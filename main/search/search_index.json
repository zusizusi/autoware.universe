{"config":{"lang":["en"],"separator":"[\\s\\-_,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Autoware Universe","text":""},{"location":"#autoware-universe","title":"Autoware Universe","text":""},{"location":"#welcome-to-autoware-universe","title":"Welcome to Autoware Universe","text":"<p>Autoware Universe serves as a foundational pillar within the Autoware ecosystem, playing a critical role in enhancing the core functionalities of autonomous driving technologies. This repository is a pivotal element of the Autoware Core/Universe concept, managing a wide array of packages that significantly extend the capabilities of autonomous vehicles.</p> <p></p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To dive into the vast world of Autoware and understand how Autoware Universe fits into the bigger picture, we recommend starting with the Autoware Documentation. This resource provides a thorough overview of the Autoware ecosystem, guiding you through its components, functionalities, and how to get started with development.</p>"},{"location":"#explore-autoware-universe-documentation","title":"Explore Autoware Universe documentation","text":"<p>For those looking to explore the specifics of Autoware Universe components, the Autoware Universe Documentation, deployed with MKDocs, offers detailed insights.</p>"},{"location":"#code-coverage-metrics","title":"Code Coverage Metrics","text":"<p>Below table shows the coverage rate of entire Autoware Universe and sub-components respectively.</p>"},{"location":"#entire-project-coverage","title":"Entire Project Coverage","text":""},{"location":"#component-wise-coverage","title":"Component-wise Coverage","text":"<p>You can check more details by clicking the badge and navigating the codecov website.</p> Component Coverage Common Control Evaluator Launch TBD Localization Map Perception Planning Sensing Simulator System Vehicle"},{"location":"CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#contributor-covenant-code-of-conduct","title":"Contributor Covenant Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall   community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of   any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email address,   without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at conduct@autoware.org. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"CODE_OF_CONDUCT/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"CODE_OF_CONDUCT/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"CODE_OF_CONDUCT/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"CODE_OF_CONDUCT/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"CONTRIBUTING/","title":"Contributing","text":""},{"location":"CONTRIBUTING/#contributing","title":"Contributing","text":"<p>See https://autowarefoundation.github.io/autoware-documentation/main/contributing/.</p>"},{"location":"DISCLAIMER/","title":"DISCLAIMER","text":"<p>DISCLAIMER</p> <p>\u201cAutoware\u201d will be provided by The Autoware Foundation under the Apache License 2.0. This \u201cDISCLAIMER\u201d will be applied to all users of Autoware (a \u201cUser\u201d or \u201cUsers\u201d) with the Apache License 2.0 and Users shall hereby approve and acknowledge all the contents specified in this disclaimer below and will be deemed to consent to this disclaimer without any objection upon utilizing or downloading Autoware.</p> <p>Disclaimer and Waiver of Warranties</p> <ol> <li> <p>AUTOWARE FOUNDATION MAKES NO REPRESENTATION OR WARRANTY OF ANY KIND,    EXPRESS OR IMPLIED, WITH RESPECT TO PROVIDING AUTOWARE (the \u201cService\u201d)    including but not limited to any representation or warranty (i) of fitness or    suitability for a particular purpose contemplated by the Users, (ii) of the    expected functions, commercial value, accuracy, or usefulness of the Service,    (iii) that the use by the Users of the Service complies with the laws and    regulations applicable to the Users or any internal rules established by    industrial organizations, (iv) that the Service will be free of interruption or    defects, (v) of the non-infringement of any third party's right and (vi) the    accuracy of the content of the Services and the software itself.</p> </li> <li> <p>The Autoware Foundation shall not be liable for any damage incurred by the    User that are attributable to the Autoware Foundation for any reasons    whatsoever. UNDER NO CIRCUMSTANCES SHALL THE AUTOWARE FOUNDATION BE LIABLE FOR    INCIDENTAL, INDIRECT, SPECIAL OR FUTURE DAMAGES OR LOSS OF PROFITS.</p> </li> <li> <p>A User shall be entirely responsible for the content posted by the User and    its use of any content of the Service or the Website. If the User is held    responsible in a civil action such as a claim for damages or even in a criminal    case, the Autoware Foundation and member companies, governments and academic &amp;    non-profit organizations and their directors, officers, employees and agents    (collectively, the \u201cIndemnified Parties\u201d) shall be completely discharged from    any rights or assertions the User may have against the Indemnified Parties, or    from any legal action, litigation or similar procedures.</p> </li> </ol> <p>Indemnity</p> <p>A User shall indemnify and hold the Indemnified Parties harmless from any of their damages, losses, liabilities, costs or expenses (including attorneys' fees or criminal compensation), or any claims or demands made against the Indemnified Parties by any third party, due to or arising out of, or in connection with utilizing Autoware (including the representations and warranties), the violation of applicable Product Liability Law of each country (including criminal case) or violation of any applicable laws by the Users, or the content posted by the User or its use of any content of the Service or the Website.</p>"},{"location":"common/","title":"Common","text":""},{"location":"common/#common","title":"Common","text":""},{"location":"common/#getting-started","title":"Getting Started","text":"<p>The Autoware Universe Common folder consists of common and testing libraries that are used by other Autoware components, as well as useful plugins for visualization in RVIZ2.</p> <p>Note</p> <p>In addition to the ones listed in this folder, users can also have a look at some of the add-ons in the <code>autoware_tools/common</code> documentation page.</p>"},{"location":"common/#highlights","title":"Highlights","text":"<p>Some of the commonly used libraries are:</p> <ol> <li><code>autoware_universe_utils</code></li> <li><code>autoware_motion_utils</code></li> </ol>"},{"location":"common/autoware_agnocast_wrapper/","title":"autoware_agnocast_wrapper","text":""},{"location":"common/autoware_agnocast_wrapper/#autoware_agnocast_wrapper","title":"autoware_agnocast_wrapper","text":"<p>The purpose of this package is to integrate Agnocast, a zero-copy middleware, into each topic in Autoware with minimal side effects. Agnocast is a library designed to work alongside ROS 2, enabling true zero-copy publish/subscribe communication for all ROS 2 message types, including unsized message types.</p> <ul> <li>Agnocast Repository: https://github.com/tier4/agnocast</li> <li>Discussion on Agnocast Integration into Autoware: https://github.com/orgs/autowarefoundation/discussions/5835</li> </ul> <p>This package provides macros that wrap functions for publish/subscribe operations and smart pointer types for handling ROS 2 messages. When Autoware is built using the default build command, Agnocast is not enabled. However, setting the environment variable <code>ENABLE_AGNOCAST=1</code> enables Agnocast and results in a build that includes its integration. This design ensures backward compatibility for users who are unaware of Agnocast, minimizing disruption.</p>"},{"location":"common/autoware_agnocast_wrapper/#how-to-use-the-macros-in-this-package","title":"How to Use the Macros in This Package","text":"<p>You can immediately understand how to use the macros just by looking at <code>autoware_agnocast_wrapper.hpp</code>. A typical callback and publisher setup looks like this:</p> <pre><code>#include &lt;autoware/agnocast_wrapper/autoware_agnocast_wrapper.hpp&gt;\n\npub_output_ = AUTOWARE_CREATE_PUBLISHER3(\n  PointCloud2,\n  \"output\",\n  rclcpp::SensorDataQoS().keep_last(max_queue_size_),\n  pub_options\n);\n\nvoid onPointCloud(const AUTOWARE_MESSAGE_PTR(const PointCloud2) input_msg) {\n  auto output = ALLOCATE_OUTPUT_MESSAGE(pub_output_);\n  ...\n  pub_output_-&gt;publish(std::move(output));\n}\n</code></pre> <p>To use the macros provided by this package in your own package, include the following lines in your <code>CMakeLists.txt</code>:</p> <pre><code>find_package(autoware_agnocast_wrapper REQUIRED)\nament_target_dependencies(target autoware_agnocast_wrapper)\ntarget_include_directories(target ${autoware_agnocast_wrapper_INCLUDE_DIRS})\nautoware_agnocast_wrapper_setup(target)\n</code></pre>"},{"location":"common/autoware_agnocast_wrapper/#how-to-enabledisable-agnocast-on-build","title":"How to Enable/Disable Agnocast on Build","text":"<p>To build Autoware with Agnocast:</p> <pre><code>export ENABLE_AGNOCAST=1\ncolcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE=Release\n</code></pre> <p>To build Autoware without Agnocast (default behavior):</p> <pre><code>colcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE=Release\n</code></pre> <p>To explicitly disable Agnocast when it has been previously enabled:</p> <pre><code>unset ENABLE_AGNOCAST\n# or\nexport ENABLE_AGNOCAST=0\n</code></pre> <p>To rebuild a specific package without Agnocast after it was previously built with Agnocast:</p> <pre><code>rm -Rf ./install/&lt;package_name&gt; ./build/&lt;package_name&gt;\nexport ENABLE_AGNOCAST=0\ncolcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE=Release --package-select &lt;package_name&gt;\n</code></pre> <p>To rebuild a specific package with Agnocast after it was previously built without it:</p> <pre><code>rm -Rf ./install/&lt;package_name&gt; ./build/&lt;package_name&gt;\nexport ENABLE_AGNOCAST=1\ncolcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE=Release --package-select &lt;package_name&gt;\n</code></pre> <p>Please note that the <code>ENABLE_AGNOCAST</code> environment variable may not behave as expected in the following scenario:</p> <ul> <li>Package A depends on build artifacts from Package B</li> <li>Both A and B were previously built with Agnocast enabled</li> <li>Rebuilding only Package A with <code>ENABLE_AGNOCAST=0</code> will not be sufficient, as compile options enabling Agnocast may propagate from Package B</li> </ul> <p>Example:</p> <ul> <li>A = <code>autoware_occupancy_grid_map_outlier_filter</code></li> <li>B = <code>autoware_pointcloud_preprocessor</code></li> </ul> <p>In such cases, rebuild both A and B with Agnocast disabled to ensure consistency. As a best practice, we recommend keeping the value of <code>ENABLE_AGNOCAST</code> consistent within a workspace to avoid unintentional mismatches and simplify build management.</p>"},{"location":"common/autoware_auto_common/design/comparisons/","title":"Comparisons","text":""},{"location":"common/autoware_auto_common/design/comparisons/#comparisons","title":"Comparisons","text":"<p>The <code>float_comparisons.hpp</code> library is a simple set of functions for performing approximate numerical comparisons. There are separate functions for performing comparisons using absolute bounds and relative bounds. Absolute comparison checks are prefixed with <code>abs_</code> and relative checks are prefixed with <code>rel_</code>.</p> <p>The <code>bool_comparisons.hpp</code> library additionally contains an XOR operator.</p> <p>The intent of the library is to improve readability of code and reduce likelihood of typographical errors when using numerical and boolean comparisons.</p>"},{"location":"common/autoware_auto_common/design/comparisons/#target-use-cases","title":"Target use cases","text":"<p>The approximate comparisons are intended to be used to check whether two numbers lie within some absolute or relative interval. The <code>exclusive_or</code> function will test whether two values cast to different boolean values.</p>"},{"location":"common/autoware_auto_common/design/comparisons/#assumptions","title":"Assumptions","text":"<ul> <li>The approximate comparisons all take an <code>epsilon</code> parameter.   The value of this parameter must be &gt;= 0.</li> <li>The library is only intended to be used with floating point types.   A static assertion will be thrown if the library is used with a non-floating point type.</li> </ul>"},{"location":"common/autoware_auto_common/design/comparisons/#example-usage","title":"Example Usage","text":"<pre><code>#include \"autoware_auto_common/common/bool_comparisons.hpp\"\n#include \"autoware_auto_common/common/float_comparisons.hpp\"\n\n#include &lt;iostream&gt;\n\n// using-directive is just for illustration; don't do this in practice\nusing namespace autoware::qp_interface::helper_functions::comparisons;\n\nstatic constexpr auto epsilon = 0.2;\nstatic constexpr auto relative_epsilon = 0.01;\n\nstd::cout &lt;&lt; exclusive_or(true, false) &lt;&lt; \"\\n\";\n// Prints: true\n\nstd::cout &lt;&lt; rel_eq(1.0, 1.1, relative_epsilon)) &lt;&lt; \"\\n\";\n// Prints: false\n\nstd::cout &lt;&lt; approx_eq(10000.0, 10010.0, epsilon, relative_epsilon)) &lt;&lt; \"\\n\";\n// Prints: true\n\nstd::cout &lt;&lt; abs_eq(4.0, 4.2, epsilon) &lt;&lt; \"\\n\";\n// Prints: true\n\nstd::cout &lt;&lt; abs_ne(4.0, 4.2, epsilon) &lt;&lt; \"\\n\";\n// Prints: false\n\nstd::cout &lt;&lt; abs_eq_zero(0.2, epsilon) &lt;&lt; \"\\n\";\n// Prints: false\n\nstd::cout &lt;&lt; abs_lt(4.0, 4.25, epsilon) &lt;&lt; \"\\n\";\n// Prints: true\n\nstd::cout &lt;&lt; abs_lte(1.0, 1.2, epsilon) &lt;&lt; \"\\n\";\n// Prints: true\n\nstd::cout &lt;&lt; abs_gt(1.25, 1.0, epsilon) &lt;&lt; \"\\n\";\n// Prints: true\n\nstd::cout &lt;&lt; abs_gte(0.75, 1.0, epsilon) &lt;&lt; \"\\n\";\n// Prints: false\n</code></pre>"},{"location":"common/autoware_component_interface_specs_universe/","title":"autoware_component_interface_specs_universe","text":""},{"location":"common/autoware_component_interface_specs_universe/#autoware_component_interface_specs_universe","title":"autoware_component_interface_specs_universe","text":"<p>This package is a specification of component interfaces.</p>"},{"location":"common/autoware_component_interface_tools/","title":"autoware_component_interface_tools","text":""},{"location":"common/autoware_component_interface_tools/#autoware_component_interface_tools","title":"autoware_component_interface_tools","text":"<p>This package provides the following tools for component interface.</p>"},{"location":"common/autoware_component_interface_tools/#service_log_checker","title":"service_log_checker","text":"<p>Monitor the service log of component_interface_utils and display if the response status is an error.</p>"},{"location":"common/autoware_component_interface_utils/","title":"autoware_component_interface_utils","text":""},{"location":"common/autoware_component_interface_utils/#autoware_component_interface_utils","title":"autoware_component_interface_utils","text":""},{"location":"common/autoware_component_interface_utils/#features","title":"Features","text":"<p>This is a utility package that provides the following features:</p> <ul> <li>Instantiation of the wrapper class</li> <li>Logging for service and client</li> <li>Service exception for response</li> <li>Relays for topic and service</li> </ul>"},{"location":"common/autoware_component_interface_utils/#design","title":"Design","text":"<p>This package provides the wrappers for the interface classes of rclcpp. The wrappers limit the usage of the original class to enforce the processing recommended by the component interface. Do not inherit the class of rclcpp, and forward or wrap the member function that is allowed to be used.</p>"},{"location":"common/autoware_component_interface_utils/#instantiation-of-the-wrapper-class","title":"Instantiation of the wrapper class","text":"<p>The wrapper class requires interface information in this format.</p> <pre><code>struct SampleService\n{\n  using Service = sample_msgs::srv::ServiceType;\n  static constexpr char name[] = \"/sample/service\";\n};\n\nstruct SampleMessage\n{\n  using Message = sample_msgs::msg::MessageType;\n  static constexpr char name[] = \"/sample/message\";\n  static constexpr size_t depth = 1;\n  static constexpr auto reliability = RMW_QOS_POLICY_RELIABILITY_RELIABLE;\n  static constexpr auto durability = RMW_QOS_POLICY_DURABILITY_TRANSIENT_LOCAL;\n};\n</code></pre> <p>Create the wrapper using the above definition as follows.</p> <pre><code>// header file\nautoware::component_interface_utils::Service&lt;SampleService&gt;::SharedPtr srv_;\nautoware::component_interface_utils::Client&lt;SampleService&gt;::SharedPtr cli_;\nautoware::component_interface_utils::Publisher&lt;SampleMessage&gt;::SharedPtr pub_;\nautoware::component_interface_utils::Subscription&lt;SampleMessage&gt;::SharedPtr sub_;\n\n// source file\nconst auto node = autoware::component_interface_utils::NodeAdaptor(this);\nnode.init_srv(srv_, callback);\nnode.init_cli(cli_);\nnode.init_pub(pub_);\nnode.init_sub(sub_, callback);\n</code></pre>"},{"location":"common/autoware_component_interface_utils/#logging-for-service-and-client","title":"Logging for service and client","text":"<p>If the wrapper class is used, logging is automatically enabled. The log level is <code>RCLCPP_INFO</code>.</p>"},{"location":"common/autoware_component_interface_utils/#service-exception-for-response","title":"Service exception for response","text":"<p>If the wrapper class is used and the service response has status, throwing <code>ServiceException</code> will automatically catch and set it to status. This is useful when returning an error from a function called from the service callback.</p> <pre><code>void service_callback(Request req, Response res)\n{\n   function();\n   res-&gt;status.success = true;\n}\n\nvoid function()\n{\n   throw ServiceException(ERROR_CODE, \"message\");\n}\n</code></pre> <p>If the wrapper class is not used or the service response has no status, manually catch the <code>ServiceException</code> as follows.</p> <pre><code>void service_callback(Request req, Response res)\n{\n   try {\n      function();\n      res-&gt;status.success = true;\n   } catch (const ServiceException &amp; error) {\n      res-&gt;status = error.status();\n   }\n}\n</code></pre>"},{"location":"common/autoware_component_interface_utils/#relays-for-topic-and-service","title":"Relays for topic and service","text":"<p>There are utilities for relaying services and messages of the same type.</p> <pre><code>const auto node = autoware::component_interface_utils::NodeAdaptor(this);\nservice_callback_group_ = create_callback_group(rclcpp::CallbackGroupType::MutuallyExclusive);\nnode.relay_message(pub_, sub_);\nnode.relay_service(cli_, srv_, service_callback_group_);  // group is for avoiding deadlocks\n</code></pre>"},{"location":"common/autoware_cuda_dependency_meta/","title":"autoware_cuda_dependency_meta","text":""},{"location":"common/autoware_cuda_dependency_meta/#autoware_cuda_dependency_meta","title":"autoware_cuda_dependency_meta","text":""},{"location":"common/autoware_cuda_dependency_meta/#purpose","title":"Purpose","text":"<p>This is a virtual package that packages on Autoware Universe that use CUDA must depend on.</p>"},{"location":"common/autoware_fake_test_node/design/fake_test_node-design/","title":"Fake Test Node","text":""},{"location":"common/autoware_fake_test_node/design/fake_test_node-design/#fake-test-node","title":"Fake Test Node","text":""},{"location":"common/autoware_fake_test_node/design/fake_test_node-design/#what-this-package-provides","title":"What this package provides","text":"<p>When writing an integration test for a node in C++ using GTest, there is quite some boilerplate code that needs to be written to set up a fake node that would publish expected messages on an expected topic and subscribes to messages on some other topic. This is usually implemented as a custom GTest fixture.</p> <p>This package contains a library that introduces two utility classes that can be used in place of custom fixtures described above to write integration tests for a node:</p> <ul> <li><code>autoware::fake_test_node::FakeTestNode</code> - to use as a custom test fixture with <code>TEST_F</code> tests</li> <li><code>autoware::fake_test_node::FakeTestNodeParametrized</code> - to use a custom test fixture with the   parametrized <code>TEST_P</code> tests (accepts a template parameter that gets forwarded to   <code>testing::TestWithParam&lt;T&gt;</code>)</li> </ul> <p>These fixtures take care of initializing and re-initializing rclcpp as well as of checking that all subscribers and publishers have a match, thus reducing the amount of boilerplate code that the user needs to write.</p>"},{"location":"common/autoware_fake_test_node/design/fake_test_node-design/#how-to-use-this-library","title":"How to use this library","text":"<p>After including the relevant header the user can use a typedef to use a custom fixture name and use the provided classes as fixtures in <code>TEST_F</code> and <code>TEST_P</code> tests directly.</p>"},{"location":"common/autoware_fake_test_node/design/fake_test_node-design/#example-usage","title":"Example usage","text":"<p>Let's say there is a node <code>NodeUnderTest</code> that requires testing. It just subscribes to <code>std_msgs::msg::Int32</code> messages and publishes a <code>std_msgs::msg::Bool</code> to indicate that the input is positive. To test such a node the following code can be used utilizing the <code>autoware::fake_test_node::FakeTestNode</code>:</p> <pre><code>using FakeNodeFixture = autoware::fake_test_node::FakeTestNode;\n\n/// @test Test that we can use a non-parametrized test.\nTEST_F(FakeNodeFixture, Test) {\n  Int32 msg{};\n  msg.data = 15;\n  const auto node = std::make_shared&lt;NodeUnderTest&gt;();\n\n  Bool::SharedPtr last_received_msg{};\n  auto fake_odom_publisher = create_publisher&lt;Int32&gt;(\"/input_topic\");\n  auto result_odom_subscription = create_subscription&lt;Bool&gt;(\"/output_topic\", *node,\n    [&amp;last_received_msg](const Bool::SharedPtr msg) {last_received_msg = msg;});\n\n  const auto dt{std::chrono::milliseconds{100LL}};\n  const auto max_wait_time{std::chrono::seconds{10LL}};\n  auto time_passed{std::chrono::milliseconds{0LL}};\n  while (!last_received_msg) {\n    fake_odom_publisher-&gt;publish(msg);\n    rclcpp::spin_some(node);\n    rclcpp::spin_some(get_fake_node());\n    std::this_thread::sleep_for(dt);\n    time_passed += dt;\n    if (time_passed &gt; max_wait_time) {\n      FAIL() &lt;&lt; \"Did not receive a message soon enough.\";\n    }\n  }\n  EXPECT_TRUE(last_received_msg-&gt;data);\n  SUCCEED();\n}\n</code></pre> <p>Here only the <code>TEST_F</code> example is shown but a <code>TEST_P</code> usage is very similar with a little bit more boilerplate to set up all the parameter values, see <code>test_fake_test_node.cpp</code> for an example usage.</p>"},{"location":"common/autoware_glog_component/","title":"glog_component","text":""},{"location":"common/autoware_glog_component/#glog_component","title":"glog_component","text":"<p>This package provides the glog (google logging library) feature as a ros2 component library. This is used to dynamically load the glog feature with container.</p> <p>See the glog github for the details of its features.</p>"},{"location":"common/autoware_glog_component/#example","title":"Example","text":"<p>When you load the <code>glog_component</code> in container, the launch file can be like below:</p> <pre><code>glog_component = ComposableNode(\n    package=\"autoware_glog_component\",\n    plugin=\"autoware::glog_component::GlogComponent\",\n    name=\"glog_component\",\n)\n\ncontainer = ComposableNodeContainer(\n    name=\"my_container\",\n    namespace=\"\",\n    package=\"rclcpp_components\",\n    executable=LaunchConfiguration(\"container_executable\"),\n    composable_node_descriptions=[\n        component1,\n        component2,\n        glog_component,\n    ],\n)\n</code></pre>"},{"location":"common/autoware_goal_distance_calculator/Readme/","title":"autoware_goal_distance_calculator","text":""},{"location":"common/autoware_goal_distance_calculator/Readme/#autoware_goal_distance_calculator","title":"autoware_goal_distance_calculator","text":""},{"location":"common/autoware_goal_distance_calculator/Readme/#purpose","title":"Purpose","text":"<p>This node publishes deviation of self-pose from goal pose.</p>"},{"location":"common/autoware_goal_distance_calculator/Readme/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"common/autoware_goal_distance_calculator/Readme/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"common/autoware_goal_distance_calculator/Readme/#input","title":"Input","text":"Name Type Description <code>/planning/mission_planning/route</code> <code>autoware_planning_msgs::msg::Route</code> Used to get goal pose <code>/tf</code> <code>tf2_msgs/TFMessage</code> TF (self-pose)"},{"location":"common/autoware_goal_distance_calculator/Readme/#output","title":"Output","text":"Name Type Description <code>deviation/lateral</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> publish lateral deviation of self-pose from goal pose[m] <code>deviation/longitudinal</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> publish longitudinal deviation of self-pose from goal pose[m] <code>deviation/yaw</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> publish yaw deviation of self-pose from goal pose[rad] <code>deviation/yaw_deg</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> publish yaw deviation of self-pose from goal pose[deg]"},{"location":"common/autoware_goal_distance_calculator/Readme/#parameters","title":"Parameters","text":""},{"location":"common/autoware_goal_distance_calculator/Readme/#node-parameters","title":"Node Parameters","text":"Name Type Default Value Explanation <code>update_rate</code> double 10.0 Timer callback period. [Hz]"},{"location":"common/autoware_goal_distance_calculator/Readme/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Explanation <code>oneshot</code> bool true publish deviations just once or repeatedly"},{"location":"common/autoware_goal_distance_calculator/Readme/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"common/autoware_grid_map_utils/","title":"Grid Map Utils","text":""},{"location":"common/autoware_grid_map_utils/#grid-map-utils","title":"Grid Map Utils","text":""},{"location":"common/autoware_grid_map_utils/#overview","title":"Overview","text":"<p>This packages contains a re-implementation of the <code>grid_map::PolygonIterator</code> used to iterate over all cells of a grid map contained inside some polygon.</p>"},{"location":"common/autoware_grid_map_utils/#algorithm","title":"Algorithm","text":"<p>This implementation uses the scan line algorithm, a common algorithm used to draw polygons on a rasterized image. The main idea of the algorithm adapted to a grid map is as follow:</p> <ul> <li>calculate intersections between rows of the grid map and the edges of the polygon edges;</li> <li>calculate for each row the column between each pair of intersections;</li> <li>the resulting <code>(row, column)</code> indexes are inside of the polygon.</li> </ul> <p>More details on the scan line algorithm can be found in the References.</p>"},{"location":"common/autoware_grid_map_utils/#api","title":"API","text":"<p>The <code>autoware::grid_map_utils::PolygonIterator</code> follows the same API as the original <code>grid_map::PolygonIterator</code>.</p>"},{"location":"common/autoware_grid_map_utils/#assumptions","title":"Assumptions","text":"<p>The behavior of the <code>autoware::grid_map_utils::PolygonIterator</code> is only guaranteed to match the <code>grid_map::PolygonIterator</code> if edges of the polygon do not exactly cross any cell center. In such a case, whether the crossed cell is considered inside or outside of the polygon can vary due to floating precision error.</p>"},{"location":"common/autoware_grid_map_utils/#performances","title":"Performances","text":"<p>Benchmarking code is implemented in <code>test/benchmarking.cpp</code> and is also used to validate that the <code>autoware::grid_map_utils::PolygonIterator</code> behaves exactly like the <code>grid_map::PolygonIterator</code>.</p> <p>The following figure shows a comparison of the runtime between the implementation of this package (<code>autoware_grid_map_utils</code>) and the original implementation (<code>grid_map</code>). The time measured includes the construction of the iterator and the iteration over all indexes and is shown using a logarithmic scale. Results were obtained varying the side size of a square grid map with <code>100 &lt;= n &lt;= 1000</code> (size=<code>n</code> means a grid of <code>n x n</code> cells), random polygons with a number of vertices <code>3 &lt;= m &lt;= 100</code> and with each parameter <code>(n,m)</code> repeated 10 times.</p> <p></p>"},{"location":"common/autoware_grid_map_utils/#future-improvements","title":"Future improvements","text":"<p>There exists variations of the scan line algorithm for multiple polygons. These can be implemented if we want to iterate over the cells contained in at least one of multiple polygons.</p> <p>The current implementation imitate the behavior of the original <code>grid_map::PolygonIterator</code> where a cell is selected if its center position is inside the polygon. This behavior could be changed for example to only return all cells overlapped by the polygon.</p>"},{"location":"common/autoware_grid_map_utils/#references","title":"References","text":"<ul> <li>https://en.wikipedia.org/wiki/Scanline_rendering</li> <li>https://web.cs.ucdavis.edu/~ma/ECS175_S00/Notes/0411_b.pdf</li> </ul>"},{"location":"common/autoware_path_distance_calculator/Readme/","title":"autoware_path_distance_calculator","text":""},{"location":"common/autoware_path_distance_calculator/Readme/#autoware_path_distance_calculator","title":"autoware_path_distance_calculator","text":""},{"location":"common/autoware_path_distance_calculator/Readme/#purpose","title":"Purpose","text":"<p>This node publishes a distance from the closest path point from the self-position to the end point of the path. Note that the distance means the arc-length along the path, not the Euclidean distance between the two points.</p>"},{"location":"common/autoware_path_distance_calculator/Readme/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"common/autoware_path_distance_calculator/Readme/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"common/autoware_path_distance_calculator/Readme/#input","title":"Input","text":"Name Type Description <code>/planning/scenario_planning/lane_driving/behavior_planning/path</code> <code>autoware_planning_msgs::msg::Path</code> Reference path <code>/tf</code> <code>tf2_msgs/TFMessage</code> TF (self-pose)"},{"location":"common/autoware_path_distance_calculator/Readme/#output","title":"Output","text":"Name Type Description <code>~/distance</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> Publish a distance from the closest path point from the self-position to the end point of the path[m]"},{"location":"common/autoware_path_distance_calculator/Readme/#parameters","title":"Parameters","text":""},{"location":"common/autoware_path_distance_calculator/Readme/#node-parameters","title":"Node Parameters","text":"<p>None.</p>"},{"location":"common/autoware_path_distance_calculator/Readme/#core-parameters","title":"Core Parameters","text":"<p>None.</p>"},{"location":"common/autoware_path_distance_calculator/Readme/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"common/autoware_polar_grid/Readme/","title":"Polar Grid","text":""},{"location":"common/autoware_polar_grid/Readme/#polar-grid","title":"Polar Grid","text":""},{"location":"common/autoware_polar_grid/Readme/#purpose","title":"Purpose","text":"<p>This plugin displays polar grid around ego vehicle in Rviz.</p>"},{"location":"common/autoware_polar_grid/Readme/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Explanation <code>Max Range</code> float 200.0f max range for polar grid. [m] <code>Wave Velocity</code> float 100.0f wave ring velocity. [m/s] <code>Delta Range</code> float 10.0f wave ring distance for polar grid. [m]"},{"location":"common/autoware_polar_grid/Readme/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"common/autoware_traffic_light_recognition_marker_publisher/Readme/","title":"Traffic Light Recognition Marker Publisher","text":""},{"location":"common/autoware_traffic_light_recognition_marker_publisher/Readme/#traffic-light-recognition-marker-publisher","title":"Traffic Light Recognition Marker Publisher","text":""},{"location":"common/autoware_traffic_light_recognition_marker_publisher/Readme/#purpose","title":"Purpose","text":"<p>This node publishes a marker array for visualizing traffic signal recognition results on Rviz.</p> <p></p>"},{"location":"common/autoware_traffic_light_recognition_marker_publisher/Readme/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"common/autoware_traffic_light_recognition_marker_publisher/Readme/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"common/autoware_traffic_light_recognition_marker_publisher/Readme/#input","title":"Input","text":"Name Type Description <code>/map/vector_map</code> <code>autoware_map_msgs::msg::LaneletMapBin</code> Vector map for getting traffic signal information <code>/perception/traffic_light_recognition/traffic_signals</code> <code>autoware_perception_msgs::msg::TrafficLightGroupArray</code> The result of traffic signal recognition"},{"location":"common/autoware_traffic_light_recognition_marker_publisher/Readme/#output","title":"Output","text":"Name Type Description <code>/perception/traffic_light_recognition/traffic_signals_marker</code> <code>visualization_msgs::msg::MarkerArray</code> Publish a marker array for visualization of traffic signal recognition results"},{"location":"common/autoware_traffic_light_recognition_marker_publisher/Readme/#parameters","title":"Parameters","text":"<p>None.</p>"},{"location":"common/autoware_traffic_light_recognition_marker_publisher/Readme/#node-parameters","title":"Node Parameters","text":"<p>None.</p>"},{"location":"common/autoware_traffic_light_recognition_marker_publisher/Readme/#core-parameters","title":"Core Parameters","text":"<p>None.</p>"},{"location":"common/autoware_traffic_light_recognition_marker_publisher/Readme/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"common/autoware_traffic_light_utils/","title":"autoware_traffic_light_utils","text":""},{"location":"common/autoware_traffic_light_utils/#autoware_traffic_light_utils","title":"autoware_traffic_light_utils","text":""},{"location":"common/autoware_traffic_light_utils/#purpose","title":"Purpose","text":"<p>This package contains a library of common functions that are useful across the traffic light recognition module. This package may include functions for handling ROI types, converting between different data types and message types, as well as common functions related to them.</p>"},{"location":"common/autoware_universe_utils/","title":"autoware_universe_utils","text":""},{"location":"common/autoware_universe_utils/#autoware_universe_utils","title":"autoware_universe_utils","text":""},{"location":"common/autoware_universe_utils/#purpose","title":"Purpose","text":"<p>This package contains many common functions used by other packages, so please refer to them as needed.</p>"},{"location":"common/autoware_universe_utils/#for-developers","title":"For developers","text":"<p><code>autoware_universe_utils.hpp</code> header file was removed because the source files that directly/indirectly include this file took a long time for preprocessing.</p>"},{"location":"common/autoware_universe_utils/#autowareuniverse_utils","title":"<code>autoware::universe_utils</code>","text":""},{"location":"common/autoware_universe_utils/#systems","title":"<code>systems</code>","text":""},{"location":"common/autoware_universe_utils/#autowareuniverse_utilstimekeeper","title":"<code>autoware::universe_utils::TimeKeeper</code>","text":""},{"location":"common/autoware_universe_utils/#constructor","title":"Constructor","text":"<pre><code>template &lt;typename... Reporters&gt;\nexplicit TimeKeeper(Reporters... reporters);\n</code></pre> <ul> <li>Initializes the <code>TimeKeeper</code> with a list of reporters.</li> </ul>"},{"location":"common/autoware_universe_utils/#methods","title":"Methods","text":"<ul> <li><code>void add_reporter(std::ostream * os);</code><ul> <li>Adds a reporter to output processing times to an <code>ostream</code>.</li> <li><code>os</code>: Pointer to the <code>ostream</code> object.</li> </ul> </li> </ul> <ul> <li><code>void add_reporter(rclcpp::Publisher&lt;ProcessingTimeDetail&gt;::SharedPtr publisher);</code><ul> <li>Adds a reporter to publish processing times to an <code>rclcpp</code> publisher.</li> <li><code>publisher</code>: Shared pointer to the <code>rclcpp</code> publisher.</li> </ul> </li> </ul> <ul> <li><code>void add_reporter(rclcpp::Publisher&lt;std_msgs::msg::String&gt;::SharedPtr publisher);</code><ul> <li>Adds a reporter to publish processing times to an <code>rclcpp</code> publisher with <code>std_msgs::msg::String</code>.</li> <li><code>publisher</code>: Shared pointer to the <code>rclcpp</code> publisher.</li> </ul> </li> </ul> <ul> <li><code>void start_track(const std::string &amp; func_name);</code><ul> <li>Starts tracking the processing time of a function.</li> <li><code>func_name</code>: Name of the function to be tracked.</li> </ul> </li> </ul> <ul> <li><code>void end_track(const std::string &amp; func_name);</code><ul> <li>Ends tracking the processing time of a function.</li> <li><code>func_name</code>: Name of the function to end tracking.</li> </ul> </li> </ul> <ul> <li><code>void comment(const std::string &amp; comment);</code><ul> <li>Adds a comment to the current function being tracked.</li> <li><code>comment</code>: Comment to be added.</li> </ul> </li> </ul>"},{"location":"common/autoware_universe_utils/#note","title":"Note","text":"<ul> <li> <p>It's possible to start and end time measurements using <code>start_track</code> and <code>end_track</code> as shown below:</p> <pre><code>time_keeper.start_track(\"example_function\");\n// Your function code here\ntime_keeper.end_track(\"example_function\");\n</code></pre> </li> </ul> <ul> <li>For safety and to ensure proper tracking, it is recommended to use <code>ScopedTimeTrack</code>.</li> </ul>"},{"location":"common/autoware_universe_utils/#example","title":"Example","text":"<pre><code>#include &lt;rclcpp/rclcpp.hpp&gt;\n\n#include &lt;std_msgs/msg/string.hpp&gt;\n\n#include &lt;chrono&gt;\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n#include &lt;thread&gt;\n\nclass ExampleNode : public rclcpp::Node\n{\npublic:\n  ExampleNode() : Node(\"time_keeper_example\")\n  {\n    publisher_ =\n      create_publisher&lt;autoware::universe_utils::ProcessingTimeDetail&gt;(\"processing_time\", 1);\n\n    time_keeper_ = std::make_shared&lt;autoware::universe_utils::TimeKeeper&gt;(publisher_, &amp;std::cerr);\n    // You can also add a reporter later by add_reporter.\n    // time_keeper_-&gt;add_reporter(publisher_);\n    // time_keeper_-&gt;add_reporter(&amp;std::cerr);\n\n    timer_ =\n      create_wall_timer(std::chrono::seconds(1), std::bind(&amp;ExampleNode::func_a, this));\n  }\n\nprivate:\n  std::shared_ptr&lt;autoware::universe_utils::TimeKeeper&gt; time_keeper_;\n  rclcpp::Publisher&lt;autoware::universe_utils::ProcessingTimeDetail&gt;::SharedPtr publisher_;\n  rclcpp::Publisher&lt;std_msgs::msg::String&gt;::SharedPtr publisher_str_;\n  rclcpp::TimerBase::SharedPtr timer_;\n\n  void func_a()\n  {\n    // Start constructing ProcessingTimeTree (because func_a is the root function)\n    autoware::universe_utils::ScopedTimeTrack st(\"func_a\", *time_keeper_);\n    std::this_thread::sleep_for(std::chrono::milliseconds(1));\n    time_keeper_-&gt;comment(\"This is a comment for func_a\");\n    func_b();\n    // End constructing ProcessingTimeTree. After this, the tree will be reported (publishing\n    // message and outputting to std::cerr)\n  }\n\n  void func_b()\n  {\n    autoware::universe_utils::ScopedTimeTrack st(\"func_b\", *time_keeper_);\n    std::this_thread::sleep_for(std::chrono::milliseconds(2));\n    time_keeper_-&gt;comment(\"This is a comment for func_b\");\n    func_c();\n  }\n\n  void func_c()\n  {\n    autoware::universe_utils::ScopedTimeTrack st(\"func_c\", *time_keeper_);\n    std::this_thread::sleep_for(std::chrono::milliseconds(3));\n    time_keeper_-&gt;comment(\"This is a comment for func_c\");\n  }\n};\n\nint main(int argc, char ** argv)\n{\n  rclcpp::init(argc, argv);\n  auto node = std::make_shared&lt;ExampleNode&gt;();\n  rclcpp::spin(node);\n  rclcpp::shutdown();\n  return 0;\n}\n</code></pre> <ul> <li> <p>Output (console)</p> <pre><code>==========================\nfunc_a (6.243ms) : This is a comment for func_a\n    \u2514\u2500\u2500 func_b (5.116ms) : This is a comment for func_b\n        \u2514\u2500\u2500 func_c (3.055ms) : This is a comment for func_c\n</code></pre> </li> </ul> <ul> <li> <p>Output (<code>ros2 topic echo /processing_time</code>)</p> <pre><code>---\nnodes:\n- id: 1\n  name: func_a\n  processing_time: 6.366\n  parent_id: 0\n  comment: This is a comment for func_a\n- id: 2\n  name: func_b\n  processing_time: 5.237\n  parent_id: 1\n  comment: This is a comment for func_b\n- id: 3\n  name: func_c\n  processing_time: 3.156\n  parent_id: 2\n  comment: This is a comment for func_c\n</code></pre> </li> </ul>"},{"location":"common/autoware_universe_utils/#autowareuniverse_utilsscopedtimetrack","title":"<code>autoware::universe_utils::ScopedTimeTrack</code>","text":""},{"location":"common/autoware_universe_utils/#description","title":"Description","text":"<p>Class for automatically tracking the processing time of a function within a scope.</p>"},{"location":"common/autoware_universe_utils/#constructor_1","title":"Constructor","text":"<pre><code>ScopedTimeTrack(const std::string &amp; func_name, TimeKeeper &amp; time_keeper);\n</code></pre> <ul> <li><code>func_name</code>: Name of the function to be tracked.</li> <li><code>time_keeper</code>: Reference to the <code>TimeKeeper</code> object.</li> </ul>"},{"location":"common/autoware_universe_utils/#destructor","title":"Destructor","text":"<pre><code>~ScopedTimeTrack();\n</code></pre> <ul> <li>Destroys the <code>ScopedTimeTrack</code> object, ending the tracking of the function.</li> </ul>"},{"location":"common/autoware_universe_utils/third_party_licenses/opencv-license/","title":"Opencv license","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,  and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by  the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all  other entities that control, are controlled by, or are under common  control with that entity. For the purposes of this definition,  \"control\" means (i) the power, direct or indirect, to cause the  direction or management of such entity, whether by contract or  otherwise, or (ii) ownership of fifty percent (50%) or more of the  outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity  exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,  including but not limited to software source code, documentation  source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical  transformation or translation of a Source form, including but  not limited to compiled object code, generated documentation,  and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or  Object form, made available under the License, as indicated by a  copyright notice that is included in or attached to the work  (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object  form, that is based on (or derived from) the Work and for which the  editorial revisions, annotations, elaborations, or other modifications  represent, as a whole, an original work of authorship. For the purposes  of this License, Derivative Works shall not include works that remain  separable from, or merely link (or bind by name) to the interfaces of,  the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including  the original version of the Work and any modifications or additions  to that Work or Derivative Works thereof, that is intentionally  submitted to Licensor for inclusion in the Work by the copyright owner  or by an individual or Legal Entity authorized to submit on behalf of  the copyright owner. For the purposes of this definition, \"submitted\"  means any form of electronic, verbal, or written communication sent  to the Licensor or its representatives, including but not limited to  communication on electronic mailing lists, source code control systems,  and issue tracking systems that are managed by, or on behalf of, the  Licensor for the purpose of discussing and improving the Work, but  excluding communication that is conspicuously marked or otherwise  designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity  on behalf of whom a Contribution has been received by Licensor and  subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of    this License, each Contributor hereby grants to You a perpetual,    worldwide, non-exclusive, no-charge, royalty-free, irrevocable    copyright license to reproduce, prepare Derivative Works of,    publicly display, publicly perform, sublicense, and distribute the    Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of    this License, each Contributor hereby grants to You a perpetual,    worldwide, non-exclusive, no-charge, royalty-free, irrevocable    (except as stated in this section) patent license to make, have made,    use, offer to sell, sell, import, and otherwise transfer the Work,    where such license applies only to those patent claims licensable    by such Contributor that are necessarily infringed by their    Contribution(s) alone or by combination of their Contribution(s)    with the Work to which such Contribution(s) was submitted. If You    institute patent litigation against any entity (including a    cross-claim or counterclaim in a lawsuit) alleging that the Work    or a Contribution incorporated within the Work constitutes direct    or contributory patent infringement, then any patent licenses    granted to You under this License for that Work shall terminate    as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the    Work or Derivative Works thereof in any medium, with or without    modifications, and in Source or Object form, provided that You    meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or  Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices  stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works  that You distribute, all copyright, patent, trademark, and  attribution notices from the Source form of the Work,  excluding those notices that do not pertain to any part of  the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its  distribution, then any Derivative Works that You distribute must  include a readable copy of the attribution notices contained  within such NOTICE file, excluding those notices that do not  pertain to any part of the Derivative Works, in at least one  of the following places: within a NOTICE text file distributed  as part of the Derivative Works; within the Source form or  documentation, if provided along with the Derivative Works; or,  within a display generated by the Derivative Works, if and  wherever such third-party notices normally appear. The contents  of the NOTICE file are for informational purposes only and  do not modify the License. You may add Your own attribution  notices within Derivative Works that You distribute, alongside  or as an addendum to the NOTICE text from the Work, provided  that such additional attribution notices cannot be construed  as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and  may provide additional or different license terms and conditions  for use, reproduction, or distribution of Your modifications, or  for any such Derivative Works as a whole, provided Your use,  reproduction, and distribution of the Work otherwise complies with  the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,    any Contribution intentionally submitted for inclusion in the Work    by You to the Licensor shall be under the terms and conditions of    this License, without any additional terms or conditions.    Notwithstanding the above, nothing herein shall supersede or modify    the terms of any separate license agreement you may have executed    with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade    names, trademarks, service marks, or product names of the Licensor,    except as required for reasonable and customary use in describing the    origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or    agreed to in writing, Licensor provides the Work (and each    Contributor provides its Contributions) on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or    implied, including, without limitation, any warranties or conditions    of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A    PARTICULAR PURPOSE. You are solely responsible for determining the    appropriateness of using or redistributing the Work and assume any    risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,    whether in tort (including negligence), contract, or otherwise,    unless required by applicable law (such as deliberate and grossly    negligent acts) or agreed to in writing, shall any Contributor be    liable to You for damages, including any direct, indirect, special,    incidental, or consequential damages of any character arising as a    result of this License or out of the use or inability to use the    Work (including but not limited to damages for loss of goodwill,    work stoppage, computer failure or malfunction, or any and all    other commercial damages or losses), even if such Contributor    has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing    the Work or Derivative Works thereof, You may choose to offer,    and charge a fee for, acceptance of support, warranty, indemnity,    or other liability obligations and/or rights consistent with this    License. However, in accepting such obligations, You may act only    on Your own behalf and on Your sole responsibility, not on behalf    of any other Contributor, and only if You agree to indemnify,    defend, and hold each Contributor harmless for any liability    incurred by, or claims asserted against, such Contributor by reason    of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <pre><code>  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"[]\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n</code></pre> <p>Copyright [yyyy] [name of copyright owner]</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <pre><code>   http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"common/tier4_api_utils/","title":"tier4_api_utils","text":""},{"location":"common/tier4_api_utils/#tier4_api_utils","title":"tier4_api_utils","text":"<p>This is an old implementation of a class that logs when calling a service. Please use component_interface_utils instead.</p>"},{"location":"control/autoware_autonomous_emergency_braking/","title":"Autonomous Emergency Braking (AEB)","text":""},{"location":"control/autoware_autonomous_emergency_braking/#autonomous-emergency-braking-aeb","title":"Autonomous Emergency Braking (AEB)","text":""},{"location":"control/autoware_autonomous_emergency_braking/#purpose-role","title":"Purpose / Role","text":"<p><code>autonomous_emergency_braking</code> is a module that prevents collisions with obstacles on the predicted path created by a control module or sensor values estimated from the control module.</p>"},{"location":"control/autoware_autonomous_emergency_braking/#assumptions","title":"Assumptions","text":"<p>This module has following assumptions.</p> <ul> <li>The predicted path of the ego vehicle can be made from either the path created from sensors or the path created from a control module, or both.</li> </ul> <ul> <li>The current speed and angular velocity can be obtained from the sensors of the ego vehicle, and it uses points as obstacles.</li> </ul> <ul> <li>The AEBs target obstacles are 2D points that can be obtained from the input point cloud or by obtaining the intersection points between the predicted ego footprint path and a predicted object's shape.</li> </ul>"},{"location":"control/autoware_autonomous_emergency_braking/#imu-path-generation-steering-angle-vs-imus-angular-velocity","title":"IMU path generation: steering angle vs IMU's angular velocity","text":"<p>Currently, the IMU-based path is generated using the angular velocity obtained by the IMU itself. It has been suggested that the steering angle could be used instead onf the angular velocity.</p> <p>The pros and cons of both approaches are:</p> <p>IMU angular velocity:</p> <ul> <li>(+) Usually, it has high accuracy</li> <li>(-) Vehicle vibration might introduce noise.</li> </ul> <p>Steering angle:</p> <ul> <li>(+) Not so noisy</li> <li>(-) May have a steering offset or a wrong gear ratio, and the steering angle of Autoware and the real steering may not be the same.</li> </ul> <p>For the moment, there are no plans to implement the steering angle on the path creation process of the AEB module.</p>"},{"location":"control/autoware_autonomous_emergency_braking/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>AEB has the following steps before it outputs the emergency stop signal.</p> <ol> <li> <p>Activate AEB if necessary.</p> </li> <li> <p>Generate a predicted path of the ego vehicle.</p> </li> <li> <p>Get target obstacles from the input point cloud and/or predicted object data.</p> </li> <li> <p>Estimate the closest obstacle speed.</p> </li> <li> <p>Collision check with target obstacles.</p> </li> <li> <p>Send emergency stop signals to <code>/diagnostics</code>.</p> </li> </ol> <p>We give more details of each section below.</p>"},{"location":"control/autoware_autonomous_emergency_braking/#1-activate-aeb-if-necessary","title":"1. Activate AEB if necessary","text":"<p>We do not activate AEB module if it satisfies the following conditions.</p> <ul> <li>Ego vehicle is not in autonomous driving state</li> </ul> <ul> <li>When the ego vehicle is not moving (Current Velocity is below a 0.1 m/s threshold)</li> </ul>"},{"location":"control/autoware_autonomous_emergency_braking/#2-generate-a-predicted-path-of-the-ego-vehicle","title":"2. Generate a predicted path of the ego vehicle","text":""},{"location":"control/autoware_autonomous_emergency_braking/#21-overview-of-imu-path-generation","title":"2.1 Overview of IMU Path Generation","text":"<p>AEB generates a predicted footprint path based on current velocity and current angular velocity obtained from attached sensors. Note that if <code>use_imu_path</code> is <code>false</code>, it skips this step. This predicted path is generated as:</p> \\[ x_{k+1} = x_k + v cos(\\theta_k) dt \\\\ y_{k+1} = y_k + v sin(\\theta_k) dt \\\\ \\theta_{k+1} = \\theta_k + \\omega dt \\] <p>where \\(v\\) and \\(\\omega\\) are current longitudinal velocity and angular velocity respectively. \\(dt\\) is time interval that users can define in advance with the <code>imu_prediction_time_interval</code> parameter. The IMU path is generated considering a time horizon, defined by the <code>imu_prediction_time_horizon</code> parameter.</p>"},{"location":"control/autoware_autonomous_emergency_braking/#22-constraints-and-countermeasures-in-imu-path-generation","title":"2.2 Constraints and Countermeasures in IMU Path Generation","text":"<p>Since the IMU path generation only uses the ego vehicle's current angular velocity, disregarding the MPC's planner steering, the shape of the IMU path tends to get distorted quite easily and protrude out of the ego vehicle's current lane, possibly causing unwanted emergency stops. There are two countermeasures for this issue:</p> <ol> <li> <p>Control using the <code>max_generated_imu_path_length</code> parameter</p> <ul> <li>Generation stops when path length exceeds the set value</li> <li>Avoid using a large <code>imu_prediction_time_horizon</code></li> </ul> </li> <li> <p>Control based on lateral deviation</p> <ul> <li>Set the <code>limit_imu_path_lat_dev</code> parameter to \"true\"</li> <li>Set deviation threshold using <code>imu_path_lat_dev_threshold</code></li> <li>Path generation stops when lateral deviation exceeds the threshold</li> </ul> </li> </ol>"},{"location":"control/autoware_autonomous_emergency_braking/#23-advantages-and-limitations-of-lateral-deviation-control","title":"2.3 Advantages and Limitations of Lateral Deviation Control","text":"<p>The advantage of setting a lateral deviation limit with the <code>limit_imu_path_lat_dev</code> parameter is that the <code>imu_prediction_time_horizon</code> and the <code>max_generated_imu_path_length</code> can be increased without worries about the IMU predicted path deforming beyond a certain threshold. The downside is that the IMU path will be cut short when the ego has a high angular velocity, in said cases, the AEB module would mostly rely on the MPC path to prevent or mitigate collisions.</p> <p>If it is assumed the ego vehicle will mostly travel along the centerline of its lanelets, it can be useful to set the lateral deviation threshold parameter <code>imu_path_lat_dev_threshold</code> to be equal to or smaller than the average lanelet width divided by 2, that way, the chance of the IMU predicted path leaving the current ego lanelet is smaller, and it is possible to increase the <code>imu_prediction_time_horizon</code> to prevent frontal collisions when the ego is mostly traveling in a straight line.</p> <p>The lateral deviation is measured using the ego vehicle's current position as a reference, and it measures the distance of the furthermost vertex of the predicted ego footprint to the predicted path. The following image illustrates how the lateral deviation of a given ego pose is measured.</p> <p></p>"},{"location":"control/autoware_autonomous_emergency_braking/#24-imu-path-generation-algorithm","title":"2.4 IMU Path Generation Algorithm","text":""},{"location":"control/autoware_autonomous_emergency_braking/#241-selection-of-lateral-deviation-check-points","title":"2.4.1 Selection of Lateral Deviation Check Points","text":"<p>Select vehicle vertices for lateral deviation checks based on the following conditions:</p> <ul> <li>Forward motion (\\(v &gt; 0\\))<ul> <li>Right turn (\\(\\omega &gt; 0\\)): Right front vertex</li> <li>Left turn (\\(\\omega &lt; 0\\)): Left front vertex</li> </ul> </li> <li>Reverse motion (\\(v &lt; 0\\))<ul> <li>Right turn (\\(\\omega &gt; 0\\)): Right rear vertex</li> <li>Left turn (\\(\\omega &lt; 0\\)): Left rear vertex</li> </ul> </li> <li>Straight motion (\\(\\omega = 0\\)): Check both front/rear vertices depending on forward/reverse motion</li> </ul>"},{"location":"control/autoware_autonomous_emergency_braking/#242-path-generation-process","title":"2.4.2 Path Generation Process","text":"<p>Execute the following steps at each time step:</p> <ol> <li> <p>State Update</p> <ul> <li>Calculate next position \\((x_{k+1}, y_{k+1})\\) and yaw angle \\(\\theta_{k+1}\\) based on current velocity \\(v\\) and angular velocity \\(\\omega\\)</li> <li>Time interval \\(dt\\) is based on the <code>imu_prediction_time_interval</code> parameter</li> </ul> </li> <li> <p>Vehicle Footprint Generation</p> <ul> <li>Place vehicle footprint at calculated position</li> <li>Calculate check point coordinates</li> </ul> </li> <li> <p>Lateral Deviation Calculation</p> <ul> <li>Calculate lateral deviation from selected vertex to path</li> <li>Update path length and elapsed time</li> </ul> </li> <li> <p>Evaluation of Termination Conditions</p> </li> </ol>"},{"location":"control/autoware_autonomous_emergency_braking/#243-termination-conditions","title":"2.4.3 Termination Conditions","text":"<p>Path generation terminates when any of the following conditions are met:</p> <ol> <li> <p>Basic Termination Conditions (both must be satisfied)</p> <ul> <li>Predicted time exceeds <code>imu_prediction_time_horizon</code></li> <li>AND path length exceeds <code>min_generated_imu_path_length</code></li> </ul> </li> <li> <p>Path Length Termination Condition</p> <ul> <li>Path length exceeds <code>max_generated_imu_path_length</code></li> </ul> </li> <li> <p>Lateral Deviation Termination Condition (when <code>limit_imu_path_lat_dev = true</code>)</p> <ul> <li>Lateral deviation of selected vertex exceeds <code>imu_path_lat_dev_threshold</code></li> </ul> </li> </ol>"},{"location":"control/autoware_autonomous_emergency_braking/#mpc-path-generation","title":"MPC path generation","text":"<p>If the <code>use_predicted_trajectory</code> parameter is set to true, the AEB module will directly use the predicted path from the MPC as a base to generate a footprint path. It will copy the ego poses generated by the MPC until a given time horizon. The <code>mpc_prediction_time_horizon</code> parameter dictates how far ahead in the future the MPC path will predict the ego vehicle's movement. Both the IMU footprint path and the MPC footprint path can be used at the same time.</p>"},{"location":"control/autoware_autonomous_emergency_braking/#3-get-target-obstacles","title":"3. Get target obstacles","text":"<p>After generating the ego footprint path(s), the target obstacles are identified. There are two methods to find target obstacles: using the input point cloud, or using the predicted object information coming from perception modules.</p>"},{"location":"control/autoware_autonomous_emergency_braking/#pointcloud-obstacle-filtering","title":"Pointcloud obstacle filtering","text":"<p>The AEB module can filter the input pointcloud to find target obstacles with which the ego vehicle might collide. This method can be enable if the <code>use_pointcloud_data</code> parameter is set to true. The pointcloud obstacle filtering has three major steps, which are rough filtering, noise filtering with clustering and rigorous filtering.</p>"},{"location":"control/autoware_autonomous_emergency_braking/#rough-filtering","title":"Rough filtering","text":"<p>In rough filtering step, we select target obstacle with simple filter. Create a search area up to a certain distance (default is half of the ego vehicle width plus the <code>path_footprint_extra_margin</code> parameter plus the <code>expand_width</code> parameter) away from the predicted path of the ego vehicle and ignore the point cloud that are not within it. The rough filtering step is illustrated below.</p> <p></p>"},{"location":"control/autoware_autonomous_emergency_braking/#noise-filtering-with-clustering-and-convex-hulls","title":"Noise filtering with clustering and convex hulls","text":"<p>To prevent the AEB from considering noisy points, euclidean clustering is performed on the filtered point cloud. The points in the point cloud that are not close enough to other points to form a cluster are discarded. Furthermore, each point in a cluster is compared against the <code>cluster_minimum_height</code> parameter, if no point inside a cluster has a height/z value greater than <code>cluster_minimum_height</code>, the whole cluster of points is discarded. The parameters <code>cluster_tolerance</code>, <code>minimum_cluster_size</code> and <code>maximum_cluster_size</code> can be used to tune the clustering and the size of objects to be ignored, for more information about the clustering method used by the AEB module, please check the official documentation on euclidean clustering of the PCL library: https://pcl.readthedocs.io/projects/tutorials/en/master/cluster_extraction.html.</p> <p>Furthermore, a 2D convex hull is created around each detected cluster, the vertices of each hull represent the most extreme/outside points of the cluster. These vertices are then checked in the next step.</p>"},{"location":"control/autoware_autonomous_emergency_braking/#rigorous-filtering","title":"Rigorous filtering","text":"<p>After Noise filtering, the module performs a geometric collision check to determine whether the filtered obstacles/hull vertices actually have possibility to collide with the ego vehicle. In this check, the ego vehicle is represented as a rectangle, and the point cloud obstacles are represented as points. Only the vertices with a possibility of collision are labeled as target obstacles.</p> <p></p>"},{"location":"control/autoware_autonomous_emergency_braking/#obstacle-labeling","title":"Obstacle labeling","text":"<p>After rigorous filtering, the remaining obstacles are labeled. An obstacle is given a \"target\" label for collision checking only if it falls within the ego vehicle's defined footprint (made using the ego vehicle's width and the <code>expand_width</code> parameter). For an emergency stop to occur, at least one obstacle needs to be labeled as a target.</p> <p></p>"},{"location":"control/autoware_autonomous_emergency_braking/#using-predicted-objects-to-get-target-obstacles","title":"Using predicted objects to get target obstacles","text":"<p>If the <code>use_predicted_object_data</code> parameter is set to true, the AEB can use predicted object data coming from the perception modules, to get target obstacle points. This is done by obtaining the 2D intersection points between the ego's predicted footprint path (made using the ego vehicle's width and the <code>expand_width</code> parameter) and each of the predicted objects enveloping polygon or bounding box. if there is no intersection, all points are discarded.</p> <p></p>"},{"location":"control/autoware_autonomous_emergency_braking/#finding-the-closest-target-obstacle","title":"Finding the closest target obstacle","text":"<p>After identifying all possible obstacles using pointcloud data and/or predicted object data, the AEB module selects the closest point to the ego vehicle as the candidate for collision checking. The \"closest object\" is defined as an obstacle within the ego vehicle's footprint, determined by its width and the <code>expand_width</code> parameter, that is closest to the ego vehicle along the longitudinal axis, using the IMU or MPC path as a reference. Target obstacles are prioritized over those outside the ego path, even if the latter are longitudinally closer. This prioritization ensures that the collision check focuses on objects that pose the highest risk based on the vehicle's trajectory.</p> <p>If no target obstacles are found, the AEB module considers other nearby obstacles outside the path. In such cases, it skips the collision check but records the position of the closest obstacle to calculate its speed (Step #4). Note that, obstacles obtained with predicted object data are all target obstacles since they are within the ego footprint path and it is not necessary to calculate their speed (it is already calculated by the perception module). Such obstacles are excluded from step #4.</p> <p></p>"},{"location":"control/autoware_autonomous_emergency_braking/#4-obstacle-velocity-estimation","title":"4. Obstacle velocity estimation","text":"<p>To begin calculating the target point's velocity, the point must enter the speed calculation area, which is defined by the <code>speed_calculation_expansion_margin</code> parameter plus the ego vehicles width and the <code>expand_width</code> parameter. Depending on the operational environment, this margin can reduce unnecessary autonomous emergency braking caused by velocity miscalculations during the initial calculation steps.</p> <p></p> <p>Once the position of the closest obstacle/point is determined, the AEB modules uses the history of previously detected objects to estimate the closest object relative speed using the following equations:</p> \\[ d_{t} = t_{1} - t_{0} \\] \\[ d_{x} = norm(o_{x} - prev_{x}) \\] \\[ v_{norm} = d_{x} / d_{t} \\] <p>Where \\(t_{1}\\) and \\(t_{0}\\) are the timestamps of the point clouds used to detect the current closest object and the closest object of the previous point cloud frame, and \\(o_{x}\\) and \\(prev_{x}\\) are the positions of those objects, respectively.</p> <p></p> <p>Note that, when the closest obstacle/point comes from using predicted object data, \\(v_{norm}\\) is calculated by directly computing the norm of the predicted object's velocity in the x and y axes.</p> <p>The velocity vector is then compared against the ego's predicted path to get the longitudinal velocity \\(v_{obj}\\):</p> \\[ v_{obj} = v_{norm} * Cos(yaw_{diff}) + v_{ego} \\] <p>where \\(yaw_{diff}\\) is the difference in yaw between the ego path and the displacement vector  and \\(v_{ego}\\) is the ego's current speed, which accounts for the movement of points caused by the ego moving and not the object. All these equations are performed disregarding the z axis (in 2D).</p> <p>Note that the object velocity is calculated against the ego's current movement direction. If the object moves in the opposite direction to the ego's movement, the object velocity will be negative, which will reduce the rss distance on the next step.</p> <p>The resulting estimated object speed is added to a queue of speeds with timestamps. The AEB then checks for expiration of past speed estimations and eliminates expired speed measurements from the queue, the object expiration is determined by checking if the time elapsed since the speed was first added to the queue is larger than the parameter <code>previous_obstacle_keep_time</code>. Finally, the median speed of the queue is calculated. The median speed will be used to calculate the RSS distance used for collision checking.</p>"},{"location":"control/autoware_autonomous_emergency_braking/#5-collision-check-with-target-obstacles-using-rss-distance","title":"5. Collision check with target obstacles using RSS distance","text":"<p>In the fifth step, the AEB module checks for collision with the closest target obstacle using RSS distance. Only the closest target object is evaluated because RSS distance is used to determine collision risk. If the nearest target point is deemed safe, all other potential obstacles within the path are also assumed to be safe.</p> <p>RSS distance is formulated as:</p> \\[ d = v_{ego}*t_{response} + v_{ego}^2/(2*a_{min}) -(sign(v_{obj})) * v_{obj}^2/(2*a_{obj_{min}}) + offset \\] <p>where \\(v_{ego}\\) and \\(v_{obj}\\) is current ego and obstacle velocity, \\(a_{min}\\) and \\(a_{obj_{min}}\\) is ego and object minimum acceleration (maximum deceleration), \\(t_{response}\\) is response time of the ego vehicle to start deceleration. Therefore the distance from the ego vehicle to the obstacle is smaller than this RSS distance \\(d\\), the ego vehicle send emergency stop signals.</p> <p>Only obstacles classified as \"targets\" (as defined in Step #3) are considered for RSS distance calculations. Among these \"target\" obstacles, the one closest to the ego vehicle is used for the calculation. If no \"target\" obstacles are present\u2014meaning no obstacles fall within the ego vehicle's predicted path (determined by its width and an expanded margin)\u2014this step is skipped. Instead, the position of the closest obstacle is recorded for future speed calculations (Step #4). In this scenario, no emergency stop diagnostic message is generated. The process is illustrated in the accompanying diagram.</p> <p></p>"},{"location":"control/autoware_autonomous_emergency_braking/#6-send-emergency-stop-signals-to-diagnostics","title":"6. Send emergency stop signals to <code>/diagnostics</code>","text":"<p>If AEB detects collision with point cloud obstacles in the previous step, it sends emergency signal to <code>/diagnostics</code> in this step. Note that in order to enable emergency stop, it has to send ERROR level emergency. Moreover, AEB user should modify the setting file to keep the emergency level, otherwise Autoware does not hold the emergency state.</p>"},{"location":"control/autoware_autonomous_emergency_braking/#use-cases","title":"Use cases","text":""},{"location":"control/autoware_autonomous_emergency_braking/#front-vehicle-suddenly-brakes","title":"Front vehicle suddenly brakes","text":"<p>The AEB can activate when a vehicle in front suddenly brakes, and a collision is detected by the AEB module. Provided the distance between the ego vehicle and the front vehicle is large enough and the ego\u2019s emergency acceleration value is high enough, it is possible to avoid or soften collisions with vehicles in front that suddenly brake. NOTE: the acceleration used by the AEB to calculate rss_distance is NOT necessarily the acceleration used by the ego while doing an emergency brake. The acceleration used by the real vehicle can be tuned by changing the mrm_emergency stop jerk and acceleration values.</p> <p></p>"},{"location":"control/autoware_autonomous_emergency_braking/#stop-for-objects-that-appear-suddenly","title":"Stop for objects that appear suddenly","text":"<p>When an object appears suddenly, the AEB can act as a fail-safe to stop the ego vehicle when other modules fail to detect the object on time. If sudden object cut ins are expected, it might be useful for the AEB module to detect collisions of objects BEFORE they enter the real ego vehicle path by increasing the <code>expand_width</code> parameter.</p> <p></p>"},{"location":"control/autoware_autonomous_emergency_braking/#preventing-collisions-with-rear-objects","title":"Preventing Collisions with rear objects","text":"<p>The AEB module can also prevent collisions when the ego vehicle is moving backwards.</p> <p></p>"},{"location":"control/autoware_autonomous_emergency_braking/#preventing-collisions-in-case-of-wrong-odometry-imu-path-only","title":"Preventing collisions in case of wrong Odometry (IMU path only)","text":"<p>When vehicle odometry information is faulty, it is possible that the MPC fails to predict a correct path for the ego vehicle. If the MPC predicted path is wrong, collision avoidance will not work as intended on the planning modules. However, the AEB\u2019s IMU path does not depend on the MPC and could be able to predict a collision when the other modules cannot. As an example you can see a figure of a hypothetical case in which the MPC path is wrong and only the AEB\u2019s IMU path detects a collision.</p> <p></p>"},{"location":"control/autoware_autonomous_emergency_braking/#parameters","title":"Parameters","text":"Name Type Description Default Range use_predicted_trajectory boolean Flag to use the predicted path from the control module. 1 N/A use_imu_path boolean Flag to use the predicted path generated by sensor data. 1 N/A limit_imu_path_lat_dev boolean Flag to limit the lateral deviation of the IMU path. 0 N/A limit_imu_path_length boolean Flag to limit the length of the IMU path. 1 N/A use_pointcloud_data boolean Flag to use point cloud data for collision detection. 1 N/A use_predicted_object_data boolean Flag to use predicted object data. 0 N/A use_object_velocity_calculation boolean Flag to use object velocity calculation. If false, object velocity is set to 0 m/s. 1 N/A check_autoware_state boolean Flag to enable or disable Autoware state check. 1 N/A imu_path_lat_dev_threshold float Lateral deviation threshold for the IMU path. 1.75 N/A min_generated_imu_path_length float Minimum distance for a predicted path generated by sensors. 0.5 N/A max_generated_imu_path_length float Maximum distance for a predicted path generated by sensors. 10 N/A imu_prediction_time_horizon float Time horizon of the predicted path generated by sensors. 1.5 N/A imu_prediction_time_interval float Time interval of the predicted path generated by sensors. 0.1 N/A mpc_prediction_time_horizon float Time horizon of the predicted path generated by MPC. 4.5 N/A mpc_prediction_time_interval float Time interval of the predicted path generated by MPC. 0.1 N/A publish_debug_pointcloud boolean Flag to publish the point cloud used for debugging. 0 N/A publish_debug_markers boolean Flag to publish debug markers. 1 N/A detection_range_min_height float Minimum height of detection range used to avoid ghost braking by false positives. 0 N/A detection_range_max_height_margin float Margin for maximum height of detection range used to avoid ghost braking. 0 N/A voxel_grid_x float Downsampling parameter for x-axis in the voxel grid filter. 0.1 N/A voxel_grid_y float Downsampling parameter for y-axis in the voxel grid filter. 0.1 N/A voxel_grid_z float Downsampling parameter for z-axis in the voxel grid filter. 0.5 N/A expand_width float Expansion width of the ego vehicle for collision checking, path cropping, and speed calculation. -0.2 N/A path_footprint_extra_margin float Extra margin added to the ego vehicle footprint for cropping the point cloud. 1 N/A speed_calculation_expansion_margin float Expansion width of the ego vehicle footprint used for speed calculation. 0.7 N/A cluster_tolerance float Maximum allowable distance between two points in the same cluster. 0.15 N/A cluster_minimum_height float Minimum height of a cluster to be considered as a target. 0.1 N/A minimum_cluster_size integer Minimum number of points in a cluster to be considered as a target. 10 N/A maximum_cluster_size integer Maximum number of points in a cluster to be considered as a target. 10000 N/A longitudinal_offset_margin float Longitudinal offset distance for collision checking. 1 N/A t_response float Response time for the ego vehicle to detect a decelerating object. 1 N/A a_ego_min float Maximum deceleration value of the ego vehicle. -3 N/A a_obj_min float Maximum deceleration value of objects. -1 N/A collision_keeping_sec float Time duration to keep detecting a collision. 3 N/A previous_obstacle_keep_time float Time duration to keep the previous obstacle detected. 1 N/A aeb_hz float Frequency at which the AEB module operates. 10 N/A"},{"location":"control/autoware_autonomous_emergency_braking/#limitations","title":"Limitations","text":"<ul> <li>The distance required to stop after collision detection depends on the ego vehicle's speed and deceleration performance. To avoid collisions, it's necessary to increase the detection distance and set a higher deceleration rate. However, this creates a trade-off as it may also increase the number of unnecessary activations. Therefore, it's essential to consider what role this module should play and adjust the parameters accordingly.</li> </ul> <ul> <li>AEB might not be able to react with obstacles that are close to the ground. It depends on the performance of the pre-processing methods applied to the point cloud.</li> </ul> <ul> <li>Longitudinal acceleration information obtained from sensors is not used due to the high amount of noise.</li> </ul> <ul> <li>The accuracy of the predicted path created from sensor data depends on the accuracy of sensors attached to the ego vehicle.</li> </ul>"},{"location":"control/autoware_collision_detector/","title":"Collision Detector","text":""},{"location":"control/autoware_collision_detector/#collision-detector","title":"Collision Detector","text":""},{"location":"control/autoware_collision_detector/#purpose","title":"Purpose","text":"<p>This module publishes an <code>ERROR</code> diagnostic if a collision is detected with the current ego footprint.</p>"},{"location":"control/autoware_collision_detector/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"control/autoware_collision_detector/#flow-chart","title":"Flow chart","text":"<ol> <li>Check input data.</li> <li>Filter dynamic objects.</li> <li>Find nearest object and its distance to ego.</li> <li>Publish an <code>ERROR</code> diagnostic depending on the recent collision detection results.</li> </ol>"},{"location":"control/autoware_collision_detector/#algorithms","title":"Algorithms","text":""},{"location":"control/autoware_collision_detector/#check-data","title":"Check data","text":"<p>Check that <code>collision_detector</code> receives no ground pointcloud, dynamic objects.</p>"},{"location":"control/autoware_collision_detector/#object-filtering","title":"Object Filtering","text":""},{"location":"control/autoware_collision_detector/#recognition-assumptions","title":"Recognition Assumptions","text":"<ol> <li>If the classification changes but it's considered the same object, the uuid does not change.</li> <li>It's possible for the same uuid to be recognized after being lost for a few frames.</li> <li>Once an object is determined to be excluded, it continues to be excluded for a certain period of time.</li> </ol>"},{"location":"control/autoware_collision_detector/#filtering-process","title":"Filtering Process","text":"<ol> <li> <p>Initial Recognition and Exclusion:</p> <ul> <li>The system checks if a newly recognized object's classification is listed in <code>nearby_object_type_filters</code>.</li> <li>If so, and the object is within the <code>nearby_filter_radius</code>, it is marked for exclusion.</li> </ul> </li> <li> <p>New Object Determination:</p> <ul> <li>An object is considered \"new\" based on its UUID.</li> <li>If the UUID is not found in recent frame data, the object is treated as new.</li> </ul> </li> <li> <p>Exclusion Mechanism:</p> <ul> <li>Newly excluded objects are recorded by their UUID.</li> <li>These objects continue to be excluded for a set period (<code>keep_ignoring_time</code>) as long as they maintain the classification specified in <code>nearby_object_type_filters</code> and remain within the <code>nearby_filter_radius</code>.</li> </ul> </li> </ol>"},{"location":"control/autoware_collision_detector/#get-distance-to-nearest-object","title":"Get distance to nearest object","text":"<p>Calculate distance between ego vehicle and the nearest object. In this function, it calculates the minimum distance between the polygon of ego vehicle and all points in pointclouds and the polygons of dynamic objects. If the minimum distance is lower than the <code>collision_distance</code> parameter, then a collision is detected.</p>"},{"location":"control/autoware_collision_detector/#time-buffer-and-distance-hysteresis","title":"Time buffer and distance hysteresis","text":"<p>Before publishing an <code>ERROR</code> diagnostic, a collision must be detected for at least a duration set by the parameter <code>time_buffer.on</code>. Once an <code>ERROR</code> diagnostic is published, the <code>time_buffer.off_distance_hysteresis</code> parameter is used to make the ego footprint larger, making it easier to detect a collision. To stop publishing the <code>ERROR</code> diagnostic, no collision must be detected for at least a duration set by the parameter <code>time_buffer.off</code>.</p>"},{"location":"control/autoware_collision_detector/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"control/autoware_collision_detector/#input","title":"Input","text":"Name Type Description <code>/perception/obstacle_segmentation/pointcloud</code> <code>sensor_msgs::msg::PointCloud2</code> Pointcloud of obstacles which the ego-vehicle should stop or avoid <code>/perception/object_recognition/objects</code> <code>autoware_perception_msgs::msg::PredictedObjects</code> Dynamic objects <code>/tf</code> <code>tf2_msgs::msg::TFMessage</code> TF <code>/tf_static</code> <code>tf2_msgs::msg::TFMessage</code> TF static"},{"location":"control/autoware_collision_detector/#output","title":"Output","text":"Name Type Description <code>/diagnostics</code> <code>diagnostic_msgs::msg::DiagnosticArray</code> Diagnostics <code>~/debug_markers</code> <code>visualization_msgs::msg::MarkerArray</code> Debug markers"},{"location":"control/autoware_collision_detector/#parameters","title":"Parameters","text":"Name Type Description Default value <code>use_pointcloud</code> <code>bool</code> Use pointcloud as obstacle check <code>true</code> <code>use_dynamic_object</code> <code>bool</code> Use dynamic object as obstacle check <code>true</code> <code>collision_distance</code> <code>double</code> Distance threshold at which an object is considered a collision. [m] 0.15 <code>nearby_filter_radius</code> <code>double</code> Distance range for filtering objects. Objects within this radius are considered. [m] 5.0 <code>keep_ignoring_time</code> <code>double</code> Time to keep filtering objects that first appeared in the vicinity [sec] 10.0 <code>nearby_object_type_filters</code> <code>object of bool values</code> Specifies which object types to filter. Only objects with <code>true</code> value will be filtered. <code>{unknown: true, others: false}</code> <code>ignore_behind_rear_axle</code> <code>bool</code> If true, collisions detected behind the rear axle of the ego vehicle are ignored <code>true</code> <code>time_buffer.on</code> <code>double</code> [s] minimum consecutive detection time before triggering the ERROR diagnostic 0.2 <code>time_buffer.off</code> <code>double</code> [s] minimum consecutive time without collision detection (including the hysteresis) before releasing the ERROR diagnostic 5.0 <code>time_buffer.off_distance_hysteresis</code> <code>double</code> [m] extra distance used to detect collisions once the diagnostic is triggered 1.0"},{"location":"control/autoware_collision_detector/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<ul> <li>This module is based on <code>surround_obstacle_checker</code></li> </ul>"},{"location":"control/autoware_control_command_gate/","title":"autoware_control_command_gate","text":""},{"location":"control/autoware_control_command_gate/#autoware_control_command_gate","title":"autoware_control_command_gate","text":""},{"location":"control/autoware_control_command_gate/#overview","title":"Overview","text":"<p>This package subscribes to multiple commands, selects one and publish it. Here, unless otherwise specified, command refers to a set of four commands: control, gear, turn_indicators, and hazard_lights. Each command input is identified as a command source, an arbitrary string. The node also applies a nominal filter to the selected commands to correct for obvious abnormal values.</p> <p></p>"},{"location":"control/autoware_control_command_gate/#requirements","title":"Requirements","text":"<ul> <li>Functional<ul> <li>Subscribe to multiple commands as command source.</li> <li>Generate a builtin stop command as command source.</li> <li>Select one from the command sources and publish it.</li> <li>Filter abnormal values in the command selected.</li> <li>Filter the command selected smoothly on mode transition when requested.</li> <li>Support parking mode for humans to explicitly stop the vehicle (T.B.D.).</li> </ul> </li> <li>Safety<ul> <li>Detect timeouts for each command source.</li> <li>Select builtin stop when selected command source is not available.</li> </ul> </li> </ul>"},{"location":"control/autoware_control_performance_analysis/","title":"autoware_control_performance_analysis","text":""},{"location":"control/autoware_control_performance_analysis/#autoware_control_performance_analysis","title":"autoware_control_performance_analysis","text":""},{"location":"control/autoware_control_performance_analysis/#purpose","title":"Purpose","text":"<p><code>autoware_control_performance_analysis</code> is the package to analyze the tracking performance of a control module and monitor the driving status of the vehicle.</p> <p>This package is used as a tool to quantify the results of the control module. That's why it doesn't interfere with the core logic of autonomous driving.</p> <p>Based on the various input from planning, control, and vehicle, it publishes the result of analysis as <code>autoware_control_performance_analysis::msg::ErrorStamped</code> defined in this package.</p> <p>All results in <code>ErrorStamped</code> message are calculated in Frenet Frame of curve. Errors and velocity errors are calculated by using paper below.</p> <p><code>Werling, Moritz &amp; Groell, Lutz &amp; Bretthauer, Georg. (2010). Invariant Trajectory Tracking With a Full-Size Autonomous Road Vehicle. IEEE Transactions on Robotics. 26. 758 - 765. 10.1109/TRO.2010.2052325.</code></p> <p>If you are interested in calculations, you can see the error and error velocity calculations in section <code>C. Asymptotical Trajectory Tracking With Orientation Control</code>.</p> <p>Error acceleration calculations are made based on the velocity calculations above. You can see below the calculation of error acceleration.</p> <p></p>"},{"location":"control/autoware_control_performance_analysis/#input-output","title":"Input / Output","text":""},{"location":"control/autoware_control_performance_analysis/#input-topics","title":"Input topics","text":"Name Type Description <code>/planning/trajectory</code> autoware_planning_msgs::msg::Trajectory Output trajectory from planning module. <code>/control/command/control_cmd</code> autoware_control_msgs::msg::Control Output control command from control module. <code>/vehicle/status/steering_status</code> autoware_vehicle_msgs::msg::SteeringReport Steering information from vehicle. <code>/localization/kinematic_state</code> nav_msgs::msg::Odometry Use twist from odometry. <code>/tf</code> tf2_msgs::msg::TFMessage Extract ego pose from tf."},{"location":"control/autoware_control_performance_analysis/#output-topics","title":"Output topics","text":"Name Type Description <code>/control_performance/performance_vars</code> autoware_control_performance_analysis::msg::ErrorStamped The result of the performance analysis. <code>/control_performance/driving_status</code> autoware_control_performance_analysis::msg::DrivingMonitorStamped Driving status (acceleration, jerk etc.) monitoring"},{"location":"control/autoware_control_performance_analysis/#outputs","title":"Outputs","text":""},{"location":"control/autoware_control_performance_analysis/#autoware_control_performance_analysismsgdrivingmonitorstamped","title":"autoware_control_performance_analysis::msg::DrivingMonitorStamped","text":"Name Type Description <code>longitudinal_acceleration</code> float \\([ \\mathrm{m/s^2} ]\\) <code>longitudinal_jerk</code> float \\([ \\mathrm{m/s^3} ]\\) <code>lateral_acceleration</code> float \\([ \\mathrm{m/s^2} ]\\) <code>lateral_jerk</code> float \\([ \\mathrm{m/s^3} ]\\) <code>desired_steering_angle</code> float \\([ \\mathrm{rad} ]\\) <code>controller_processing_time</code> float Timestamp between last two control command messages \\([ \\mathrm{ms} ]\\)"},{"location":"control/autoware_control_performance_analysis/#autoware_control_performance_analysismsgerrorstamped","title":"autoware_control_performance_analysis::msg::ErrorStamped","text":"Name Type Description <code>lateral_error</code> float \\([ \\mathrm{m} ]\\) <code>lateral_error_velocity</code> float \\([ \\mathrm{m/s} ]\\) <code>lateral_error_acceleration</code> float \\([ \\mathrm{m/s^2} ]\\) <code>longitudinal_error</code> float \\([ \\mathrm{m} ]\\) <code>longitudinal_error_velocity</code> float \\([ \\mathrm{m/s} ]\\) <code>longitudinal_error_acceleration</code> float \\([ \\mathrm{m/s^2} ]\\) <code>heading_error</code> float \\([ \\mathrm{rad} ]\\) <code>heading_error_velocity</code> float \\([ \\mathrm{rad/s} ]\\) <code>control_effort_energy</code> float \\([ \\mathbf{u}^\\top \\mathbf{R} \\mathbf{u} ]\\) simplified to \\([ R \\cdot u^2 ]\\) <code>error_energy</code> float \\(e_{\\text{lat}}^2 + e_\\theta^2\\) (squared lateral error + squared heading error) <code>value_approximation</code> float \\(V = \\mathbf{x}^\\top \\mathbf{P} \\mathbf{x}\\); Value function from DARE Lyapunov matrix \\(\\mathbf{P}\\) <code>curvature_estimate</code> float \\([ \\mathrm{1/m} ]\\) <code>curvature_estimate_pp</code> float \\([ \\mathrm{1/m} ]\\) <code>vehicle_velocity_error</code> float \\([ \\mathrm{m/s} ]\\) <code>tracking_curvature_discontinuity_ability</code> float Measures the ability to track curvature changes \\(\\frac{\\lvert \\Delta(\\text{curvature}) \\rvert}{1 + \\lvert \\Delta(e_{\\text{lat}}) \\rvert}\\)"},{"location":"control/autoware_control_performance_analysis/#parameters","title":"Parameters","text":"Name Type Description <code>curvature_interval_length</code> double Used for estimating current curvature <code>prevent_zero_division_value</code> double Value to avoid zero division. Default is <code>0.001</code> <code>odom_interval</code> unsigned integer Interval between odom messages, increase it for smoother curve. <code>acceptable_max_distance_to_waypoint</code> double Maximum distance between trajectory point and vehicle [m] <code>acceptable_max_yaw_difference_rad</code> double Maximum yaw difference between trajectory point and vehicle [rad] <code>low_pass_filter_gain</code> double Low pass filter gain"},{"location":"control/autoware_control_performance_analysis/#usage","title":"Usage","text":"<ul> <li>After launched simulation and control module, launch the <code>control_performance_analysis.launch.xml</code>.</li> <li>You should be able to see the driving monitor and error variables in topics.</li> <li>If you want to visualize the results, you can use <code>Plotjuggler</code> and use <code>config/controller_monitor.xml</code> as layout.</li> <li>After import the layout, please specify the topics that are listed below.</li> </ul> <ul> <li>/localization/kinematic_state</li> <li>/vehicle/status/steering_status</li> <li>/control_performance/driving_status</li> <li>/control_performance/performance_vars</li> </ul> <ul> <li>In <code>Plotjuggler</code> you can export the statistic (max, min, average) values as csv file. Use that statistics to compare the control modules.</li> </ul>"},{"location":"control/autoware_control_performance_analysis/#future-improvements","title":"Future Improvements","text":"<ul> <li>Implement a LPF by cut-off frequency, differential equation and discrete state space update.</li> </ul>"},{"location":"control/autoware_control_validator/","title":"Control Validator","text":""},{"location":"control/autoware_control_validator/#control-validator","title":"Control Validator","text":"<p>The <code>control_validator</code> is a module that checks the validity of the output of the control component. The status of the validation can be viewed in the <code>/diagnostics</code> topic.</p> <p></p>"},{"location":"control/autoware_control_validator/#supported-features","title":"Supported features","text":"<p>The following features are supported for the validation and can have thresholds set by parameters. The listed features below does not always correspond to the latest implementation.</p> Description Arguments Diagnostic equation Inverse velocity: Measured velocity has a different sign from the target velocity. measured velocity \\(v\\), target velocity \\(\\hat{v}\\), and velocity parameter \\(c\\) \\(v \\hat{v} &lt; 0, \\quad \\lvert v \\rvert &gt; c\\) Overspeed: Measured speed exceeds target speed significantly. measured velocity \\(v\\), target velocity \\(\\hat{v}\\), ratio parameter \\(r\\), and offset parameter \\(c\\) \\(\\lvert v \\rvert &gt; (1 + r) \\lvert \\hat{v} \\rvert + c\\) Overrun estimation: estimate overrun even if decelerate by assumed rate. assumed deceleration, assumed delay <ul> <li>Lateral jerk : invalid when the lateral jerk exceeds the configured threshold. The validation uses the vehicle's velocity and steering angle rate to calculate the resulting lateral jerk. The calculation assumes constant velocity (acceleration is zero).</li> <li>Deviation check between reference trajectory and predicted trajectory : invalid when the largest deviation between the predicted trajectory and reference trajectory is greater than the given threshold.</li> <li>Yaw deviation: invalid when the difference between the yaw of the ego vehicle and the nearest (interpolated) trajectory yaw is greater than the given threshold.<ul> <li>2 thresholds are implemented, one to trigger a warning diagnostic, and one to trigger an error diagnostic.</li> </ul> </li> </ul> <p></p>"},{"location":"control/autoware_control_validator/#inputsoutputs","title":"Inputs/Outputs","text":""},{"location":"control/autoware_control_validator/#inputs","title":"Inputs","text":"<p>The <code>control_validator</code> takes in the following inputs:</p> Name Type Description <code>~/input/kinematics</code> nav_msgs/Odometry ego pose and twist <code>~/input/reference_trajectory</code> autoware_planning_msgs/Trajectory reference trajectory which is outputted from planning module to to be followed <code>~/input/predicted_trajectory</code> autoware_planning_msgs/Trajectory predicted trajectory which is outputted from control module"},{"location":"control/autoware_control_validator/#outputs","title":"Outputs","text":"<p>It outputs the following:</p> Name Type Description <code>~/output/validation_status</code> control_validator/ControlValidatorStatus validator status to inform the reason why the trajectory is valid/invalid <code>/diagnostics</code> diagnostic_msgs/DiagnosticStatus diagnostics to report errors"},{"location":"control/autoware_control_validator/#parameters","title":"Parameters","text":"<p>The following parameters can be set for the <code>control_validator</code>:</p>"},{"location":"control/autoware_control_validator/#system-parameters","title":"System parameters","text":"Name Type Description Default value <code>publish_diag</code> bool if true, diagnostics msg is published. true <code>diag_error_count_threshold</code> int the Diag will be set to ERROR when the number of consecutive invalid trajectory exceeds this threshold. (For example, threshold = 1 means, even if the trajectory is invalid, the Diag will not be ERROR if the next trajectory is valid.) true <code>display_on_terminal</code> bool show error msg on terminal true"},{"location":"control/autoware_control_validator/#algorithm-parameters","title":"Algorithm parameters","text":""},{"location":"control/autoware_control_validator/#thresholds","title":"Thresholds","text":"<p>The input trajectory is detected as invalid if the index exceeds the following thresholds.</p> Name Type Description Default value <code>thresholds.max_distance_deviation</code> double invalid threshold of the max distance deviation between the predicted path and the reference trajectory [m] 1.0 <code>thresholds.lateral_jerk</code> double invalid threshold of the lateral jerk for steering rate validation [m/s^3] 10.0 <code>thresholds.rolling_back_velocity</code> double threshold velocity to validate the vehicle velocity [m/s] 0.5 <code>thresholds.over_velocity_offset</code> double threshold velocity offset to validate the vehicle velocity [m/s] 2.0 <code>thresholds.over_velocity_ratio</code> double threshold ratio to validate the vehicle velocity [*] 0.2 <code>thresholds.overrun_stop_point_dist</code> double threshold distance to overrun stop point [m] 0.8 <code>thresholds.acc_error_offset</code> double threshold ratio to validate the vehicle acceleration [*] 0.8 <code>thresholds.acc_error_scale</code> double threshold acceleration to validate the vehicle acceleration [m] 0.2 <code>thresholds.will_overrun_stop_point_dist</code> double threshold distance to overrun stop point [m] 1.0 <code>thresholds.assumed_limit_acc</code> double assumed acceleration for over run estimation [m] 5.0 <code>thresholds.assumed_delay_time</code> double assumed delay for over run estimation [m] 0.2 <code>thresholds.yaw_deviation_error</code> double threshold angle to validate the vehicle yaw related to the nearest trajectory yaw [rad] 1.0 <code>thresholds.yaw_deviation_warn</code> double threshold angle to trigger a WARN diagnostic [rad] 0.5 <code>over_velocity.vel_lpf_gain</code> double low-pass filter gain for filtering vehicle and target velocity [*] (time constant 2.0s at 30msec sampling period) 0.985"},{"location":"control/autoware_external_cmd_selector/","title":"autoware_external_cmd_selector","text":""},{"location":"control/autoware_external_cmd_selector/#autoware_external_cmd_selector","title":"autoware_external_cmd_selector","text":""},{"location":"control/autoware_external_cmd_selector/#purpose","title":"Purpose","text":"<p><code>autoware_external_cmd_selector</code> is the package to publish <code>external_control_cmd</code>, <code>gear_cmd</code>, <code>hazard_lights_cmd</code>, <code>heartbeat</code> and <code>turn_indicators_cmd</code>, according to the current mode, which is <code>remote</code> or <code>local</code>.</p> <p>The current mode is set via service, <code>remote</code> is remotely operated, <code>local</code> is to use the values calculated by Autoware.</p>"},{"location":"control/autoware_external_cmd_selector/#input-output","title":"Input / Output","text":""},{"location":"control/autoware_external_cmd_selector/#input-topics","title":"Input topics","text":"Name Type Description <code>/api/external/set/command/local/control</code> TBD Local. Calculated control value. <code>/api/external/set/command/local/heartbeat</code> TBD Local. Heartbeat. <code>/api/external/set/command/local/shift</code> TBD Local. Gear shift like drive, rear and etc. <code>/api/external/set/command/local/turn_signal</code> TBD Local. Turn signal like left turn, right turn and etc. <code>/api/external/set/command/remote/control</code> TBD Remote. Calculated control value. <code>/api/external/set/command/remote/heartbeat</code> TBD Remote. Heartbeat. <code>/api/external/set/command/remote/shift</code> TBD Remote. Gear shift like drive, rear and etc. <code>/api/external/set/command/remote/turn_signal</code> TBD Remote. Turn signal like left turn, right turn and etc."},{"location":"control/autoware_external_cmd_selector/#output-topics","title":"Output topics","text":"Name Type Description <code>/control/external_cmd_selector/current_selector_mode</code> TBD Current selected mode, remote or local. <code>/diagnostics</code> diagnostic_msgs::msg::DiagnosticArray Check if node is active or not. <code>/external/selected/external_control_cmd</code> TBD Pass through control command with current mode. <code>/external/selected/gear_cmd</code> autoware_vehicle_msgs::msg::GearCommand Pass through gear command with current mode. <code>/external/selected/hazard_lights_cmd</code> autoware_vehicle_msgs::msg::HazardLightsCommand Pass through hazard light with current mode. <code>/external/selected/heartbeat</code> TBD Pass through heartbeat with current mode. <code>/external/selected/turn_indicators_cmd</code> autoware_vehicle_msgs::msg::TurnIndicatorsCommand Pass through turn indicator with current mode."},{"location":"control/autoware_external_cmd_selector/#parameters","title":"Parameters","text":"Name Type Description Default Range update_rate float The rate in Hz at which the external command selector node updates. 10.0 N/A initial_selector_mode string The initial mode for command selection, either 'local' or 'remote'. local ['local', 'remote']"},{"location":"control/autoware_joy_controller/","title":"autoware_joy_controller","text":""},{"location":"control/autoware_joy_controller/#autoware_joy_controller","title":"autoware_joy_controller","text":""},{"location":"control/autoware_joy_controller/#role","title":"Role","text":"<p><code>autoware_joy_controller</code> is the package to convert a joy msg to autoware commands (e.g. steering wheel, shift, turn signal, engage) for a vehicle.</p>"},{"location":"control/autoware_joy_controller/#usage","title":"Usage","text":""},{"location":"control/autoware_joy_controller/#ros-2-launch","title":"ROS 2 launch","text":"<pre><code># With default config (ds4)\nros2 launch autoware_joy_controller joy_controller.launch.xml\n\n# Default config but select from the existing parameter files\nros2 launch autoware_joy_controller joy_controller_param_selection.launch.xml joy_type:=ds4 # or g29, p65, xbox\n\n# Override the param file\nros2 launch autoware_joy_controller joy_controller.launch.xml config_file:=/path/to/your/param.yaml\n</code></pre>"},{"location":"control/autoware_joy_controller/#input-output","title":"Input / Output","text":""},{"location":"control/autoware_joy_controller/#input-topics","title":"Input topics","text":"Name Type Description <code>~/input/joy</code> sensor_msgs::msg::Joy joy controller command <code>~/input/odometry</code> nav_msgs::msg::Odometry ego vehicle odometry to get twist"},{"location":"control/autoware_joy_controller/#output-topics","title":"Output topics","text":"Name Type Description <code>~/output/control_command</code> autoware_control_msgs::msg::Control lateral and longitudinal control command <code>~/output/external_control_command</code> tier4_external_api_msgs::msg::ControlCommandStamped lateral and longitudinal control command <code>~/output/shift</code> tier4_external_api_msgs::msg::GearShiftStamped gear command <code>~/output/turn_signal</code> tier4_external_api_msgs::msg::TurnSignalStamped turn signal command <code>~/output/gate_mode</code> tier4_control_msgs::msg::GateMode gate mode (Auto or External) <code>~/output/heartbeat</code> tier4_external_api_msgs::msg::Heartbeat heartbeat <code>~/output/vehicle_engage</code> autoware_vehicle_msgs::msg::Engage vehicle engage"},{"location":"control/autoware_joy_controller/#parameters","title":"Parameters","text":"Parameter Type Description <code>joy_type</code> string joy controller type (default: DS4) <code>update_rate</code> double update rate to publish control commands <code>accel_ratio</code> double ratio to calculate acceleration (commanded acceleration is ratio * operation) <code>brake_ratio</code> double ratio to calculate deceleration (commanded acceleration is -ratio * operation) <code>steer_ratio</code> double ratio to calculate deceleration (commanded steer is ratio * operation) <code>steering_angle_velocity</code> double steering angle velocity for operation <code>accel_sensitivity</code> double sensitivity to calculate acceleration for external API (commanded acceleration is pow(operation, 1 / sensitivity)) <code>brake_sensitivity</code> double sensitivity to calculate deceleration for external API (commanded acceleration is pow(operation, 1 / sensitivity)) <code>raw_control</code> bool skip input odometry if true <code>velocity_gain</code> double ratio to calculate velocity by acceleration <code>max_forward_velocity</code> double absolute max velocity to go forward <code>max_backward_velocity</code> double absolute max velocity to go backward <code>backward_accel_ratio</code> double ratio to calculate deceleration (commanded acceleration is -ratio * operation)"},{"location":"control/autoware_joy_controller/#p65-joystick-key-map","title":"P65 Joystick Key Map","text":"Action Button Acceleration R2 Brake L2 Steering Left Stick Left Right Shift up Cursor Up Shift down Cursor Down Shift Drive Cursor Left Shift Reverse Cursor Right Turn Signal Left L1 Turn Signal Right R1 Clear Turn Signal A Gate Mode B Emergency Stop Select Clear Emergency Stop Start Autoware Engage X Autoware Disengage Y Vehicle Engage PS Vehicle Disengage Right Trigger"},{"location":"control/autoware_joy_controller/#ds4-joystick-key-map","title":"DS4 Joystick Key Map","text":"Action Button Acceleration R2, \u00d7, or Right Stick Up Brake L2, \u25a1, or Right Stick Down Steering Left Stick Left Right Shift up Cursor Up Shift down Cursor Down Shift Drive Cursor Left Shift Reverse Cursor Right Turn Signal Left L1 Turn Signal Right R1 Clear Turn Signal SHARE Gate Mode OPTIONS Emergency Stop PS Clear Emergency Stop PS Autoware Engage \u25cb Autoware Disengage \u25cb Vehicle Engage \u25b3 Vehicle Disengage \u25b3"},{"location":"control/autoware_joy_controller/#xbox-joystick-key-map","title":"XBOX Joystick Key Map","text":"Action Button Acceleration RT Brake LT Steering Left Stick Left Right Shift up Cursor Up Shift down Cursor Down Shift Drive Cursor Left Shift Reverse Cursor Right Turn Signal Left LB Turn Signal Right RB Clear Turn Signal A Gate Mode B Emergency Stop View Clear Emergency Stop Menu Autoware Engage X Autoware Disengage Y Vehicle Engage Left Stick Button Vehicle Disengage Right Stick Button"},{"location":"control/autoware_lane_departure_checker/","title":"Lane Departure Checker","text":""},{"location":"control/autoware_lane_departure_checker/#lane-departure-checker","title":"Lane Departure Checker","text":"<p>The Lane Departure Checker checks if vehicle follows a trajectory. If it does not follow the trajectory, it reports its status via <code>diagnostic_updater</code>.</p>"},{"location":"control/autoware_lane_departure_checker/#features","title":"Features","text":"<p>This package includes the following features:</p> <ul> <li>Lane Departure: Check if ego vehicle is going to be out of lane boundaries based on output from control module (predicted trajectory).</li> <li>Trajectory Deviation: Check if ego vehicle's pose does not deviate from the trajectory. Checking lateral, longitudinal and yaw deviation.</li> <li>Road Border Departure: Check if ego vehicle's footprint, generated from the control's output, extends beyond the road border.</li> </ul>"},{"location":"control/autoware_lane_departure_checker/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"control/autoware_lane_departure_checker/#how-to-extend-footprint-by-covariance","title":"How to extend footprint by covariance","text":"<ol> <li> <p>Calculate the standard deviation of error ellipse(covariance) in vehicle coordinate.</p> <p>1.Transform covariance into vehicle coordinate.</p> <p> </p> <p>Calculate covariance in vehicle coordinate.</p> <p> </p> <p>2.The longitudinal length we want to expand is correspond to marginal distribution of \\(x_{vehicle}\\), which is represented in \\(Cov_{vehicle}(0,0)\\). In the same way, the lateral length is represented in \\(Cov_{vehicle}(1,1)\\). Wikipedia reference here.</p> </li> <li> <p>Expand footprint based on the standard deviation multiplied with <code>footprint_margin_scale</code>.</p> </li> </ol>"},{"location":"control/autoware_lane_departure_checker/#interface","title":"Interface","text":""},{"location":"control/autoware_lane_departure_checker/#input","title":"Input","text":"<ul> <li>/localization/kinematic_state [<code>nav_msgs::msg::Odometry</code>]</li> <li>/map/vector_map [<code>autoware_map_msgs::msg::LaneletMapBin</code>]</li> <li>/planning/mission_planning/route [<code>autoware_planning_msgs::msg::LaneletRoute</code>]</li> <li>/planning/trajectory [<code>autoware_planning_msgs::msg::Trajectory</code>]</li> <li>/control/trajectory_follower/predicted_trajectory [<code>autoware_planning_msgs::msg::Trajectory</code>]</li> </ul>"},{"location":"control/autoware_lane_departure_checker/#output","title":"Output","text":"<ul> <li>[<code>diagnostic_updater</code>] lane_departure : Update diagnostic level when ego vehicle is out of lane.</li> </ul>"},{"location":"control/autoware_lane_departure_checker/#parameters","title":"Parameters","text":""},{"location":"control/autoware_lane_departure_checker/#node-parameters","title":"Node Parameters","text":""},{"location":"control/autoware_lane_departure_checker/#general-parameters","title":"General Parameters","text":"Name Type Description Default value will_out_of_lane_checker bool Enable checker whether ego vehicle footprint will depart from lane True out_of_lane_checker bool Enable checker whether ego vehicle footprint is out of lane True boundary_departure_checker bool Enable checker whether ego vehicle footprint wil depart from boundary specified by boundary_types_to_detect False update_rate double Frequency for publishing [Hz] 10.0 visualize_lanelet bool Flag for visualizing lanelet False"},{"location":"control/autoware_lane_departure_checker/#parameters-for-lane-departure","title":"Parameters For Lane Departure","text":"Name Type Description Default value include_right_lanes bool Flag for including right lanelet in borders False include_left_lanes bool Flag for including left lanelet in borders False include_opposite_lanes bool Flag for including opposite lanelet in borders False include_conflicting_lanes bool Flag for including conflicting lanelet in borders False"},{"location":"control/autoware_lane_departure_checker/#parameters-for-road-border-departure","title":"Parameters For Road Border Departure","text":"Name Type Description Default value boundary_types_to_detect std::vector\\&lt;std::string&gt; line_string types to detect with boundary_departure_checker [road_border]"},{"location":"control/autoware_lane_departure_checker/#core-parameters","title":"Core Parameters","text":"Name Type Description Default value footprint_margin_scale double Coefficient for expanding footprint margin. Multiplied by 1 standard deviation. 1.0 footprint_extra_margin double Coefficient for expanding footprint margin. When checking for lane departure 0.0 resample_interval double Minimum Euclidean distance between points when resample trajectory.[m] 0.3 max_deceleration double Maximum deceleration when calculating braking distance. 2.8 delay_time double Delay time which took to actuate brake when calculating braking distance. [second] 1.3 max_lateral_deviation double Maximum lateral deviation in vehicle coordinate. [m] 2.0 max_longitudinal_deviation double Maximum longitudinal deviation in vehicle coordinate. [m] 2.0 max_yaw_deviation_deg double Maximum ego yaw deviation from trajectory. [deg] 60.0"},{"location":"control/autoware_mpc_lateral_controller/","title":"MPC Lateral Controller","text":""},{"location":"control/autoware_mpc_lateral_controller/#mpc-lateral-controller","title":"MPC Lateral Controller","text":"<p>This is the design document for the lateral controller node in the <code>autoware_trajectory_follower_node</code> package.</p>"},{"location":"control/autoware_mpc_lateral_controller/#purpose-use-cases","title":"Purpose / Use cases","text":"<p>This node is used to general lateral control commands (steering angle and steering rate) when following a path.</p>"},{"location":"control/autoware_mpc_lateral_controller/#design","title":"Design","text":"<p>The node uses an implementation of linear model predictive control (MPC) for accurate path tracking. The MPC uses a model of the vehicle to simulate the trajectory resulting from the control command. The optimization of the control command is formulated as a Quadratic Program (QP).</p> <p>Different vehicle models are implemented:</p> <ul> <li>kinematics : bicycle kinematics model with steering 1st-order delay.</li> <li>kinematics_no_delay : bicycle kinematics model without steering delay.</li> <li>dynamics : bicycle dynamics model considering slip angle.   The kinematics model is being used by default. Please see the reference [1] for more details.</li> </ul> <p>For the optimization, a Quadratic Programming (QP) solver is used and two options are currently implemented:</p> <ul> <li>unconstraint_fast : use least square method to solve unconstraint QP with eigen.</li> <li>osqp: run the following ADMM   algorithm (for more details see the related papers at   the Citing OSQP section):</li> </ul>"},{"location":"control/autoware_mpc_lateral_controller/#filtering","title":"Filtering","text":"<p>Filtering is required for good noise reduction. A Butterworth filter is employed for processing the yaw and lateral errors, which are used as inputs for the MPC, as well as for refining the output steering angle. Other filtering methods can be considered as long as the noise reduction performances are good enough. The moving average filter for example is not suited and can yield worse results than without any filtering.</p>"},{"location":"control/autoware_mpc_lateral_controller/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>The tracking is not accurate if the first point of the reference trajectory is at or in front of the current ego pose.</p>"},{"location":"control/autoware_mpc_lateral_controller/#inputs-outputs-api","title":"Inputs / Outputs / API","text":""},{"location":"control/autoware_mpc_lateral_controller/#inputs","title":"Inputs","text":"<p>Set the following from the controller_node</p> <ul> <li><code>autoware_planning_msgs/Trajectory</code> : reference trajectory to follow.</li> <li><code>nav_msgs/Odometry</code>: current odometry</li> <li><code>autoware_vehicle_msgs/SteeringReport</code>: current steering</li> </ul>"},{"location":"control/autoware_mpc_lateral_controller/#outputs","title":"Outputs","text":"<p>Return LateralOutput which contains the following to the controller node</p> <ul> <li><code>autoware_control_msgs/Lateral</code></li> <li>LateralSyncData<ul> <li>steer angle convergence</li> </ul> </li> </ul> <p>Publish the following messages.</p> Name Type Description <code>~/output/predicted_trajectory</code> autoware_planning_msgs::Trajectory Predicted trajectory calculated by MPC. The trajectory size will be empty when the controller is in an emergency such as too large deviation from the planning trajectory."},{"location":"control/autoware_mpc_lateral_controller/#mpc-class","title":"MPC class","text":"<p>The <code>MPC</code> class (defined in <code>mpc.hpp</code>) provides the interface with the MPC algorithm. Once a vehicle model, a QP solver, and the reference trajectory to follow have been set (using <code>setVehicleModel()</code>, <code>setQPSolver()</code>, <code>setReferenceTrajectory()</code>), a lateral control command can be calculated by providing the current steer, velocity, and pose to function <code>calculateMPC()</code>.</p>"},{"location":"control/autoware_mpc_lateral_controller/#parameter-description","title":"Parameter description","text":"<p>The default parameters defined in <code>param/lateral_controller_defaults.param.yaml</code> are adjusted to the AutonomouStuff Lexus RX 450h for under 40 km/h driving.</p>"},{"location":"control/autoware_mpc_lateral_controller/#system","title":"System","text":"Name Type Description Default Range traj_resample_dist float distance of waypoints in resampling [m] 0.1 N/A use_steer_prediction boolean flag for using steer prediction (do not use steer measurement) 0 N/A use_delayed_initial_state boolean flag to use x0_delayed as initial state for predicted trajectory 1 N/A"},{"location":"control/autoware_mpc_lateral_controller/#path-smoothing","title":"Path Smoothing","text":"Name Type Description Default Range enable_path_smoothing boolean path smoothing flag. This should be true when uses path resampling to reduce resampling noise. False N/A path_filter_moving_ave_num integer number of data points moving average filter for path smoothing 25 N/A curvature_smoothing_num_traj integer index distance of points used in curvature calculation for trajectory: p(i-num), p(i), p(i+num). Larger num makes less noisy values. 15 N/A curvature_smoothing_num_ref_steer integer index distance of points used in curvature calculation for reference steering command: p(i-num), p(i), p(i+num). Larger num makes less noisy values. 15 N/A"},{"location":"control/autoware_mpc_lateral_controller/#trajectory-extending","title":"Trajectory Extending","text":"Name Type Description Default Range extend_trajectory_for_end_yaw_control boolean trajectory extending flag for end yaw control True N/A"},{"location":"control/autoware_mpc_lateral_controller/#mpc-optimization","title":"MPC Optimization","text":"Name Type Description Default Range qp_solver_type string QP solver option. described below in detail. osqp N/A mpc_prediction_horizon integer total prediction step for MPC 50 N/A mpc_prediction_dt float prediction period for one step [s] 0.1 N/A mpc_weight_lat_error float weight for lateral error 1.0 N/A mpc_weight_heading_error float weight for heading error 0.0 N/A mpc_weight_heading_error_squared_vel float weight for heading error * velocity 0.3 N/A mpc_weight_steering_input float weight for steering error (steer command - reference steer) 1.0 N/A mpc_weight_steering_input_squared_vel float weight for steering error (steer command - reference steer) * velocity 0.25 N/A mpc_weight_lat_jerk float weight for lateral jerk (steer(i) - steer(i-1)) * velocity 0.1 N/A mpc_weight_steer_rate float weight for steering rate [rad/s] 0.0 N/A mpc_weight_steer_acc float weight for derivatives of the steering rate [rad/ss] 1e-06 N/A mpc_low_curvature_weight_lat_error float [used in a low curvature trajectory] weight for lateral error 0.1 N/A mpc_low_curvature_weight_heading_error float [used in a low curvature trajectory] weight for heading error 0.0 N/A mpc_low_curvature_weight_heading_error_squared_vel float [used in a low curvature trajectory] weight for heading error * velocity 0.3 N/A mpc_low_curvature_weight_steering_input float [used in a low curvature trajectory] weight for steering error (steer command - reference steer) 1.0 N/A mpc_low_curvature_weight_steering_input_squared_vel float [used in a low curvature trajectory] weight for steering error (steer command - reference steer) * velocity 0.25 N/A mpc_low_curvature_weight_lat_jerk float [used in a low curvature trajectory] weight for lateral jerk (steer(i) - steer(i-1)) * velocity 0.0 N/A mpc_low_curvature_weight_steer_rate float [used in a low curvature trajectory] weight for steering rate [rad/s] 0.0 N/A mpc_low_curvature_weight_steer_acc float [used in a low curvature trajectory] weight for derivatives of the steering rate [rad/ss] 1e-06 N/A mpc_low_curvature_thresh_curvature float threshold of curvature to use 'low_curvature' parameter 0.0 N/A mpc_weight_terminal_lat_error float terminal lateral error weight in matrix Q to improve mpc stability 1.0 N/A mpc_weight_terminal_heading_error float terminal heading error weight in matrix Q to improve mpc stability 0.1 N/A mpc_zero_ff_steer_deg float threshold that feed-forward angle becomes zero 0.5 N/A mpc_acceleration_limit float limit on the vehicle's acceleration 2.0 N/A mpc_velocity_time_constant float time constant used for velocity smoothing 0.3 N/A mpc_min_prediction_length float minimum prediction length 5.0 N/A"},{"location":"control/autoware_mpc_lateral_controller/#vehicle-model","title":"Vehicle Model","text":"Name Type Description Default Range vehicle_model_type string vehicle model type for mpc prediction kinematics N/A input_delay float steering input delay time for delay compensation 0.24 N/A vehicle_model_steer_tau float steering dynamics time constant (1d approximation) [s] 0.3 N/A steer_rate_lim_dps_list_by_curvature array steering angle rate limit list depending on curvature [deg/s] [40.0, 50.0, 60.0] N/A curvature_list_for_steer_rate_lim array curvature list for steering angle rate limit interpolation in ascending order [/m] [0.001, 0.002, 0.01] N/A steer_rate_lim_dps_list_by_velocity array steering angle rate limit list depending on velocity [deg/s] [60.0, 50.0, 40.0] N/A velocity_list_for_steer_rate_lim array velocity list for steering angle rate limit interpolation in ascending order [m/s] [10.0, 15.0, 20.0] N/A acceleration_limit float acceleration limit for trajectory velocity modification [m/ss] 2.0 N/A velocity_time_constant float velocity dynamics time constant for trajectory velocity modification [s] 0.3 N/A"},{"location":"control/autoware_mpc_lateral_controller/#lowpass-filter-for-noise-reduction","title":"Lowpass Filter for Noise Reduction","text":"Name Type Description Default Range steering_lpf_cutoff_hz float cutoff frequency of lowpass filter for steering output command [hz] 3 N/A error_deriv_lpf_cutoff_hz float cutoff frequency of lowpass filter for error derivative [Hz] 5 N/A"},{"location":"control/autoware_mpc_lateral_controller/#stop-state","title":"Stop State","text":"Name Type Description Default Range stop_state_entry_ego_speed float threshold value of the ego vehicle speed used to the stop state entry condition 0.001 N/A stop_state_entry_target_speed float threshold value of the target speed used to the stop state entry condition 0.001 N/A converged_steer_rad float threshold value of the steer convergence 0.1 N/A keep_steer_control_until_converged boolean keep steer control until steer is converged 1 N/A new_traj_duration_time float threshold value of the time to be considered as new trajectory 1 N/A new_traj_end_dist float threshold value of the distance between trajectory ends to be considered as new trajectory 0.3 N/A mpc_converged_threshold_rps float threshold value to be sure output of the optimization is converged, it is used in stopped state 0.01 N/A <p>(stop_state_entry_ego_speed and stop_state_entry_target_speed) To prevent unnecessary steering movement, the steering command is fixed to the previous value in the stop state.</p>"},{"location":"control/autoware_mpc_lateral_controller/#steer-offset","title":"Steer Offset","text":"<p>Defined in the <code>steering_offset</code> namespace. This logic is designed as simple as possible, with minimum design parameters.</p> Name Type Description Default Range enable_auto_steering_offset_removal boolean Estimate the steering offset and apply compensation 1 N/A update_vel_threshold float If the velocity is smaller than this value, the data is not used for the offset estimation 5.56 N/A update_steer_threshold float If the steering angle is larger than this value, the data is not used for the offset estimation 0.035 N/A average_num integer The average of this number of data is used as a steering offset 1000 N/A steering_offset_limit float The angle limit to be applied to the offset compensation 0.02 N/A"},{"location":"control/autoware_mpc_lateral_controller/#for-dynamics-model-wip","title":"For dynamics model (WIP)","text":"Name Type Description Default Range cg_to_front_m float distance from baselink to the front axle [m] 1.228 N/A cg_to_rear_m float distance from baselink to the rear axle [m] 1.5618 N/A mass_fl float mass applied to front left tire [kg] 600 N/A mass_fr float mass applied to front right tire [kg] 600 N/A mass_rl float mass applied to rear left tire [kg] 600 N/A mass_rr float mass applied to rear right tire [kg] 600 N/A cf float front cornering power [N/rad] 155495 N/A cr float rear cornering power [N/rad] 155495 N/A"},{"location":"control/autoware_mpc_lateral_controller/#publish-debug-predicted-trajectory-in-frenet-coordinate","title":"publish debug predicted trajectory in Frenet coordinate","text":"Name Type Description Default Range debug_publish_predicted_trajectory boolean publish debug predicted trajectory in Frenet coordinate False N/A"},{"location":"control/autoware_mpc_lateral_controller/#debug","title":"Debug","text":"Name Type Description Default value publish_debug_trajectories boolean publish predicted trajectory and resampled reference trajectory for debug purpose true"},{"location":"control/autoware_mpc_lateral_controller/#how-to-tune-mpc-parameters","title":"How to tune MPC parameters","text":""},{"location":"control/autoware_mpc_lateral_controller/#set-kinematics-information","title":"Set kinematics information","text":"<p>First, it's important to set the appropriate parameters for vehicle kinematics. This includes parameters like <code>wheelbase</code>, which represents the distance between the front and rear wheels, and <code>max_steering_angle</code>, which indicates the maximum tire steering angle. These parameters should be set in the <code>vehicle_info.param.yaml</code>.</p>"},{"location":"control/autoware_mpc_lateral_controller/#set-dynamics-information","title":"Set dynamics information","text":"<p>Next, you need to set the proper parameters for the dynamics model. These include the time constant <code>steering_tau</code> and time delay <code>steering_delay</code> for steering dynamics, and the maximum acceleration <code>mpc_acceleration_limit</code> and the time constant <code>mpc_velocity_time_constant</code> for velocity dynamics.</p>"},{"location":"control/autoware_mpc_lateral_controller/#confirmation-of-the-input-information","title":"Confirmation of the input information","text":"<p>It's also important to make sure the input information is accurate. Information such as the velocity of the center of the rear wheel [m/s] and the steering angle of the tire [rad] is required. Please note that there have been frequent reports of performance degradation due to errors in input information. For instance, there are cases where the velocity of the vehicle is offset due to an unexpected difference in tire radius, or the tire angle cannot be accurately measured due to a deviation in the steering gear ratio or midpoint. It is suggested to compare information from multiple sensors (e.g., integrated vehicle speed and GNSS position, steering angle and IMU angular velocity), and ensure the input information for MPC is appropriate.</p>"},{"location":"control/autoware_mpc_lateral_controller/#mpc-weight-tuning","title":"MPC weight tuning","text":"<p>Then, tune the weights of the MPC. One simple approach of tuning is to keep the weight for the lateral deviation (<code>weight_lat_error</code>) constant, and vary the input weight (<code>weight_steering_input</code>) while observing the trade-off between steering oscillation and control accuracy.</p> <p>Here, <code>weight_lat_error</code> acts to suppress the lateral error in path following, while <code>weight_steering_input</code> works to adjust the steering angle to a standard value determined by the path's curvature. When <code>weight_lat_error</code> is large, the steering moves significantly to improve accuracy, which can cause oscillations. On the other hand, when <code>weight_steering_input</code> is large, the steering doesn't respond much to tracking errors, providing stable driving but potentially reducing tracking accuracy.</p> <p>The steps are as follows:</p> <ol> <li>Set <code>weight_lat_error</code> = 0.1, <code>weight_steering_input</code> = 1.0 and other weights to 0.</li> <li>If the vehicle oscillates when driving, set <code>weight_steering_input</code> larger.</li> <li>If the tracking accuracy is low, set <code>weight_steering_input</code> smaller.</li> </ol> <p>If you want to adjust the effect only in the high-speed range, you can use <code>weight_steering_input_squared_vel</code>. This parameter corresponds to the steering weight in the high-speed range.</p>"},{"location":"control/autoware_mpc_lateral_controller/#descriptions-for-weights","title":"Descriptions for weights","text":"<ul> <li><code>weight_lat_error</code>: Reduce lateral tracking error. This acts like P gain in PID.</li> <li><code>weight_heading_error</code>: Make a drive straight. This acts like D gain in PID.</li> <li><code>weight_heading_error_squared_vel_coeff</code> : Make a drive straight in high speed range.</li> <li><code>weight_steering_input</code>: Reduce oscillation of tracking.</li> <li><code>weight_steering_input_squared_vel_coeff</code>: Reduce oscillation of tracking in high speed range.</li> <li><code>weight_lat_jerk</code>: Reduce lateral jerk.</li> <li><code>weight_terminal_lat_error</code>: Preferable to set a higher value than normal lateral weight <code>weight_lat_error</code> for stability.</li> <li><code>weight_terminal_heading_error</code>: Preferable to set a higher value than normal heading weight <code>weight_heading_error</code> for stability.</li> </ul>"},{"location":"control/autoware_mpc_lateral_controller/#other-tips-for-tuning","title":"Other tips for tuning","text":"<p>Here are some tips for adjusting other parameters:</p> <ul> <li>In theory, increasing terminal weights, <code>weight_terminal_lat_error</code> and <code>weight_terminal_heading_error</code>, can enhance the tracking stability. This method sometimes proves effective.</li> <li>A larger <code>prediction_horizon</code> and a smaller <code>prediction_sampling_time</code> are efficient for tracking performance. However, these come at the cost of higher computational costs.</li> <li>If you want to modify the weight according to the trajectory curvature (for instance, when you're driving on a sharp curve and want a larger weight), use <code>mpc_low_curvature_thresh_curvature</code> and adjust <code>mpc_low_curvature_weight_**</code> weights.</li> <li>If you want to adjust the steering rate limit based on the vehicle speed and trajectory curvature, you can modify the values of <code>steer_rate_lim_dps_list_by_curvature</code>, <code>curvature_list_for_steer_rate_lim</code>, <code>steer_rate_lim_dps_list_by_velocity</code>, <code>velocity_list_for_steer_rate_lim</code>. By doing this, you can enforce the steering rate limit during high-speed driving or relax it while curving.</li> <li>In case your target curvature appears jagged, adjusting <code>curvature_smoothing</code> becomes critically important for accurate curvature calculations. A larger value yields a smooth curvature calculation which reduces noise but can cause delay in feedforward computation and potentially degrade performance.</li> <li>Adjusting the <code>steering_lpf_cutoff_hz</code> value can also be effective to forcefully reduce computational noise. This refers to the cutoff frequency in the second order Butterworth filter installed in the final layer. The smaller the cutoff frequency, the stronger the noise reduction, but it also induce operation delay.</li> <li>If the vehicle consistently deviates laterally from the trajectory, it's most often due to the offset of the steering sensor or self-position estimation. It's preferable to eliminate these biases before inputting into MPC, but it's also possible to remove this bias within MPC. To utilize this, set <code>enable_auto_steering_offset_removal</code> to true and activate the steering offset remover. The steering offset estimation logic works when driving at high speeds with the steering close to the center, applying offset removal.</li> <li>If the onset of steering in curves is late, it's often due to incorrect delay time and time constant in the steering model. Please recheck the values of <code>input_delay</code> and <code>vehicle_model_steer_tau</code>. Additionally, as a part of its debug information, MPC outputs the current steering angle assumed by the MPC model, so please check if that steering angle matches the actual one.</li> </ul>"},{"location":"control/autoware_mpc_lateral_controller/#references-external-links","title":"References / External links","text":"<ul> <li>[1] Jarrod M. Snider, \"Automatic Steering Methods for Autonomous Automobile Path Tracking\",   Robotics Institute, Carnegie Mellon University, February 2009.</li> </ul>"},{"location":"control/autoware_mpc_lateral_controller/#related-issues","title":"Related issues","text":""},{"location":"control/autoware_mpc_lateral_controller/model_predictive_control_algorithm/","title":"MPC Algorithm","text":""},{"location":"control/autoware_mpc_lateral_controller/model_predictive_control_algorithm/#mpc-algorithm","title":"MPC Algorithm","text":""},{"location":"control/autoware_mpc_lateral_controller/model_predictive_control_algorithm/#introduction","title":"Introduction","text":"<p>Model Predictive Control (MPC) is a control method that solves an optimization problem during each control cycle to determine an optimal control sequence based on a given vehicle model. The calculated sequence of control inputs is used to control the system.</p> <p>In simpler terms, an MPC controller calculates a series of control inputs that optimize the state and output trajectories to achieve the desired behavior. The key characteristics of an MPC control system can be summarized as follows:</p> <ol> <li>Prediction of Future Trajectories: MPC computes a control sequence by predicting the future state and output trajectories. The first control input is applied to the system, and this process repeats in a receding horizon manner at each control cycle.</li> <li>Handling of Constraints: MPC is capable of handling constraints on the state and input variables during the optimization phase. This ensures that the system operates within specified limits.</li> <li>Handling of Complex Dynamics: MPC algorithms can handle complex dynamics, whether they are linear or nonlinear in nature.</li> </ol> <p>The choice between a linear or nonlinear model or constraint equation depends on the specific formulation of the MPC problem. If any nonlinear expressions are present in the motion equation or constraints, the optimization problem becomes nonlinear. In the following sections, we provide a step-by-step explanation of how linear and nonlinear optimization problems are solved within the MPC framework. Note that in this documentation, we utilize the linearization method to accommodate the nonlinear model.</p>"},{"location":"control/autoware_mpc_lateral_controller/model_predictive_control_algorithm/#linear-mpc-formulation","title":"Linear MPC formulation","text":""},{"location":"control/autoware_mpc_lateral_controller/model_predictive_control_algorithm/#formulate-as-an-optimization-problem","title":"Formulate as an optimization problem","text":"<p>This section provides an explanation of MPC specifically for linear systems. In the following section, it also demonstrates the formulation of a vehicle path following problem as an application.</p> <p>In the linear MPC formulation, all motion and constraint expressions are linear. For the path following problem, let's assume that the system's motion can be described by a set of equations, denoted as (1). The state evolution and measurements are presented in a discrete state space format, where matrices \\(A\\), \\(B\\), and \\(C\\) represent the state transition, control, and measurement matrices, respectively.</p> \\[ \\begin{gather} x_{k+1}=Ax_{k}+Bu_{k}+w_{k}, y_{k}=Cx_{k} \\tag{1} \\\\\\ x_{k}\\in R^{n},u_{k}\\in R^{m},w_{k}\\in R^{n}, y_{k}\\in R^{l}, A\\in R^{n\\times n}, B\\in R^{n\\times m}, C\\in R^{l \\times n} \\end{gather} \\] <p>Equation (1) represents the state-space equation, where \\(x_k\\) represents the internal states, \\(u_k\\) denotes the input, and \\(w_k\\) represents a known disturbance caused by linearization or problem structure. The measurements are indicated by the variable \\(y_k\\).</p> <p>It's worth noting that another advantage of MPC is its ability to effectively handle the disturbance term \\(w\\). While it is referred to as a disturbance here, it can take various forms as long as it adheres to the equation's structure.</p> <p>The state transition and measurement equations in (1) are iterative, moving from time \\(k\\) to time \\(k+1\\). By propagating the equation starting from an initial state and control pair \\((x_0, u_0)\\) along with a specified horizon of \\(N\\) steps, one can predict the trajectories of states and measurements.</p> <p>For simplicity, let's assume the initial state is \\(x_0\\) with \\(k=0\\).</p> <p>To begin, we can compute the state \\(x_1\\) at \\(k=1\\) using equation (1) by substituting the initial state into the equation. Since we are seeking a solution for the input sequence, we represent the inputs as decision variables in the symbolic expressions.</p> \\[ \\begin{align} x_{1} = Ax_{0} + Bu_{0} + w_{0} \\tag{2} \\end{align} \\] <p>Then, when \\(k=2\\), using also equation (2), we get</p> \\[ \\begin{align} x_{2} &amp; = Ax_{1} + Bu_{1} + w_{1} \\\\\\ &amp; = A(Ax_{0} + Bu_{0} + w_{0}) + Bu_{1} + w_{1} \\\\\\ &amp; = A^{2}x_{0} + ABu_{0} + Aw_{0} + Bu_{1} + w_{1} \\\\\\ &amp; = A^{2}x_{0} + \\begin{bmatrix}AB &amp; B \\end{bmatrix}\\begin{bmatrix}u_{0}\\\\\\ u_{1} \\end{bmatrix} + \\begin{bmatrix}A &amp; I \\end{bmatrix}\\begin{bmatrix}w_{0}\\\\\\ w_{1} \\end{bmatrix} \\tag{3} \\end{align} \\] <p>When \\(k=3\\) , from equation (3)</p> \\[ \\begin{align} x_{3} &amp; = Ax_{2} + Bu_{2} + w_{2} \\\\\\ &amp; = A(A^{2}x_{0} + ABu_{0} + Bu_{1} + Aw_{0} + w_{1} ) + Bu_{2} + w_{2} \\\\\\ &amp; = A^{3}x_{0} + A^{2}Bu_{0} + ABu_{1} + A^{2}w_{0} + Aw_{1} + Bu_{2} + w_{2} \\\\\\ &amp; = A^{3}x_{0} + \\begin{bmatrix}A^{2}B &amp; AB &amp; B  \\end{bmatrix}\\begin{bmatrix}u_{0}\\\\\\ u_{1} \\\\\\ u_{2} \\end{bmatrix} + \\begin{bmatrix} A^{2} &amp; A &amp; I \\end{bmatrix}\\begin{bmatrix}w_{0}\\\\\\ w_{1} \\\\\\ w_{2} \\end{bmatrix} \\tag{4} \\end{align} \\] <p>If \\(k=n\\) , then</p> \\[ \\begin{align} x_{n} = A^{n}x_{0} + \\begin{bmatrix}A^{n-1}B &amp; A^{n-2}B &amp; \\dots  &amp; B  \\end{bmatrix}\\begin{bmatrix}u_{0}\\\\\\ u_{1} \\\\\\ \\vdots  \\\\\\ u_{n-1} \\end{bmatrix} + \\begin{bmatrix} A^{n-1} &amp; A^{n-2} &amp; \\dots &amp; I \\end{bmatrix}\\begin{bmatrix}w_{0}\\\\\\ w_{1} \\\\\\ \\vdots \\\\\\ w_{n-1} \\end{bmatrix} \\tag{5} \\end{align} \\] <p>Putting all of them together with (2) to (5) yields the following matrix equation;</p> \\[ \\begin{align} \\begin{bmatrix}x_{1}\\\\\\ x_{2} \\\\\\ x_{3} \\\\\\ \\vdots  \\\\\\ x_{n} \\end{bmatrix} = \\begin{bmatrix}A^{1}\\\\\\ A^{2} \\\\\\ A^{3} \\\\\\ \\vdots  \\\\\\ A^{n} \\end{bmatrix}x_{0} + \\begin{bmatrix}B &amp; 0 &amp; \\dots  &amp; &amp; 0 \\\\\\ AB &amp; B &amp; 0 &amp; \\dots &amp; 0  \\\\\\ A^{2}B &amp; AB &amp; B &amp; \\dots &amp; 0 \\\\\\ \\vdots &amp; \\vdots &amp; &amp; &amp; 0 \\\\\\ A^{n-1}B &amp; A^{n-2}B &amp; \\dots &amp; AB &amp; B \\end{bmatrix}\\begin{bmatrix}u_{0}\\\\\\ u_{1} \\\\\\ u_{2} \\\\\\ \\vdots  \\\\\\ u_{n-1} \\end{bmatrix} \\\\\\ + \\begin{bmatrix}I &amp; 0 &amp; \\dots  &amp; &amp; 0 \\\\\\ A &amp; I &amp; 0 &amp; \\dots &amp; 0  \\\\\\ A^{2} &amp; A &amp; I &amp; \\dots &amp; 0 \\\\\\ \\vdots &amp; \\vdots &amp; &amp; &amp; 0 \\\\\\ A^{n-1} &amp; A^{n-2} &amp; \\dots &amp; A &amp; I \\end{bmatrix}\\begin{bmatrix}w_{0}\\\\\\ w_{1} \\\\\\ w_{2} \\\\\\ \\vdots  \\\\\\ w_{n-1} \\end{bmatrix} \\tag{6} \\end{align} \\] <p>In this case, the measurements (outputs) become; \\(y_{k}=Cx_{k}\\), so</p> \\[ \\begin{align} \\begin{bmatrix}y_{1}\\\\\\ y_{2} \\\\\\ y_{3} \\\\\\ \\vdots  \\\\\\ y_{n} \\end{bmatrix} = \\begin{bmatrix}C &amp; 0 &amp; \\dots  &amp; &amp; 0 \\\\\\ 0 &amp; C &amp; 0 &amp; \\dots &amp; 0  \\\\\\ 0 &amp; 0 &amp; C &amp; \\dots &amp; 0 \\\\\\ \\vdots &amp; &amp; &amp; \\ddots &amp; 0 \\\\\\ 0 &amp; \\dots &amp; 0 &amp; 0 &amp; C \\end{bmatrix}\\begin{bmatrix}x_{1}\\\\\\ x_{2} \\\\\\ x_{3} \\\\\\ \\vdots  \\\\\\ x_{n} \\end{bmatrix} \\tag{7} \\end{align} \\] <p>We can combine equations (6) and (7) into the following form:</p> \\[ \\begin{align} X = Fx_{0} + GU +SW, Y=HX \\tag{8} \\end{align} \\] <p>This form is similar to the original state-space equations (1), but it introduces new matrices: the state transition matrix \\(F\\), control matrix \\(G\\), disturbance matrix \\(W\\), and measurement matrix \\(H\\). In these equations, \\(X\\) represents the predicted states, given by \\(\\begin{bmatrix}x_{1} &amp; x_{2} &amp; \\dots &amp; x_{n} \\end{bmatrix}^{T}\\).</p> <p>Now that \\(G\\), \\(S\\), \\(W\\), and \\(H\\) are known, we can express the output behavior \\(Y\\) for the next \\(n\\) steps as a function of the input \\(U\\). This allows us to calculate the control input \\(U\\) so that \\(Y(U)\\) follows the target trajectory \\(Y_{ref}\\).</p> <p>The next step is to define a cost function. The cost function generally uses the following quadratic form;</p> \\[ \\begin{align} J = (Y - Y_{ref})^{T}Q(Y - Y_{ref}) + (U - U_{ref})^{T}R(U - U_{ref}) \\tag{9} \\end{align} \\] <p>where \\(U_{ref}\\) is the target or steady-state input around which the system is linearized for \\(U\\).</p> <p>This cost function is the same as that of the LQR controller. The first term of \\(J\\) penalizes the deviation from the reference trajectory. The second term penalizes the deviation from the reference (or steady-state) control trajectory. The \\(Q\\) and \\(R\\) are the cost weights Positive and Positive semi-semidefinite matrices.</p> <p>Note: in some cases, \\(U_{ref}=0\\) is used, but this can mean the steering angle should be set to \\(0\\) even if the vehicle is turning a curve. Thus \\(U_{ref}\\) is used for the explanation here. This \\(U_{ref}\\) can be pre-calculated from the curvature of the target trajectory or the steady-state analyses.</p> <p>As the resulting trajectory output is now \\(Y=Y(x_{0}, U)\\), the cost function depends only on U and the initial state conditions which yields the cost \\(J=J(x_{0}, U)\\). Let\u2019s find the \\(U\\) that minimizes this.</p> <p>Substituting equation (8) into equation (9) and tidying up the equation for \\(U\\).</p> \\[ \\begin{align} J(U) &amp;= (H(Fx_{0}+GU+SW)-Y_{ref})^{T}Q(H(Fx_{0}+GU+SW)-Y_{ref})+(U-U_{ref})^{T}R(U-U_{ref}) \\\\\\ &amp; =U^{T}(G^{T}H^{T}QHG+R)U+2\\lbrace\\{(H(Fx_{0}+SW)-Y_{ref})^{T}QHG-U_{ref}^{T}R\\rbrace\\}U +(\\rm{constant}) \\tag{10} \\end{align} \\] <p>This equation is a quadratic form of \\(U\\) (i.e. \\(U^{T}AU+B^{T}U\\))</p> <p>The coefficient matrix of the quadratic term of \\(U\\), \\(G^{T}C^{T}QCG+R\\) , is positive definite due to the positive and semi-positive definiteness requirement for \\(Q\\) and \\(R\\). Therefore, the cost function is a convex quadratic function in U, which can efficiently be solved by convex optimization.</p>"},{"location":"control/autoware_mpc_lateral_controller/model_predictive_control_algorithm/#apply-to-vehicle-path-following-problem-nonlinear-problem","title":"Apply to vehicle path-following problem (nonlinear problem)","text":"<p>Because the path-following problem with a kinematic vehicle model is nonlinear, we cannot directly use the linear MPC methods described in the preceding section. There are several ways to deal with a nonlinearity such as using the nonlinear optimization solver. Here, the linearization is applied to the nonlinear vehicle model along the reference trajectory, and consequently, the nonlinear model is converted into a linear time-varying model.</p> <p>For a nonlinear kinematic vehicle model, the discrete-time update equations are as follows:</p> \\[ \\begin{align} x_{k+1} &amp;= x_{k} + v\\cos\\theta_{k} \\text{d}t \\\\\\ y_{k+1} &amp;= y_{k} + v\\sin\\theta_{k} \\text{d}t \\\\\\ \\theta_{k+1} &amp;= \\theta_{k} + \\frac{v\\tan\\delta_{k}}{L} \\text{d}t \\tag{11} \\\\\\ \\delta_{k+1} &amp;= \\delta_{k} - \\tau^{-1}\\left(\\delta_{k}-\\delta_{des}\\right)\\text{d}t \\end{align} \\] <p></p> <p>The vehicle reference is the center of the rear axle and all states are measured at this point. The states, parameters, and control variables are shown in the following table.</p> Symbol Represent \\(v\\) Vehicle speed measured at the center of rear axle \\(\\theta\\) Yaw (heading angle) in global coordinate system \\(\\delta\\) Vehicle steering angle \\(\\delta_{des}\\) Vehicle target steering angle \\(L\\) Vehicle wheelbase (distance between the rear and front axles) \\(\\tau\\) Time constant for the first order steering dynamics <p>We assume in this example that the MPC only generates the steering control, and the trajectory generator gives the vehicle speed along the trajectory.</p> <p>The kinematic vehicle model discrete update equations contain trigonometric functions; sin and cos, and the vehicle coordinates \\(x\\), \\(y\\), and yaw angles are global coordinates. In path tracking applications, it is common to reformulate the model in error dynamics to convert the control into a regulator problem in which the targets become zero (zero error).</p> <p></p> <p>We make small angle assumptions for the following derivations of linear equations. Given the nonlinear dynamics and omitting the longitudinal coordinate \\(x\\), the resulting set of equations become;</p> \\[ \\begin{align} y_{k+1} &amp;= y_{k} + v\\sin\\theta_{k} \\text{d}t \\\\\\ \\theta_{k+1} &amp;= \\theta_{k} + \\frac{v\\tan\\delta_{k}}{L} \\text{d}t - \\kappa_{r}v\\cos\\theta_{k}\\text{d}t \\tag{12} \\\\\\ \\delta_{k+1} &amp;= \\delta_{k} - \\tau^{-1}\\left(\\delta_{k}-\\delta_{des}\\right)\\text{d}t \\end{align} \\] <p>Where \\(\\kappa_{r}\\left(s\\right)\\) is the curvature along the trajectory parametrized by the arc length.</p> <p>There are three expressions in the update equations that are subject to linear approximation: the lateral deviation (or lateral coordinate) \\(y\\), the heading angle (or the heading angle error) \\(\\theta\\), and the steering \\(\\delta\\). We can make a small angle assumption on the heading angle \\(\\theta\\).</p> <p>In the path tracking problem, the curvature of the trajectory \\(\\kappa_{r}\\) is known in advance. At the lower speeds, the Ackermann formula approximates the reference steering angle \\(\\theta_{r}\\)(this value corresponds to the \\(U_{ref}\\) mentioned above). The Ackermann steering expression can be written as;</p> \\[ \\begin{align} \\delta_{r} = \\arctan\\left(L\\kappa_{r}\\right) \\end{align} \\] <p>When the vehicle is turning a path, its steer angle \\(\\delta\\) should be close to the value \\(\\delta_{r}\\). Therefore, \\(\\delta\\) can be expressed,</p> \\[ \\begin{align} \\delta = \\delta_{r} + \\Delta \\delta, \\Delta\\delta \\ll 1 \\end{align} \\] <p>Substituting this equation into equation (12), and approximate \\(\\Delta\\delta\\) to be small.</p> \\[ \\begin{align} \\tan\\delta &amp;\\simeq \\tan\\delta_{r} + \\frac{\\text{d}\\tan\\delta}{\\text{d}\\delta} \\Biggm|_{\\delta=\\delta_{r}}\\Delta\\delta \\\\\\ &amp;= \\tan \\delta_{r} + \\frac{1}{\\cos^{2}\\delta_{r}}\\Delta\\delta \\\\\\ &amp;= \\tan \\delta_{r} + \\frac{1}{\\cos^{2}\\delta_{r}}\\left(\\delta-\\delta_{r}\\right) \\\\\\ &amp;= \\tan \\delta_{r} - \\frac{\\delta_{r}}{\\cos^{2}\\delta_{r}} + \\frac{1}{\\cos^{2}\\delta_{r}}\\delta \\end{align} \\] <p>Using this, \\(\\theta_{k+1}\\) can be expressed</p> \\[ \\begin{align} \\theta_{k+1} &amp;= \\theta_{k} + \\frac{v\\tan\\delta_{k}}{L}\\text{d}t - \\kappa_{r}v\\cos\\delta_{k}\\text{d}t \\\\\\ &amp;\\simeq \\theta_{k} + \\frac{v}{L}\\text{d}t\\left(\\tan\\delta_{r} - \\frac{\\delta_{r}}{\\cos^{2}\\delta_{r}} + \\frac{1}{\\cos^{2}\\delta_{r}}\\delta_{k} \\right) - \\kappa_{r}v\\text{d}t \\\\\\ &amp;= \\theta_{k} + \\frac{v}{L}\\text{d}t\\left(L\\kappa_{r} - \\frac{\\delta_{r}}{\\cos^{2}\\delta_{r}} + \\frac{1}{\\cos^{2}\\delta_{r}}\\delta_{k} \\right) - \\kappa_{r}v\\text{d}t \\\\\\ &amp;= \\theta_{k} + \\frac{v}{L}\\frac{\\text{d}t}{\\cos^{2}\\delta_{r}}\\delta_{k} - \\frac{v}{L}\\frac{\\delta_{r}\\text{d}t}{\\cos^{2}\\delta_{r}} \\end{align} \\] <p>Finally, the linearized time-varying model equation becomes;</p> \\[ \\begin{align} \\begin{bmatrix} y_{k+1} \\\\\\ \\theta_{k+1} \\\\\\ \\delta_{k+1} \\end{bmatrix} = \\begin{bmatrix} 1 &amp; v\\text{d}t &amp; 0 \\\\\\ 0 &amp; 1 &amp; \\frac{v}{L}\\frac{\\text{d}t}{\\cos^{2}\\delta_{r}} \\\\\\ 0 &amp; 0 &amp; 1 - \\tau^{-1}\\text{d}t \\end{bmatrix} \\begin{bmatrix} y_{k} \\\\\\ \\theta_{k} \\\\\\ \\delta_{k} \\end{bmatrix} + \\begin{bmatrix} 0 \\\\\\ 0 \\\\\\ \\tau^{-1}\\text{d}t \\end{bmatrix}\\delta_{des} + \\begin{bmatrix} 0 \\\\\\ -\\frac{v}{L}\\frac{\\delta_{r}\\text{d}t}{\\cos^{2}\\delta_{r}} \\\\\\ 0 \\end{bmatrix} \\end{align} \\] <p>This equation has the same form as equation (1) of the linear MPC assumption, but the matrices \\(A\\), \\(B\\), and \\(w\\) change depending on the coordinate transformation. To make this explicit, the entire equation is written as follows</p> \\[ \\begin{align} x_{k+1} = A_{k}x_{k} + B_{k}u_{k}+w_{k} \\end{align} \\] <p>Comparing equation (1), \\(A \\rightarrow A_{k}\\). This means that the \\(A\\) matrix is a linear approximation in the vicinity of the trajectory after \\(k\\) steps (i.e., \\(k* \\text{d}t\\) seconds), and it can be obtained if the trajectory is known in advance.</p> <p>Using this equation, write down the update equation likewise (2) ~ (6)</p> \\[ \\begin{align} \\begin{bmatrix}  x_{1} \\\\\\ x_{2} \\\\\\ x_{3} \\\\\\ \\vdots \\\\\\ x_{n} \\end{bmatrix} = \\begin{bmatrix}  A_{1} \\\\\\ A_{1}A_{0} \\\\\\ A_{2}A_{1}A_{0} \\\\\\ \\vdots \\\\\\ \\prod_{i=0}^{n-1} A_{k} \\end{bmatrix} x_{0} + \\begin{bmatrix}  B_{0} &amp; 0 &amp; \\dots &amp; &amp; 0 \\\\\\ A_{1}B_{0} &amp; B_{1} &amp; 0 &amp; \\dots &amp; 0 \\\\\\ A_{2}A_{1}B_{0} &amp; A_{2}B_{1} &amp; B_{2} &amp; \\dots &amp; 0 \\\\\\ \\vdots &amp; \\vdots &amp; &amp;\\ddots &amp; 0 \\\\\\ \\prod_{i=1}^{n-1} A_{k}B_{0} &amp; \\prod_{i=2}^{n-1} A_{k}B_{1} &amp; \\dots &amp; A_{n-1}B_{n-1} &amp; B_{n-1} \\end{bmatrix} \\begin{bmatrix}  u_{0} \\\\\\ u_{1} \\\\\\ u_{2} \\\\\\ \\vdots \\\\\\ u_{n-1} \\end{bmatrix} + \\begin{bmatrix} I &amp; 0 &amp; \\dots &amp; &amp; 0 \\\\\\ A_{1} &amp; I &amp; 0 &amp; \\dots &amp; 0 \\\\\\ A_{2}A_{1} &amp; A_{2} &amp; I &amp; \\dots &amp; 0 \\\\\\ \\vdots &amp; \\vdots &amp; &amp;\\ddots &amp; 0 \\\\\\ \\prod_{i=1}^{n-1} A_{k} &amp; \\prod_{i=2}^{n-1} A_{k} &amp; \\dots &amp; A_{n-1} &amp; I \\end{bmatrix} \\begin{bmatrix}  w_{0} \\\\\\ w_{1} \\\\\\ w_{2} \\\\\\ \\vdots \\\\\\ w_{n-1} \\end{bmatrix} \\end{align} \\] <p>As it has the same form as equation (6), convex optimization is applicable for as much as the model in the former section.</p>"},{"location":"control/autoware_mpc_lateral_controller/model_predictive_control_algorithm/#the-cost-functions-and-constraints","title":"The cost functions and constraints","text":"<p>In this section, we give the details on how to set up the cost function and constraint conditions.</p>"},{"location":"control/autoware_mpc_lateral_controller/model_predictive_control_algorithm/#the-cost-function","title":"The cost function","text":""},{"location":"control/autoware_mpc_lateral_controller/model_predictive_control_algorithm/#weight-for-error-and-input","title":"Weight for error and input","text":"<p>MPC states and control weights appear in the cost function in a similar way as LQR (9). In the vehicle path following the problem described above, if C is the unit matrix, the output \\(y = x = \\left[y, \\theta, \\delta\\right]\\). (To avoid confusion with the y-directional deviation, here \\(e\\) is used for the lateral deviation.)</p> <p>As an example, let's determine the weight matrix \\(Q_{1}\\) of the evaluation function for the number of prediction steps \\(n=2\\) system as follows.</p> \\[ \\begin{align} Q_{1} = \\begin{bmatrix} q_{e} &amp; 0 &amp; 0 &amp; 0 &amp; 0&amp; 0 \\\\\\ 0 &amp; q_{\\theta} &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\\\ 0 &amp; 0 &amp; 0 &amp; q_{e} &amp; 0 &amp; 0 \\\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; q_{\\theta} &amp; 0 \\\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix} \\end{align} \\] <p>The first term in the cost function (9) with \\(n=2\\), is shown as follow (\\(Y_{ref}\\) is set to \\(0\\))</p> \\[ \\begin{align} q_{e}\\left(e_{0}^{2} + e_{1}^{2} \\right) + q_{\\theta}\\left(\\theta_{0}^{2} + \\theta_{1}^{2} \\right) \\end{align} \\] <p>This shows that \\(q_{e}\\) is the weight for the lateral error and \\(q\\) is for the angular error. In this example, \\(q_{e}\\) acts as the proportional - P gain and \\(q_{\\theta}\\) as the derivative - D gain for the lateral tracking error. The balance of these factors (including R) will be determined through actual experiments.</p>"},{"location":"control/autoware_mpc_lateral_controller/model_predictive_control_algorithm/#weight-for-non-diagonal-term","title":"Weight for non-diagonal term","text":"<p>MPC can handle the non-diagonal term in its calculation (as long as the resulting matrix is positive definite).</p> <p>For instance, write \\(Q_{2}\\) as follows for the \\(n=2\\) system.</p> \\[ \\begin{align} Q_{2} = \\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\\\ 0 &amp; 0 &amp; q_{d} &amp; 0 &amp; 0 &amp; -q_{d} \\\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\\\ 0 &amp; 0 &amp; -q_{d} &amp; 0 &amp; 0 &amp; q_{d} \\end{bmatrix} \\end{align} \\] <p>Expanding the first term of the evaluation function using \\(Q_{2}\\)</p> \\[ \\begin{align} q_{d}\\left(\\delta_{0}^{2} -2\\delta_{0}\\delta_{1} + \\delta_{1}^{2} \\right) = q_{d}\\left( \\delta_{0} - \\delta_{1}\\right)^{2} \\end{align} \\] <p>The value of \\(q_{d}\\) is weighted by the amount of change in \\(\\delta\\), which will prevent the tire from moving quickly. By adding this section, the system can evaluate the balance between tracking accuracy and change of steering wheel angle.</p> <p>Since the weight matrix can be added linearly, the final weight can be set as \\(Q = Q_{1} + Q_{2}\\).</p> <p>Furthermore, MPC optimizes over a period of time, the time-varying weight can be considered in the optimization.</p>"},{"location":"control/autoware_mpc_lateral_controller/model_predictive_control_algorithm/#constraints","title":"Constraints","text":""},{"location":"control/autoware_mpc_lateral_controller/model_predictive_control_algorithm/#input-constraint","title":"Input constraint","text":"<p>The main advantage of MPC controllers is the capability to deal with any state or input constraints. The constraints can be expressed as box constraints, such as \"the tire angle must be within \u00b130 degrees\", and can be put in the following form;</p> \\[ \\begin{align} u_{min} &lt; u &lt; u_{max} \\end{align} \\] <p>The constraints must be linear and convex in the linear MPC applications.</p>"},{"location":"control/autoware_mpc_lateral_controller/model_predictive_control_algorithm/#constraints-on-the-derivative-of-the-input","title":"Constraints on the derivative of the input","text":"<p>We can also put constraints on the input deviations. As the derivative of steering angle is \\(\\dot{u}\\), its box constraint is</p> \\[ \\begin{align} \\dot u_{min} &lt; \\dot u &lt; \\dot u_{max} \\end{align} \\] <p>We discretize \\(\\dot{u}\\) as \\(\\left(u_{k} - u_{k-1}\\right)/\\text{d}t\\) and multiply both sides by dt, and the resulting constraint become linear and convex</p> \\[ \\begin{align} \\dot u_{min}\\text{d}t &lt; u_{k} - u_{k-1} &lt; \\dot u_{max}\\text{d}t \\end{align} \\] <p>Along the prediction or control horizon, i.e for setting \\(n=3\\)</p> \\[ \\begin{align} \\dot u_{min}\\text{d}t &lt; u_{1} - u_{0} &lt; \\dot u_{max}\\text{d}t \\\\\\ \\dot u_{min}\\text{d}t &lt; u_{2} - u_{1} &lt; \\dot u_{max}\\text{d}t \\end{align} \\] <p>and aligning the inequality signs</p> \\[ \\begin{align} u_{1} - u_{0} &amp;&lt; \\dot u_{max}\\text{d}t \\\\\\ - u_{1} + u_{0} &amp;&lt; -\\dot u_{min}\\text{d}t \\\\\\ u_{2} - u_{1} &amp;&lt; \\dot u_{max}\\text{d}t \\\\\\ - u_{2} + u_{1} &amp;&lt; - \\dot u_{min}\\text{d}t \\end{align} \\] <p>We can obtain a matrix expression for the resulting constraint equation in the form of</p> \\[ \\begin{align} Ax \\leq b \\end{align} \\] <p>Thus, putting this inequality to fit the form above, the constraints against \\(\\dot{u}\\) can be included at the first-order approximation level.</p> \\[ \\begin{align} \\begin{bmatrix} -1 &amp; 1 &amp; 0 \\\\\\ 1 &amp; -1 &amp; 0 \\\\\\ 0 &amp; -1 &amp; 1 \\\\\\ 0 &amp; 1 &amp; -1 \\end{bmatrix}\\begin{bmatrix} u_{0} \\\\\\ u_{1} \\\\\\ u_{2} \\end{bmatrix} \\leq \\begin{bmatrix} \\dot u_{max}\\text{d}t \\\\\\ -\\dot u_{min}\\text{d}t \\\\\\ \\dot u_{max}\\text{d}t \\\\\\ -\\dot u_{min}\\text{d}t \\end{bmatrix} \\end{align} \\]"},{"location":"control/autoware_obstacle_collision_checker/","title":"obstacle_collision_checker","text":""},{"location":"control/autoware_obstacle_collision_checker/#obstacle_collision_checker","title":"obstacle_collision_checker","text":""},{"location":"control/autoware_obstacle_collision_checker/#purpose","title":"Purpose","text":"<p><code>obstacle_collision_checker</code> is a module to check obstacle collision for predicted trajectory and publish diagnostic errors if collision is found.</p>"},{"location":"control/autoware_obstacle_collision_checker/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"control/autoware_obstacle_collision_checker/#flow-chart","title":"Flow chart","text":""},{"location":"control/autoware_obstacle_collision_checker/#algorithms","title":"Algorithms","text":""},{"location":"control/autoware_obstacle_collision_checker/#check-data","title":"Check data","text":"<p>Check that <code>obstacle_collision_checker</code> receives no ground pointcloud, predicted_trajectory, reference trajectory, and current velocity data.</p>"},{"location":"control/autoware_obstacle_collision_checker/#diagnostic-update","title":"Diagnostic update","text":"<p>If any collision is found on predicted path, this module sets <code>ERROR</code> level as diagnostic status else sets <code>OK</code>.</p>"},{"location":"control/autoware_obstacle_collision_checker/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"control/autoware_obstacle_collision_checker/#input","title":"Input","text":"Name Type Description <code>~/input/trajectory</code> <code>autoware_planning_msgs::msg::Trajectory</code> Reference trajectory <code>~/input/trajectory</code> <code>autoware_planning_msgs::msg::Trajectory</code> Predicted trajectory <code>/perception/obstacle_segmentation/pointcloud</code> <code>sensor_msgs::msg::PointCloud2</code> Pointcloud of obstacles which the ego-vehicle should stop or avoid <code>/tf</code> <code>tf2_msgs::msg::TFMessage</code> TF <code>/tf_static</code> <code>tf2_msgs::msg::TFMessage</code> TF static"},{"location":"control/autoware_obstacle_collision_checker/#output","title":"Output","text":"Name Type Description <code>~/debug/marker</code> <code>visualization_msgs::msg::MarkerArray</code> Marker for visualization"},{"location":"control/autoware_obstacle_collision_checker/#parameters","title":"Parameters","text":"Name Type Description Default value <code>delay_time</code> <code>double</code> Delay time of vehicle [s] 0.3 <code>footprint_margin</code> <code>double</code> Foot print margin [m] 0.0 <code>max_deceleration</code> <code>double</code> Max deceleration for ego vehicle to stop [m/s^2] 2.0 <code>resample_interval</code> <code>double</code> Interval for resampling trajectory [m] 0.3 <code>search_radius</code> <code>double</code> Search distance from trajectory to point cloud [m] 5.0"},{"location":"control/autoware_obstacle_collision_checker/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>To perform proper collision check, it is necessary to get probably predicted trajectory and obstacle pointclouds without noise.</p>"},{"location":"control/autoware_operation_mode_transition_manager/","title":"autoware_operation_mode_transition_manager","text":""},{"location":"control/autoware_operation_mode_transition_manager/#autoware_operation_mode_transition_manager","title":"autoware_operation_mode_transition_manager","text":""},{"location":"control/autoware_operation_mode_transition_manager/#note","title":"Note","text":"<p>The state management functionality of this package is currently being moved to the command_mode_decider package. When used with the command_mode_decider package, this package is only responsible for determining transition conditions. Also, the data <code>status</code>, <code>in_autoware_control</code>, and <code>in_transition</code> contained in the debug topic is not available, so use the <code>/system/command_mode_decider/debug</code> topic instead.</p> <ul> <li>in_autoware_control: True when the current command mode is manual mode (default value 1000).</li> <li>in_transition: See the is_in_transition function in the command_mode_decider package.</li> <li>status:<ul> <li>If in_autoware_control, <code>DISENGAGE (autoware mode = curr_mode)</code></li> <li>Else if in_transition, <code>curr_mode (in transition from prev_mode)</code></li> <li>Else <code>curr_mode</code></li> <li>Note: <code>curr_mode = current operation mode</code> and <code>prev_mode = last operation mode</code></li> </ul> </li> </ul>"},{"location":"control/autoware_operation_mode_transition_manager/#purpose-use-cases","title":"Purpose / Use cases","text":"<p>This module is responsible for managing the different modes of operation for the Autoware system. The possible modes are:</p> <ul> <li><code>Autonomous</code>: the vehicle is fully controlled by the autonomous driving system</li> <li><code>Local</code>: the vehicle is controlled by a physically connected control system such as a joy stick</li> <li><code>Remote</code>: the vehicle is controlled by a remote controller</li> <li><code>Stop</code>: the vehicle is stopped and there is no active control system.</li> </ul> <p>There is also an <code>In Transition</code> state that occurs during each mode transitions. During this state, the transition to the new operator is not yet complete, and the previous operator is still responsible for controlling the system until the transition is complete. Some actions may be restricted during the <code>In Transition</code> state, such as sudden braking or steering. (This is restricted by the <code>vehicle_cmd_gate</code>).</p>"},{"location":"control/autoware_operation_mode_transition_manager/#features","title":"Features","text":"<ul> <li>Transit mode between <code>Autonomous</code>, <code>Local</code>, <code>Remote</code> and <code>Stop</code> based on the indication command.</li> <li>Check whether the each transition is available (safe or not).</li> <li>Limit some sudden motion control in <code>In Transition</code> mode (this is done with <code>vehicle_cmd_gate</code> feature).</li> <li>Check whether the transition is completed.</li> </ul> <ul> <li>Transition between the <code>Autonomous</code>, <code>Local</code>, <code>Remote</code>, and <code>Stop</code> modes based on the indicated command.</li> <li>Determine whether each transition is safe to execute.</li> <li>Restrict certain sudden motion controls during the <code>In Transition</code> mode (using the <code>vehicle_cmd_gate</code> feature).</li> <li>Verify that the transition is complete.</li> </ul>"},{"location":"control/autoware_operation_mode_transition_manager/#design","title":"Design","text":"<p>A rough design of the relationship between `autoware_operation_mode_transition_manager`` and the other nodes is shown below.</p> <p></p> <p>A more detailed structure is below.</p> <p></p> <p>Here we see that <code>autoware_operation_mode_transition_manager</code> has multiple state transitions as follows</p> <ul> <li>AUTOWARE ENABLED &lt;---&gt; DISABLED<ul> <li>ENABLED: the vehicle is controlled by Autoware.</li> <li>DISABLED: the vehicle is out of Autoware control, expecting the e.g. manual driving.</li> </ul> </li> <li>AUTOWARE ENABLED &lt;---&gt; AUTO/LOCAL/REMOTE/NONE<ul> <li>AUTO: the vehicle is controlled by Autoware, with the autonomous control command calculated by the planning/control component.</li> <li>LOCAL: the vehicle is controlled by Autoware, with the locally connected operator, e.g. joystick controller.</li> <li>REMOTE: the vehicle is controlled by Autoware, with the remotely connected operator.</li> <li>NONE: the vehicle is not controlled by any operator.</li> </ul> </li> <li>IN TRANSITION &lt;---&gt; COMPLETED<ul> <li>IN TRANSITION: the mode listed above is in the transition process, expecting the former operator to have a responsibility to confirm the transition is completed.</li> <li>COMPLETED: the mode transition is completed.</li> </ul> </li> </ul>"},{"location":"control/autoware_operation_mode_transition_manager/#inputs-outputs-api","title":"Inputs / Outputs / API","text":""},{"location":"control/autoware_operation_mode_transition_manager/#inputs","title":"Inputs","text":"<p>For the mode transition:</p> <ul> <li>/system/operation_mode/change_autoware_control [<code>autoware_system_msgs/srv/ChangeAutowareControl</code>]: change operation mode to Autonomous</li> <li>/system/operation_mode/change_operation_mode [<code>autoware_system_msgs/srv/ChangeOperationMode</code>]: change operation mode</li> </ul> <p>For the transition availability/completion check:</p> <ul> <li>/control/command/control_cmd [<code>autoware_control_msgs/msg/Control</code>]: vehicle control signal</li> <li>/localization/kinematic_state [<code>nav_msgs/msg/Odometry</code>]: ego vehicle state</li> <li>/planning/trajectory [<code>autoware_planning_msgs/msg/Trajectory</code>]: planning trajectory</li> <li>/vehicle/status/control_mode [<code>autoware_vehicle_msgs/msg/ControlModeReport</code>]: vehicle control mode (autonomous/manual)</li> <li>/control/vehicle_cmd_gate/operation_mode [<code>autoware_adapi_v1_msgs/msg/OperationModeState</code>]: the operation mode in the <code>vehicle_cmd_gate</code>. (To be removed)</li> </ul> <p>For the backward compatibility (to be removed):</p> <ul> <li>/api/autoware/get/engage [<code>autoware_vehicle_msgs/msg/Engage</code>]</li> <li>/control/current_gate_mode [<code>tier4_control_msgs/msg/GateMode</code>]</li> <li>/control/external_cmd_selector/current_selector_mode [<code>tier4_control_msgs/msg/ExternalCommandSelectorMode</code>]</li> </ul>"},{"location":"control/autoware_operation_mode_transition_manager/#outputs","title":"Outputs","text":"<ul> <li>/system/operation_mode/state [<code>autoware_adapi_v1_msgs/msg/OperationModeState</code>]: to inform the current operation mode</li> <li>/control/autoware_operation_mode_transition_manager/debug_info [<code>autoware_operation_mode_transition_manager/msg/OperationModeTransitionManagerDebug</code>]: detailed information about the operation mode transition</li> </ul> <ul> <li>/control/gate_mode_cmd [<code>tier4_control_msgs/msg/GateMode</code>]: to change the <code>vehicle_cmd_gate</code> state to use its features (to be removed)</li> <li>/autoware/engage [<code>autoware_vehicle_msgs/msg/Engage</code>]:</li> </ul> <ul> <li>/control/control_mode_request [<code>autoware_vehicle_msgs/srv/ControlModeCommand</code>]: to change the vehicle control mode (autonomous/manual)</li> <li>/control/external_cmd_selector/select_external_command [<code>tier4_control_msgs/srv/ExternalCommandSelect</code>]:</li> </ul>"},{"location":"control/autoware_operation_mode_transition_manager/#parameters","title":"Parameters","text":"Name Type Description Default Range transition_timeout float If the state transition is not completed within this time, it is considered a transition failure. 10.0 \u22650.0 frequency_hz float running hz 10.0 \u22650.0 enable_engage_on_driving boolean Set true if you want to engage the autonomous driving mode while the vehicle is driving. If set to false, it will deny Engage in any situation where the vehicle speed is not zero. Note that if you use this feature without adjusting the parameters, it may cause issues like sudden deceleration. Before using, please ensure the engage condition and the vehicle_cmd_gate transition filter are appropriately adjusted. false N/A check_engage_condition boolean If false, autonomous transition is always available. false N/A nearest_dist_deviation_threshold float distance threshold used to find nearest trajectory point [m] 3.0 \u22650.0 nearest_yaw_deviation_threshold float angle threshold used to find nearest trajectory point [rad] 1.57 \u2265-3.142 engage_acceptable_limits.allow_autonomous_in_stopped boolean If true, autonomous transition is available when the vehicle is stopped even if other checks fail. true N/A engage_acceptable_limits.dist_threshold float The distance between the trajectory and ego vehicle must be within this distance for Autonomous transition. 1.5 \u22650.0 engage_acceptable_limits.yaw_threshold float The yaw angle between trajectory and ego vehicle must be within this threshold for Autonomous transition. 0.524 \u2265-3.142 engage_acceptable_limits.speed_upper_threshold float The velocity deviation between control command and ego vehicle must be within this threshold for Autonomous transition. 10.0 N/A engage_acceptable_limits.speed_lower_threshold float The velocity deviation between control command and ego vehicle must be within this threshold for Autonomous transition. -10.0 N/A engage_acceptable_limits.acc_threshold float The control command acceleration must be less than this threshold for Autonomous transition. 1.5 \u22650.0 engage_acceptable_limits.lateral_acc_threshold float The control command lateral acceleration must be less than this threshold for Autonomous transition. 1.0 \u22650.0 engage_acceptable_limits.lateral_acc_diff_threshold float The lateral acceleration deviation between the control command must be less than this threshold for Autonomous transition. 0.5 \u22650.0 stable_check.duration float The stable condition must be satisfied for this duration to complete the transition. 0.1 \u22650.0 stable_check.dist_threshold float The distance between the trajectory and ego vehicle must be within this distance to complete Autonomous transition. 1.5 \u22650.0 stable_check.speed_upper_threshold float The velocity deviation between control command and ego vehicle must be within this threshold to complete Autonomous transition. 2.0 N/A stable_check.speed_lower_threshold float The velocity deviation between control command and ego vehicle must be within this threshold to complete Autonomous transition. -2.0 N/A stable_check.yaw_threshold float The yaw angle between trajectory and ego vehicle must be within this threshold to complete Autonomous transition. 0,262 \u2265-3.142 Name Type Description Default value <code>transition_timeout</code> <code>double</code> If the state transition is not completed within this time, it is considered a transition failure. 10.0 <code>frequency_hz</code> <code>double</code> running hz 10.0 <code>enable_engage_on_driving</code> <code>bool</code> Set true if you want to engage the autonomous driving mode while the vehicle is driving. If set to false, it will deny Engage in any situation where the vehicle speed is not zero. Note that if you use this feature without adjusting the parameters, it may cause issues like sudden deceleration. Before using, please ensure the engage condition and the vehicle_cmd_gate transition filter are appropriately adjusted. 0.1 <code>check_engage_condition</code> <code>bool</code> If false, autonomous transition is always available 0.1 <code>nearest_dist_deviation_threshold</code> <code>double</code> distance threshold used to find nearest trajectory point 3.0 <code>nearest_yaw_deviation_threshold</code> <code>double</code> angle threshold used to find nearest trajectory point 1.57 <p>For <code>engage_acceptable_limits</code> related parameters:</p> Name Type Description Default value <code>allow_autonomous_in_stopped</code> <code>bool</code> If true, autonomous transition is available when the vehicle is stopped even if other checks fail. true <code>dist_threshold</code> <code>double</code> the distance between the trajectory and ego vehicle must be within this distance for <code>Autonomous</code> transition. 1.5 <code>yaw_threshold</code> <code>double</code> the yaw angle between trajectory and ego vehicle must be within this threshold for <code>Autonomous</code> transition. 0.524 <code>speed_upper_threshold</code> <code>double</code> the velocity deviation between control command and ego vehicle must be within this threshold for <code>Autonomous</code> transition. 10.0 <code>speed_lower_threshold</code> <code>double</code> the velocity deviation between the control command and ego vehicle must be within this threshold for <code>Autonomous</code> transition. -10.0 <code>acc_threshold</code> <code>double</code> the control command acceleration must be less than this threshold for <code>Autonomous</code> transition. 1.5 <code>lateral_acc_threshold</code> <code>double</code> the control command lateral acceleration must be less than this threshold for <code>Autonomous</code> transition. 1.0 <code>lateral_acc_diff_threshold</code> <code>double</code> the lateral acceleration deviation between the control command must be less than this threshold for <code>Autonomous</code> transition. 0.5 <p>For <code>stable_check</code> related parameters:</p> Name Type Description Default value <code>duration</code> <code>double</code> the stable condition must be satisfied for this duration to complete the transition. 0.1 <code>dist_threshold</code> <code>double</code> the distance between the trajectory and ego vehicle must be within this distance to complete <code>Autonomous</code> transition. 1.5 <code>yaw_threshold</code> <code>double</code> the yaw angle between trajectory and ego vehicle must be within this threshold to complete <code>Autonomous</code> transition. 0.262 <code>speed_upper_threshold</code> <code>double</code> the velocity deviation between control command and ego vehicle must be within this threshold to complete <code>Autonomous</code> transition. 2.0 <code>speed_lower_threshold</code> <code>double</code> the velocity deviation between control command and ego vehicle must be within this threshold to complete <code>Autonomous</code> transition. 2.0"},{"location":"control/autoware_operation_mode_transition_manager/#engage-check-behavior-on-each-parameter-setting","title":"Engage check behavior on each parameter setting","text":"<p>This matrix describes the scenarios in which the vehicle can be engaged based on the combinations of parameter settings:</p> <code>enable_engage_on_driving</code> <code>check_engage_condition</code> <code>allow_autonomous_in_stopped</code> Scenarios where engage is permitted x x x Only when the vehicle is stationary. x x o Only when the vehicle is stationary. x o x When the vehicle is stationary and all engage conditions are met. x o o Only when the vehicle is stationary. o x x At any time (Caution: Not recommended). o x o At any time (Caution: Not recommended). o o x When all engage conditions are met, regardless of vehicle status. o o o When all engage conditions are met or the vehicle is stationary."},{"location":"control/autoware_operation_mode_transition_manager/#future-extensions-unimplemented-parts","title":"Future extensions / Unimplemented parts","text":"<ul> <li>Need to remove backward compatibility interfaces.</li> <li>This node should be merged to the <code>vehicle_cmd_gate</code> due to its strong connection.</li> </ul>"},{"location":"control/autoware_pid_longitudinal_controller/","title":"PID Longitudinal Controller","text":""},{"location":"control/autoware_pid_longitudinal_controller/#pid-longitudinal-controller","title":"PID Longitudinal Controller","text":""},{"location":"control/autoware_pid_longitudinal_controller/#purpose-use-cases","title":"Purpose / Use cases","text":"<p>The longitudinal_controller computes the target acceleration to achieve the target velocity set at each point of the target trajectory using a feed-forward/back control.</p> <p>It also contains a slope force correction that takes into account road slope information, and a delay compensation function. It is assumed that the target acceleration calculated here will be properly realized by the vehicle interface.</p> <p>Note that the use of this module is not mandatory for Autoware if the vehicle supports the \"target speed\" interface.</p>"},{"location":"control/autoware_pid_longitudinal_controller/#design-inner-workings-algorithms","title":"Design / Inner-workings / Algorithms","text":""},{"location":"control/autoware_pid_longitudinal_controller/#states","title":"States","text":"<p>This module has four state transitions as shown below in order to handle special processing in a specific situation.</p> <ul> <li>DRIVE<ul> <li>Executes target velocity tracking by PID control.</li> <li>It also applies the delay compensation and slope compensation.</li> </ul> </li> <li>STOPPING<ul> <li>Controls the motion just before stopping.</li> <li>Special sequence is performed to achieve accurate and smooth stopping.</li> </ul> </li> <li>STOPPED<ul> <li>Performs operations in the stopped state (e.g. brake hold)</li> </ul> </li> <li>EMERGENCY.<ul> <li>Enters an emergency state when certain conditions are met (e.g., when the vehicle has crossed a certain distance of a stop line).</li> <li>The recovery condition (whether or not to keep emergency state until the vehicle completely stops) or the deceleration in the emergency state are defined by parameters.</li> </ul> </li> </ul> <p>The state transition diagram is shown below.</p> <p></p>"},{"location":"control/autoware_pid_longitudinal_controller/#logics","title":"Logics","text":""},{"location":"control/autoware_pid_longitudinal_controller/#control-block-diagram","title":"Control Block Diagram","text":""},{"location":"control/autoware_pid_longitudinal_controller/#feedforward-ff","title":"FeedForward (FF)","text":"<p>The reference acceleration set in the trajectory and slope compensation terms are output as a feedforward. Under ideal conditions with no modeling error, this FF term alone should be sufficient for velocity tracking.</p> <p>Tracking errors causing modeling or discretization errors are removed by the feedback control (now using PID).</p>"},{"location":"control/autoware_pid_longitudinal_controller/#brake-keeping","title":"Brake keeping","text":"<p>From the viewpoint of ride comfort, stopping with 0 acceleration is important because it reduces the impact of braking. However, if the target acceleration when stopping is 0, the vehicle may cross over the stop line or accelerate a little in front of the stop line due to vehicle model error or gradient estimation error.</p> <p>For reliable stopping, the target acceleration calculated by the FeedForward system is limited to a negative acceleration when stopping.</p> <p></p>"},{"location":"control/autoware_pid_longitudinal_controller/#slope-compensation","title":"Slope compensation","text":"<p>Based on the slope information, a compensation term is added to the target acceleration.</p> <p>There are two sources of the slope information, which can be switched by a parameter.</p> <ul> <li>Pitch of the estimated ego-pose (default)<ul> <li>Calculates the current slope from the pitch angle of the estimated ego-pose</li> <li>Pros: Easily available</li> <li>Cons: Cannot extract accurate slope information due to the influence of vehicle vibration.</li> </ul> </li> <li>Z coordinate on the trajectory<ul> <li>Calculates the road slope from the difference of z-coordinates between the front and rear wheel positions in the target trajectory</li> <li>Pros: More accurate than pitch information, if the z-coordinates of the route are properly maintained</li> <li>Pros: Can be used in combination with delay compensation (not yet implemented)</li> <li>Cons: z-coordinates of high-precision map is needed.</li> <li>Cons: Does not support free space planning (for now)</li> </ul> </li> </ul> <p>We also offer the options to switch between these, depending on driving conditions.</p> <p>Notation: This function works correctly only in a vehicle system that does not have acceleration feedback in the low-level control system.</p> <p>This compensation adds gravity correction to the target acceleration, resulting in an output value that is no longer equal to the target acceleration that the autonomous driving system desires. Therefore, it conflicts with the role of the acceleration feedback in the low-level controller. For instance, if the vehicle is attempting to start with an acceleration of <code>1.0 m/s^2</code> and a gravity correction of <code>-1.0 m/s^2</code> is applied, the output value will be <code>0</code>. If this output value is mistakenly treated as the target acceleration, the vehicle will not start.</p> <p>A suitable example of a vehicle system for the slope compensation function is one in which the output acceleration from the longitudinal_controller is converted into target accel/brake pedal input without any feedbacks. In this case, the output acceleration is just used as a feedforward term to calculate the target pedal, and hence the issue mentioned above does not arise.</p> <p>Note: The angle of the slope is defined as positive for an uphill slope, while the pitch angle of the ego pose is defined as negative when facing upward. They have an opposite definition.</p> <p></p>"},{"location":"control/autoware_pid_longitudinal_controller/#pid-control","title":"PID control","text":"<p>For deviations that cannot be handled by FeedForward control, such as model errors, PID control is used to construct a feedback system.</p> <p>This PID control calculates the target acceleration from the deviation between the current ego-velocity and the target velocity.</p> <p>This PID logic has a maximum value for the output of each term. This is to prevent the following:</p> <ul> <li>Large integral terms may cause unintended behavior by users.</li> <li>Unintended noise may cause the output of the derivative term to be very large.</li> </ul> <p>Note: by default, the integral term in the control system is not accumulated when the vehicle is stationary. This precautionary measure aims to prevent unintended accumulation of the integral term in scenarios where Autoware assumes the vehicle is engaged, but an external system has immobilized the vehicle to initiate startup procedures.</p> <p>However, certain situations may arise, such as when the vehicle encounters a depression in the road surface during startup or if the slope compensation is inaccurately estimated (lower than necessary), leading to a failure to initiate motion. To address these scenarios, it is possible to activate error integration even when the vehicle is at rest by setting the <code>enable_integration_at_low_speed</code> parameter to true.</p> <p>When <code>enable_integration_at_low_speed</code> is set to true, the PID controller will initiate integration of the acceleration error after a specified duration defined by the <code>time_threshold_before_pid_integration</code> parameter has elapsed without the vehicle surpassing a minimum velocity set by the <code>current_vel_threshold_pid_integration</code> parameter.</p> <p>The presence of the <code>time_threshold_before_pid_integration</code> parameter is important for practical PID tuning. Integrating the error when the vehicle is stationary or at low speed can complicate PID tuning. This parameter effectively introduces a delay before the integral part becomes active, preventing it from kicking in immediately. This delay allows for more controlled and effective tuning of the PID controller.</p> <p>At present, PID control is implemented from the viewpoint of trade-off between development/maintenance cost and performance. This may be replaced by a higher performance controller (adaptive control or robust control) in future development.</p>"},{"location":"control/autoware_pid_longitudinal_controller/#time-delay-compensation","title":"Time delay compensation","text":"<p>At high speeds, the delay of actuator systems such as gas pedals and brakes has a significant impact on driving accuracy. Depending on the actuating principle of the vehicle, the mechanism that physically controls the gas pedal and brake typically has a delay of about a hundred millisecond.</p> <p>In this controller, the predicted ego-velocity and the target velocity after the delay time are calculated and used for the feedback to address the time delay problem.</p>"},{"location":"control/autoware_pid_longitudinal_controller/#slope-compensation_1","title":"Slope compensation","text":"<p>Based on the slope information, a compensation term is added to the target acceleration.</p> <p>There are two sources of the slope information, which can be switched by a parameter.</p> <ul> <li>Pitch of the estimated ego-pose (default)<ul> <li>Calculates the current slope from the pitch angle of the estimated ego-pose</li> <li>Pros: Easily available</li> <li>Cons: Cannot extract accurate slope information due to the influence of vehicle vibration.</li> </ul> </li> <li>Z coordinate on the trajectory<ul> <li>Calculates the road slope from the difference of z-coordinates between the front and rear wheel positions in the target trajectory</li> <li>Pros: More accurate than pitch information, if the z-coordinates of the route are properly maintained</li> <li>Pros: Can be used in combination with delay compensation (not yet implemented)</li> <li>Cons: z-coordinates of high-precision map is needed.</li> <li>Cons: Does not support free space planning (for now)</li> </ul> </li> </ul>"},{"location":"control/autoware_pid_longitudinal_controller/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<ol> <li>Smoothed target velocity and its acceleration shall be set in the trajectory<ol> <li>The velocity command is not smoothed inside the controller (only noise may be removed).</li> <li>For step-like target signal, tracking is performed as fast as possible.</li> </ol> </li> <li>The vehicle velocity must be an appropriate value<ol> <li>The ego-velocity must be a signed-value corresponding to the forward/backward direction</li> <li>The ego-velocity should be given with appropriate noise processing.</li> <li>If there is a large amount of noise in the ego-velocity, the tracking performance will be significantly reduced.</li> </ol> </li> <li>The output of this controller must be achieved by later modules (e.g. vehicle interface).<ol> <li>If the vehicle interface does not have the target velocity or acceleration interface (e.g., the vehicle only has a gas pedal and brake interface), an appropriate conversion must be done after this controller.</li> </ol> </li> </ol>"},{"location":"control/autoware_pid_longitudinal_controller/#inputs-outputs-api","title":"Inputs / Outputs / API","text":""},{"location":"control/autoware_pid_longitudinal_controller/#input","title":"Input","text":"<p>Set the following from the controller_node</p> <ul> <li><code>autoware_planning_msgs/Trajectory</code> : reference trajectory to follow.</li> <li><code>nav_msgs/Odometry</code>: current odometry</li> </ul>"},{"location":"control/autoware_pid_longitudinal_controller/#output","title":"Output","text":"<p>Return LongitudinalOutput which contains the following to the controller node</p> <ul> <li><code>autoware_control_msgs/Longitudinal</code>: command to control the longitudinal motion of the vehicle. It contains the target velocity and target acceleration.</li> <li>LongitudinalSyncData<ul> <li>velocity convergence(currently not used)</li> </ul> </li> </ul>"},{"location":"control/autoware_pid_longitudinal_controller/#pidcontroller-class","title":"PIDController class","text":"<p>The <code>PIDController</code> class is straightforward to use. First, gains and limits must be set (using <code>setGains()</code> and <code>setLimits()</code>) for the proportional (P), integral (I), and derivative (D) components. Then, the velocity can be calculated by providing the current error and time step duration to the <code>calculate()</code> function.</p>"},{"location":"control/autoware_pid_longitudinal_controller/#parameter-description","title":"Parameter description","text":"<p>The default parameters defined in <code>param/lateral_controller_defaults.param.yaml</code> are adjusted to the AutonomouStuff Lexus RX 450h for under 40 km/h driving.</p> Name Type Description Default value delay_compensation_time double delay for longitudinal control [s] 0.17 enable_smooth_stop bool flag to enable transition to STOPPING true enable_overshoot_emergency bool flag to enable transition to EMERGENCY when the ego is over the stop line with a certain distance. See <code>emergency_state_overshoot_stop_dist</code>. true enable_large_tracking_error_emergency bool flag to enable transition to EMERGENCY when the closest trajectory point search is failed due to a large deviation between trajectory and ego pose. true enable_slope_compensation bool flag to modify output acceleration for slope compensation. The source of the slope angle can be selected from ego-pose or trajectory angle. See <code>use_trajectory_for_pitch_calculation</code>. true enable_brake_keeping_before_stop bool flag to keep a certain acceleration during DRIVE state before the ego stops. See Brake keeping. false enable_keep_stopped_until_steer_convergence bool flag to keep stopped condition until until the steer converges. true max_acc double max value of output acceleration [m/s^2] 3.0 min_acc double min value of output acceleration [m/s^2] -5.0 max_jerk double max value of jerk of output acceleration [m/s^3] 2.0 min_jerk double min value of jerk of output acceleration [m/s^3] -5.0 use_trajectory_for_pitch_calculation bool If true, the slope is estimated from trajectory z-level. Otherwise the pitch angle of the ego pose is used. false lpf_pitch_gain double gain of low-pass filter for pitch estimation 0.95 max_pitch_rad double max value of estimated pitch [rad] 0.1 min_pitch_rad double min value of estimated pitch [rad] -0.1"},{"location":"control/autoware_pid_longitudinal_controller/#state-transition","title":"State transition","text":"Name Type Description Default value drive_state_stop_dist double The state will transit to DRIVE when the distance to the stop point is longer than <code>drive_state_stop_dist</code> + <code>drive_state_offset_stop_dist</code> [m] 0.5 drive_state_offset_stop_dist double The state will transit to DRIVE when the distance to the stop point is longer than <code>drive_state_stop_dist</code> + <code>drive_state_offset_stop_dist</code> [m] 1.0 stopping_state_stop_dist double The state will transit to STOPPING when the distance to the stop point is shorter than <code>stopping_state_stop_dist</code> [m] 0.5 stopped_state_entry_vel double threshold of the ego velocity in transition to the STOPPED state [m/s] 0.01 stopped_state_entry_acc double threshold of the ego acceleration in transition to the STOPPED state [m/s^2] 0.1 emergency_state_overshoot_stop_dist double If <code>enable_overshoot_emergency</code> is true and the ego is <code>emergency_state_overshoot_stop_dist</code>-meter ahead of the stop point, the state will transit to EMERGENCY. [m] 1.5"},{"location":"control/autoware_pid_longitudinal_controller/#drive-parameter","title":"DRIVE Parameter","text":"Name Type Description Default value kp double p gain for longitudinal control 1.0 ki double i gain for longitudinal control 0.1 kd double d gain for longitudinal control 0.0 max_out double max value of PID's output acceleration during DRIVE state [m/s^2] 1.0 min_out double min value of PID's output acceleration during DRIVE state [m/s^2] -1.0 max_p_effort double max value of acceleration with p gain 1.0 min_p_effort double min value of acceleration with p gain -1.0 max_i_effort double max value of acceleration with i gain 0.3 min_i_effort double min value of acceleration with i gain -0.3 max_d_effort double max value of acceleration with d gain 0.0 min_d_effort double min value of acceleration with d gain 0.0 lpf_vel_error_gain double gain of low-pass filter for velocity error 0.9 enable_integration_at_low_speed bool Whether to enable integration of acceleration errors when the vehicle speed is lower than <code>current_vel_threshold_pid_integration</code> or not. false current_vel_threshold_pid_integration double Velocity error is integrated for I-term only when the absolute value of current velocity is larger than this parameter. [m/s] 0.5 time_threshold_before_pid_integration double How much time without the vehicle moving must past to enable PID error integration. [s] 5.0 brake_keeping_acc double If <code>enable_brake_keeping_before_stop</code> is true, a certain acceleration is kept during DRIVE state before the ego stops [m/s^2] See Brake keeping. 0.2"},{"location":"control/autoware_pid_longitudinal_controller/#stopping-parameter-smooth-stop","title":"STOPPING Parameter (smooth stop)","text":"<p>Smooth stop is enabled if <code>enable_smooth_stop</code> is true. In smooth stop, strong acceleration (<code>strong_acc</code>) will be output first to decrease the ego velocity. Then weak acceleration (<code>weak_acc</code>) will be output to stop smoothly by decreasing the ego jerk. If the ego does not stop in a certain time or some-meter over the stop point, weak acceleration to stop right (<code>weak_stop_acc</code>) now will be output. If the ego is still running, strong acceleration (<code>strong_stop_acc</code>) to stop right now will be output.</p> Name Type Description Default value smooth_stop_max_strong_acc double max strong acceleration [m/s^2] -0.5 smooth_stop_min_strong_acc double min strong acceleration [m/s^2] -0.8 smooth_stop_weak_acc double weak acceleration [m/s^2] -0.3 smooth_stop_weak_stop_acc double weak acceleration to stop right now [m/s^2] -0.8 smooth_stop_strong_stop_acc double strong acceleration to be output when the ego is <code>smooth_stop_strong_stop_dist</code>-meter over the stop point. [m/s^2] -3.4 smooth_stop_max_fast_vel double max fast vel to judge the ego is running fast [m/s]. If the ego is running fast, strong acceleration will be output. 0.5 smooth_stop_min_running_vel double min ego velocity to judge if the ego is running or not [m/s] 0.01 smooth_stop_min_running_acc double min ego acceleration to judge if the ego is running or not [m/s^2] 0.01 smooth_stop_weak_stop_time double max time to output weak acceleration [s]. After this, strong acceleration will be output. 0.8 smooth_stop_weak_stop_dist double Weak acceleration will be output when the ego is <code>smooth_stop_weak_stop_dist</code>-meter before the stop point. [m] -0.3 smooth_stop_strong_stop_dist double Strong acceleration will be output when the ego is <code>smooth_stop_strong_stop_dist</code>-meter over the stop point. [m] -0.5"},{"location":"control/autoware_pid_longitudinal_controller/#stopped-parameter","title":"STOPPED Parameter","text":"<p>The <code>STOPPED</code> state assumes that the vehicle is completely stopped with the brakes fully applied. Therefore, <code>stopped_acc</code> should be set to a value that allows the vehicle to apply the strongest possible brake. If <code>stopped_acc</code> is not sufficiently low, there is a possibility of sliding down on steep slopes.</p> Name Type Description Default value stopped_vel double target velocity in STOPPED state [m/s] 0.0 stopped_acc double target acceleration in STOPPED state [m/s^2] -3.4 stopped_jerk double target jerk in STOPPED state [m/s^3] -5.0"},{"location":"control/autoware_pid_longitudinal_controller/#emergency-parameter","title":"EMERGENCY Parameter","text":"Name Type Description Default value emergency_vel double target velocity in EMERGENCY state [m/s] 0.0 emergency_acc double target acceleration in an EMERGENCY state [m/s^2] -5.0 emergency_jerk double target jerk in an EMERGENCY state [m/s^3] -3.0"},{"location":"control/autoware_pid_longitudinal_controller/#references-external-links","title":"References / External links","text":""},{"location":"control/autoware_pid_longitudinal_controller/#future-extensions-unimplemented-parts","title":"Future extensions / Unimplemented parts","text":""},{"location":"control/autoware_pid_longitudinal_controller/#related-issues","title":"Related issues","text":""},{"location":"control/autoware_predicted_path_checker/","title":"Predicted Path Checker","text":""},{"location":"control/autoware_predicted_path_checker/#predicted-path-checker","title":"Predicted Path Checker","text":""},{"location":"control/autoware_predicted_path_checker/#purpose","title":"Purpose","text":"<p>The Predicted Path Checker package is designed for autonomous vehicles to check the predicted path generated by control modules. It handles potential collisions that the planning module might not be able to handle and that in the brake distance. In case of collision in brake distance, the package will send a diagnostic message labeled \"ERROR\" to alert the system to send emergency and in the case of collisions in outside reference trajectory, it sends pause request to pause interface to make the vehicle stop.</p> <p></p>"},{"location":"control/autoware_predicted_path_checker/#algorithm","title":"Algorithm","text":"<p>The package algorithm evaluates the predicted trajectory against the reference trajectory and the predicted objects in the environment. It checks for potential collisions and, if necessary, generates an appropriate response to avoid them ( emergency or pause request).</p>"},{"location":"control/autoware_predicted_path_checker/#inner-algorithm","title":"Inner Algorithm","text":"<p>cutTrajectory() -&gt; It cuts the predicted trajectory with input length. Length is calculated by multiplying the velocity of ego vehicle with \"trajectory_check_time\" parameter and \"min_trajectory_length\".</p> <p>filterObstacles() -&gt; It filters the predicted objects in the environment. It filters the objects which are not in front of the vehicle and far away from predicted trajectory.</p> <p>checkTrajectoryForCollision() -&gt; It checks the predicted trajectory for collision with the predicted objects. It calculates both polygon of trajectory points and predicted objects and checks intersection of both polygons. If there is an intersection, it calculates the nearest collision point. It returns the nearest collision point of polygon and the predicted object. It also checks predicted objects history which are intersect with the footprint before to avoid unexpected behaviors. Predicted objects history stores the objects if it was detected below the \"chattering_threshold\" seconds ago.</p> <p>If the \"enable_z_axis_obstacle_filtering\" parameter is set to true, it filters the predicted objects in the Z-axis by using \"z_axis_filtering_buffer\". If the object does not intersect with the Z-axis, it is filtered out.</p> <p></p> <p>calculateProjectedVelAndAcc() -&gt; It calculates the projected velocity and acceleration of the predicted object on predicted trajectory's collision point's axes.</p> <p>isInBrakeDistance() -&gt; It checks if the stop point is in brake distance. It gets relative velocity and acceleration of ego vehicle with respect to the predicted object. It calculates the brake distance, if the point in brake distance, it returns true.</p> <p>isItDiscretePoint() -&gt; It checks if the stop point on predicted trajectory is discrete point or not. If it is not discrete point, planning should handle the stop.</p> <p>isThereStopPointOnRefTrajectory() -&gt; It checks if there is a stop point on reference trajectory. If there is a stop point before the stop index, it returns true. Otherwise, it returns false, and node is going to call pause interface to make the vehicle stop.</p>"},{"location":"control/autoware_predicted_path_checker/#inputs","title":"Inputs","text":"Name Type Description <code>~/input/reference_trajectory</code> <code>autoware_planning_msgs::msg::Trajectory</code> Reference trajectory <code>~/input/predicted_trajectory</code> <code>autoware_planning_msgs::msg::Trajectory</code> Predicted trajectory <code>~/input/objects</code> <code>autoware_perception_msgs::msg::PredictedObject</code> Dynamic objects in the environment <code>~/input/odometry</code> <code>nav_msgs::msg::Odometry</code> Odometry message of vehicle to get current velocity <code>~/input/current_accel</code> <code>geometry_msgs::msg::AccelWithCovarianceStamped</code> Current acceleration <code>/control/vehicle_cmd_gate/is_paused</code> <code>tier4_control_msgs::msg::IsPaused</code> Current pause state of the vehicle"},{"location":"control/autoware_predicted_path_checker/#outputs","title":"Outputs","text":"Name Type Description <code>~/debug/marker</code> <code>visualization_msgs::msg::MarkerArray</code> Marker for visualization <code>~/debug/virtual_wall</code> <code>visualization_msgs::msg::MarkerArray</code> Virtual wall marker for visualization <code>/control/vehicle_cmd_gate/set_pause</code> <code>tier4_control_msgs::srv::SetPause</code> Pause service to make the vehicle stop <code>/diagnostics</code> <code>diagnostic_msgs::msg::DiagnosticStatus</code> Diagnostic status of vehicle"},{"location":"control/autoware_predicted_path_checker/#parameters","title":"Parameters","text":""},{"location":"control/autoware_predicted_path_checker/#node-parameters","title":"Node Parameters","text":"Name Type Description Default value <code>update_rate</code> <code>double</code> The update rate [Hz] 10.0 <code>delay_time</code> <code>double</code> he time delay considered for the emergency response [s] 0.17 <code>max_deceleration</code> <code>double</code> Max deceleration for ego vehicle to stop [m/s^2] 1.5 <code>resample_interval</code> <code>double</code> Interval for resampling trajectory [m] 0.5 <code>stop_margin</code> <code>double</code> The stopping margin [m] 0.5 <code>ego_nearest_dist_threshold</code> <code>double</code> The nearest distance threshold for ego vehicle [m] 3.0 <code>ego_nearest_yaw_threshold</code> <code>double</code> The nearest yaw threshold for ego vehicle [rad] 1.046 <code>min_trajectory_check_length</code> <code>double</code> The minimum trajectory check length in meters [m] 1.5 <code>trajectory_check_time</code> <code>double</code> The trajectory check time in seconds. [s] 3.0 <code>distinct_point_distance_threshold</code> <code>double</code> The distinct point distance threshold [m] 0.3 <code>distinct_point_yaw_threshold</code> <code>double</code> The distinct point yaw threshold [deg] 5.0 <code>filtering_distance_threshold</code> <code>double</code> It ignores the objects if distance is higher than this [m] 1.5 <code>use_object_prediction</code> <code>bool</code> If true, node predicts current pose of the objects wrt delta time [-] true"},{"location":"control/autoware_predicted_path_checker/#collision-checker-parameters","title":"Collision Checker Parameters","text":"Name Type Description Default value <code>width_margin</code> <code>double</code> The width margin for collision checking [Hz] 0.2 <code>chattering_threshold</code> <code>double</code> The chattering threshold for collision detection [s] 0.2 <code>z_axis_filtering_buffer</code> <code>double</code> The Z-axis filtering buffer [m] 0.3 <code>enable_z_axis_obstacle_filtering</code> <code>bool</code> A boolean flag indicating if Z-axis obstacle filtering is enabled false"},{"location":"control/autoware_pure_pursuit/","title":"Pure Pursuit Controller","text":""},{"location":"control/autoware_pure_pursuit/#pure-pursuit-controller","title":"Pure Pursuit Controller","text":"<p>The Pure Pursuit Controller module calculates the steering angle for tracking a desired trajectory using the pure pursuit algorithm. This is used as a lateral controller plugin in the <code>autoware_trajectory_follower_node</code>.</p>"},{"location":"control/autoware_pure_pursuit/#inputs","title":"Inputs","text":"<p>Set the following from the controller_node</p> <ul> <li><code>autoware_planning_msgs/Trajectory</code> : reference trajectory to follow.</li> <li><code>nav_msgs/Odometry</code>: current ego pose and velocity information</li> </ul>"},{"location":"control/autoware_pure_pursuit/#outputs","title":"Outputs","text":"<p>Return LateralOutput which contains the following to the controller node</p> <ul> <li><code>autoware_control_msgs/Lateral</code>: target steering angle</li> <li>LateralSyncData<ul> <li>steer angle convergence</li> </ul> </li> <li><code>autoware_planning_msgs/Trajectory</code>: predicted path for ego vehicle</li> </ul>"},{"location":"control/autoware_pure_pursuit/#parameters","title":"Parameters","text":"Name Type Description Default Range ld_velocity_ratio float Velocity ratio for lookahead distance. 2.4 N/A ld_lateral_error_ratio float Lateral error ratio for lookahead distance. 3.6 N/A ld_curvature_ratio float Curvature ratio for lookahead distance. 120 N/A long_ld_lateral_error_threshold float Threshold for lateral error in long lookahead distance. 0.5 N/A min_lookahead_distance float Minimum lookahead distance. 4.35 N/A max_lookahead_distance float Maximum lookahead distance. 15 N/A converged_steer_rad float Steering angle considered as converged. 0.1 N/A reverse_min_lookahead_distance float Minimum lookahead distance for reversing. 7 N/A prediction_ds float Prediction step size. 0.3 N/A prediction_distance_length float Prediction distance length. 21 N/A resampling_ds float Resampling step size. 0.1 N/A curvature_calculation_distance float Distance for curvature calculation. 4 N/A enable_path_smoothing boolean Enable or disable path smoothing. 0 N/A path_filter_moving_ave_num float Number of points for moving average path filter. 25 N/A"},{"location":"control/autoware_shift_decider/","title":"Shift Decider","text":""},{"location":"control/autoware_shift_decider/#shift-decider","title":"Shift Decider","text":""},{"location":"control/autoware_shift_decider/#purpose","title":"Purpose","text":"<p><code>autoware_shift_decider</code> is a module to decide shift from ackermann control command.</p>"},{"location":"control/autoware_shift_decider/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"control/autoware_shift_decider/#flow-chart","title":"Flow chart","text":""},{"location":"control/autoware_shift_decider/#algorithms","title":"Algorithms","text":""},{"location":"control/autoware_shift_decider/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"control/autoware_shift_decider/#input","title":"Input","text":"Name Type Description <code>~/input/control_cmd</code> <code>autoware_control_msgs::msg::Control</code> Control command for vehicle."},{"location":"control/autoware_shift_decider/#output","title":"Output","text":"Name Type Description <code>~output/gear_cmd</code> <code>autoware_vehicle_msgs::msg::GearCommand</code> Gear for drive forward / backward."},{"location":"control/autoware_shift_decider/#parameters","title":"Parameters","text":"<p>none.</p>"},{"location":"control/autoware_shift_decider/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"control/autoware_smart_mpc_trajectory_follower/","title":"Index","text":""},{"location":"control/autoware_smart_mpc_trajectory_follower/#smart-mpc-trajectory-follower","title":"Smart MPC Trajectory Follower","text":"<p>Smart MPC (Model Predictive Control) is a control algorithm that combines model predictive control and machine learning. While inheriting the advantages of model predictive control, it solves its disadvantage of modeling difficulty with a data-driven method using machine learning.</p> <p>This technology makes it relatively easy to operate model predictive control, which is expensive to implement, as long as an environment for collecting data can be prepared.</p> <p> </p>"},{"location":"control/autoware_smart_mpc_trajectory_follower/#provided-features","title":"Provided features","text":"<p>This package provides smart MPC logic for path-following control as well as mechanisms for learning and evaluation. These features are described below.</p>"},{"location":"control/autoware_smart_mpc_trajectory_follower/#trajectory-following-control-based-on-ilqrmppi","title":"Trajectory following control based on iLQR/MPPI","text":"<p>The control mode can be selected from \"ilqr\", \"mppi\", or \"mppi_ilqr\", and can be set as <code>mpc_parameter:system:mode</code> in mpc_param.yaml. In \"mppi_ilqr\" mode, the initial value of iLQR is given by the MPPI solution.</p> <p>[!NOTE] With the default settings, the performance of \"mppi\" mode is limited due to an insufficient number of samples. This issue is being addressed with ongoing work to introduce GPU support.</p> <p>To perform a simulation, run the following command:</p> <pre><code>ros2 launch autoware_launch planning_simulator.launch.xml map_path:=$HOME/autoware_map/sample-map-planning vehicle_model:=sample_vehicle sensor_model:=sample_sensor_kit trajectory_follower_mode:=smart_mpc_trajectory_follower\n</code></pre> <p>[!NOTE] When running with the nominal model set in nominal_param.yaml, set <code>trained_model_parameter:control_application:use_trained_model</code> to <code>false</code> in trained_model_param.yaml. To run using the trained model, set <code>trained_model_parameter:control_application:use_trained_model</code> to <code>true</code>, but the trained model must have been generated according to the following procedure.</p>"},{"location":"control/autoware_smart_mpc_trajectory_follower/#training-of-model-and-reflection-in-control","title":"Training of model and reflection in control","text":"<p>To obtain training data, start autoware, perform a drive, and record rosbag data with the following commands.</p> <pre><code>ros2 bag record /localization/kinematic_state /localization/acceleration /vehicle/status/steering_status /control/command/control_cmd /control/trajectory_follower/control_cmd /control/trajectory_follower/lane_departure_checker_node/debug/deviation/lateral /control/trajectory_follower/lane_departure_checker_node/debug/deviation/yaw /system/operation_mode/state /vehicle/status/control_mode /sensing/imu/imu_data /debug_mpc_x_des /debug_mpc_y_des /debug_mpc_v_des /debug_mpc_yaw_des /debug_mpc_acc_des /debug_mpc_steer_des /debug_mpc_X_des_converted /debug_mpc_x_current /debug_mpc_error_prediction /debug_mpc_max_trajectory_err /debug_mpc_emergency_stop_mode /debug_mpc_goal_stop_mode /debug_mpc_total_ctrl_time /debug_mpc_calc_u_opt_time\n</code></pre> <p>Move rosbag2.bash to the rosbag directory recorded above and execute the following command on the directory</p> <pre><code>bash rosbag2.bash\n</code></pre> <p>This converts rosbag data into CSV format for training models.</p> <p>[!NOTE] Note that a large number of terminals are automatically opened at runtime, but they are automatically closed after rosbag data conversion is completed. From the time you begin this process until all terminals are closed, autoware should not be running.</p> <p>Instead, the same result can be obtained by executing the following command in a python environment:</p> <pre><code>from autoware_smart_mpc_trajectory_follower.training_and_data_check import train_drive_NN_model\nmodel_trainer = train_drive_NN_model.train_drive_NN_model()\nmodel_trainer.transform_rosbag_to_csv(rosbag_dir)\n</code></pre> <p>Here, <code>rosbag_dir</code> represents the rosbag directory. At this time, all CSV files in <code>rosbag_dir</code> are automatically deleted first.</p> <p>We move on to an explanation of how the model is trained. If <code>trained_model_parameter:memory_for_training:use_memory_for_training</code> in trained_model_param.yaml is set to <code>true</code>, training is performed on models that include LSTM, and if it is set to <code>false</code>, training is performed on models that do not include LSTM. When using LSTM, cell states and hidden states are updated based on historical time series data and reflected in the prediction.</p> <p>The paths of the rosbag directories used for training and validation, <code>dir_0</code>, <code>dir_1</code>, <code>dir_2</code>,..., <code>dir_val_0</code>, <code>dir_val_1</code>, <code>dir_val_2</code>,... and the directory <code>save_dir</code> where you save the models, the model can be saved in the python environment as follows:</p> <pre><code>from autoware_smart_mpc_trajectory_follower.training_and_data_check import train_drive_NN_model\nmodel_trainer = train_drive_NN_model.train_drive_NN_model()\nmodel_trainer.add_data_from_csv(dir_0, add_mode=\"as_train\")\nmodel_trainer.add_data_from_csv(dir_1, add_mode=\"as_train\")\nmodel_trainer.add_data_from_csv(dir_2, add_mode=\"as_train\")\n...\nmodel_trainer.add_data_from_csv(dir_val_0, add_mode=\"as_val\")\nmodel_trainer.add_data_from_csv(dir_val_1, add_mode=\"as_val\")\nmodel_trainer.add_data_from_csv(dir_val_2, add_mode=\"as_val\")\n...\nmodel_trainer.get_trained_model()\nmodel_trainer.save_models(save_dir)\n</code></pre> <p>If <code>add_mode</code> is not specified or validation data is not added, the training data is split to be used for training and validation.</p> <p>After performing the polynomial regression, the NN can be trained on the residuals as follows:</p> <pre><code>model_trainer.get_trained_model(use_polynomial_reg=True)\n</code></pre> <p>[!NOTE] In the default setting, regression is performed by several preselected polynomials. When <code>use_selected_polynomial=False</code> is set as the argument of get_trained_model, the <code>deg</code> argument allows setting the maximum degree of the polynomial to be used.</p> <p>If only polynomial regression is performed and no NN model is used, run the following command:</p> <pre><code>model_trainer.get_trained_model(use_polynomial_reg=True,force_NN_model_to_zero=True)\n</code></pre> <p>Move <code>model_for_test_drive.pth</code> and <code>polynomial_reg_info.npz</code> saved in <code>save_dir</code> to the home directory and set <code>trained_model_parameter:control_application:use_trained_model</code> in trained_model_param.yaml to <code>true</code> to reflect the trained model in the control.</p>"},{"location":"control/autoware_smart_mpc_trajectory_follower/#performance-evaluation","title":"Performance evaluation","text":"<p>Here, as an example, we describe the verification of the adaptive performance when the wheel base of the sample_vehicle is 2.79 m, but an incorrect value of 2.0 m is given to the controller side. To give the controller 2.0 m as the wheel base, set the value of <code>nominal_parameter:vehicle_info:wheel_base</code> in nominal_param.yaml to 2.0, and run the following command:</p> <pre><code>python3 -m smart_mpc_trajectory_follower.clear_pycache\n</code></pre>"},{"location":"control/autoware_smart_mpc_trajectory_follower/#test-on-autoware","title":"Test on autoware","text":"<p>To perform a control test on autoware with the nominal model before training, make sure that <code>trained_model_parameter:control_application:use_trained_model</code> in trained_model_param.yaml is <code>false</code> and launch autoware in the manner described in \"Trajectory following control based on iLQR/MPPI\". This time, the following route will be used for the test:</p> <p></p> <p>Record rosbag and train the model in the manner described in \"Training of model and reflection in control\", and move the generated files <code>model_for_test_drive.pth</code> and <code>polynomial_reg_info.npz</code> to the home directory. Sample models, which work under the condition that<code>trained_model_parameter:memory_for_training:use_memory_for_training</code> in trained_model_param.yaml is set to <code>true</code>, can be obtained at sample_models/wheel_base_changed.</p> <p>[!NOTE] Although the data used for training is small, for the sake of simplicity, we will see how much performance can be improved with this amount of data.</p> <p>To control using the trained model obtained here, set <code>trained_model_parameter:control_application:use_trained_model</code> to <code>true</code>, start autoware in the same way, and drive the same route recording rosbag. After the driving is complete, convert the rosbag file to CSV format using the method described in \"Training of model and reflection in control\". A plot of the lateral deviation is obtained by running the <code>lateral_error_visualize</code> function in <code>control/autoware_smart_mpc_trajectory_follower/autoware_smart_mpc_trajectory_follower/training_and_data_check/data_checker.ipynb</code> for the nominal and training model rosbag files <code>rosbag_nominal</code> and <code>rosbag_trained</code>, respectively, as follows:</p> <pre><code>lateral_error_visualize(dir_name=rosbag_nominal,ylim=[-1.2,1.2])\nlateral_error_visualize(dir_name=rosbag_trained,ylim=[-1.2,1.2])\n</code></pre> <p>The following results were obtained.</p>"},{"location":"control/autoware_smart_mpc_trajectory_follower/#test-on-python-simulator","title":"Test on python simulator","text":"<p>First, to give wheel base 2.79 m in the python simulator, create the following file and save it in <code>control/autoware_smart_mpc_trajectory_follower/autoware_smart_mpc_trajectory_follower/python_simulator</code> with the name <code>sim_setting.json</code>:</p> <pre><code>{ \"wheel_base\": 2.79 }\n</code></pre> <p>Next, after moving to <code>control/autoware_smart_mpc_trajectory_follower/autoware_smart_mpc_trajectory_follower/python_simulator</code>, run the following commands to test the slalom driving on the python simulator with the nominal control:</p> <pre><code>python3 run_python_simulator.py nominal_test\n</code></pre> <p>The result of the driving is stored in <code>test_python_nominal_sim</code>.</p> <p>The following results were obtained.</p> <p> </p> <p>The center of the upper row represents the lateral deviation.</p> <p>Run the following commands to perform training using figure eight driving data under the control of pure pursuit.</p> <p>To perform training using a figure eight driving and driving based on the obtained model, run the following commands:</p> <pre><code>python3 run_python_simulator.py\n</code></pre> <p>The result of the driving is stored in <code>test_python_trined_sim</code>.</p> <p>When <code>trained_model_parameter:memory_for_training:use_memory_for_training</code> in trained_model_param.yaml is set to <code>true</code>, the following results were obtained.</p> <p> </p> <p>When <code>trained_model_parameter:memory_for_training:use_memory_for_training</code> in trained_model_param.yaml is set to <code>false</code>, the following results were obtained.</p> <p> </p> <p>It can be seen that the lateral deviation has improved significantly. However, the difference in driving with and without LSTM is not very apparent.</p> <p>To see the difference, for example, we can experiment with parameters such as steer_time_delay.</p> <p>First, to restore nominal model settings to default values, set the value of <code>nominal_parameter:vehicle_info:wheel_base</code> in nominal_param.yaml to 2.79, and run the following command:</p> <pre><code>python3 -m smart_mpc_trajectory_follower.clear_pycache\n</code></pre> <p>Next, modify <code>sim_setting.json</code> as follows:</p> <pre><code>{ \"steer_time_delay\": 1.01 }\n</code></pre> <p>In this way, an experiment is performed when <code>steer_time_delay</code> is set to 1.01 sec.</p> <p>The result of the driving using the nominal model is as follows:</p> <p> </p> <p>The result of the driving using the trained model with LSTM is as follows:</p> <p> </p> <p>The result of the driving using the trained model without LSTM is as follows:</p> <p> </p> <p>It can be seen that the performance with the model that includes LSTM is significantly better than with the model that does not.</p> <p>The parameters that can be passed to the python simulator are as follows.</p> Parameter Type Description steer_bias float steer bias [rad] steer_rate_lim float steer rate limit [rad/s] vel_rate_lim float acceleration limit [m/s^2] wheel_base float wheel base [m] steer_dead_band float steer dead band [rad] adaptive_gear_ratio_coef list[float] List of floats of length 6 specifying information on speed-dependent gear ratios from tire angle to steering wheel angle. acc_time_delay float acceleration time delay [s] steer_time_delay float steer time delay [s] acc_time_constant float acceleration time constant [s] steer_time_constant float steer time constant [s] accel_map_scale float Parameter that magnifies the corresponding distortion from acceleration input values to actual acceleration realizations.  Correspondence information is kept in <code>control/autoware_smart_mpc_trajectory_follower/autoware_smart_mpc_trajectory_follower/python_simulator/accel_map.csv</code>. acc_scaling float acceleration scaling steer_scaling float steer scaling vehicle_type int Take values from 0 to 4 for pre-designed vehicle types.  A description of each vehicle type is given below. <p>For example, to give the simulation side 0.01 [rad] of steer bias and 0.001 [rad] of steer dead band, edit the <code>sim_setting.json</code> as follows.</p> <pre><code>{ \"steer_bias\": 0.01, \"steer_dead_band\": 0.001 }\n</code></pre>"},{"location":"control/autoware_smart_mpc_trajectory_follower/#vehicle_type_0","title":"vehicle_type_0","text":"<p>This vehicle type matches the default vehicle type used in the control.</p> Parameter value wheel_base 2.79 acc_time_delay 0.1 steer_time_delay 0.27 acc_time_constant 0.1 steer_time_constant 0.24 acc_scaling 1.0"},{"location":"control/autoware_smart_mpc_trajectory_follower/#vehicle_type_1","title":"vehicle_type_1","text":"<p>This vehicle type is intended for a heavy bus.</p> Parameter value wheel_base 4.76 acc_time_delay 1.0 steer_time_delay 1.0 acc_time_constant 1.0 steer_time_constant 1.0 acc_scaling 0.2"},{"location":"control/autoware_smart_mpc_trajectory_follower/#vehicle_type_2","title":"vehicle_type_2","text":"<p>This vehicle type is intended for a light bus.</p> Parameter value wheel_base 4.76 acc_time_delay 0.5 steer_time_delay 0.5 acc_time_constant 0.5 steer_time_constant 0.5 acc_scaling 0.5"},{"location":"control/autoware_smart_mpc_trajectory_follower/#vehicle_type_3","title":"vehicle_type_3","text":"<p>This vehicle type is intended for a small vehicle.</p> Parameter value wheel_base 1.335 acc_time_delay 0.3 steer_time_delay 0.3 acc_time_constant 0.3 steer_time_constant 0.3 acc_scaling 1.5"},{"location":"control/autoware_smart_mpc_trajectory_follower/#vehicle_type_4","title":"vehicle_type_4","text":"<p>This vehicle type is intended for a small robot.</p> Parameter value wheel_base 0.395 acc_time_delay 0.2 steer_time_delay 0.2 acc_time_constant 0.2 steer_time_constant 0.2 acc_scaling 1.0"},{"location":"control/autoware_smart_mpc_trajectory_follower/#auto-test-on-python-simulator","title":"Auto test on python simulator","text":"<p>Here, we describe a method for testing adaptive performance by giving the simulation side a predefined range of model parameters while the control side is given constant model parameters.</p> <p>To run a driving experiment within the parameter change range set in run_sim.py, for example, move to <code>control/autoware_smart_mpc_trajectory_follower/autoware_smart_mpc_trajectory_follower/python_simulator</code> and run the following command:</p> <pre><code>python3 run_sim.py --param_name steer_bias\n</code></pre> <p>Here we described the experimental procedure for steer bias, and the same method can be used for other parameters.</p> <p>To run the test for all parameters except limits at once, run the following command:</p> <pre><code>python3 run_auto_test.py\n</code></pre> <p>The results are stored in the <code>auto_test</code> directory. After the executions were completed, the following results were obtained by running plot_auto_test_result.ipynb:</p> <p> </p> <p>The orange line shows the intermediate model trained using pure pursuit figure eight drive, and the blue line shows the final model trained using data from both the intermediate model and the figure eight drive. In most cases, sufficient performance is obtained, but for <code>vehicle_type_1</code>, which is intended for a heavy bus, a lateral deviation of about 2 m was observed, which is not satisfactory.</p> <p>In <code>run_sim.py</code>, the following parameters can be set:</p> Parameter Type Description USE_TRAINED_MODEL_DIFF bool Whether the derivative of the trained model is reflected in the control DATA_COLLECTION_MODE DataCollectionMode Which method will be used to collect the training data  \"DataCollectionMode.ff\": Straight line driving with feed-forward input  \"DataCollectionMode.pp\": Figure eight driving with pure pursuit control  \"DataCollectionMode.mpc\": Slalom driving with mpc USE_POLYNOMIAL_REGRESSION bool Whether to perform polynomial regression before NN USE_SELECTED_POLYNOMIAL bool When USE_POLYNOMIAL_REGRESSION is True, perform polynomial regression using only some preselected polynomials.  The choice of polynomials is intended to be able to absorb the contribution of some parameter shifts based on the nominal model of the vehicle. FORCE_NN_MODEL_TO_ZERO bool Whether to force the NN model to zero (i.e., erase the contribution of the NN model).  When USE_POLYNOMIAL_REGRESSION is True, setting FORCE_MODEL_TO_ZERO to True allows the control to reflect the results of polynomial regression only, without using NN models. FIT_INTERCEPT bool Whether to include bias in polynomial regression.  If it is False, perform the regression with a polynomial of the first degree or higher. USE_INTERCEPT bool When a polynomial regression including bias is performed, whether to use or discard the resulting bias information.  It is meaningful only if FIT_INTERCEPT is True. If it is False, discard the bias in the polynomial regression in the hope that the NN model can remove the bias term, even if the polynomial regression is performed with the bias included. <p>[!NOTE] When <code>run_sim.py</code> is run, the <code>use_trained_model_diff</code> set in <code>run_sim.py</code> takes precedence over the <code>trained_model_parameter:control_application:use_trained_model_diff</code> set in trained_model_param.yaml.</p>"},{"location":"control/autoware_smart_mpc_trajectory_follower/#kernel-density-estimation-of-pure-pursuit-driving-data","title":"Kernel density estimation of pure pursuit driving data","text":"<p>The distribution of data obtained from pure pursuit runs can be displayed using Kernel density estimation. To do this, run density_estimation.ipynb.</p> <p>The correlation between the minimum value of the density estimate and the lateral deviation of the run results is low. A scalar indicator that better predicts the value of lateral deviation is under development.</p>"},{"location":"control/autoware_smart_mpc_trajectory_follower/#change-of-nominal-parameters-and-their-reloading","title":"Change of nominal parameters and their reloading","text":"<p>The nominal parameters of vehicle model can be changed by editing the file nominal_param.yaml. After changing the nominal parameters, the cache must be deleted by running the following command:</p> <pre><code>python3 -m smart_mpc_trajectory_follower.clear_pycache\n</code></pre> <p>The nominal parameters include the following:</p> Parameter Type Description nominal_parameter:vehicle_info:wheel_base float wheel base [m] nominal_parameter:acceleration:acc_time_delay float acceleration time delay [s] nominal_parameter:acceleration:acc_time_constant float acceleration time constant [s] nominal_parameter:steering:steer_time_delay float steer time delay [s] nominal_parameter:steering:steer_time_constant float steer time constant [s]"},{"location":"control/autoware_smart_mpc_trajectory_follower/#change-of-control-parameters-and-their-reloading","title":"Change of control parameters and their reloading","text":"<p>The control parameters can be changed by editing files mpc_param.yaml and trained_model_param.yaml. Although it is possible to reflect parameter changes by restarting autoware, the following command allows us to do so without leaving autoware running:</p> <pre><code>ros2 topic pub /pympc_reload_mpc_param_trigger std_msgs/msg/String \"data: ''\" --once\n</code></pre> <p>The main parameters among the control parameters are as follows.</p>"},{"location":"control/autoware_smart_mpc_trajectory_follower/#mpc_paramyaml","title":"<code>mpc_param.yaml</code>","text":"Parameter Type Description mpc_parameter:system:mode str control mode \"ilqr\": iLQR mode  \"mppi\": MPPI mode  \"mppi_ilqr\": the initial value of iLQR is given by the MPPI solution. mpc_parameter:cost_parameters:Q list[float] Stage cost for states.  List of length 8, in order: straight deviation, lateral deviation, velocity deviation, yaw angle deviation, acceleration deviation, steer deviation, acceleration input deviation, steer input deviation cost weights. mpc_parameter:cost_parameters:Q_c list[float] Cost in the horizon corresponding to the following timing_Q_c for the states.  The correspondence of the components of the list is the same as for Q. mpc_parameter:cost_parameters:Q_f list[float] Termination cost for the states.  The correspondence of the components of the list is the same as for Q. mpc_parameter:cost_parameters:R list[float] A list of length 2 where R[0] is weight of cost for the change rate of acceleration input value and R[1] is weight of cost for the change rate of steer input value. mpc_parameter:mpc_setting:timing_Q_c list[int] Horizon numbers such that the stage cost for the states is set to Q_c. mpc_parameter:compensation:acc_fb_decay float Coefficient of damping in integrating the error between the observed and predicted acceleration values in the compensator outside the MPC. mpc_parameter:compensation:acc_fb_gain float Gain of acceleration compensation. mpc_parameter:compensation:max_error_acc float Maximum acceleration compensation (m/s^2) mpc_parameter:compensation:steer_fb_decay float Coefficient of damping in integrating the error between the observed and predicted steering values in the compensator outside the MPC. mpc_parameter:compensation:steer_fb_gain float Gain of steering compensation. mpc_parameter:compensation:max_error_steer float Maximum steering compensation (rad)"},{"location":"control/autoware_smart_mpc_trajectory_follower/#trained_model_paramyaml","title":"<code>trained_model_param.yaml</code>","text":"Parameter Type Description trained_model_parameter:control_application:use_trained_model bool Whether the trained model is reflected in the control or not. trained_model_parameter:control_application:use_trained_model_diff bool Whether the derivative of the trained model is reflected on the control or not.  It is meaningful only when use_trained_model is True, and if False, the nominal model is used for the derivative of the dynamics, and trained model is used only for prediction. trained_model_parameter:memory_for_training:use_memory_for_training bool Whether to use the model that includes LSTM for learning or not. trained_model_parameter:memory_for_training:use_memory_diff bool Whether the derivative with respect to the cell state and hidden state at the previous time of LSTM is reflected in the control or not."},{"location":"control/autoware_smart_mpc_trajectory_follower/#request-to-release-the-slow-stop-mode","title":"Request to release the slow stop mode","text":"<p>If the predicted trajectory deviates too far from the target trajectory, the system enters a slow stop mode and the vehicle stops moving. To cancel the slow stop mode and make the vehicle ready to run again, run the following command:</p> <pre><code>ros2 topic pub /pympc_stop_mode_reset_request std_msgs/msg/String \"data: ''\" --once\n</code></pre>"},{"location":"control/autoware_smart_mpc_trajectory_follower/#limitation","title":"Limitation","text":"<ul> <li>May not be able to start when initial position/posture is far from the target.</li> </ul> <ul> <li>It may take some time until the end of the planning to compile numba functions at the start of the first control.</li> </ul> <ul> <li>In the stopping action near the goal our control switches to another simple control law. As a result, the stopping action may not work except near the goal. Stopping is also difficult if the acceleration map is significantly shifted.</li> </ul> <ul> <li>If the dynamics deviates too much from the nominal model, as in <code>vehicle_type_1</code>, which is intended for heavy buses, it may not be well controlled.</li> </ul>"},{"location":"control/autoware_stop_mode_operator/","title":"autoware_stop_mode_operator","text":""},{"location":"control/autoware_stop_mode_operator/#autoware_stop_mode_operator","title":"autoware_stop_mode_operator","text":"<p>This package publishes a stop command used when stop mode is selected.</p>"},{"location":"control/autoware_stop_mode_operator/#auto-parking","title":"Auto parking","text":"<p>If <code>enable_auto_parking</code> is set to <code>true</code>, automatically shifts gear to parking when route is unset/arrived and the vehicle is stopped. Since the stop mode needs to work independently of localization, velocity should be taken from vehicle status instead of kinematic state.</p>"},{"location":"control/autoware_trajectory_follower_base/","title":"Trajectory Follower","text":""},{"location":"control/autoware_trajectory_follower_base/#trajectory-follower","title":"Trajectory Follower","text":"<p>This is the design document for the <code>trajectory_follower</code> package.</p>"},{"location":"control/autoware_trajectory_follower_base/#purpose-use-cases","title":"Purpose / Use cases","text":"<p>This package provides the interface of longitudinal and lateral controllers used by the node of the <code>autoware_trajectory_follower_node</code> package. We can implement a detailed controller by deriving the longitudinal and lateral base interfaces.</p>"},{"location":"control/autoware_trajectory_follower_base/#design","title":"Design","text":"<p>There are lateral and longitudinal base interface classes and each algorithm inherits from this class to implement. The interface class has the following base functions.</p> <ul> <li><code>isReady()</code>: Check if the control is ready to compute.</li> <li><code>run()</code>: Compute control commands and return to Trajectory Follower Nodes. This must be implemented by inherited algorithms.</li> <li><code>sync()</code>: Input the result of running the other controller.<ul> <li>steer angle convergence<ul> <li>allow keeping stopped until steer is converged.</li> </ul> </li> <li>velocity convergence(currently not used)</li> </ul> </li> </ul> <p>See the Design of Trajectory Follower Nodes for how these functions work in the node.</p>"},{"location":"control/autoware_trajectory_follower_base/#separated-lateral-steering-and-longitudinal-velocity-controls","title":"Separated lateral (steering) and longitudinal (velocity) controls","text":"<p>This longitudinal controller assumes that the roles of lateral and longitudinal control are separated as follows.</p> <ul> <li>Lateral control computes a target steering to keep the vehicle on the trajectory, assuming perfect velocity tracking.</li> <li>Longitudinal control computes a target velocity/acceleration to keep the vehicle velocity on the trajectory speed, assuming perfect trajectory tracking.</li> </ul> <p>Ideally, dealing with the lateral and longitudinal control as a single mixed problem can achieve high performance. In contrast, there are two reasons to provide velocity controller as a stand-alone function, described below.</p>"},{"location":"control/autoware_trajectory_follower_base/#complex-requirements-for-longitudinal-motion","title":"Complex requirements for longitudinal motion","text":"<p>The longitudinal vehicle behavior that humans expect is difficult to express in a single logic. For example, the expected behavior just before stopping differs depending on whether the ego-position is ahead/behind of the stop line, or whether the current speed is higher/lower than the target speed to achieve a human-like movement.</p> <p>In addition, some vehicles have difficulty measuring the ego-speed at extremely low speeds. In such cases, a configuration that can improve the functionality of the longitudinal control without affecting the lateral control is important.</p> <p>There are many characteristics and needs that are unique to longitudinal control. Designing them separately from the lateral control keeps the modules less coupled and improves maintainability.</p>"},{"location":"control/autoware_trajectory_follower_base/#nonlinear-coupling-of-lateral-and-longitudinal-motion","title":"Nonlinear coupling of lateral and longitudinal motion","text":"<p>The lat-lon mixed control problem is very complex and uses nonlinear optimization to achieve high performance. Since it is difficult to guarantee the convergence of the nonlinear optimization, a simple control logic is also necessary for development.</p> <p>Also, the benefits of simultaneous longitudinal and lateral control are small if the vehicle doesn't move at high speed.</p>"},{"location":"control/autoware_trajectory_follower_base/#related-issues","title":"Related issues","text":""},{"location":"control/autoware_trajectory_follower_node/","title":"Trajectory Follower Nodes","text":""},{"location":"control/autoware_trajectory_follower_node/#trajectory-follower-nodes","title":"Trajectory Follower Nodes","text":""},{"location":"control/autoware_trajectory_follower_node/#purpose","title":"Purpose","text":"<p>Generate control commands to follow a given Trajectory.</p>"},{"location":"control/autoware_trajectory_follower_node/#design","title":"Design","text":"<p>This is a node of the functionalities implemented in the controller class derived from autoware_trajectory_follower_base package. It has instances of those functionalities, gives them input data to perform calculations, and publishes control commands.</p> <p>By default, the controller instance with the <code>Controller</code> class as follows is used.</p> <p></p> <p>The process flow of <code>Controller</code> class is as follows.</p> <pre><code>// 1. create input data\nconst auto input_data = createInputData(*get_clock());\nif (!input_data) {\n  return;\n}\n\n// 2. check if controllers are ready\nconst bool is_lat_ready = lateral_controller_-&gt;isReady(*input_data);\nconst bool is_lon_ready = longitudinal_controller_-&gt;isReady(*input_data);\nif (!is_lat_ready || !is_lon_ready) {\n  return;\n}\n\n// 3. run controllers\nconst auto lat_out = lateral_controller_-&gt;run(*input_data);\nconst auto lon_out = longitudinal_controller_-&gt;run(*input_data);\n\n// 4. sync with each other controllers\nlongitudinal_controller_-&gt;sync(lat_out.sync_data);\nlateral_controller_-&gt;sync(lon_out.sync_data);\n\n// 5. publish control command\ncontrol_cmd_pub_-&gt;publish(out);\n</code></pre> <p>Giving the longitudinal controller information about steer convergence allows it to control steer when stopped if following parameters are <code>true</code></p> <ul> <li>lateral controller<ul> <li><code>keep_steer_control_until_converged</code></li> </ul> </li> <li>longitudinal controller<ul> <li><code>enable_keep_stopped_until_steer_convergence</code></li> </ul> </li> </ul>"},{"location":"control/autoware_trajectory_follower_node/#inputs-outputs-api","title":"Inputs / Outputs / API","text":""},{"location":"control/autoware_trajectory_follower_node/#inputs","title":"Inputs","text":"<ul> <li><code>autoware_planning_msgs/Trajectory</code> : reference trajectory to follow.</li> <li><code>nav_msgs/Odometry</code>: current odometry</li> <li><code>autoware_vehicle_msgs/SteeringReport</code> current steering</li> </ul>"},{"location":"control/autoware_trajectory_follower_node/#outputs","title":"Outputs","text":"<ul> <li><code>autoware_control_msgs/Control</code>: message containing both lateral and longitudinal commands.</li> <li><code>autoware_control_msgs/ControlHorizon</code>: message containing both lateral and longitudinal horizon commands. this is NOT published by default. by using this, the performance of vehicle control may be improved, and by turning the default on, it can be used as an experimental topic.</li> </ul>"},{"location":"control/autoware_trajectory_follower_node/#parameter","title":"Parameter","text":"<ul> <li><code>ctrl_period</code>: control commands publishing period</li> <li><code>timeout_thr_sec</code>: duration in second after which input messages are discarded.<ul> <li>Each time the node receives lateral and longitudinal commands from each controller, it publishes an <code>Control</code> if the following two conditions are met.<ol> <li>Both commands have been received.</li> <li>The last received commands are not older than defined by <code>timeout_thr_sec</code>.</li> </ol> </li> </ul> </li> <li><code>lateral_controller_mode</code>: <code>mpc</code> or <code>pure_pursuit</code><ul> <li>(currently there is only <code>PID</code> for longitudinal controller)</li> </ul> </li> <li><code>enable_control_cmd_horizon_pub</code>: publish <code>ControlHorizon</code> or not (default: false)</li> </ul>"},{"location":"control/autoware_trajectory_follower_node/#debugging","title":"Debugging","text":"<p>Debug information are published by the lateral and longitudinal controller using <code>autoware_internal_debug_msgs/Float32MultiArrayStamped</code> messages.</p> <p>A configuration file for PlotJuggler is provided in the <code>config</code> folder which, when loaded, allow to automatically subscribe and visualize information useful for debugging.</p> <p>In addition, the predicted MPC trajectory is published on topic <code>output/lateral/predicted_trajectory</code> and can be visualized in Rviz.</p>"},{"location":"control/autoware_trajectory_follower_node/design/simple_trajectory_follower-design/","title":"Simple Trajectory Follower","text":""},{"location":"control/autoware_trajectory_follower_node/design/simple_trajectory_follower-design/#simple-trajectory-follower","title":"Simple Trajectory Follower","text":""},{"location":"control/autoware_trajectory_follower_node/design/simple_trajectory_follower-design/#purpose","title":"Purpose","text":"<p>Provide a base trajectory follower code that is simple and flexible to use. This node calculates control command based on a reference trajectory and an ego vehicle kinematics.</p>"},{"location":"control/autoware_trajectory_follower_node/design/simple_trajectory_follower-design/#design","title":"Design","text":""},{"location":"control/autoware_trajectory_follower_node/design/simple_trajectory_follower-design/#inputs-outputs","title":"Inputs / Outputs","text":"<p>Inputs</p> <ul> <li><code>input/reference_trajectory</code> [autoware_planning_msgs::msg::Trajectory] : reference trajectory to follow.</li> <li><code>input/current_kinematic_state</code> [nav_msgs::msg::Odometry] : current state of the vehicle (position, velocity, etc).</li> <li>Output</li> <li><code>output/control_cmd</code> [autoware_control_msgs::msg::Control] : generated control command.</li> </ul>"},{"location":"control/autoware_trajectory_follower_node/design/simple_trajectory_follower-design/#parameters","title":"Parameters","text":"Name Type Description Default value use_external_target_vel bool use external target velocity defined by parameter when true, else follow the velocity on target trajectory points. false external_target_vel float target velocity used when <code>use_external_target_vel</code> is true. 0.0 lateral_deviation float target lateral deviation when following. 0.0"},{"location":"control/autoware_vehicle_cmd_gate/","title":"vehicle_cmd_gate","text":""},{"location":"control/autoware_vehicle_cmd_gate/#vehicle_cmd_gate","title":"vehicle_cmd_gate","text":""},{"location":"control/autoware_vehicle_cmd_gate/#purpose","title":"Purpose","text":"<p><code>vehicle_cmd_gate</code> is the package to get information from emergency handler, planning module, and external controller, and send a message to the vehicle.</p>"},{"location":"control/autoware_vehicle_cmd_gate/#role","title":"Role","text":"<p>Receive multiple control commands and select one to forward to the vehicle.</p>"},{"location":"control/autoware_vehicle_cmd_gate/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"control/autoware_vehicle_cmd_gate/#input","title":"Input","text":"Name Type Description <code>~/input/steering</code> <code>autoware_vehicle_msgs::msg::SteeringReport</code> steering status <code>~/input/auto/control_cmd</code> <code>autoware_control_msgs::msg::Control</code> command for lateral and longitudinal velocity from planning module <code>~/input/auto/turn_indicators_cmd</code> <code>autoware_vehicle_msgs::msg::TurnIndicatorsCommand</code> turn indicators command from planning module <code>~/input/auto/hazard_lights_cmd</code> <code>autoware_vehicle_msgs::msg::HazardLightsCommand</code> hazard lights command from planning module <code>~/input/auto/gear_cmd</code> <code>autoware_vehicle_msgs::msg::GearCommand</code> gear command from planning module <code>~/input/external/control_cmd</code> <code>autoware_control_msgs::msg::Control</code> command for lateral and longitudinal velocity from external <code>~/input/external/turn_indicators_cmd</code> <code>autoware_vehicle_msgs::msg::TurnIndicatorsCommand</code> turn indicators command from external <code>~/input/external/hazard_lights_cmd</code> <code>autoware_vehicle_msgs::msg::HazardLightsCommand</code> hazard lights command from external <code>~/input/external/gear_cmd</code> <code>autoware_vehicle_msgs::msg::GearCommand</code> gear command from external <code>~/input/external_emergency_stop_heartbeat</code> <code>tier4_external_api_msgs::msg::Heartbeat</code> heartbeat <code>~/input/gate_mode</code> <code>tier4_control_msgs::msg::GateMode</code> gate mode (AUTO or EXTERNAL) <code>~/input/emergency/control_cmd</code> <code>autoware_control_msgs::msg::Control</code> command for lateral and longitudinal velocity from emergency handler <code>~/input/emergency/turn_indicators_cmd</code> <code>autoware_vehicle_msgs::msg::TurnIndicatorsCommand</code> turn indicators command from emergency handler <code>~/input/emergency/hazard_lights_cmd</code> <code>autoware_vehicle_msgs::msg::HazardLightsCommand</code> hazard lights command from emergency handler <code>~/input/emergency/gear_cmd</code> <code>autoware_vehicle_msgs::msg::GearCommand</code> gear command from emergency handler <code>~/input/engage</code> <code>autoware_vehicle_msgs::msg::Engage</code> engage signal <code>~/input/operation_mode</code> <code>autoware_adapi_v1_msgs::msg::OperationModeState</code> operation mode of Autoware"},{"location":"control/autoware_vehicle_cmd_gate/#output","title":"Output","text":"Name Type Description <code>~/output/vehicle_cmd_emergency</code> <code>tier4_vehicle_msgs::msg::VehicleEmergencyStamped</code> emergency state which was originally in vehicle command <code>~/output/command/control_cmd</code> <code>autoware_control_msgs::msg::Control</code> command for lateral and longitudinal velocity to vehicle <code>~/output/command/turn_indicators_cmd</code> <code>autoware_vehicle_msgs::msg::TurnIndicatorsCommand</code> turn indicators command to vehicle <code>~/output/command/hazard_lights_cmd</code> <code>autoware_vehicle_msgs::msg::HazardLightsCommand</code> hazard lights command to vehicle <code>~/output/command/gear_cmd</code> <code>autoware_vehicle_msgs::msg::GearCommand</code> gear command to vehicle <code>~/output/gate_mode</code> <code>tier4_control_msgs::msg::GateMode</code> gate mode (AUTO or EXTERNAL) <code>~/output/engage</code> <code>autoware_vehicle_msgs::msg::Engage</code> engage signal <code>~/output/external_emergency</code> <code>tier4_external_api_msgs::msg::Emergency</code> external emergency signal <code>~/output/operation_mode</code> <code>tier4_system_msgs::msg::OperationMode</code> current operation mode of the vehicle_cmd_gate"},{"location":"control/autoware_vehicle_cmd_gate/#parameters","title":"Parameters","text":"Parameter Type Description <code>update_period</code> double update period <code>use_emergency_handling</code> bool true when emergency handler is used <code>check_external_emergency_heartbeat</code> bool true when checking heartbeat for emergency stop <code>system_emergency_heartbeat_timeout</code> double timeout for system emergency <code>external_emergency_stop_heartbeat_timeout</code> double timeout for external emergency <code>filter_activated_count_threshold</code> int threshold for filter activation <code>filter_activated_velocity_threshold</code> double velocity threshold for filter activation <code>stop_hold_acceleration</code> double longitudinal acceleration cmd when vehicle should stop <code>emergency_acceleration</code> double longitudinal acceleration cmd when vehicle stop with emergency <code>moderate_stop_service_acceleration</code> double longitudinal acceleration cmd when vehicle stop with moderate stop service <code>nominal.vel_lim</code> double limit of longitudinal velocity (activated in AUTONOMOUS operation mode) <code>nominal.reference_speed_points</code> velocity point used as a reference when calculate control command limit (activated in AUTONOMOUS operation mode). The size of this array must be equivalent to the size of the limit array. <code>nominal.lon_acc_lim_for_lon_vel</code> array of limits for longitudinal acceleration (activated in AUTONOMOUS operation mode) <code>nominal.lon_jerk_lim_for_lon_acc</code> array of limits for longitudinal jerk (activated in AUTONOMOUS operation mode) <code>nominal.lat_acc_lim_for_steer_cmd</code> array of limits for lateral acceleration (activated in AUTONOMOUS operation mode) <code>nominal.lat_jerk_lim_for_steer_cmd</code> array of limits for lateral jerk (activated in AUTONOMOUS operation mode) <code>nominal.steer_cmd_lim</code> array of limits for steering angle (activated in AUTONOMOUS operation mode) <code>nominal.steer_rate_lim_for_steer_cmd</code> array of limits for command steering rate (activated in AUTONOMOUS operation mode) <code>nominal.lat_jerk_lim_for_steer_rate</code> double limit for lateral jerk constraint on steering rate (activated in AUTONOMOUS operation mode) <code>nominal.steer_cmd_diff_lim_from_current_steer</code> array of limits for difference between current and command steering angle (activated in AUTONOMOUS operation mode) <code>on_transition.vel_lim</code> double limit of longitudinal velocity (activated in TRANSITION operation mode) <code>on_transition.reference_speed_points</code> velocity point used as a reference when calculate control command limit (activated in TRANSITION operation mode). The size of this array must be equivalent to the size of the limit array. <code>on_transition.lon_acc_lim_for_lon_vel</code> array of limits for longitudinal acceleration (activated in TRANSITION operation mode) <code>on_transition.lon_jerk_lim_for_lon_acc</code> array of limits for longitudinal jerk (activated in TRANSITION operation mode) <code>on_transition.lat_acc_lim_for_steer_cmd</code> array of limits for lateral acceleration (activated in TRANSITION operation mode) <code>on_transition.lat_jerk_lim_for_steer_cmd</code> array of limits for lateral jerk (activated in TRANSITION operation mode) <code>on_transition.steer_cmd_lim</code> array of limits for steering angle (activated in TRANSITION operation mode) <code>on_transition.steer_rate_lim_for_steer_cmd</code> array of limits for command steering rate (activated in TRANSITION operation mode) <code>on_transition.lat_jerk_lim_for_steer_rate</code> double limit for lateral jerk constraint on steering rate (activated in TRANSITION operation mode) <code>on_transition.steer_cmd_diff_lim_from_current_steer</code> array of limits for difference between current and command steering angle (activated in TRANSITION operation mode)"},{"location":"control/autoware_vehicle_cmd_gate/#parameter-naming-convention","title":"Parameter Naming Convention","text":"<p>The parameters follow specific naming patterns to clearly distinguish between different types of constraints and their relationships:</p>"},{"location":"control/autoware_vehicle_cmd_gate/#pattern-1-constraint_lim_for_target","title":"Pattern 1: <code>[constraint]_lim_for_[target]</code>","text":"<ul> <li>Format: <code>[physical_constraint]_lim_for_[controlled_variable]</code></li> <li>Description: Defines limits based on physical constraints (acceleration, jerk, etc.) applied to control variables</li> </ul>"},{"location":"control/autoware_vehicle_cmd_gate/#pattern-2-target_constraint_lim_from_reference","title":"Pattern 2: <code>[target]_[constraint]_lim_from_[reference]</code>","text":"<ul> <li>Format: <code>[controlled_variable]_[constraint_type]_lim_from_[reference_variable]</code></li> <li>Description: Defines limits on the difference or deviation of a control variable from a reference value</li> </ul>"},{"location":"control/autoware_vehicle_cmd_gate/#pattern-3-target_lim","title":"Pattern 3: <code>[target]_lim</code>","text":"<ul> <li>Format: <code>[controlled_variable]_lim</code></li> <li>Description: Defines absolute limits for control variables</li> </ul>"},{"location":"control/autoware_vehicle_cmd_gate/#functionality","title":"Functionality","text":""},{"location":"control/autoware_vehicle_cmd_gate/#main-functionality","title":"Main Functionality","text":"<ul> <li>Receive multiple control commands (from Autoware planning, emergency handler, remote control, etc.) and select one to forward to the vehicle.</li> <li>Apply a final guard on the selected command to enforce absolute safety limits (e.g., maximum steering rate). This is not a comfort filter.</li> <li>Enforce transition guards during mode changes into autonomous driving (e.g., remote\u2192autonomous, manual\u2192autonomous) to limit sudden changes. Integration with the Operation Transition Manager is recommended, though code boundaries should be maintained due to its complexity.</li> </ul>"},{"location":"control/autoware_vehicle_cmd_gate/#sub-functionality","title":"Sub-Functionality","text":"<ul> <li>Check heartbeat signals to verify connectivity for each input (e.g., emergency external heartbeat).</li> <li>Publish status indicating whether the final guard is active. Active guard in autonomous mode implies an unexpected constraint in command generation and requires attention.</li> <li>Leverage guard status during mode transitions to notify operators/drivers that a strong constraint is active (focus on \"transition in progress\" rather than simple filter activation).</li> </ul>"},{"location":"control/autoware_vehicle_cmd_gate/#filter-function","title":"Filter function","text":"<p>This module incorporates a limitation filter to the control command right before its published. Primarily for safety, this filter restricts the output range of all control commands published through Autoware.</p> <p>The limitation values are calculated based on the 1D interpolation of the limitation array parameters. Here is an example for the longitudinal jerk limit.</p> <p></p> <p>Notation: this filter is not designed to enhance ride comfort. Its main purpose is to detect and remove abnormal values in the control outputs during the final stages of Autoware. If this filter is frequently active, it implies the control module may need tuning. If you're aiming to smoothen the signal via a low-pass filter or similar techniques, that should be handled in the control module. When the filter is activated, the topic <code>~/is_filter_activated</code> is published.</p> <p>Notation 2: If you use vehicles in which the driving force is controlled by the accelerator/brake pedal, the jerk limit, denoting the pedal rate limit, must be sufficiently relaxed at low speeds. Otherwise, quick pedal changes at start/stop will not be possible, resulting in slow starts and creep down on hills. This functionality for starting/stopping was embedded in the source code but was removed because it was complex and could be achieved by parameters.</p>"},{"location":"control/autoware_vehicle_cmd_gate/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"control/autoware_vehicle_cmd_gate/#external-emergency-heartbeat","title":"External Emergency Heartbeat","text":"<p>The parameter <code>check_external_emergency_heartbeat</code> (true by default) enables an emergency stop request from external modules. This feature requires a <code>~/input/external_emergency_stop_heartbeat</code> topic for health monitoring of the external module, and the vehicle_cmd_gate module will not start without the topic. The <code>check_external_emergency_heartbeat</code> parameter must be false when the \"external emergency stop\" function is not used.</p>"},{"location":"control/autoware_vehicle_cmd_gate/#commands-on-mode-changes","title":"Commands on Mode changes","text":"<p>Output commands' topics: <code>turn_indicators_cmd</code>, <code>hazard_light</code> and <code>gear_cmd</code> are selected based on <code>gate_mode</code>. However, to ensure the continuity of commands, these commands will not change until the topics of new input commands arrive, even if a mode change occurs.</p>"},{"location":"control/autoware_vehicle_cmd_gate/#caution","title":"Caution","text":"<ul> <li>This node depends on the Operation Mode Transition Manager for Engage state transitions at the design level.</li> <li>Tests are essential and must be retained.</li> </ul>"},{"location":"evaluator/autoware_control_evaluator/","title":"Control Evaluator","text":""},{"location":"evaluator/autoware_control_evaluator/#control-evaluator","title":"Control Evaluator","text":""},{"location":"evaluator/autoware_control_evaluator/#purpose","title":"Purpose","text":"<p>This package provides nodes that generate metrics to evaluate the quality of control.</p> <p>It publishes metric information about control modules' outputs as well as the ego vehicle's current kinematics and position.</p>"},{"location":"evaluator/autoware_control_evaluator/#evaluated-metrics","title":"Evaluated metrics","text":"<p>The control evaluator uses the metrics defined in <code>include/autoware/control_evaluator/metrics/metric.hpp</code>to calculate deviations in yaw and lateral distance from the ego's set-point. The control_evaluator can also be customized to offer metrics/evaluation about other control modules. Currently, the control_evaluator offers a simple metric output based on the autonomous_emergency_braking node's output, but this functionality can be extended to evaluate other control modules' performance.</p>"},{"location":"evaluator/autoware_control_evaluator/#kinematics-output","title":"Kinematics output","text":"<p>The control evaluator module also constantly publishes information regarding the ego vehicle's kinematics and position. It publishes the current ego lane id with the longitudinal <code>s</code> and lateral <code>t</code> arc coordinates. It also publishes the current ego speed, acceleration and jerk in its metric messages.</p> <p>This information can be used by other nodes to establish automated evaluation using rosbags: by crosschecking the ego position and kinematics with the evaluated control module's output, it is possible to judge if the evaluated control modules reacted in a satisfactory way at certain interesting points of the rosbag reproduction.</p>"},{"location":"evaluator/autoware_kinematic_evaluator/","title":"autoware_kinematic_evaluator","text":""},{"location":"evaluator/autoware_kinematic_evaluator/#autoware_kinematic_evaluator","title":"autoware_kinematic_evaluator","text":"<p>TBD</p>"},{"location":"evaluator/autoware_kinematic_evaluator/#parameters","title":"Parameters","text":"Name Type Description Default Range output_file string Name of the file to which kinematic metrics are written. If empty, metrics are not written to a file. kinematic_metrics.results N/A selected_metrics string The specific metrics selected for evaluation. velocity_stats N/A"},{"location":"evaluator/autoware_localization_evaluator/","title":"Localization Evaluator","text":""},{"location":"evaluator/autoware_localization_evaluator/#localization-evaluator","title":"Localization Evaluator","text":"<p>The Localization Evaluator evaluates the performance of the localization system and provides metrics</p>"},{"location":"evaluator/autoware_localization_evaluator/#parameters","title":"Parameters","text":"Name Type Description Default Range output_file string if empty, metrics are not written to file loc_metrics.results N/A selected_metrics array metrics to be calculated ['lateral_error', 'absolute_error'] N/A"},{"location":"evaluator/autoware_perception_online_evaluator/","title":"Perception Evaluator","text":""},{"location":"evaluator/autoware_perception_online_evaluator/#perception-evaluator","title":"Perception Evaluator","text":"<p>A node for evaluating the output of perception systems.</p>"},{"location":"evaluator/autoware_perception_online_evaluator/#purpose","title":"Purpose","text":"<p>This module allows for the evaluation of how accurately perception results are generated without the need for annotations. It is capable of confirming performance and can evaluate results from a few seconds prior, enabling online execution.</p>"},{"location":"evaluator/autoware_perception_online_evaluator/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The evaluated metrics are as follows:</p> <ul> <li>predicted_path_deviation</li> <li>predicted_path_deviation_variance</li> <li>lateral_deviation</li> <li>yaw_deviation</li> <li>yaw_rate</li> <li>total_objects_count</li> <li>average_objects_count</li> <li>interval_objects_count</li> </ul>"},{"location":"evaluator/autoware_perception_online_evaluator/#predicted-path-deviation-predicted-path-deviation-variance","title":"Predicted Path Deviation / Predicted Path Deviation Variance","text":"<p>Compare the predicted path of past objects with their actual traveled path to determine the deviation for MOVING OBJECTS. For each object, calculate the mean distance between the predicted path points and the corresponding points on the actual path, up to the specified time step. In other words, this calculates the Average Displacement Error (ADE). The target object to be evaluated is the object from \\(T_N\\) seconds ago, where \\(T_N\\) is the maximum value of the prediction time horizon \\([T_1, T_2, ..., T_N]\\).</p> <p>[!NOTE] The object from \\(T_N\\) seconds ago is the target object for all metrics. This is to unify the time of the target object across metrics.</p> <p></p> \\[ \\begin{align} n_{points} = T / dt \\\\ ADE = \\Sigma_{i=1}^{n_{points}} d_i / n_{points}\u3000\\\\ Var = \\Sigma_{i=1}^{n_{points}} (d_i - ADE)^2 / n_{points} \\end{align} \\] <ul> <li>\\(n_{points}\\) : Number of points in the predicted path</li> <li>\\(T\\) : Time horizon for prediction evaluation.</li> <li>\\(dt\\) : Time interval of the predicted path</li> <li>\\(d_i\\) : Distance between the predicted path and the actual traveled path at path point \\(i\\)</li> <li>\\(ADE\\) : Mean deviation of the predicted path for the target object.</li> <li>\\(Var\\) : Variance of the predicted path deviation for the target object.</li> </ul> <p>The final predicted path deviation metrics are calculated by averaging the mean deviation of the predicted path for all objects of the same class, and then calculating the mean, maximum, and minimum values of the mean deviation.</p> <p></p> \\[ \\begin{align} ADE_{mean} = \\Sigma_{j=1}^{n_{objects}} ADE_j / n_{objects} \\\\ ADE_{max} = max(ADE_j) \\\\ ADE_{min} = min(ADE_j) \\end{align} \\] \\[ \\begin{align} Var_{mean} = \\Sigma_{j=1}^{n_{objects}} Var_j / n_{objects} \\\\ Var_{max} = max(Var_j) \\\\ Var_{min} = min(Var_j) \\end{align} \\] <ul> <li>\\(n_{objects}\\) : Number of objects</li> <li>\\(ADE_{mean}\\) : Mean deviation of the predicted path through all objects</li> <li>\\(ADE_{max}\\) : Maximum deviation of the predicted path through all objects</li> <li>\\(ADE_{min}\\) : Minimum deviation of the predicted path through all objects</li> <li>\\(Var_{mean}\\) : Mean variance of the predicted path deviation through all objects</li> <li>\\(Var_{max}\\) : Maximum variance of the predicted path deviation through all objects</li> <li>\\(Var_{min}\\) : Minimum variance of the predicted path deviation through all objects</li> </ul> <p>The actual metric name is determined by the object class and time horizon. For example, <code>predicted_path_deviation_variance_CAR_5.00</code></p>"},{"location":"evaluator/autoware_perception_online_evaluator/#lateral-deviation","title":"Lateral Deviation","text":"<p>Calculates lateral deviation between the smoothed traveled trajectory and the perceived position to evaluate the stability of lateral position recognition for MOVING OBJECTS. The smoothed traveled trajectory is calculated by applying a centered moving average filter whose window size is specified by the parameter <code>smoothing_window_size</code>. The lateral deviation is calculated by comparing the smoothed traveled trajectory with the perceived position of the past object whose timestamp is \\(T=T_n\\) seconds ago. For stopped objects, the smoothed traveled trajectory is unstable, so this metric is not calculated.</p> <p></p>"},{"location":"evaluator/autoware_perception_online_evaluator/#yaw-deviation","title":"Yaw Deviation","text":"<p>Calculates the deviation between the recognized yaw angle of an past object and the yaw azimuth angle of the smoothed traveled trajectory for MOVING OBJECTS. The smoothed traveled trajectory is calculated by applying a centered moving average filter whose window size is specified by the parameter <code>smoothing_window_size</code>. The yaw deviation is calculated by comparing the yaw azimuth angle of smoothed traveled trajectory with the perceived orientation of the past object whose timestamp is \\(T=T_n\\) seconds ago. For stopped objects, the smoothed traveled trajectory is unstable, so this metric is not calculated.</p> <p></p>"},{"location":"evaluator/autoware_perception_online_evaluator/#yaw-rate","title":"Yaw Rate","text":"<p>Calculates the yaw rate of an object based on the change in yaw angle from the previous time step. It is evaluated for STATIONARY OBJECTS and assesses the stability of yaw rate recognition. The yaw rate is calculated by comparing the yaw angle of the past object with the yaw angle of the object received in the previous cycle. Here, t2 is the timestamp that is \\(T_n\\) seconds ago.</p> <p></p>"},{"location":"evaluator/autoware_perception_online_evaluator/#object-counts","title":"Object Counts","text":"<p>Counts the number of detections for each object class within the specified detection range. These metrics are measured for the most recent object not past objects.</p> <p></p> <p>In the provided illustration, the range \\(R\\) is determined by a combination of lists of radii (e.g., \\(r_1, r_2, \\ldots\\)) and heights (e.g., \\(h_1, h_2, \\ldots\\)). For example,</p> <ul> <li>the number of CAR in range \\(R = (r_1, h_1)\\) equals 1</li> <li>the number of CAR in range \\(R = (r_1, h_2)\\) equals 2</li> <li>the number of CAR in range \\(R = (r_2, h_1)\\) equals 3</li> <li>the number of CAR in range \\(R = (r_2, h_2)\\) equals 4</li> </ul>"},{"location":"evaluator/autoware_perception_online_evaluator/#total-object-count","title":"Total Object Count","text":"<p>Counts the number of unique objects for each class within the specified detection range. The total object count is calculated as follows:</p> \\[ \\begin{align} \\text{Total Object Count (Class, Range)} = \\left| \\bigcup_{t=0}^{T_{\\text{now}}} \\{ \\text{uuid} \\mid \\text{class}(t, \\text{uuid}) = C \\wedge \\text{position}(t, \\text{uuid}) \\in R \\} \\right| \\end{align} \\] <p>where:</p> <ul> <li>\\(\\bigcup\\) represents the union across all frames from \\(t = 0\\) to \\(T_{\\text{now}}\\), which ensures that each uuid is counted only once.</li> <li>\\(\\text{class}(t, \\text{uuid}) = C\\) specifies that the object with uuid at time \\(t\\) belongs to class \\(C\\).</li> <li>\\(\\text{position}(t, \\text{uuid}) \\in R\\) indicates that the object with uuid at time \\(t\\) is within the specified range \\(R\\).</li> <li>\\(\\left| \\{ \\ldots \\} \\right|\\) denotes the cardinality of the set, which counts the number of unique uuids that meet the class and range criteria across all considered times.</li> </ul>"},{"location":"evaluator/autoware_perception_online_evaluator/#average-object-count","title":"Average Object Count","text":"<p>Counts the average number of objects for each class within the specified detection range. This metric measures how many objects were detected in one frame, without considering uuids. The average object count is calculated as follows:</p> \\[ \\begin{align} \\text{Average Object Count (Class, Range)} = \\frac{1}{N} \\sum_{t=0}^{T_{\\text{now}}} \\left| \\{ \\text{object} \\mid \\text{class}(t, \\text{object}) = C \\wedge \\text{position}(t, \\text{object}) \\in R \\} \\right| \\end{align} \\] <p>where:</p> <ul> <li>\\(N\\) represents the total number of frames within the time period time to \\(T\\_{\\text{now}}\\) (it is precisely <code>detection_count_purge_seconds</code>)</li> <li>\\(text{object}\\) denotes the number of objects that meet the class and range criteria at time \\(t\\).</li> </ul>"},{"location":"evaluator/autoware_perception_online_evaluator/#interval-object-count","title":"Interval Object Count","text":"<p>Counts the average number of objects for each class within the specified detection range over the last <code>objects_count_window_seconds</code>. This metric measures how many objects were detected in one frame, without considering uuids. The interval object count is calculated as follows:</p> \\[ \\begin{align} \\text{Interval Object Count (Class, Range)} = \\frac{1}{W} \\sum_{t=T_{\\text{now}} - T_W}^{T_{\\text{now}}} \\left| \\{ \\text{object} \\mid \\text{class}(t, \\text{object}) = C \\wedge \\text{position}(t, \\text{object}) \\in R \\} \\right| \\end{align} \\] <p>where:</p> <ul> <li>\\(W\\) represents the total number of frames within the last <code>objects_count_window_seconds</code>.</li> <li>\\(T_W\\) represents the time window <code>objects_count_window_seconds</code></li> </ul>"},{"location":"evaluator/autoware_perception_online_evaluator/#inputs-outputs","title":"Inputs / Outputs","text":"Name Type Description <code>~/input/objects</code> <code>autoware_perception_msgs::msg::PredictedObjects</code> The predicted objects to evaluate. <code>~/metrics</code> <code>tier4_metric_msgs::msg::MetricArray</code> Metric information about perception accuracy. <code>~/markers</code> <code>visualization_msgs::msg::MarkerArray</code> Visual markers for debugging and visualization."},{"location":"evaluator/autoware_perception_online_evaluator/#parameters","title":"Parameters","text":"Name Type Description <code>selected_metrics</code> List Metrics to be evaluated, such as lateral deviation, yaw deviation, and predicted path deviation. <code>smoothing_window_size</code> Integer Determines the window size for smoothing path, should be an odd number. <code>prediction_time_horizons</code> list[double] Time horizons for prediction evaluation in seconds. <code>stopped_velocity_threshold</code> double threshold velocity to check if vehicle is stopped <code>detection_radius_list</code> list[double] Detection radius for objects to be evaluated.(used for objects count only) <code>detection_height_list</code> list[double] Detection height for objects to be evaluated. (used for objects count only) <code>detection_count_purge_seconds</code> double Time window for purging object detection counts. <code>objects_count_window_seconds</code> double Time window for keeping object detection counts. The number of object detections within this time window is stored in <code>detection_count_vector_</code> <code>target_object.*.check_lateral_deviation</code> bool Whether to check lateral deviation for specific object types (car, truck, etc.). <code>target_object.*.check_yaw_deviation</code> bool Whether to check yaw deviation for specific object types (car, truck, etc.). <code>target_object.*.check_predicted_path_deviation</code> bool Whether to check predicted path deviation for specific object types (car, truck, etc.). <code>target_object.*.check_yaw_rate</code> bool Whether to check yaw rate for specific object types (car, truck, etc.). <code>target_object.*.check_total_objects_count</code> bool Whether to check total object count for specific object types (car, truck, etc.). <code>target_object.*.check_average_objects_count</code> bool Whether to check average object count for specific object types (car, truck, etc.). <code>target_object.*.check_interval_average_objects_count</code> bool Whether to check interval average object count for specific object types (car, truck, etc.). <code>debug_marker.*</code> bool Debugging parameters for marker visualization (history path, predicted path, etc.)."},{"location":"evaluator/autoware_perception_online_evaluator/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>It is assumed that the current positions of PredictedObjects are reasonably accurate.</p>"},{"location":"evaluator/autoware_perception_online_evaluator/#future-extensions-unimplemented-parts","title":"Future extensions / Unimplemented parts","text":"<ul> <li>Increase rate in recognition per class</li> <li>Metrics for objects with strange physical behavior (e.g., going through a fence)</li> <li>Metrics for splitting objects</li> <li>Metrics for problems with objects that are normally stationary but move</li> <li>Disappearing object metrics</li> </ul>"},{"location":"evaluator/autoware_planning_evaluator/","title":"Planning Evaluator","text":""},{"location":"evaluator/autoware_planning_evaluator/#planning-evaluator","title":"Planning Evaluator","text":""},{"location":"evaluator/autoware_planning_evaluator/#purpose","title":"Purpose","text":"<p>This package provides nodes that generate various metrics to evaluate the quality of planning.</p> <p>Metrics can be published in real time and saved to a JSON file when the node is shut down:</p> <ul> <li><code>metrics_for_publish</code>:<ul> <li>Metrics listed in <code>metrics_for_publish</code> are calculated and published to the topic.</li> </ul> </li> </ul> <ul> <li><code>metrics_for_output</code>:<ul> <li>Metrics listed in <code>metrics_for_output</code> are saved to a JSON file when the node shuts down (if <code>output_metrics</code> is set to <code>true</code>).</li> <li>These metrics include statistics derived from <code>metrics_for_publish</code> and additional information such as parameters and descriptions.</li> </ul> </li> </ul>"},{"location":"evaluator/autoware_planning_evaluator/#metrics","title":"Metrics","text":"<p>All possible metrics are defined in the <code>Metric</code> enumeration located in:</p> <ul> <li><code>include/autoware/planning_evaluator/metrics/metric.hpp</code></li> <li><code>include/autoware/planning_evaluator/metrics/output_metric.hpp</code></li> </ul> <p>These files also provide string conversions and human-readable descriptions for use in output files.</p>"},{"location":"evaluator/autoware_planning_evaluator/#metric-classifications","title":"Metric Classifications","text":""},{"location":"evaluator/autoware_planning_evaluator/#by-data-type","title":"By Data Type","text":"<ol> <li> <p>Statistics-based Metrics:</p> <ul> <li>Calculated using <code>autoware_utils::Accumulator</code>, which tracks minimum, maximum, mean, and count values.</li> <li>Sub-metrics: <code>/mean</code>, <code>/min</code>, <code>/max</code>, and <code>/count</code>.</li> </ul> </li> <li> <p>Value-based Metrics:</p> <ul> <li>Metrics with a single value.</li> <li>Sub-metrics: <code>/value</code>.</li> <li>Some metrics with older implementations use the statistics-based format of <code>/mean</code>, <code>/min</code>, <code>/max</code>, but all values are the same.</li> </ul> </li> <li> <p>Count-based Metrics:</p> <ul> <li>Count occurrences of specific events over a time period.</li> <li>Sub-metrics: <code>/count</code>, or <code>/count_in_duration</code>.</li> </ul> </li> </ol>"},{"location":"evaluator/autoware_planning_evaluator/#by-purpose","title":"By Purpose","text":"<ol> <li>Trajectory Metrics</li> <li>Trajectory Deviation Metrics</li> <li>Trajectory Stability Metrics</li> <li>Trajectory Obstacle Metrics</li> <li>Modified Goal Metrics</li> <li>Planning Factor Metrics</li> <li>Steering Metrics</li> <li>Blinker Metrics</li> <li>Other Information</li> </ol>"},{"location":"evaluator/autoware_planning_evaluator/#detailed-metrics","title":"Detailed Metrics","text":""},{"location":"evaluator/autoware_planning_evaluator/#trajectory-metrics","title":"Trajectory Metrics","text":"<p>Evaluates the current trajectory <code>T(0)</code> itself. Metrics are calculated and published when a trajectory is received.</p>"},{"location":"evaluator/autoware_planning_evaluator/#implemented-metrics","title":"Implemented metrics","text":"<ul> <li><code>curvature</code>: Statistics of curvature at each trajectory point.<ul> <li>Sub-metrics to publish: <code>/mean</code>, <code>/min</code>, <code>/max</code>.</li> <li>Sub-metrics to output: The same as above but take the published data as data point instead of each trajectory point.</li> </ul> </li> </ul> <ul> <li><code>point_interval</code>: Statistics of distances between consecutive trajectory points.<ul> <li>Sub-metrics to publish: <code>/mean</code>, <code>/min</code>, <code>/max</code>.</li> <li>Sub-metrics to output: The same as above but take the published data as data point instead of each trajectory point.</li> </ul> </li> </ul> <ul> <li><code>relative_angle</code>: Statistics of angles between consecutive trajectory points.<ul> <li>Parameters: <code>trajectory.min_point_dist_m</code> (minimum distance between points).</li> <li>Sub-metrics to publish: <code>/mean</code>, <code>/min</code>, <code>/max</code>.</li> <li>Sub-metrics to output: The same as above but take the published data as data point instead of each trajectory point.</li> </ul> </li> </ul> <ul> <li><code>resampled_relative_angle</code>: Similar to <code>relative_angle</code>, but considers a point at a fixed distance (e.g., half the vehicle length) for angle calculation from the current point as the next point to calculate the relative angle, instead of using the immediately adjacent point.</li> </ul> <ul> <li><code>length</code>: Total trajectory length.<ul> <li>Sub-metrics: Value-based metric, but using the statistics-based format of <code>/mean</code>, <code>/min</code>, <code>/max</code> with the same value.</li> <li>Sub-metrics to output: <code>/mean</code>, <code>/min</code>, <code>/max</code> for the published data.</li> </ul> </li> </ul> <ul> <li><code>duration</code>: Expected driving time to travel the trajectory.<ul> <li>Sub-metrics: Value-based metric, but using the statistics-based format of <code>/mean</code>, <code>/min</code>, <code>/max</code> with the same value.</li> <li>Sub-metrics to output: <code>/mean</code>, <code>/min</code>, <code>/max</code> for the published data.</li> </ul> </li> </ul> <ul> <li><code>velocity</code>: Statistics of velocity at each trajectory point.<ul> <li>Sub-metrics to publish: <code>/mean</code>, <code>/min</code>, <code>/max</code>.</li> <li>Sub-metrics to output: The same as above but take the published data as data point instead of each trajectory point.</li> </ul> </li> </ul> <ul> <li><code>acceleration</code>: Statistics of acceleration at each trajectory point.<ul> <li>Sub-metrics to publish: <code>/mean</code>, <code>/min</code>, <code>/max</code>.</li> <li>Sub-metrics to output: The same as above but take the published data as data point instead of each trajectory point.</li> </ul> </li> </ul> <ul> <li><code>jerk</code>: Statistics of jerk at each trajectory point.<ul> <li>Sub-metrics to publish: <code>/mean</code>, <code>/min</code>, <code>/max</code>.</li> <li>Sub-metrics to output: The same as above but take the published data as data point instead of each trajectory point.</li> </ul> </li> </ul>"},{"location":"evaluator/autoware_planning_evaluator/#trajectory-deviation-metrics","title":"Trajectory Deviation Metrics","text":"<p>Evaluates the trajectory deviation by comparing the trajectory <code>T(0)</code> and the reference trajectory.</p> <p>Metrics are calculated and publish only when the node receives a trajectory.</p> <p>The following information are used to calculate metrics:</p> <ul> <li>the trajectory <code>T(0)</code>.</li> <li>the reference trajectory assumed to be used as the reference to plan <code>T(0)</code>.</li> </ul>"},{"location":"evaluator/autoware_planning_evaluator/#implemented-metrics_1","title":"Implemented metrics","text":"<ul> <li><code>lateral_deviation</code>: Statistics of the lateral deviation between trajectory points and the closest reference trajectory points.<ul> <li>Sub-metrics to publish: <code>/mean</code>, <code>/min</code>, <code>/max</code>.</li> <li>Sub-metrics to output: The same as above but take the published data as data point instead of each trajectory point.</li> </ul> </li> </ul> <ul> <li><code>yaw_deviation</code>: Statistics of the yaw deviation between trajectory points and the closest reference trajectory points.<ul> <li>Sub-metrics to publish: <code>/mean</code>, <code>/min</code>, <code>/max</code>.</li> <li>Sub-metrics to output: The same as above but take the published data as data point instead of each trajectory point.</li> </ul> </li> </ul> <ul> <li><code>velocity_deviation</code>: Statistics of the velocity deviation between trajectory points and the closest reference trajectory points.<ul> <li>Sub-metrics to publish: <code>/mean</code>, <code>/min</code>, <code>/max</code>.</li> <li>Sub-metrics to output: The same as above but take the published data as data point instead of each trajectory point.</li> </ul> </li> </ul>"},{"location":"evaluator/autoware_planning_evaluator/#trajectory-stability-metrics","title":"Trajectory Stability Metrics","text":"<p>Evaluates the trajectory stability by comparing the trajectory <code>T(0)</code> and previous trajectory <code>T(-1)</code>.</p> <p>Metrics are calculated and publish only when the node receives a trajectory.</p> <p>The following information are used to calculate metrics:</p> <ul> <li>the trajectory <code>T(0)</code> itself.</li> <li>the previous trajectory <code>T(-1)</code>.</li> <li>the current ego odometry.</li> </ul>"},{"location":"evaluator/autoware_planning_evaluator/#implemented-metrics_2","title":"Implemented metrics","text":"<p><code>stability</code>: Statistics of the lateral deviation between <code>T(0)</code> and <code>T(-1)</code> within a lookahead duration and distance.</p> <ul> <li>Parameters: <code>trajectory.lookahead.max_time_s</code>, <code>trajectory.lookahead.max_dist_m</code>.</li> <li>Sub-metrics to publish: <code>/mean</code>, <code>/min</code>, <code>/max</code>.</li> <li>Sub-metrics to output: The same as above but take the published data as data point instead of each trajectory point.</li> </ul> <ul> <li><code>stability_frechet</code>: Frechet distance between <code>T(0)</code> and <code>T(-1)</code> within a lookahead duration and distance.<ul> <li>Parameters: Same as <code>stability</code>.</li> <li>Sub-metrics to publish: <code>/mean</code>, <code>/min</code>, <code>/max</code>.</li> <li>Sub-metrics to output: The same as above but take the published data as data point instead of each trajectory point.</li> </ul> </li> </ul> <ul> <li><code>lateral_trajectory_displacement_local</code>: Absolute lateral displacement between <code>T(0)</code> and <code>T(-1)</code> at the ego position.<ul> <li>Sub-metrics to publish: Value-based metric, but using the statistics-based format.</li> <li>Sub-metrics to output: <code>/mean</code>, <code>/min</code>, <code>/max</code> for the published data.</li> </ul> </li> </ul> <ul> <li><code>lateral_trajectory_displacement_lookahead</code>: Statistics of absolute lateral displacement between <code>T(0)</code> and <code>T(-1)</code> within a lookahead duration.</li> <li>Parameters: <code>trajectory.evaluation_time_s</code>.</li> <li>Sub-metrics to publish: <code>/mean</code>, <code>/min</code>, <code>/max</code>.</li> <li>Sub-metrics to output: The same as above but take the published data as data point instead of each trajectory point.</li> </ul>"},{"location":"evaluator/autoware_planning_evaluator/#trajectory-obstacle-metrics","title":"Trajectory Obstacle Metrics","text":"<p>Evaluate the safety of <code>T(0)</code> with respect to obstacles.</p> <p>Metrics are calculated and publish only when the node receives a trajectory.</p> <p>The following information are used to calculate metrics:</p> <ul> <li>the trajectory <code>T(0)</code>.</li> <li>the set of objects in the environment.</li> </ul>"},{"location":"evaluator/autoware_planning_evaluator/#implemented-metrics_3","title":"Implemented metrics","text":"<ul> <li><code>obstacle_distance</code>: Statistics of the distance between each object and the closest trajectory point.<ul> <li>Sub-metrics to publish: <code>/mean</code>, <code>/min</code>, <code>/max</code>.</li> <li>Sub-metrics to output: The same as above but take the published data as data point instead of each trajectory point.</li> </ul> </li> </ul> <ul> <li><code>obstacle_ttc</code>: Statistics of the time-to-collision (TTC) of objects and ego. Predicted path of objects and ego trajectory are considered.<ul> <li>Parameters: <code>obstacle.dist_thr_m</code> (distance margin to consider a collision occurs between object footprints and ego trajectory footprints).</li> <li>Sub-metrics to publish: <code>/mean</code>, <code>/min</code>, <code>/max</code>.</li> <li>Sub-metrics to output: The same as above but take the published data as data point instead of each trajectory point.</li> </ul> </li> </ul>"},{"location":"evaluator/autoware_planning_evaluator/#modified-goal-metrics","title":"Modified Goal Metrics","text":"<p>Evaluate deviations between the modified goal and the ego position.</p> <p>Metrics are calculated and publish only when the node receives a modified goal message.</p>"},{"location":"evaluator/autoware_planning_evaluator/#implemented-metrics_4","title":"Implemented metrics","text":"<ul> <li><code>modified_goal_longitudinal_deviation</code>: Statistics of the longitudinal deviation between the modified goal and the current ego position.<ul> <li>Sub-metrics to publish: Value-based metric, but using the statics-based format.</li> <li>Sub-metrics to output: <code>/mean</code>, <code>/min</code>, <code>/max</code> for the published data.</li> </ul> </li> </ul> <ul> <li><code>modified_goal_lateral_deviation</code>: Statistics of the lateral deviation between the modified goal and the current ego position.<ul> <li>Sub-metrics to publish: Value-based metric, but using the statics-based format.</li> <li>Sub-metrics to output: <code>/mean</code>, <code>/min</code>, <code>/max</code> for the published data.</li> </ul> </li> </ul> <ul> <li><code>modified_goal_yaw_deviation</code>: Statistics of the yaw deviation between the modified goal and the current ego position.<ul> <li>Sub-metrics to publish: Value-based metric, but using the statics-based format.</li> <li>Sub-metrics to output: <code>/mean</code>, <code>/min</code>, <code>/max</code> for the published data.</li> </ul> </li> </ul>"},{"location":"evaluator/autoware_planning_evaluator/#planning-factor-metrics","title":"Planning Factor Metrics","text":"<p>Evaluates the behavior of each planning module by checking planning factors.</p> <p>Metrics are calculated and publish only when the node receives that planning factors published by each module.</p> <p>The modules listed in the <code>module_list</code> in the parameter file are evaluated.</p>"},{"location":"evaluator/autoware_planning_evaluator/#implemented-metrics_5","title":"Implemented metrics","text":"<ul> <li><code>stop_decision</code>: Evaluate stop decisions for each module.<ul> <li>Parameters:<ul> <li><code>stop_decision.time_count_threshold_s</code>: time threshold to count a stop decision as a new one.</li> <li><code>stop_decision.dist_count_threshold_m</code>: distance threshold to count a stop decision as a new one.</li> <li><code>stop_decision.topic_prefix</code>: topic prefix for planning factors</li> <li><code>stop_decision.module_list</code>: list of modules' names to check, the <code>{topic_prefix}/{module_name}</code> should be a valid topic.</li> </ul> </li> <li>Sub-metrics to publish:<ul> <li><code>/{module_name}/keep_duration</code>(value-based): the time current stop decision keeps.</li> <li><code>/{module_name}/distance_to_stop</code>(value-based): the distance to the stop line.</li> <li><code>/{module_name}/count</code>(count-based): the stop decision index, in other words, the number of stop decisions made by the module.</li> </ul> </li> <li>Sub-metrics to output:<ul> <li><code>/{module_name}/keep_duration/mean</code>, <code>/{module_name}/keep_duration/min</code>, <code>/{module_name}/keep_duration/max</code>: the statistics of the published keep_duration.</li> <li><code>/{module_name}/count</code>: the total number of stop decisions.</li> </ul> </li> </ul> </li> </ul> <ul> <li><code>abnormal_stop_decision</code>: Evaluate abnormal stop decisions for each module.<ul> <li>A stop decision is considered as abnormal if the ego cannot stop with the current velocity and the maximum deceleration limit.</li> <li>Parameters:<ul> <li><code>stop_decision.abnormal_deceleration_threshold_mps2</code>: maximum deceleration limit to consider the stop decision as abnormal.</li> <li>Other parameters are shared with <code>stop_decision</code>.</li> </ul> </li> <li>Sub-metrics to publish: The same as <code>stop_decision</code>.</li> <li>Sub-metrics to output: The same as <code>stop_decision</code>.</li> </ul> </li> </ul>"},{"location":"evaluator/autoware_planning_evaluator/#blinker-metrics","title":"Blinker Metrics","text":"<p>Evaluates the blinker status of the vehicle.</p> <p>Metrics are calculated and publish only when the node receives a turn indicators report message.</p>"},{"location":"evaluator/autoware_planning_evaluator/#implemented-metrics_6","title":"Implemented metrics","text":"<ul> <li><code>blinker_change_count</code>: Count blinker status changes.<ul> <li>A change is counted when the blinker status transitions from off/left to right, or from off/right to left.</li> </ul> </li> <li>Parameters:<ul> <li><code>blinker_change_count.window_duration_s</code>: duration to count the changes for publishing.</li> </ul> </li> <li>Sub-metrics to publish: <code>/count_in_duration</code>.</li> <li>Sub-metrics to output:<ul> <li><code>/count_in_duration/min</code>, <code>/count_in_duration/max</code>, <code>/count_in_duration/mean</code>: the statistics of the published keep_duration.</li> <li><code>/count</code>: total number of changes.</li> </ul> </li> </ul>"},{"location":"evaluator/autoware_planning_evaluator/#steering-metrics","title":"Steering Metrics","text":"<p>Evaluates the steering status of the vehicle.</p> <p>Metrics are calculated and publish only when the node receives a steering report message.</p>"},{"location":"evaluator/autoware_planning_evaluator/#implemented-metrics_7","title":"Implemented metrics","text":"<ul> <li><code>steer_change_count</code>: Count steering rate changes.<ul> <li>A change is counted when the steering rate changes from positive/0 to negative or from negative/0 to positive.</li> <li>Parameters:<ul> <li><code>steer_change_count.window_duration_s</code>: duration to count the changes for publishing.</li> <li><code>steer_change_count.steer_rate_margin</code>: margin to consider the steer rate as 0.</li> </ul> </li> <li>Sub-metrics to publish: <code>/count_in_duration</code>.</li> <li>Sub-metrics to output:<ul> <li><code>/count_in_duration/min</code>, <code>/count_in_duration/max</code>, <code>/count_in_duration/mean</code>: the statistics of the published keep_duration.</li> <li><code>/count</code>: total number of changes.</li> </ul> </li> </ul> </li> </ul>"},{"location":"evaluator/autoware_planning_evaluator/#other-information","title":"Other Information","text":"<p>Additional useful information related to planning:</p>"},{"location":"evaluator/autoware_planning_evaluator/#implemented-metrics_8","title":"Implemented metrics","text":"<ul> <li><code>kinematic_state</code>: Current kinematic state of the vehicle<ul> <li>Sub-metrics to publish:<ul> <li><code>/velocity</code>: current ego velocity.</li> <li><code>/acceleration</code>: current ego acceleration.</li> <li><code>/jerk</code>: current ego jerk.</li> </ul> </li> </ul> </li> </ul> <ul> <li><code>ego_lane_info</code>: Lanelet information.<ul> <li>Sub-metrics to publish:<ul> <li><code>/lanelet_id</code>: ID of the lanelet where the ego is located.</li> <li><code>/s</code>: Arc length of ego position in the lanelet.</li> <li><code>/t</code>: Lateral offset of ego position in the lanelet.</li> </ul> </li> </ul> </li> </ul>"},{"location":"evaluator/autoware_planning_evaluator/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"evaluator/autoware_planning_evaluator/#inputs","title":"Inputs","text":"Name Type Description <code>~/input/trajectory</code> <code>autoware_planning_msgs::msg::Trajectory</code> Main trajectory to evaluate <code>~/input/reference_trajectory</code> <code>autoware_planning_msgs::msg::Trajectory</code> Reference trajectory to use for deviation metrics <code>~/input/objects</code> <code>autoware_perception_msgs::msg::PredictedObjects</code> Obstacles <code>~/input/modified_goal</code> <code>autoware_planning_msgs::msg::PoseWithUuidStamped</code> Modified goal <code>~/input/odometry</code> <code>nav_msgs::msg::Odometry</code> Current odometry of the vehicle <code>~/input/route</code> <code>autoware_planning_msgs::msg::LaneletRoute</code> Route information <code>~/input/vector_map</code> <code>autoware_map_msgs::msg::LaneletMapBin</code> Vector map information <code>~/input/acceleration</code> <code>geometry_msgs::msg::AccelWithCovarianceStamped</code> Current acceleration of the vehicle <code>~/input/steering_status</code> <code>autoware_vehicle_msgs::msg::SteeringReport</code> Current steering of the vehicle <code>~/input/turn_indicators_status</code> <code>autoware_vehicle_msgs::msg::TurnIndicatorsReport</code> Current blinker status of the vehicle <code>{topic_prefix}/{module_name}</code> <code>autoware_internal_planning_msgs::msg::PlanningFactorArray</code> Planning factors of each module to evaluate"},{"location":"evaluator/autoware_planning_evaluator/#outputs","title":"Outputs","text":"<p>Each publishing-based metric is published on the same topic.</p> Name Type Description <code>~/metrics</code> <code>tier4_metric_msgs::msg::MetricArray</code> MetricArray with all published metrics <code>~/debug/processing_time_ms</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> Node processing time in milliseconds <ul> <li>If <code>output_metrics = true</code>, the evaluation node writes the output-based metrics measured during its lifetime   to <code>&lt;ros2_logging_directory&gt;/autoware_metrics/&lt;node_name&gt;-&lt;time_stamp&gt;.json</code> when shut down.</li> </ul>"},{"location":"evaluator/autoware_planning_evaluator/#parameters","title":"Parameters","text":"Name Type Description Default Range ego_frame string reference frame of ego base_link N/A metrics_for_publish array metrics to collect and publish ['curvature', 'point_interval', 'relative_angle', 'resampled_relative_angle', 'length', 'duration', 'velocity', 'acceleration', 'jerk', 'lateral_deviation', 'yaw_deviation', 'velocity_deviation', 'lateral_trajectory_displacement_local', 'lateral_trajectory_displacement_global', 'stability', 'stability_frechet', 'obstacle_distance', 'obstacle_ttc', 'modified_goal_longitudinal_deviation', 'modified_goal_lateral_deviation', 'modified_goal_yaw_deviation', 'stop_decision', 'abnormal_stop_decision', 'blinker_change_count', 'steer_change_count'] N/A metrics_for_output array metrics to output to json file when the node is terminated ['curvature', 'point_interval', 'relative_angle', 'resampled_relative_angle', 'length', 'duration', 'velocity', 'acceleration', 'jerk', 'lateral_deviation', 'yaw_deviation', 'velocity_deviation', 'lateral_trajectory_displacement_local', 'lateral_trajectory_displacement_global', 'stability', 'stability_frechet', 'obstacle_distance', 'obstacle_ttc', 'modified_goal_longitudinal_deviation', 'modified_goal_lateral_deviation', 'modified_goal_yaw_deviation', 'stop_decision', 'abnormal_stop_decision', 'blinker_change_count', 'steer_change_count'] N/A trajectory.min_point_dist_m float minimum distance between two successive points to use for angle calculation 0.1 N/A lookahead.max_dist_m float maximum distance from ego along the trajectory to use for calculation 5.0 N/A lookahead.max_time_s float maximum time ahead of ego along the trajectory to use for calculation 3.0 N/A obstacle.dist_thr_m float distance between ego and the obstacle below which a collision is considered 0.0 N/A stop_decision.topic_prefix string prefix of the topic to subscribe to for planning factors /planning/planning_factors/ N/A stop_decision.time_count_threshold_s float time [s] threshold to count a stop as a new one 60.0 N/A stop_decision.dist_count_threshold_m float distance [m] threshold to count a stop as a new one 5.0 N/A stop_decision.abnormal_deceleration_threshold_mps2 float abnormal deceleration threshold [m/s^2] to count a stop as abnormal 2.0 N/A stop_decision.module_list array list of modules to be checked for stop decision, the <code>topic_prefix</code>/<code>module</code> topic should be available and publish the <code>autoware_internal_planning_msgs::PlanningFactorArray</code> message ['avoidance_by_lane_change', 'behavior_path_planner', 'blind_spot', 'crosswalk', 'detection_area', 'dynamic_obstacle_avoidance', 'dynamic_obstacle_stop', 'goal_planner', 'intersection', 'lane_change_left', 'lane_change_right', 'motion_velocity_planner', 'merge_from_private', 'no_drivable_lane', 'no_stopping_area', 'obstacle_cruise', 'obstacle_slow_down', 'obstacle_stop', 'occlusion_spot', 'out_of_lane', 'run_out', 'side_shift', 'start_planner', 'static_obstacle_avoidance', 'stop_line', 'surround_obstacle_checker', 'traffic_light', 'virtual_traffic_light', 'walkway'] N/A blinker_change_count.window_duration_s float window duration [s] to count blinker changes for publishing 5.0 N/A steer_change_count.window_duration_s float window duration [s] to count steer changes for publishing 5.0 N/A steer_change_count.steer_rate_margin float margin of steer_rate [rad/s] around 0 to count as steer change 0.1 N/A"},{"location":"evaluator/autoware_planning_evaluator/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>There is a strong assumption that when receiving a trajectory <code>T(0)</code>, it has been generated using the last received reference trajectory and objects. This can be wrong if a new reference trajectory or objects are published while <code>T(0)</code> is being calculated.</p> <p>Precision is currently limited by the resolution of the trajectories. It is possible to interpolate the trajectory and reference trajectory to increase precision but would make computation significantly more expensive.</p>"},{"location":"evaluator/autoware_planning_evaluator/#future-extensions-unimplemented-parts","title":"Future extensions / Unimplemented parts","text":"<ul> <li>Use <code>Route</code> or <code>Path</code> messages as reference trajectory.</li> <li>RSS metrics (done in another node https://tier4.atlassian.net/browse/AJD-263).</li> <li><code>motion_evaluator_node</code>.<ul> <li>Node which constructs a trajectory over time from the real motion of ego.</li> <li>Only a proof of concept is currently implemented.</li> </ul> </li> <li>Take into account the shape, not only the centroid of the object for the obstacle metrics.</li> </ul>"},{"location":"evaluator/autoware_scenario_simulator_v2_adapter/","title":"scenario_simulator_v2 Adapter","text":""},{"location":"evaluator/autoware_scenario_simulator_v2_adapter/#scenario_simulator_v2-adapter","title":"scenario_simulator_v2 Adapter","text":""},{"location":"evaluator/autoware_scenario_simulator_v2_adapter/#purpose","title":"Purpose","text":"<p>This package provides a node to convert various messages from the Autoware into <code>tier4_simulation_msgs::msg::UserDefinedValue</code> messages for the scenario_simulator_v2. Currently, this node supports conversion of:</p> <ul> <li><code>tier4_metric_msgs::msg::MetricArray</code> for metric topics</li> </ul>"},{"location":"evaluator/autoware_scenario_simulator_v2_adapter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<ul> <li>For <code>tier4_metric_msgs::msg::MetricArray</code>,   The node subscribes to all topics listed in the parameter <code>metric_topic_list</code>.   Each time such message is received, it is converted into as many <code>UserDefinedValue</code> messages as the number of <code>Metric</code> objects.   The format of the output topic is detailed in the output section.</li> </ul>"},{"location":"evaluator/autoware_scenario_simulator_v2_adapter/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"evaluator/autoware_scenario_simulator_v2_adapter/#inputs","title":"Inputs","text":"<p>The node listens to <code>MetricArray</code> messages on the topics specified in <code>metric_topic_list</code>.</p>"},{"location":"evaluator/autoware_scenario_simulator_v2_adapter/#outputs","title":"Outputs","text":"<p>The node outputs <code>UserDefinedValue</code> messages that are converted from the received messages.</p> <ul> <li>For <code>MetricArray</code> messages,   The name of the output topics are generated from the corresponding input topic, the name of the metric.<ul> <li>For example, we might listen to topic <code>/planning/planning_evaluator/metrics</code> and receive a <code>MetricArray</code> with 2 metrics:<ul> <li>metric with <code>name: \"metricA/x\"</code></li> <li>metric with <code>name: \"metricA/y\"</code></li> </ul> </li> <li>The resulting topics to publish the <code>UserDefinedValue</code> are as follows:<ul> <li><code>/planning/planning_evaluator/metrics/metricA/x</code></li> <li><code>/planning/planning_evaluator/metrics/metricA/y</code></li> </ul> </li> </ul> </li> </ul>"},{"location":"evaluator/autoware_scenario_simulator_v2_adapter/#parameters","title":"Parameters","text":"Name Type Description Default Range metric_topic_list array The topic name list of the processing time. [] N/A"},{"location":"evaluator/autoware_scenario_simulator_v2_adapter/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>Values in the <code>Metric</code> objects of a <code>MetricArray</code> are assumed to be of type <code>double</code>.</p>"},{"location":"evaluator/autoware_scenario_simulator_v2_adapter/#future-extensions-unimplemented-parts","title":"Future extensions / Unimplemented parts","text":""},{"location":"launch/tier4_autoware_api_launch/","title":"tier4_autoware_api_launch","text":""},{"location":"launch/tier4_autoware_api_launch/#tier4_autoware_api_launch","title":"tier4_autoware_api_launch","text":""},{"location":"launch/tier4_autoware_api_launch/#description","title":"Description","text":"<p>This package contains launch files that run nodes to convert Autoware internal topics into consistent API used by external software (e.g., fleet management system, simulator).</p>"},{"location":"launch/tier4_autoware_api_launch/#package-dependencies","title":"Package Dependencies","text":"<p>Please see <code>&lt;exec_depend&gt;</code> in <code>package.xml</code>.</p>"},{"location":"launch/tier4_autoware_api_launch/#usage","title":"Usage","text":"<p>You can include as follows in <code>*.launch.xml</code> to use <code>autoware_api.launch.xml</code>.</p> <pre><code>  &lt;include file=\"$(find-pkg-share tier4_autoware_api_launch)/launch/autoware_api.launch.xml\"/&gt;\n</code></pre>"},{"location":"launch/tier4_autoware_api_launch/#notes","title":"Notes","text":"<p>For reducing processing load, we use the Component feature in ROS 2 (similar to Nodelet in ROS 1 )</p>"},{"location":"launch/tier4_control_launch/","title":"tier4_control_launch","text":""},{"location":"launch/tier4_control_launch/#tier4_control_launch","title":"tier4_control_launch","text":""},{"location":"launch/tier4_control_launch/#structure","title":"Structure","text":""},{"location":"launch/tier4_control_launch/#package-dependencies","title":"Package Dependencies","text":"<p>Please see <code>&lt;exec_depend&gt;</code> in <code>package.xml</code>.</p>"},{"location":"launch/tier4_control_launch/#usage","title":"Usage","text":"<p>You can include as follows in <code>*.launch.xml</code> to use <code>control.launch.py</code>.</p> <p>Note that you should provide parameter paths as <code>PACKAGE_param_path</code>. The list of parameter paths you should provide is written at the top of <code>planning.launch.xml</code>.</p> <pre><code>&lt;include file=\"$(find-pkg-share tier4_control_launch)/launch/control.launch.py\"&gt;\n  &lt;!-- options for lateral_controller_mode: mpc_follower, pure_pursuit --&gt;\n  &lt;!-- Parameter files --&gt;\n  &lt;arg name=\"FOO_NODE_param_path\" value=\"...\"/&gt;\n  &lt;arg name=\"BAR_NODE_param_path\" value=\"...\"/&gt;\n  ...\n  &lt;arg name=\"lateral_controller_mode\" value=\"mpc_follower\" /&gt;\n&lt;/include&gt;\n</code></pre>"},{"location":"launch/tier4_control_launch/#notes","title":"Notes","text":"<p>For reducing processing load, we use the Component feature in ROS 2 (similar to Nodelet in ROS 1 )</p>"},{"location":"launch/tier4_localization_launch/","title":"tier4_localization_launch","text":""},{"location":"launch/tier4_localization_launch/#tier4_localization_launch","title":"tier4_localization_launch","text":""},{"location":"launch/tier4_localization_launch/#structure","title":"Structure","text":""},{"location":"launch/tier4_localization_launch/#package-dependencies","title":"Package Dependencies","text":"<p>Please see <code>&lt;exec_depend&gt;</code> in <code>package.xml</code>.</p>"},{"location":"launch/tier4_localization_launch/#usage","title":"Usage","text":"<p>Include <code>localization.launch.xml</code> in other launch files as follows.</p> <p>You can select which methods in localization to launch as <code>pose_estimator</code> or <code>twist_estimator</code> by specifying <code>pose_source</code> and <code>twist_source</code>.</p> <p>In addition, you should provide parameter paths as <code>PACKAGE_param_path</code>. The list of parameter paths you should provide is written at the top of <code>localization.launch.xml</code>.</p> <pre><code>  &lt;include file=\"$(find-pkg-share tier4_localization_launch)/launch/localization.launch.xml\"&gt;\n    &lt;!-- Localization methods --&gt;\n    &lt;arg name=\"pose_source\" value=\"...\"/&gt;\n    &lt;arg name=\"twist_source\" value=\"...\"/&gt;\n\n    &lt;!-- Parameter files --&gt;\n    &lt;arg name=\"FOO_param_path\" value=\"...\"/&gt;\n    &lt;arg name=\"BAR_param_path\" value=\"...\"/&gt;\n    ...\n  &lt;/include&gt;\n</code></pre>"},{"location":"launch/tier4_map_launch/","title":"tier4_map_launch","text":""},{"location":"launch/tier4_map_launch/#tier4_map_launch","title":"tier4_map_launch","text":""},{"location":"launch/tier4_map_launch/#structure","title":"Structure","text":""},{"location":"launch/tier4_map_launch/#package-dependencies","title":"Package Dependencies","text":"<p>Please see <code>&lt;exec_depend&gt;</code> in <code>package.xml</code>.</p>"},{"location":"launch/tier4_map_launch/#usage","title":"Usage","text":"<p>You can include as follows in <code>*.launch.xml</code> to use <code>map.launch.py</code>.</p> <p>Note that you should provide parameter paths as <code>PACKAGE_param_path</code>. The list of parameter paths you should provide is written at the top of <code>map.launch.xml</code>.</p> <pre><code>&lt;arg name=\"map_path\" description=\"point cloud and lanelet2 map directory path\"/&gt;\n&lt;arg name=\"lanelet2_map_file\" default=\"lanelet2_map.osm\" description=\"lanelet2 map file name\"/&gt;\n&lt;arg name=\"pointcloud_map_file\" default=\"pointcloud_map.pcd\" description=\"pointcloud map file name\"/&gt;\n\n&lt;include file=\"$(find-pkg-share tier4_map_launch)/launch/map.launch.py\"&gt;\n  &lt;arg name=\"lanelet2_map_path\" value=\"$(var map_path)/$(var lanelet2_map_file)\" /&gt;\n  &lt;arg name=\"pointcloud_map_path\" value=\"$(var map_path)/$(var pointcloud_map_file)\"/&gt;\n\n  &lt;!-- Parameter files --&gt;\n  &lt;arg name=\"FOO_param_path\" value=\"...\"/&gt;\n  &lt;arg name=\"BAR_param_path\" value=\"...\"/&gt;\n  ...\n&lt;/include&gt;\n</code></pre>"},{"location":"launch/tier4_map_launch/#notes","title":"Notes","text":"<p>For reducing processing load, we use the Component feature in ROS 2 (similar to Nodelet in ROS 1 )</p>"},{"location":"launch/tier4_perception_launch/","title":"tier4_perception_launch","text":""},{"location":"launch/tier4_perception_launch/#tier4_perception_launch","title":"tier4_perception_launch","text":""},{"location":"launch/tier4_perception_launch/#structure","title":"Structure","text":""},{"location":"launch/tier4_perception_launch/#package-dependencies","title":"Package Dependencies","text":"<p>Please see <code>&lt;exec_depend&gt;</code> in <code>package.xml</code>.</p>"},{"location":"launch/tier4_perception_launch/#usage","title":"Usage","text":"<p>You can include as follows in <code>*.launch.xml</code> to use <code>perception.launch.xml</code>.</p> <p>Note that you should provide parameter paths as <code>PACKAGE_param_path</code>. The list of parameter paths you should provide is written at the top of <code>perception.launch.xml</code>.</p> <pre><code>  &lt;include file=\"$(find-pkg-share tier4_perception_launch)/launch/perception.launch.xml\"&gt;\n    &lt;!-- options for mode: camera_lidar_fusion, lidar, camera --&gt;\n    &lt;arg name=\"mode\" value=\"lidar\" /&gt;\n\n    &lt;!-- Parameter files --&gt;\n    &lt;arg name=\"FOO_param_path\" value=\"...\"/&gt;\n    &lt;arg name=\"BAR_param_path\" value=\"...\"/&gt;\n    ...\n  &lt;/include&gt;\n</code></pre>"},{"location":"launch/tier4_planning_launch/","title":"tier4_planning_launch","text":""},{"location":"launch/tier4_planning_launch/#tier4_planning_launch","title":"tier4_planning_launch","text":""},{"location":"launch/tier4_planning_launch/#structure","title":"Structure","text":""},{"location":"launch/tier4_planning_launch/#package-dependencies","title":"Package Dependencies","text":"<p>Please see <code>&lt;exec_depend&gt;</code> in <code>package.xml</code>.</p>"},{"location":"launch/tier4_planning_launch/#usage","title":"Usage","text":"<p>Note that you should provide parameter paths as <code>PACKAGE_param_path</code>. The list of parameter paths you should provide is written at the top of <code>planning.launch.xml</code>.</p> <pre><code>&lt;include file=\"$(find-pkg-share tier4_planning_launch)/launch/planning.launch.xml\"&gt;\n  &lt;!-- Parameter files --&gt;\n  &lt;arg name=\"FOO_NODE_param_path\" value=\"...\"/&gt;\n  &lt;arg name=\"BAR_NODE_param_path\" value=\"...\"/&gt;\n  ...\n&lt;/include&gt;\n</code></pre>"},{"location":"launch/tier4_sensing_launch/","title":"tier4_sensing_launch","text":""},{"location":"launch/tier4_sensing_launch/#tier4_sensing_launch","title":"tier4_sensing_launch","text":""},{"location":"launch/tier4_sensing_launch/#structure","title":"Structure","text":""},{"location":"launch/tier4_sensing_launch/#package-dependencies","title":"Package Dependencies","text":"<p>Please see <code>&lt;exec_depend&gt;</code> in <code>package.xml</code>.</p>"},{"location":"launch/tier4_sensing_launch/#usage","title":"Usage","text":"<p>You can include as follows in <code>*.launch.xml</code> to use <code>sensing.launch.xml</code>.</p> <pre><code>  &lt;include file=\"$(find-pkg-share tier4_sensing_launch)/launch/sensing.launch.xml\"&gt;\n    &lt;arg name=\"launch_driver\" value=\"true\"/&gt;\n    &lt;arg name=\"sensor_model\" value=\"$(var sensor_model)\"/&gt;\n    &lt;arg name=\"vehicle_param_file\" value=\"$(find-pkg-share $(var vehicle_model)_description)/config/vehicle_info.param.yaml\"/&gt;\n    &lt;arg name=\"vehicle_mirror_param_file\" value=\"$(find-pkg-share $(var vehicle_model)_description)/config/mirror.param.yaml\"/&gt;\n  &lt;/include&gt;\n</code></pre>"},{"location":"launch/tier4_sensing_launch/#launch-directory-structure","title":"Launch Directory Structure","text":"<p>This package finds sensor settings of specified sensor model in <code>launch</code>.</p> <pre><code>launch/\n\u251c\u2500\u2500 aip_x1 # Sensor model name\n\u2502   \u251c\u2500\u2500 camera.launch.xml # Camera\n\u2502   \u251c\u2500\u2500 gnss.launch.xml # GNSS\n\u2502   \u251c\u2500\u2500 imu.launch.xml # IMU\n\u2502   \u251c\u2500\u2500 lidar.launch.xml # LiDAR\n\u2502   \u2514\u2500\u2500 pointcloud_preprocessor.launch.py # for preprocessing pointcloud\n...\n</code></pre>"},{"location":"launch/tier4_sensing_launch/#notes","title":"Notes","text":"<p>This package finds settings with variables.</p> <p>ex.)</p> <pre><code>&lt;include file=\"$(find-pkg-share tier4_sensing_launch)/launch/$(var sensor_model)/lidar.launch.xml\"&gt;\n</code></pre>"},{"location":"launch/tier4_simulator_launch/","title":"tier4_simulator_launch","text":""},{"location":"launch/tier4_simulator_launch/#tier4_simulator_launch","title":"tier4_simulator_launch","text":""},{"location":"launch/tier4_simulator_launch/#structure","title":"Structure","text":""},{"location":"launch/tier4_simulator_launch/#package-dependencies","title":"Package Dependencies","text":"<p>Please see <code>&lt;exec_depend&gt;</code> in <code>package.xml</code>.</p>"},{"location":"launch/tier4_simulator_launch/#usage","title":"Usage","text":"<pre><code>  &lt;include file=\"$(find-pkg-share tier4_simulator_launch)/launch/simulator.launch.xml\"&gt;\n    &lt;arg name=\"vehicle_info_param_file\" value=\"VEHICLE_INFO_PARAM_FILE\" /&gt;\n    &lt;arg name=\"vehicle_model\" value=\"VEHICLE_MODEL\"/&gt;\n  &lt;/include&gt;\n</code></pre> <p>The simulator model used in simple_planning_simulator is loaded from \"config/simulator_model.param.yaml\" in the \"<code>VEHICLE_MODEL</code>_description\" package.</p>"},{"location":"launch/tier4_system_launch/","title":"tier4_system_launch","text":""},{"location":"launch/tier4_system_launch/#tier4_system_launch","title":"tier4_system_launch","text":""},{"location":"launch/tier4_system_launch/#structure","title":"Structure","text":""},{"location":"launch/tier4_system_launch/#package-dependencies","title":"Package Dependencies","text":"<p>Please see <code>&lt;exec_depend&gt;</code> in <code>package.xml</code>.</p>"},{"location":"launch/tier4_system_launch/#usage","title":"Usage","text":"<p>Note that you should provide parameter paths as <code>PACKAGE_param_path</code>. The list of parameter paths you should provide is written at the top of <code>system.launch.xml</code>.</p> <pre><code>  &lt;include file=\"$(find-pkg-share tier4_system_launch)/launch/system.launch.xml\"&gt;\n    &lt;arg name=\"run_mode\" value=\"online\"/&gt;\n    &lt;arg name=\"sensor_model\" value=\"SENSOR_MODEL\"/&gt;\n\n    &lt;!-- Parameter files --&gt;\n    &lt;arg name=\"FOO_param_path\" value=\"...\"/&gt;\n    &lt;arg name=\"BAR_param_path\" value=\"...\"/&gt;\n    ...\n  &lt;/include&gt;\n</code></pre>"},{"location":"launch/tier4_vehicle_launch/","title":"tier4_vehicle_launch","text":""},{"location":"launch/tier4_vehicle_launch/#tier4_vehicle_launch","title":"tier4_vehicle_launch","text":""},{"location":"launch/tier4_vehicle_launch/#structure","title":"Structure","text":""},{"location":"launch/tier4_vehicle_launch/#package-dependencies","title":"Package Dependencies","text":"<p>Please see <code>&lt;exec_depend&gt;</code> in <code>package.xml</code>.</p>"},{"location":"launch/tier4_vehicle_launch/#usage","title":"Usage","text":"<p>You can include as follows in <code>*.launch.xml</code> to use <code>vehicle.launch.xml</code>.</p> <pre><code>  &lt;arg name=\"vehicle_model\" default=\"sample_vehicle\" description=\"vehicle model name\"/&gt;\n  &lt;arg name=\"sensor_model\" default=\"sample_sensor_kit\" description=\"sensor model name\"/&gt;\n\n  &lt;include file=\"$(find-pkg-share tier4_vehicle_launch)/launch/vehicle.launch.xml\"&gt;\n    &lt;arg name=\"vehicle_model\" value=\"$(var vehicle_model)\"/&gt;\n    &lt;arg name=\"sensor_model\" value=\"$(var sensor_model)\"/&gt;\n  &lt;/include&gt;\n</code></pre>"},{"location":"launch/tier4_vehicle_launch/#notes","title":"Notes","text":"<p>This package finds some external packages and settings with variables and package names.</p> <p>ex.)</p> <pre><code>&lt;let name=\"vehicle_model_pkg\" value=\"$(find-pkg-share $(var vehicle_model)_description)\"/&gt;\n</code></pre> <pre><code>&lt;arg name=\"config_dir\" default=\"$(find-pkg-share $(var sensor_model)_description)/config\"/&gt;\n</code></pre>"},{"location":"launch/tier4_vehicle_launch/#vehiclexacro","title":"vehicle.xacro","text":""},{"location":"launch/tier4_vehicle_launch/#arguments","title":"Arguments","text":"Name Type Description Default sensor_model String sensor model name \"\" vehicle_model String vehicle model name \"\""},{"location":"launch/tier4_vehicle_launch/#usage_1","title":"Usage","text":"<p>You can write as follows in <code>*.launch.xml</code>.</p> <pre><code>  &lt;arg name=\"vehicle_model\" default=\"sample_vehicle\" description=\"vehicle model name\"/&gt;\n  &lt;arg name=\"sensor_model\" default=\"sample_sensor_kit\" description=\"sensor model name\"/&gt;\n  &lt;arg name=\"model\" default=\"$(find-pkg-share tier4_vehicle_launch)/urdf/vehicle.xacro\"/&gt;\n\n  &lt;node name=\"robot_state_publisher\" pkg=\"robot_state_publisher\" exec=\"robot_state_publisher\"&gt;\n    &lt;param name=\"robot_description\" value=\"$(command 'xacro $(var model) vehicle_model:=$(var vehicle_model) sensor_model:=$(var sensor_model)')\"/&gt;\n  &lt;/node&gt;\n</code></pre>"},{"location":"localization/autoware_geo_pose_projector/","title":"autoware_geo_pose_projector","text":""},{"location":"localization/autoware_geo_pose_projector/#autoware_geo_pose_projector","title":"autoware_geo_pose_projector","text":""},{"location":"localization/autoware_geo_pose_projector/#overview","title":"Overview","text":"<p>This node is a simple node that subscribes to the geo-referenced pose topic and publishes the pose in the map frame.</p>"},{"location":"localization/autoware_geo_pose_projector/#subscribed-topics","title":"Subscribed Topics","text":"Name Type Description <code>input_geo_pose</code> <code>geographic_msgs::msg::GeoPoseWithCovarianceStamped</code> geo-referenced pose <code>/map/map_projector_info</code> <code>autoware_map_msgs::msg::MapProjectedObjectInfo</code> map projector info"},{"location":"localization/autoware_geo_pose_projector/#published-topics","title":"Published Topics","text":"Name Type Description <code>output_pose</code> <code>geometry_msgs::msg::PoseWithCovarianceStamped</code> pose in map frame <code>/tf</code> <code>tf2_msgs::msg::TFMessage</code> tf from parent link to the child link"},{"location":"localization/autoware_geo_pose_projector/#parameters","title":"Parameters","text":"Name Type Description Default Range publish_tf boolean whether to publish tf True N/A parent_frame string parent frame for published tf map N/A child_frame string child frame for published tf pose_estimator_base_link N/A"},{"location":"localization/autoware_geo_pose_projector/#limitations","title":"Limitations","text":"<p>The covariance conversion may be incorrect depending on the projection type you are using. The covariance of input topic is expressed in (Latitude, Longitude, Altitude) as a diagonal matrix. Currently, we assume that the x axis is the east direction and the y axis is the north direction. Thus, the conversion may be incorrect when this assumption breaks, especially when the covariance of latitude and longitude is different.</p>"},{"location":"localization/autoware_landmark_based_localizer/","title":"Landmark Based Localizer","text":""},{"location":"localization/autoware_landmark_based_localizer/#landmark-based-localizer","title":"Landmark Based Localizer","text":"<p>This directory contains packages for landmark-based localization.</p> <p>Landmarks are, for example</p> <ul> <li>AR tags detected by camera</li> <li>Boards characterized by intensity detected by LiDAR</li> </ul> <p>etc.</p> <p>Since these landmarks are easy to detect and estimate pose, the ego pose can be calculated from the pose of the detected landmark if the pose of the landmark is written on the map in advance.</p> <p>Currently, landmarks are assumed to be flat.</p> <p>The following figure shows the principle of localization in the case of <code>ar_tag_based_localizer</code>.</p> <p></p> <p>This calculated ego pose is passed to the EKF, where it is fused with the twist information and used to estimate a more accurate ego pose.</p>"},{"location":"localization/autoware_landmark_based_localizer/#node-diagram","title":"Node diagram","text":""},{"location":"localization/autoware_landmark_based_localizer/#landmark_manager","title":"<code>landmark_manager</code>","text":"<p>The definitions of the landmarks written to the map are introduced in the next section. See <code>Map Specifications</code>.</p> <p>The <code>landmark_manager</code> is a utility package to load landmarks from the map.</p> <ul> <li>Translation : The center of the four vertices of the landmark</li> <li>Rotation : Let the vertex numbers be 1, 2, 3, 4 counterclockwise as shown in the next section. Direction is defined as the cross product of the vector from 1 to 2 and the vector from 2 to 3.</li> </ul> <p>Users can define landmarks as Lanelet2 4-vertex polygons. In this case, it is possible to define an arrangement in which the four vertices cannot be considered to be on the same plane. The direction of the landmark in that case is difficult to calculate. So, if the 4 vertices are considered as forming a tetrahedron and its volume exceeds the <code>volume_threshold</code> parameter, the landmark will not publish tf_static.</p>"},{"location":"localization/autoware_landmark_based_localizer/#landmark-based-localizer-packages","title":"Landmark based localizer packages","text":"<ul> <li>ar_tag_based_localizer</li> <li>etc.</li> </ul>"},{"location":"localization/autoware_landmark_based_localizer/#map-specifications","title":"Map specifications","text":"<p>See https://github.com/autowarefoundation/autoware_lanelet2_extension/blob/main/autoware_lanelet2_extension/docs/lanelet2_format_extension.md#localization-landmarks</p>"},{"location":"localization/autoware_landmark_based_localizer/#about-consider_orientation","title":"About <code>consider_orientation</code>","text":"<p>The <code>calculate_new_self_pose</code> function in the <code>LandmarkManager</code> class includes a boolean argument named <code>consider_orientation</code>. This argument determines the method used to calculate the new self pose based on detected and mapped landmarks. The following image illustrates the difference between the two methods.</p> <p></p>"},{"location":"localization/autoware_landmark_based_localizer/#consider_orientation-true","title":"<code>consider_orientation = true</code>","text":"<p>In this mode, the new self pose is calculated so that the relative Pose of the \"landmark detected from the current self pose\" is equal to the relative Pose of the \"landmark mapped from the new self pose\". This method can correct for orientation, but is strongly affected by the orientation error of the landmark detection.</p>"},{"location":"localization/autoware_landmark_based_localizer/#consider_orientation-false","title":"<code>consider_orientation = false</code>","text":"<p>In this mode, the new self pose is calculated so that only the relative position is correct for x, y, and z.</p> <p>This method can not correct for orientation, but it is not affected by the orientation error of the landmark detection.</p>"},{"location":"localization/autoware_landmark_based_localizer/autoware_ar_tag_based_localizer/","title":"AR Tag Based Localizer","text":""},{"location":"localization/autoware_landmark_based_localizer/autoware_ar_tag_based_localizer/#ar-tag-based-localizer","title":"AR Tag Based Localizer","text":"<p>ArTagBasedLocalizer is a vision-based localization node.</p> <p></p> <p>This node uses the ArUco library to detect AR-Tags from camera images and calculates and publishes the pose of the ego vehicle based on these detections. The positions and orientations of the AR-Tags are assumed to be written in the Lanelet2 format.</p>"},{"location":"localization/autoware_landmark_based_localizer/autoware_ar_tag_based_localizer/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"localization/autoware_landmark_based_localizer/autoware_ar_tag_based_localizer/#ar_tag_based_localizer-node","title":"<code>ar_tag_based_localizer</code> node","text":""},{"location":"localization/autoware_landmark_based_localizer/autoware_ar_tag_based_localizer/#input","title":"Input","text":"Name Type Description <code>~/input/lanelet2_map</code> <code>autoware_map_msgs::msg::LaneletMapBin</code> Data of lanelet2 <code>~/input/image</code> <code>sensor_msgs::msg::Image</code> Camera Image <code>~/input/camera_info</code> <code>sensor_msgs::msg::CameraInfo</code> Camera Info <code>~/input/ekf_pose</code> <code>geometry_msgs::msg::PoseWithCovarianceStamped</code> EKF Pose without IMU correction. It is used to validate detected AR tags by filtering out False Positives. Only if the EKF Pose and the AR tag-detected Pose are within a certain temporal and spatial range, the AR tag-detected Pose is considered valid and published."},{"location":"localization/autoware_landmark_based_localizer/autoware_ar_tag_based_localizer/#output","title":"Output","text":"Name Type Description <code>~/output/pose_with_covariance</code> <code>geometry_msgs::msg::PoseWithCovarianceStamped</code> Estimated Pose <code>~/debug/result</code> <code>sensor_msgs::msg::Image</code> [debug topic] Image in which marker detection results are superimposed on the input image <code>~/debug/marker</code> <code>visualization_msgs::msg::MarkerArray</code> [debug topic] Loaded landmarks to visualize in Rviz as thin boards <code>/tf</code> <code>geometry_msgs::msg::TransformStamped</code> [debug topic] TF from camera to detected tag <code>/diagnostics</code> <code>diagnostic_msgs::msg::DiagnosticArray</code> Diagnostics outputs"},{"location":"localization/autoware_landmark_based_localizer/autoware_ar_tag_based_localizer/#parameters","title":"Parameters","text":"Name Type Description Default Range marker_size float marker_size 0.6 N/A target_tag_ids array target_tag_ids ['0','1','2','3','4','5','6'] N/A base_covariance array base_covariance [0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02] N/A distance_threshold float distance_threshold(m) 13.0 N/A consider_orientation boolean consider_orientation false N/A detection_mode string detection_mode select from [DM_NORMAL, DM_FAST, DM_VIDEO_FAST] DM_NORMAL N/A min_marker_size float min_marker_size 0.02 N/A ekf_time_tolerance float ekf_time_tolerance(sec) 5.0 N/A ekf_position_tolerance float ekf_position_tolerance(m) 10.0 N/A"},{"location":"localization/autoware_landmark_based_localizer/autoware_ar_tag_based_localizer/#how-to-launch","title":"How to launch","text":"<p>When launching Autoware, set <code>artag</code> for <code>pose_source</code>.</p> <pre><code>ros2 launch autoware_launch ... \\\n    pose_source:=artag \\\n    ...\n</code></pre>"},{"location":"localization/autoware_landmark_based_localizer/autoware_ar_tag_based_localizer/#rosbag","title":"Rosbag","text":""},{"location":"localization/autoware_landmark_based_localizer/autoware_ar_tag_based_localizer/#sample-rosbag-and-map-awsim-data","title":"Sample rosbag and map (AWSIM data)","text":"<p>This data is simulated data created by AWSIM. Essentially, AR tag-based self-localization is not intended for such public road driving, but for driving in a smaller area, so the max driving speed is set at 15 km/h.</p> <p>It is a known problem that the timing of when each AR tag begins to be detected can cause significant changes in estimation.</p> <p></p>"},{"location":"localization/autoware_landmark_based_localizer/autoware_ar_tag_based_localizer/#sample-rosbag-and-map-real-world-data","title":"Sample rosbag and map (Real world data)","text":"<p>Please remap the topic names and play it.</p> <pre><code>ros2 bag play /path/to/ar_tag_based_localizer_sample_bag/ -r 0.5 -s sqlite3 \\\n     --remap /sensing/camera/front/image:=/sensing/camera/traffic_light/image_raw \\\n             /sensing/camera/front/image/info:=/sensing/camera/traffic_light/camera_info\n</code></pre> <p>This dataset contains issues such as missing IMU data, and overall the accuracy is low. Even when running AR tag-based self-localization, significant difference from the true trajectory can be observed.</p> <p>The image below shows the trajectory when the sample is executed and plotted.</p> <p></p> <p>The pull request video below should also be helpful.</p> <p>https://github.com/autowarefoundation/autoware_universe/pull/4347#issuecomment-1663155248</p>"},{"location":"localization/autoware_landmark_based_localizer/autoware_ar_tag_based_localizer/#principle","title":"Principle","text":""},{"location":"localization/autoware_landmark_based_localizer/autoware_lidar_marker_localizer/","title":"LiDAR Marker Localizer","text":""},{"location":"localization/autoware_landmark_based_localizer/autoware_lidar_marker_localizer/#lidar-marker-localizer","title":"LiDAR Marker Localizer","text":"<p>LiDARMarkerLocalizer is a detect-reflector-based localization node .</p>"},{"location":"localization/autoware_landmark_based_localizer/autoware_lidar_marker_localizer/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"localization/autoware_landmark_based_localizer/autoware_lidar_marker_localizer/#lidar_marker_localizer-node","title":"<code>lidar_marker_localizer</code> node","text":""},{"location":"localization/autoware_landmark_based_localizer/autoware_lidar_marker_localizer/#input","title":"Input","text":"Name Type Description <code>~/input/lanelet2_map</code> <code>autoware_map_msgs::msg::HADMapBin</code> Data of lanelet2 <code>~/input/pointcloud</code> <code>sensor_msgs::msg::PointCloud2</code> PointCloud <code>~/input/ekf_pose</code> <code>geometry_msgs::msg::PoseWithCovarianceStamped</code> EKF Pose"},{"location":"localization/autoware_landmark_based_localizer/autoware_lidar_marker_localizer/#output","title":"Output","text":"Name Type Description <code>~/output/pose_with_covariance</code> <code>geometry_msgs::msg::PoseWithCovarianceStamped</code> Estimated pose <code>~/debug/pose_with_covariance</code> <code>geometry_msgs::msg::PoseWithCovarianceStamped</code> [debug topic] Estimated pose <code>~/debug/marker_detected</code> <code>geometry_msgs::msg::PoseArray</code> [debug topic] Detected marker poses <code>~/debug/marker_mapped</code> <code>visualization_msgs::msg::MarkerArray</code> [debug topic] Loaded landmarks to visualize in Rviz as thin boards <code>~/debug/marker_pointcloud</code> <code>sensor_msgs::msg::PointCloud2</code> [debug topic] PointCloud of the detected marker <code>/diagnostics</code> <code>diagnostic_msgs::msg::DiagnosticArray</code> Diagnostics outputs"},{"location":"localization/autoware_landmark_based_localizer/autoware_lidar_marker_localizer/#parameters","title":"Parameters","text":"Name Type Description Default Range marker_name string The name of the markers listed in the HD map. reflector N/A resolution float Grid size for marker detection algorithm. [m] 0.05 \u22650.0 intensity_pattern array A sequence of high/low intensity to perform pattern matching. 1: high intensity (positive match), 0: not consider, -1: low intensity (negative match) [-1, -1, 0, 1, 1, 1, 1, 1, 0, -1, -1] N/A match_intensity_difference_threshold float Threshold for determining high/low. 20 \u22650 positive_match_num_threshold float Threshold for the number of required matches with the pattern. 3 \u22650 negative_match_num_threshold float Threshold for the number of required matches with the pattern. 3 \u22650 vote_threshold_for_detect_marker float Threshold for the number of rings matching the pattern needed to detect it as a marker. 20 \u22650 marker_height_from_ground float Height from the ground to the center of the marker. [m] 1.075 N/A self_pose_timeout_sec float Timeout for self pose. [sec] 1.0 \u22650.0 self_pose_distance_tolerance_m float Tolerance for the distance between two points when performing linear interpolation. [m] 1.0 \u22650.0 limit_distance_from_self_pose_to_nearest_marker float Distance limit for the purpose of determining whether the node should detect a marker. [m] 2.0 \u22650.0 limit_distance_from_self_pose_to_marker float Distance limit for avoiding miss detection. [m] 2.0 \u22650.0 base_covariance array Output covariance in the base_link coordinate. [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.569e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.569e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00030625] N/A marker_width float Width of a marker. This param is used for visualizing the detected marker pointcloud[m] 0.8 \u22650.0 enable_save_log boolean False N/A save_file_directory_path string $(env HOME)/detected_reflector_intensity N/A save_file_name string detected_reflector_intensity N/A save_frame_id string velodyne_top N/A"},{"location":"localization/autoware_landmark_based_localizer/autoware_lidar_marker_localizer/#how-to-launch","title":"How to launch","text":"<p>When launching Autoware, set <code>lidar-marker</code> for <code>pose_source</code>.</p> <pre><code>ros2 launch autoware_launch ... \\\n    pose_source:=lidar-marker \\\n    ...\n</code></pre>"},{"location":"localization/autoware_landmark_based_localizer/autoware_lidar_marker_localizer/#design","title":"Design","text":""},{"location":"localization/autoware_landmark_based_localizer/autoware_lidar_marker_localizer/#flowchart","title":"Flowchart","text":""},{"location":"localization/autoware_landmark_based_localizer/autoware_lidar_marker_localizer/#detection-algorithm","title":"Detection Algorithm","text":"<ol> <li>Split the LiDAR point cloud into rings along the x-axis of the base_link coordinate system at intervals of the <code>resolution</code> size.</li> <li>Find the portion of intensity that matches the <code>intensity_pattern</code>.</li> <li>Perform steps 1 and 2 for each ring, accumulate the matching indices, and detect portions where the count exceeds the <code>vote_threshold_for_detect_marker</code> as markers.</li> </ol>"},{"location":"localization/autoware_landmark_based_localizer/autoware_lidar_marker_localizer/#sample-dataset","title":"Sample Dataset","text":"<ul> <li>Sample rosbag and map</li> </ul> <p>This dataset was acquired in National Institute for Land and Infrastructure Management, Full-scale tunnel experiment facility. The reflectors were installed by Taisei Corporation.</p>"},{"location":"localization/autoware_landmark_based_localizer/autoware_lidar_marker_localizer/#collaborators","title":"Collaborators","text":"<ul> <li>TIER IV</li> <li>Taisei Corporation</li> <li>Yuri Shimizu</li> </ul>"},{"location":"localization/autoware_localization_error_monitor/","title":"autoware_localization_error_monitor","text":""},{"location":"localization/autoware_localization_error_monitor/#autoware_localization_error_monitor","title":"autoware_localization_error_monitor","text":""},{"location":"localization/autoware_localization_error_monitor/#purpose","title":"Purpose","text":"<p>autoware_localization_error_monitor is a package for diagnosing localization errors by monitoring uncertainty of the localization results. The package monitors the following two values:</p> <ul> <li>size of long radius of confidence ellipse</li> <li>size of confidence ellipse along lateral direction (body-frame)</li> </ul>"},{"location":"localization/autoware_localization_error_monitor/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"localization/autoware_localization_error_monitor/#input","title":"Input","text":"Name Type Description <code>input/odom</code> <code>nav_msgs::msg::Odometry</code> localization result"},{"location":"localization/autoware_localization_error_monitor/#output","title":"Output","text":"Name Type Description <code>debug/ellipse_marker</code> <code>visualization_msgs::msg::Marker</code> ellipse marker <code>diagnostics</code> <code>diagnostic_msgs::msg::DiagnosticArray</code> diagnostics outputs"},{"location":"localization/autoware_localization_error_monitor/#parameters","title":"Parameters","text":"Name Type Description Default Range scale float scale factor for monitored values 3 N/A error_ellipse_size float error threshold for long radius of confidence ellipse [m] 1.5 N/A warn_ellipse_size float warning threshold for long radius of confidence ellipse [m] 1.2 N/A error_ellipse_size_lateral_direction float error threshold for size of confidence ellipse along lateral direction [m] 0.3 N/A warn_ellipse_size_lateral_direction float warning threshold for size of confidence ellipse along lateral direction [m] 0.25 N/A"},{"location":"localization/autoware_pose2twist/","title":"autoware_pose2twist","text":""},{"location":"localization/autoware_pose2twist/#autoware_pose2twist","title":"autoware_pose2twist","text":""},{"location":"localization/autoware_pose2twist/#purpose","title":"Purpose","text":"<p>This <code>autoware_pose2twist</code> calculates the velocity from the input pose history. In addition to the computed twist, this node outputs the linear-x and angular-z components as a float message to simplify debugging.</p> <p>The <code>twist.linear.x</code> is calculated as <code>sqrt(dx * dx + dy * dy + dz * dz) / dt</code>, and the values in the <code>y</code> and <code>z</code> fields are zero. The <code>twist.angular</code> is calculated as <code>relative_rotation_vector / dt</code> for each field.</p>"},{"location":"localization/autoware_pose2twist/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"localization/autoware_pose2twist/#input","title":"Input","text":"Name Type Description pose geometry_msgs::msg::PoseStamped pose source to used for the velocity calculation."},{"location":"localization/autoware_pose2twist/#output","title":"Output","text":"Name Type Description twist geometry_msgs::msg::TwistStamped twist calculated from the input pose history. linear_x autoware_internal_debug_msgs::msg::Float32Stamped linear-x field of the output twist. angular_z autoware_internal_debug_msgs::msg::Float32Stamped angular-z field of the output twist."},{"location":"localization/autoware_pose2twist/#parameters","title":"Parameters","text":"<p>none.</p>"},{"location":"localization/autoware_pose2twist/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>none.</p>"},{"location":"localization/autoware_pose_covariance_modifier/","title":"Autoware Pose Covariance Modifier Node","text":""},{"location":"localization/autoware_pose_covariance_modifier/#autoware-pose-covariance-modifier-node","title":"Autoware Pose Covariance Modifier Node","text":""},{"location":"localization/autoware_pose_covariance_modifier/#purpose","title":"Purpose","text":"<p>This package makes it possible to use GNSS and NDT poses together in real time localization.</p>"},{"location":"localization/autoware_pose_covariance_modifier/#function","title":"Function","text":"<p>This package takes in GNSS (Global Navigation Satellite System) and NDT (Normal Distribution Transform) poses with covariances.</p> <p>It outputs a single pose with covariance:</p> <ul> <li>Directly the GNSS pose and its covariance.</li> <li>Directly the NDT pose and its covariance.</li> <li>Both GNSS and NDT poses with modified covariances.</li> </ul> <ul> <li>This package doesn't modify the pose information it receives.</li> <li>It only modifies NDT covariance values under certain conditions.</li> </ul>"},{"location":"localization/autoware_pose_covariance_modifier/#assumptions","title":"Assumptions","text":"<ul> <li>The NDT matcher provides a pose with a fixed covariance.</li> <li>The NDT matcher is unable to provide a dynamic, reliable covariance value.</li> </ul>"},{"location":"localization/autoware_pose_covariance_modifier/#requirements","title":"Requirements","text":"<ul> <li>The GNSS/INS module must provide standard deviation values (its error / RMSE) for the position and orientation.</li> <li>It probably needs RTK support to provide accurate position and orientation information.</li> <li>You need to have a geo-referenced map.</li> <li>GNSS/INS module and the base_link frame must be calibrated well enough.</li> <li>In an environment where GNSS/INS and NDT systems work well, the <code>base_link</code> poses from both systems should be close to   each other.</li> </ul>"},{"location":"localization/autoware_pose_covariance_modifier/#description","title":"Description","text":"<p>GNSS and NDT nodes provide the pose with covariance data utilized in an Extended Kalman Filter (EKF).</p> <p>Accurate covariance values are crucial for the effectiveness of the EKF in estimating the state.</p> <p>The GNSS system generates reliable standard deviation values, which can be transformed into covariance measures.</p> <p>But we currently don't have a reliable way to determine the covariance values for the NDT poses. And the NDT matching system in Autoware outputs poses with preset covariance values.</p> <p>For this reason, this package is designed to manage the selection of the pose source, based on the standard deviation values provided by the GNSS system.</p> <p>It also tunes the covariance values of the NDT poses, based on the GNSS standard deviation values.</p>"},{"location":"localization/autoware_pose_covariance_modifier/#flowcharts","title":"Flowcharts","text":""},{"location":"localization/autoware_pose_covariance_modifier/#without-this-package","title":"Without this package","text":"<p>Only NDT pose is used in localization. GNSS pose is only used for initialization.</p> <pre><code>graph TD\n    ndt_scan_matcher[\"ndt_scan_matcher\"] --&gt; |\"/localization/pose_estimator/pose_with_covariance\"| ekf_localizer[\"ekf_localizer\"]\n\nclassDef cl_node fill:#FFF2CC,stroke-width:3px,stroke:#D6B656;\n\nclass ndt_scan_matcher cl_node;\nclass ekf_localizer cl_node;</code></pre>"},{"location":"localization/autoware_pose_covariance_modifier/#with-this-package","title":"With this package","text":"<p>Both NDT and GNSS poses are used in localization, depending on the standard deviation values coming from the GNSS system.</p> <p>Here is a flowchart depicting the process and the predefined thresholds:</p> <pre><code>graph TD\n    gnss_poser[\"gnss_poser\"] --&gt; |\"/sensing/gnss/&lt;br/&gt;pose_with_covariance\"| pose_covariance_modifier_node\n    ndt_scan_matcher[\"ndt_scan_matcher\"] --&gt; |\"/localization/pose_estimator/ndt_scan_matcher/&lt;br/&gt;pose_with_covariance\"| pose_covariance_modifier_node\n\n    subgraph pose_covariance_modifier_node [\"Pose Covariance Modifier Node\"]\n        pc1gnss_pose_yaw&lt;br/&gt;stddev\n        pc1 --&gt;|\"&lt;= 0.3 rad\"| pc2gnss_pose_z&lt;br/&gt;stddev\n        pc2 --&gt;|\"&lt;= 0.1 m\"| pc3gnss_pose_xy&lt;br/&gt;stddev\n        pc2 --&gt;|\"&amp;gt; 0.1 m\"| ndt_pose(\"NDT Pose\")\n        pc3 --&gt;|\"&lt;= 0.1 m\"| gnss_pose(\"GNSS Pose\")\n        pc3 --&gt;|\"0.1 m &lt; x &lt;= 0.2 m\"| gnss_ndt_pose(\"`Both GNSS and NDT Pose\n        (_with modified covariance_)`\")\n        pc3 --&gt;|\"&amp;gt; 0.2 m\"| ndt_pose\n        pc1 --&gt;|\"&amp;gt; 0.3 rad\"| ndt_pose\n    end\n\n    pose_covariance_modifier_node --&gt;|\"/localization/pose_estimator/pose_with_covariance\"| ekf_localizer[\"ekf_localizer\"]\n\nclassDef cl_node fill:#FFF2CC,stroke-width:3px,stroke:#D6B656;\nclassDef cl_conditional fill:#FFE6CC,stroke-width:3px,stroke:#D79B00;\nclassDef cl_output fill:#D5E8D4,stroke-width:3px,stroke:#82B366;\n\nclass gnss_poser cl_node;\nclass ndt_scan_matcher cl_node;\nclass ekf_localizer cl_node;\nclass pose_covariance_modifier_node cl_node;\n\nclass pc1 cl_conditional;\nclass pc2 cl_conditional;\nclass pc3 cl_conditional;\n\nclass ndt_pose cl_output;\nclass gnss_pose cl_output;\nclass gnss_ndt_pose cl_output;</code></pre>"},{"location":"localization/autoware_pose_covariance_modifier/#how-to-use-this-package","title":"How to use this package","text":"<p>This package is disabled by default in Autoware, you need to manually enable it.</p> <p>To enable this package, you need to change the <code>use_autoware_pose_covariance_modifier</code> parameter to <code>true</code> within the pose_twist_estimator.launch.xml.</p>"},{"location":"localization/autoware_pose_covariance_modifier/#without-this-condition-default","title":"Without this condition (default)","text":"<ul> <li>The output of the ndt_scan_matcher is directly sent   to ekf_localizer.<ul> <li>It has a preset covariance value.</li> <li>topic name: <code>/localization/pose_estimator/pose_with_covariance</code></li> </ul> </li> <li>The GNSS pose does not enter the ekf_localizer.</li> <li>This node does not launch.</li> </ul>"},{"location":"localization/autoware_pose_covariance_modifier/#with-this-condition","title":"With this condition","text":"<ul> <li>The output of the ndt_scan_matcher is renamed<ul> <li>from: <code>/localization/pose_estimator/pose_with_covariance</code>.</li> <li>to: <code>/localization/pose_estimator/ndt_scan_matcher/pose_with_covariance</code>.</li> </ul> </li> <li>The <code>ndt_scan_matcher</code> output enters the <code>autoware_pose_covariance_modifier</code>.</li> <li>The output of this package goes to ekf_localizer with:<ul> <li>topic name: <code>/localization/pose_estimator/pose_with_covariance</code>.</li> </ul> </li> </ul>"},{"location":"localization/autoware_pose_covariance_modifier/#node","title":"Node","text":""},{"location":"localization/autoware_pose_covariance_modifier/#subscribed-topics","title":"Subscribed topics","text":"Name Type Description <code>input_gnss_pose_with_cov_topic</code> <code>geometry_msgs::msg::PoseWithCovarianceStamped</code> Input GNSS pose topic. <code>input_ndt_pose_with_cov_topic</code> <code>geometry_msgs::msg::PoseWithCovarianceStamped</code> Input NDT pose topic."},{"location":"localization/autoware_pose_covariance_modifier/#published-topics","title":"Published topics","text":"Name Type Description <code>output_pose_with_covariance_topic</code> <code>geometry_msgs::msg::PoseWithCovarianceStamped</code> Output pose topic. This topic is used by the ekf_localizer package. <code>selected_pose_type</code> <code>std_msgs::msg::String</code> Declares which pose sources are used in the output of this package <code>output/ndt_position_stddev</code> <code>std_msgs::msg::Float64</code> Output pose ndt average standard deviation in position xy. It is published only when the enable_debug_topics is true. <code>output/gnss_position_stddev</code> <code>std_msgs::msg::Float64</code> Output pose gnss average standard deviation in position xy. It is published only when the enable_debug_topics is true."},{"location":"localization/autoware_pose_covariance_modifier/#parameters","title":"Parameters","text":"<p>The parameters are set in config/pose_covariance_modifier.param.yaml .</p> Name Type Description Default Range threshold_gnss_stddev_yaw_deg_max float If GNSS yaw standard deviation values are larger than this, trust only NDT 0.3 N/A threshold_gnss_stddev_z_max float If GNSS position Z standard deviation values are larger than this, trust only NDT 0.1 N/A threshold_gnss_stddev_xy_bound_lower float If GNSS position XY standard deviation values are lower than this, trust only GNSS 0.1 N/A threshold_gnss_stddev_xy_bound_upper float If GNSS position XY standard deviation values are higher than this, trust only NDT 0.25 N/A ndt_std_dev_bound_lower float Lower bound value for standard deviation of NDT positions (x, y, z) when used with GNSS 0.15 N/A ndt_std_dev_bound_upper float Upper bound value for standard deviation of NDT positions (x, y, z) when used with GNSS 0.3 N/A gnss_pose_timeout_sec float If GNSS data is not received for this duration, trust only NDT 1 N/A enable_debug_topics boolean Publish additional debug topics 1 N/A"},{"location":"localization/autoware_pose_covariance_modifier/#faq","title":"FAQ","text":""},{"location":"localization/autoware_pose_covariance_modifier/#how-are-varying-frequency-rates-handled","title":"How are varying frequency rates handled?","text":"<p>The GNSS and NDT pose topics may have different frequencies. The GNSS pose topic may have a higher frequency than the NDT.</p> <p>Let's assume that the inputs have the following frequencies:</p> Source Frequency GNSS 200 Hz NDT 10 Hz <p>This package publishes the output poses as they come in, depending on the mode.</p> <p>End result:</p> Mode Output Freq GNSS Only 200 Hz GNSS + NDT 210 Hz NDT Only 10 Hz"},{"location":"localization/autoware_pose_covariance_modifier/#how-and-when-are-the-ndt-covariance-values-overwritten","title":"How and when are the NDT covariance values overwritten?","text":"Mode Outputs, Covariance GNSS Only GNSS, Unmodified GNSS + NDT GNSS: Unmodified, NDT: Interpolated NDT Only NDT, Unmodified <p>NDT covariance values overwritten only for the <code>GNSS + NDT</code> mode.</p> <p>This enables a smooth transition between <code>GNSS Only</code> and <code>NDT Only</code> modes.</p> <p>In this mode, both NDT and GNSS poses are published from this node.</p>"},{"location":"localization/autoware_pose_covariance_modifier/#ndt-covariance-calculation","title":"NDT covariance calculation","text":"<p>As the <code>gnss_std_dev</code> increases within its bounds, <code>ndt_std_dev</code> should proportionally decrease within its own bounds.</p> <p>To achieve this, we first linearly interpolate:</p> <ul> <li>Base value: <code>gnss_std_dev</code></li> <li>Base range: [<code>threshold_gnss_stddev_xy_bound_lower</code>, <code>threshold_gnss_stddev_xy_bound_upper</code>]</li> <li>Target range: [<code>ndt_std_dev_bound_lower</code>, <code>ndt_std_dev_bound_upper</code>]</li> <li>Target value: <code>ndt_std_dev_target</code></li> </ul> <ul> <li>Final value = <code>ndt_std_dev_bound_lower</code> + <code>ndt_std_dev_bound_upper</code> - <code>ndt_std_dev_target</code> (to get the inverse)</li> </ul> <p></p>"},{"location":"localization/autoware_pose_estimator_arbiter/","title":"autoware_pose_estimator_arbiter","text":""},{"location":"localization/autoware_pose_estimator_arbiter/#autoware_pose_estimator_arbiter","title":"autoware_pose_estimator_arbiter","text":"<p>Table of contents:</p> <ul> <li>Abstract</li> <li>Interface</li> <li>Architecture</li> <li>How to launch</li> <li>Switching Rules</li> <li>Pose Initialization</li> <li>Future Plans</li> </ul>"},{"location":"localization/autoware_pose_estimator_arbiter/#abstract","title":"Abstract","text":"<p>This package launches multiple pose estimators and provides the capability to stop or resume specific pose estimators based on the situation. It provides provisional switching rules and will be adaptable to a wide variety of rules in the future.</p> <p>Please refer to this discussion about other ideas on implementation.</p>"},{"location":"localization/autoware_pose_estimator_arbiter/#why-do-we-need-a-stopresume-mechanism","title":"Why do we need a stop/resume mechanism?","text":"<p>It is possible to launch multiple pose_estimators and fuse them using a Kalman filter by editing launch files. However, this approach is not preferable due to computational costs.</p> <p>Particularly, NDT and YabLoc are computationally intensive, and it's not recommended to run them simultaneously. Also, even if both can be activated at the same time, the Kalman Filter may be affected by one of them giving bad output.</p> <p>[!NOTE] Currently, there is ONLY A RULE implemented that always enables all pose_estimators. If users want to toggle pose_estimator with their own rules, they need to add new rules. by referring to example_rule. The example_rule has source code that can be used as a reference for implementing the rules.</p>"},{"location":"localization/autoware_pose_estimator_arbiter/#supporting-pose_estimators","title":"Supporting pose_estimators","text":"<ul> <li>ndt_scan_matcher</li> <li>eagleye</li> <li>yabloc</li> <li>landmark_based_localizer</li> </ul>"},{"location":"localization/autoware_pose_estimator_arbiter/#demonstration","title":"Demonstration","text":"<p>The following video demonstrates the switching of four different pose estimators.</p> <p>Users can reproduce the demonstration using the following data and launch command:</p> <p>sample data (rosbag &amp; map) The rosbag is simulated data created by AWSIM. The map is an edited version of the original map data published on the AWSIM documentation page to make it suitable for multiple pose_estimators.</p> <pre><code>ros2 launch autoware_launch logging_simulator.launch.xml \\\n  map_path:=&lt;your-map-path&gt; \\\n  vehicle_model:=sample_vehicle \\\n  sensor_model:=awsim_sensor_kit \\\n  pose_source:=ndt_yabloc_artag_eagleye\n</code></pre>"},{"location":"localization/autoware_pose_estimator_arbiter/#interfaces","title":"Interfaces","text":"Click to show details  ### Parameters  There are no parameters.  ### Services  | Name             | Type                            | Description                     | | ---------------- | ------------------------------- | ------------------------------- | | `/config_logger` | logging_demo::srv::ConfigLogger | service to modify logging level |  ### Clients  | Name                  | Type                  | Description                       | | --------------------- | --------------------- | --------------------------------- | | `/yabloc_suspend_srv` | std_srv::srv::SetBool | service to stop or restart yabloc |  ### Subscriptions  For pose estimator arbitration:  | Name                                  | Type                                          | Description    | | ------------------------------------- | --------------------------------------------- | -------------- | | `/input/artag/image`                  | sensor_msgs::msg::Image                       | ArTag input    | | `/input/yabloc/image`                 | sensor_msgs::msg::Image                       | YabLoc input   | | `/input/eagleye/pose_with_covariance` | geometry_msgs::msg::PoseWithCovarianceStamped | Eagleye output | | `/input/ndt/pointcloud`               | sensor_msgs::msg::PointCloud2                 | NDT input      |  For switching rule:  | Name                          | Type                                                         | Description                       | | ----------------------------- | ------------------------------------------------------------ | --------------------------------- | | `/input/vector_map`           | autoware_map_msgs::msg::LaneletMapBin                        | vector map                        | | `/input/pose_with_covariance` | geometry_msgs::msg::PoseWithCovarianceStamped                | localization final output         | | `/input/initialization_state` | autoware_adapi_v1_msgs::msg::LocalizationInitializationState | localization initialization state |  ### Publications  | Name                                   | Type                                          | Description                                            | | -------------------------------------- | --------------------------------------------- | ------------------------------------------------------ | | `/output/artag/image`                  | sensor_msgs::msg::Image                       | relayed ArTag input                                    | | `/output/yabloc/image`                 | sensor_msgs::msg::Image                       | relayed YabLoc input                                   | | `/output/eagleye/pose_with_covariance` | geometry_msgs::msg::PoseWithCovarianceStamped | relayed Eagleye output                                 | | `/output/ndt/pointcloud`               | sensor_msgs::msg::PointCloud2                 | relayed NDT input                                      | | `/output/debug/marker_array`           | visualization_msgs::msg::MarkerArray          | [debug topic] everything for visualization             | | `/output/debug/string`                 | visualization_msgs::msg::MarkerArray          | [debug topic] debug information such as current status |"},{"location":"localization/autoware_pose_estimator_arbiter/#trouble-shooting","title":"Trouble Shooting","text":"<p>If it does not seems to work, users can get more information in the following ways.</p> <p>[!TIP]</p> <pre><code>ros2 service call /localization/autoware_pose_estimator_arbiter/config_logger logging_demo/srv/ConfigLogger \\\n  '{logger_name: localization.autoware_pose_estimator_arbiter, level: debug}'\n</code></pre>"},{"location":"localization/autoware_pose_estimator_arbiter/#architecture","title":"Architecture","text":"Click to show details  ### Case of running a single pose estimator  When each pose_estimator is run alone, this package does nothing. Following figure shows the node configuration when NDT, YabLoc Eagleye and AR-Tag are run independently.    ### Case of running multiple pose estimators  When running multiple pose_estimators, autoware_pose_estimator_arbiter is executed. It comprises a **switching rule** and **stoppers** corresponding to each pose_estimator.  - Stoppers control the pose_estimator activity by relaying inputs or outputs, or by requesting a suspend service. - Switching rules determine which pose_estimator to use.  Which stoppers and switching rules are instantiated depends on the runtime arguments at startup.  Following figure shows the node configuration when all pose_estimator are run simultaneously.    - **NDT**  The NDT stopper relays topics in the front side of the point cloud pre-processor.  - **YabLoc**  The YabLoc stopper relays input image topics in the frontend of the image pre-processor. YabLoc includes a particle filter process that operates on a timer, and even when image topics are not streamed, the particle prediction process continues to work. To address this, the YabLoc stopper also has a service client for explicitly stopping and resuming YabLoc.  - **Eagleye**  The Eagleye stopper relays Eagleye's output pose topics in the backend of Eagleye's estimation process. Eagleye performs time-series processing internally, and it can't afford to stop the input stream. Furthermore, Eagleye's estimation process is lightweight enough to be run continuously without a significant load, so the relay is inserted in the backend.  - **ArTag**  The ArTag stopper relays image topics in the front side of the landmark localizer."},{"location":"localization/autoware_pose_estimator_arbiter/#how-to-launch","title":"How to launch","text":"Click to show details  The user can launch the desired pose_estimators by giving the pose_estimator names as a concatenation of underscores for the runtime argument `pose_source`.  <pre><code>ros2 launch autoware_launch logging_simulator.launch.xml \\\n  map_path:=&lt;your-map-path&gt; \\\n  vehicle_model:=sample_vehicle \\\n  sensor_model:=awsim_sensor_kit \\\n  pose_source:=ndt_yabloc_artag_eagleye\n</code></pre>  Even if `pose_source` includes an unexpected string, it will be filtered appropriately. Please see the table below for details.  | given runtime argument                      | parsed autoware_pose_estimator_arbiter's param (pose_sources) | | ------------------------------------------- | ------------------------------------------------------------- | | `pose_source:=ndt`                          | `[\"ndt\"]`                                                     | | `pose_source:=nan`                          | `[]`                                                          | | `pose_source:=yabloc_ndt`                   | `[\"ndt\",\"yabloc\"]`                                            | | `pose_source:=yabloc_ndt_ndt_ndt`           | `[\"ndt\",\"yabloc\"]`                                            | | `pose_source:=ndt_yabloc_eagleye`           | `[\"ndt\",\"yabloc\",\"eagleye\"]`                                  | | `pose_source:=ndt_yabloc_nan_eagleye_artag` | `[\"ndt\",\"yabloc\",\"eagleye\",\"artag\"]`                          |"},{"location":"localization/autoware_pose_estimator_arbiter/#switching-rules","title":"Switching Rules","text":"Click to show details  Currently, **ONLY ONE RULE** (`enable_all_rule`) is implemented. In the future, several rules will be implemented and users will be able to select rules.  &gt; [!TIP] &gt; There are presets available to extend the rules. If you want to extend the rules, please see [example_rule](./example_rule/README.md).  ### Enable All Rule  This is the default and simplest rule. This rule enables all pose_estimators regardless of their current state.  <pre><code>flowchart LR\n  A{ }\n  A --whatever --&gt; _A[enable all pose_estimators]</code></pre>"},{"location":"localization/autoware_pose_estimator_arbiter/#pose-initialization","title":"Pose Initialization","text":"<p>When using multiple pose_estimators, it is necessary to appropriately adjust the parameters provided to the <code>pose_initializer</code>.</p> Click to show details  The following table is based on the runtime argument \"pose_source\" indicating which initial pose estimation methods are available and the parameters that should be provided to the pose_initialization node. To avoid making the application too complicated, a priority is established so that NDT is always used when it is available. (The pose_initializer will only perform NDT-based initial pose estimation when `ndt_enabled` and `yabloc_enabled` are both `true`).  This table's usage is described from three perspectives:  - **Autoware Users:** Autoware users do not need to consult this table.   They simply provide the desired combinations of pose_estimators, and the appropriate parameters are automatically provided to the pose_initializer. - **Autoware Developers:** Autoware developers can consult this table to know which parameters are assigned. - **Who implements New Pose Estimator Switching:**   Developers must extend this table and implement the assignment of appropriate parameters to the pose_initializer.  |         pose_source         | invoked initialization method | `ndt_enabled` | `yabloc_enabled` | `gnss_enabled` | `sub_gnss_pose_cov`                          | | :-------------------------: | ----------------------------- | ------------- | ---------------- | -------------- | -------------------------------------------- | |             ndt             | ndt                           | true          | false            | true           | /sensing/gnss/pose_with_covariance           | |           yabloc            | yabloc                        | false         | true             | true           | /sensing/gnss/pose_with_covariance           | |           eagleye           | vehicle needs run for a while | false         | false            | true           | /localization/pose_estimator/eagleye/...     | |            artag            | 2D Pose Estimate (RViz)       | false         | false            | true           | /sensing/gnss/pose_with_covariance           | |         ndt, yabloc         | ndt                           | true          | true             | true           | /sensing/gnss/pose_with_covariance           | |        ndt, eagleye         | ndt                           | true          | false            | true           | /sensing/gnss/pose_with_covariance           | |         ndt, artag          | ndt                           | true          | false            | true           | /sensing/gnss/pose_with_covariance           | |       yabloc, eagleye       | yabloc                        | false         | true             | true           | /sensing/gnss/pose_with_covariance           | |        yabloc, artag        | yabloc                        | false         | true             | true           | /sensing/gnss/pose_with_covariance           | |       eagleye, artag        | vehicle needs run for a while | false         | false            | true           | /localization/pose_estimator/eagleye/pose... | |    ndt, yabloc, eagleye     | ndt                           | true          | true             | true           | /sensing/gnss/pose_with_covariance           | |     ndt, eagleye, artag     | ndt                           | true          | false            | true           | /sensing/gnss/pose_with_covariance           | |   yabloc, eagleye, artag    | yabloc                        | false         | true             | true           | /sensing/gnss/pose_with_covariance           | | ndt, yabloc, eagleye, artag | ndt                           | true          | true             | true           | /sensing/gnss/pose_with_covariance           |"},{"location":"localization/autoware_pose_estimator_arbiter/#future-plans","title":"Future Plans","text":"Click to show details  ### gradually switching  In the future, this package will provide not only ON/OFF switching, but also a mechanism for low frequency operation, such as 50% NDT &amp; 50% YabLoc.  ### stopper for pose_estimators to be added in the future  The basic strategy is to realize ON/OFF switching by relaying the input or output topics of that pose_estimator. If pose_estimator involves time-series processing with heavy computations, it's not possible to pause and resume with just topic relaying.  In such cases, there may not be generally applicable solutions, but the following methods may help:  1. Completely stop and **reinitialize** time-series processing, as seen in the case of YabLoc. 2. Subscribe to `localization/kinematic_state` and **keep updating states** to ensure that the estimation does not break (relying on the output of the active pose_estimator). 3. The multiple pose_estimator **does not support** that particular pose_estimator.  Please note that this issue is fundamental to realizing multiple pose_estimators, and it will arise regardless of the architecture proposed in this case."},{"location":"localization/autoware_pose_estimator_arbiter/example_rule/","title":"example rule","text":""},{"location":"localization/autoware_pose_estimator_arbiter/example_rule/#example-rule","title":"example rule","text":"<p>The example rule provides a sample rule for controlling the arbiter. By combining the provided rules, it is possible to achieve demonstrations as follows. Users can extend the rules as needed by referencing this code, allowing them to control the arbiter as desired.</p>"},{"location":"localization/autoware_pose_estimator_arbiter/example_rule/#demonstration","title":"Demonstration","text":"<p>The following video demonstrates the switching of four different pose estimators.</p>"},{"location":"localization/autoware_pose_estimator_arbiter/example_rule/#switching-rules","title":"Switching Rules","text":""},{"location":"localization/autoware_pose_estimator_arbiter/example_rule/#pcd-map-based-rule","title":"Pcd Map Based Rule","text":"<pre><code>flowchart LR\n  A{PCD is enough dense }\n  A --true--&gt; B[enable NDT]\n  A --false--&gt; C[enable YabLoc]</code></pre>"},{"location":"localization/autoware_pose_estimator_arbiter/example_rule/#vector-map-based-rule","title":"Vector Map Based Rule","text":"<pre><code>flowchart LR\n  A{ }\n  A --whatever --&gt; _A[When the ego vehicle is in a predetermined pose_estimator_area,\\n it enables the corresponding pose_estamtor.]</code></pre>"},{"location":"localization/autoware_pose_estimator_arbiter/example_rule/#rule-helpers","title":"Rule helpers","text":"<p>Rule helpers are auxiliary tools for describing switching rules.</p> <ul> <li>PCD occupancy</li> <li>Pose estimator area</li> </ul>"},{"location":"localization/autoware_pose_estimator_arbiter/example_rule/#pcd-occupancy","title":"PCD occupancy","text":""},{"location":"localization/autoware_pose_estimator_arbiter/example_rule/#pose-estimator-area","title":"Pose estimator area","text":"<p>The pose_estimator_area is a planar area described by polygon in lanelet2. The height of the area is meaningless; it judges if the projection of its self-position is contained within the polygon or not.</p> <p></p> <p>A sample pose_estimator_area is shown below. The values provided below are placeholders. To be correctly read, the area should have the type \"pose_estimator_specify\" and the subtype should be one of ndt, yabloc, eagleye, or artag.</p> <pre><code>  &lt;node id=\"1\" lat=\"35.8xxxxx\" lon=\"139.6xxxxx\"&gt;\n    &lt;tag k=\"mgrs_code\" v=\"54SUE000000\"/&gt;\n    &lt;tag k=\"local_x\" v=\"10.0\"/&gt;\n    &lt;tag k=\"local_y\" v=\"10.0\"/&gt;\n    &lt;tag k=\"ele\" v=\"1.0\"/&gt;\n  &lt;/node&gt;\n  &lt;node id=\"2\" lat=\"35.8xxxxx\" lon=\"139.6xxxxx\"&gt;\n    &lt;tag k=\"mgrs_code\" v=\"54SUE000000\"/&gt;\n    &lt;tag k=\"local_x\" v=\"10.0\"/&gt;\n    &lt;tag k=\"local_y\" v=\"20.0\"/&gt;\n    &lt;tag k=\"ele\" v=\"1.0\"/&gt;\n  &lt;/node&gt;\n  &lt;node id=\"3\" lat=\"35.8xxxxx\" lon=\"139.6xxxxx\"&gt;\n    &lt;tag k=\"mgrs_code\" v=\"54SUE000000\"/&gt;\n    &lt;tag k=\"local_x\" v=\"20.0\"/&gt;\n    &lt;tag k=\"local_y\" v=\"20.0\"/&gt;\n    &lt;tag k=\"ele\" v=\"1.0\"/&gt;\n  &lt;/node&gt;\n  &lt;node id=\"4\" lat=\"35.8xxxxx\" lon=\"139.6xxxxx\"&gt;\n    &lt;tag k=\"mgrs_code\" v=\"54SUE000000\"/&gt;\n    &lt;tag k=\"local_x\" v=\"10.0\"/&gt;\n    &lt;tag k=\"local_y\" v=\"20.0\"/&gt;\n    &lt;tag k=\"ele\" v=\"1.0\"/&gt;\n  &lt;/node&gt;\n\n...\n\n  &lt;way id=\"5\"&gt;\n    &lt;nd ref=\"1\"/&gt;\n    &lt;nd ref=\"2\"/&gt;\n    &lt;nd ref=\"3\"/&gt;\n    &lt;nd ref=\"4\"/&gt;\n    &lt;tag k=\"type\" v=\"pose_estimator_specify\"/&gt;\n    &lt;tag k=\"subtype\" v=\"eagleye\"/&gt;\n    &lt;tag k=\"area\" v=\"yes\"/&gt;\n  &lt;/way&gt;\n\n  &lt;way id=\"6\"&gt;\n    &lt;nd ref=\"7\"/&gt;\n    &lt;nd ref=\"8\"/&gt;\n    &lt;nd ref=\"9\"/&gt;\n    &lt;nd ref=\"10\"/&gt;\n    &lt;tag k=\"type\" v=\"pose_estimator_specify\"/&gt;\n    &lt;tag k=\"subtype\" v=\"yabloc\"/&gt;\n    &lt;tag k=\"area\" v=\"yes\"/&gt;\n  &lt;/way&gt;\n</code></pre>"},{"location":"localization/autoware_pose_instability_detector/","title":"autoware_pose_instability_detector","text":""},{"location":"localization/autoware_pose_instability_detector/#autoware_pose_instability_detector","title":"autoware_pose_instability_detector","text":"<p>The <code>pose_instability_detector</code> is a node designed to monitor the stability of <code>/localization/kinematic_state</code>, which is an output topic of the Extended Kalman Filter (EKF).</p> <p>This node triggers periodic timer callbacks to compare two poses:</p> <ul> <li>The pose calculated by dead reckoning starting from the pose of <code>/localization/kinematic_state</code> obtained <code>timer_period</code> seconds ago.</li> <li>The latest pose from <code>/localization/kinematic_state</code>.</li> </ul> <p>The results of this comparison are then output to the <code>/diagnostics</code> topic.</p> <p></p> <p></p> <p>If this node outputs ERROR messages to <code>/diagnostics</code>, it means that the EKF output is significantly different from the integrated twist values. In other words, ERROR outputs indicate that the vehicle has moved to a place outside the expected range based on the twist values. This discrepancy suggests that there may be an issue with either the estimated pose or the input twist.</p> <p>The following diagram provides an overview of how the procedure looks like:</p> <p></p>"},{"location":"localization/autoware_pose_instability_detector/#dead-reckoning-algorithm","title":"Dead reckoning algorithm","text":"<p>Dead reckoning is a method of estimating the position of a vehicle based on its previous position and velocity. The procedure for dead reckoning is as follows:</p> <ol> <li>Capture the necessary twist values from the <code>/input/twist</code> topic.</li> <li>Integrate the twist values to calculate the pose transition.</li> <li>Apply the pose transition to the previous pose to obtain the current pose.</li> </ol>"},{"location":"localization/autoware_pose_instability_detector/#collecting-twist-values","title":"Collecting twist values","text":"<p>The <code>pose_instability_detector</code> node collects the twist values from the <code>~/input/twist</code> topic to perform dead reckoning. Ideally, <code>pose_instability_detector</code> needs the twist values between the previous pose and the current pose. Therefore, <code>pose_instability_detector</code> snips the twist buffer and apply interpolations and extrapolations to obtain the twist values at the desired time.</p> <p></p>"},{"location":"localization/autoware_pose_instability_detector/#linear-transition-and-angular-transition","title":"Linear transition and angular transition","text":"<p>After the twist values are collected, the node calculates the linear transition and angular transition based on the twist values and add them to the previous pose.</p>"},{"location":"localization/autoware_pose_instability_detector/#threshold-definition","title":"Threshold definition","text":"<p>The <code>pose_instability_detector</code> node compares the pose calculated by dead reckoning with the latest pose from the EKF output. These two pose are ideally the same, but in reality, they are not due to the error in the twist values the pose observation. If these two poses are significantly different so that the absolute value exceeds the threshold, the node outputs a WARN message to the <code>/diagnostics</code> topic. There are six thresholds (x, y, z, roll, pitch, and yaw) to determine whether the poses are significantly different, and these thresholds are determined by the following subsections.</p>"},{"location":"localization/autoware_pose_instability_detector/#diff_position_x","title":"<code>diff_position_x</code>","text":"<p>This threshold examines the difference in the longitudinal axis between the two poses, and check whether the vehicle goes beyond the expected error. This threshold is a sum of \"maximum longitudinal error due to velocity scale factor error\" and \"pose estimation error tolerance\".</p> \\[ \\tau_x = v_{\\rm max}\\frac{\\beta_v}{100} \\Delta t + \\epsilon_x\\\\ \\] Symbol Description Unit \\(\\tau_x\\) Threshold for the difference in the longitudinal axis \\(m\\) \\(v_{\\rm max}\\) Maximum velocity \\(m/s\\) \\(\\beta_v\\) Scale factor tolerance for the maximum velocity \\(\\%\\) \\(\\Delta t\\) Time interval \\(s\\) \\(\\epsilon_x\\) Pose estimator (e. g. ndt_scan_matcher) error tolerance in the longitudinal axis \\(m\\)"},{"location":"localization/autoware_pose_instability_detector/#diff_position_y-and-diff_position_z","title":"<code>diff_position_y</code> and <code>diff_position_z</code>","text":"<p>These thresholds examine the difference in the lateral and vertical axes between the two poses, and check whether the vehicle goes beyond the expected error. The <code>pose_instability_detector</code> calculates the possible range where the vehicle goes, and get the maximum difference between the nominal dead reckoning pose and the maximum limit pose.</p> <p></p> <p>Addition to this, the <code>pose_instability_detector</code> node considers the pose estimation error tolerance to determine the threshold.</p> \\[ \\tau_y = l + \\epsilon_y \\] Symbol Description Unit \\(\\tau_y\\) Threshold for the difference in the lateral axis \\(m\\) \\(l\\) Maximum lateral distance described in the image above (See the appendix how this is calculated) \\(m\\) \\(\\epsilon_y\\) Pose estimator (e. g. ndt_scan_matcher) error tolerance in the lateral axis \\(m\\) <p>Note that <code>pose_instability_detector</code> sets the threshold for the vertical axis as the same as the lateral axis. Only the pose estimator error tolerance is different.</p>"},{"location":"localization/autoware_pose_instability_detector/#diff_angle_x-diff_angle_y-and-diff_angle_z","title":"<code>diff_angle_x</code>, <code>diff_angle_y</code>, and <code>diff_angle_z</code>","text":"<p>These thresholds examine the difference in the roll, pitch, and yaw angles between the two poses. This threshold is a sum of \"maximum angular error due to velocity scale factor error and bias error\" and \"pose estimation error tolerance\".</p> \\[ \\tau_\\phi = \\tau_\\theta = \\tau_\\psi = \\left(\\omega_{\\rm max}\\frac{\\beta_\\omega}{100} + b \\right) \\Delta t + \\epsilon_\\psi \\] Symbol Description Unit \\(\\tau_\\phi\\) Threshold for the difference in the roll angle \\({\\rm rad}\\) \\(\\tau_\\theta\\) Threshold for the difference in the pitch angle \\({\\rm rad}\\) \\(\\tau_\\psi\\) Threshold for the difference in the yaw angle \\({\\rm rad}\\) \\(\\omega_{\\rm max}\\) Maximum angular velocity \\({\\rm rad}/s\\) \\(\\beta_\\omega\\) Scale factor tolerance for the maximum angular velocity \\(\\%\\) \\(b\\) Bias tolerance of the angular velocity \\({\\rm rad}/s\\) \\(\\Delta t\\) Time interval \\(s\\) \\(\\epsilon_\\psi\\) Pose estimator (e. g. ndt_scan_matcher) error tolerance in the yaw angle \\({\\rm rad}\\)"},{"location":"localization/autoware_pose_instability_detector/#parameters","title":"Parameters","text":"Name Type Description Default Range timer_period float The period of timer_callback (sec). 0.5 &gt;0 heading_velocity_maximum float The maximum of heading velocity (m/s). 16.667 \u22650.0 heading_velocity_scale_factor_tolerance float The tolerance of heading velocity scale factor (%). 3 \u22650.0 angular_velocity_maximum float The maximum of angular velocity (rad/s). 0.523 \u22650.0 angular_velocity_scale_factor_tolerance float The tolerance of angular velocity scale factor (%). 0.2 \u22650.0 angular_velocity_bias_tolerance float The tolerance of angular velocity bias (rad/s). 0.00698 \u22650.0 pose_estimator_longitudinal_tolerance float The tolerance of longitudinal position of pose estimator (m). 0.11 \u22650.0 pose_estimator_lateral_tolerance float The tolerance of lateral position of pose estimator (m). 0.11 \u22650.0 pose_estimator_vertical_tolerance float The tolerance of vertical position of pose estimator (m). 0.5 \u22650.0 pose_estimator_angular_tolerance float The tolerance of roll angle of pose estimator (rad). 0.0175 \u22650.0 enable_validation.position_x boolean Enable validation for x dimension of the pose position. 1 N/A enable_validation.position_y boolean Enable validation for y dimension of the pose position. 1 N/A enable_validation.position_z boolean Enable validation for z dimension of the pose position. 1 N/A enable_validation.angle_x boolean Enable validation for x dimension (roll) of the pose orientation. 0 N/A enable_validation.angle_y boolean Enable validation for y dimension (pitch) of the pose orientation. 0 N/A enable_validation.angle_z boolean Enable validation for z dimension (yaw) of the pose orientation. 1 N/A"},{"location":"localization/autoware_pose_instability_detector/#input","title":"Input","text":"Name Type Description <code>~/input/odometry</code> nav_msgs::msg::Odometry Pose estimated by EKF <code>~/input/twist</code> geometry_msgs::msg::TwistWithCovarianceStamped Twist"},{"location":"localization/autoware_pose_instability_detector/#output","title":"Output","text":"Name Type Description <code>~/debug/diff_pose</code> geometry_msgs::msg::PoseStamped diff_pose <code>/diagnostics</code> diagnostic_msgs::msg::DiagnosticArray Diagnostics"},{"location":"localization/autoware_pose_instability_detector/#appendix","title":"Appendix","text":"<p>On calculating the maximum lateral distance \\(l\\), the <code>pose_instability_detector</code> node will estimate the following poses.</p> Pose heading velocity \\(v\\) angular velocity \\(\\omega\\) Nominal dead reckoning pose \\(v_{\\rm max}\\) \\(\\omega_{\\rm max}\\) Dead reckoning pose of corner A \\(\\left(1+\\frac{\\beta_v}{100}\\right) v_{\\rm max}\\) \\(\\left(1+\\frac{\\beta_\\omega}{100}\\right) \\omega_{\\rm max} + b\\) Dead reckoning pose of corner B \\(\\left(1-\\frac{\\beta_v}{100}\\right) v_{\\rm max}\\) \\(\\left(1+\\frac{\\beta_\\omega}{100}\\right) \\omega_{\\rm max} + b\\) Dead reckoning pose of corner C \\(\\left(1-\\frac{\\beta_v}{100}\\right) v_{\\rm max}\\) \\(\\left(1-\\frac{\\beta_\\omega}{100}\\right) \\omega_{\\rm max} - b\\) Dead reckoning pose of corner D \\(\\left(1+\\frac{\\beta_v}{100}\\right) v_{\\rm max}\\) \\(\\left(1-\\frac{\\beta_\\omega}{100}\\right) \\omega_{\\rm max} - b\\) <p>Given a heading velocity \\(v\\) and \\(\\omega\\), the 2D theoretical variation seen from the previous pose is calculated as follows:</p> \\[ \\begin{align*} \\left[     \\begin{matrix}     \\Delta x\\\\     \\Delta y     \\end{matrix} \\right] &amp;= \\left[     \\begin{matrix}     \\int_{0}^{\\Delta t} v \\cos(\\omega t) dt\\\\     \\int_{0}^{\\Delta t} v \\sin(\\omega t) dt     \\end{matrix} \\right] \\\\ &amp;= \\left[     \\begin{matrix}     \\frac{v}{\\omega} \\sin(\\omega \\Delta t)\\\\     \\frac{v}{\\omega} \\left(1 - \\cos(\\omega \\Delta t)\\right)     \\end{matrix} \\right] \\end{align*} \\] <p>We calculate this variation for each corner and get the maximum value of the lateral distance \\(l\\) by comparing the distance between the nominal dead reckoning pose and the corner poses.</p>"},{"location":"localization/yabloc/","title":"YabLoc","text":""},{"location":"localization/yabloc/#yabloc","title":"YabLoc","text":"<p>YabLoc is vision-based localization with vector map. https://youtu.be/Eaf6r_BNFfk</p> <p></p> <p>It estimates position by matching road surface markings extracted from images with a vector map. Point cloud maps and LiDAR are not required. YabLoc enables users localize vehicles that are not equipped with LiDAR and in environments where point cloud maps are not available.</p>"},{"location":"localization/yabloc/#packages","title":"Packages","text":"<ul> <li>yabloc_common</li> <li>yabloc_image_processing</li> <li>yabloc_particle_filter</li> <li>yabloc_pose_initializer</li> </ul>"},{"location":"localization/yabloc/#how-to-launch-yabloc-instead-of-ndt","title":"How to launch YabLoc instead of NDT","text":"<p>When launching autoware, if you set <code>pose_source:=yabloc</code> as an argument, YabLoc will be launched instead of NDT. By default, <code>pose_source</code> is <code>ndt</code>.</p> <p>A sample command to run YabLoc is as follows</p> <pre><code>ros2 launch autoware_launch logging_simulator.launch.xml \\\n  map_path:=$HOME/autoware_map/sample-map-rosbag\\\n  vehicle_model:=sample_vehicle \\\n  sensor_model:=sample_sensor_kit \\\n  pose_source:=yabloc\n</code></pre>"},{"location":"localization/yabloc/#architecture","title":"Architecture","text":""},{"location":"localization/yabloc/#principle","title":"Principle","text":"<p>The diagram below illustrates the basic principle of YabLoc. It extracts road surface markings by extracting the line segments using the road area obtained from graph-based segmentation. The red line at the center-top of the diagram represents the line segments identified as road surface markings. YabLoc transforms these segments for each particle and determines the particle's weight by comparing them with the cost map generated from Lanelet2.</p> <p></p>"},{"location":"localization/yabloc/#visualization","title":"Visualization","text":""},{"location":"localization/yabloc/#core-visualization-topics","title":"Core visualization topics","text":"<p>These topics are not visualized by default.</p> <p></p> index topic name description 1 <code>/localization/yabloc/pf/predicted_particle_marker</code> particle distribution of particle filter. Red particles are probable candidate. 2 <code>/localization/yabloc/pf/scored_cloud</code> 3D projected line segments. the color indicates how well they match the map. 3 <code>/localization/yabloc/image_processing/lanelet2_overlay_image</code> overlay of lanelet2 (yellow lines) onto image based on estimated pose. If they match well with the actual road markings, it means that the localization performs well."},{"location":"localization/yabloc/#image-topics-for-debug","title":"Image topics for debug","text":"<p>These topics are not visualized by default.</p> <p></p> index topic name description 1 <code>/localization/yabloc/pf/cost_map_image</code> cost map made from lanelet2 2 <code>/localization/yabloc/pf/match_image</code> projected line segments 3 <code>/localization/yabloc/image_processing/image_with_colored_line_segment</code> classified line segments. green line segments are used in particle correction 4 <code>/localization/yabloc/image_processing/lanelet2_overlay_image</code> overlay of lanelet2 5 <code>/localization/yabloc/image_processing/segmented_image</code> graph based segmentation result"},{"location":"localization/yabloc/#limitation","title":"Limitation","text":"<ul> <li>Running YabLoc and NDT simultaneously is not supported.<ul> <li>This is because running both at the same time may be computationally too expensive.</li> <li>Also, in most cases, NDT is superior to YabLoc, so there is less benefit to running them at the same time.</li> </ul> </li> <li>It does not estimate roll and pitch, therefore some of the perception nodes may not work well.</li> <li>It does not support multiple cameras now. But it will in the future.</li> <li>In places where there are few road surface markings, such as intersections, the estimation heavily relies on GNSS, IMU, and vehicle's wheel odometry.</li> <li>If the road boundary or road surface markings are not included in the Lanelet2, the estimation is likely to fail.</li> <li>The sample rosbag provided in the autoware tutorial does not include images, so it is not possible to run YabLoc with it.<ul> <li>If you want to test the functionality of YabLoc, the sample test data provided in this PR is useful.</li> </ul> </li> </ul>"},{"location":"localization/yabloc/yabloc_common/","title":"yabloc_common","text":""},{"location":"localization/yabloc/yabloc_common/#yabloc_common","title":"yabloc_common","text":"<p>This package contains some executable nodes related to map. Also, This provides some yabloc common library.</p> <ul> <li>ground_server</li> <li>ll2_decomposer</li> </ul>"},{"location":"localization/yabloc/yabloc_common/#ground_server","title":"ground_server","text":""},{"location":"localization/yabloc/yabloc_common/#purpose","title":"Purpose","text":"<p>It estimates the height and tilt of the ground from lanelet2.</p>"},{"location":"localization/yabloc/yabloc_common/#input-outputs","title":"Input / Outputs","text":""},{"location":"localization/yabloc/yabloc_common/#input","title":"Input","text":"Name Type Description <code>input/vector_map</code> <code>autoware_map_msgs::msg::LaneletMapBin</code> vector map <code>input/pose</code> <code>geometry_msgs::msg::PoseStamped</code> estimated self pose"},{"location":"localization/yabloc/yabloc_common/#output","title":"Output","text":"Name Type Description <code>output/ground</code> <code>std_msgs::msg::Float32MultiArray</code> estimated ground parameters. it contains x, y, z, normal_x, normal_y, normal_z. <code>output/ground_markers</code> <code>visualization_msgs::msg::Marker</code> visualization of estimated ground plane <code>output/ground_status</code> <code>std_msgs::msg::String</code> status log of ground plane estimation <code>output/height</code> <code>std_msgs::msg::Float32</code> altitude <code>output/near_cloud</code> <code>sensor_msgs::msg::PointCloud2</code> point cloud extracted from lanelet2 and used for ground tilt estimation"},{"location":"localization/yabloc/yabloc_common/#parameters","title":"Parameters","text":"Name Type Description Default Range force_zero_tilt boolean if true, the tilt is always determined to be horizontal False N/A K float the number of neighbors for ground search on a map 50 N/A R float radius for ground search on a map [m] 10 N/A"},{"location":"localization/yabloc/yabloc_common/#ll2_decomposer","title":"ll2_decomposer","text":""},{"location":"localization/yabloc/yabloc_common/#purpose_1","title":"Purpose","text":"<p>This node extracts the elements related to the road surface markings and yabloc from lanelet2.</p>"},{"location":"localization/yabloc/yabloc_common/#input-outputs_1","title":"Input / Outputs","text":""},{"location":"localization/yabloc/yabloc_common/#input_1","title":"Input","text":"Name Type Description <code>input/vector_map</code> <code>autoware_map_msgs::msg::LaneletMapBin</code> vector map"},{"location":"localization/yabloc/yabloc_common/#output_1","title":"Output","text":"Name Type Description <code>output/ll2_bounding_box</code> <code>sensor_msgs::msg::PointCloud2</code> bounding boxes extracted from lanelet2 <code>output/ll2_road_marking</code> <code>sensor_msgs::msg::PointCloud2</code> road surface markings extracted from lanelet2 <code>output/ll2_sign_board</code> <code>sensor_msgs::msg::PointCloud2</code> traffic sign boards extracted from lanelet2 <code>output/sign_board_marker</code> <code>visualization_msgs::msg::MarkerArray</code> visualized traffic sign boards"},{"location":"localization/yabloc/yabloc_common/#parameters_1","title":"Parameters","text":"Name Type Description Default Range road_marking_labels array line string types that indicating road surface markings in lanelet2 ['cross_walk', 'zebra_marking', 'line_thin', 'line_thick', 'pedestrian_marking', 'stop_line', 'road_border'] N/A sign_board_labels array line string types that indicating traffic sign boards in lanelet2 ['sign-board'] N/A bounding_box_labels array line string types that indicating not mapped areas in lanelet2 ['none'] N/A"},{"location":"localization/yabloc/yabloc_image_processing/","title":"yabloc_image_processing","text":""},{"location":"localization/yabloc/yabloc_image_processing/#yabloc_image_processing","title":"yabloc_image_processing","text":"<p>This package contains some executable nodes related to image processing.</p> <ul> <li>line_segment_detector</li> <li>graph_segmentation</li> <li>segment_filter</li> <li>undistort</li> <li>lanelet2_overlay</li> <li>line_segments_overlay</li> </ul>"},{"location":"localization/yabloc/yabloc_image_processing/#line_segment_detector","title":"line_segment_detector","text":""},{"location":"localization/yabloc/yabloc_image_processing/#purpose","title":"Purpose","text":"<p>This node extract all line segments from gray scale image.</p>"},{"location":"localization/yabloc/yabloc_image_processing/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"localization/yabloc/yabloc_image_processing/#input","title":"Input","text":"Name Type Description <code>input/image_raw</code> <code>sensor_msgs::msg::Image</code> undistorted image"},{"location":"localization/yabloc/yabloc_image_processing/#output","title":"Output","text":"Name Type Description <code>output/image_with_line_segments</code> <code>sensor_msgs::msg::Image</code> image with line segments highlighted <code>output/line_segments_cloud</code> <code>sensor_msgs::msg::PointCloud2</code> detected line segments as point cloud. each point contains x,y,z, normal_x, normal_y, normal_z and z, and normal_z are always empty."},{"location":"localization/yabloc/yabloc_image_processing/#graph_segmentation","title":"graph_segmentation","text":""},{"location":"localization/yabloc/yabloc_image_processing/#purpose_1","title":"Purpose","text":"<p>This node extract road surface region by graph-based-segmentation.</p>"},{"location":"localization/yabloc/yabloc_image_processing/#inputs-outputs_1","title":"Inputs / Outputs","text":""},{"location":"localization/yabloc/yabloc_image_processing/#input_1","title":"Input","text":"Name Type Description <code>input/image_raw</code> <code>sensor_msgs::msg::Image</code> undistorted image"},{"location":"localization/yabloc/yabloc_image_processing/#output_1","title":"Output","text":"Name Type Description <code>output/mask_image</code> <code>sensor_msgs::msg::Image</code> image with masked segments determined as road surface area <code>output/segmented_image</code> <code>sensor_msgs::msg::Image</code> segmented image for visualization"},{"location":"localization/yabloc/yabloc_image_processing/#parameters","title":"Parameters","text":"Name Type Description Default Range target_height_ratio float height on the image to retrieve the candidate road surface 0.85 N/A target_candidate_box_width float size of the square area to search for candidate road surfaces 15 N/A pickup_additional_graph_segment boolean if this is true, additional regions of similar color are retrieved 1 N/A similarity_score_threshold float threshold for picking up additional areas 0.8 N/A sigma float parameter for cv::ximgproc::segmentation::GraphSegmentation 0.5 N/A k float parameter for cv::ximgproc::segmentation::GraphSegmentation 300 N/A min_size float parameter for cv::ximgproc::segmentation::GraphSegmentation 100 N/A"},{"location":"localization/yabloc/yabloc_image_processing/#segment_filter","title":"segment_filter","text":""},{"location":"localization/yabloc/yabloc_image_processing/#purpose_2","title":"Purpose","text":"<p>This is a node that integrates the results of graph_segment and lsd to extract road surface markings.</p>"},{"location":"localization/yabloc/yabloc_image_processing/#inputs-outputs_2","title":"Inputs / Outputs","text":""},{"location":"localization/yabloc/yabloc_image_processing/#input_2","title":"Input","text":"Name Type Description <code>input/line_segments_cloud</code> <code>sensor_msgs::msg::PointCloud2</code> detected line segment <code>input/mask_image</code> <code>sensor_msgs::msg::Image</code> image with masked segments determined as road surface area <code>input/camera_info</code> <code>sensor_msgs::msg::CameraInfo</code> undistorted camera info"},{"location":"localization/yabloc/yabloc_image_processing/#output_2","title":"Output","text":"Name Type Description <code>output/line_segments_cloud</code> <code>sensor_msgs::msg::PointCloud2</code> filtered line segments for visualization <code>output/projected_image</code> <code>sensor_msgs::msg::Image</code> projected filtered line segments for visualization <code>output/projected_line_segments_cloud</code> <code>sensor_msgs::msg::PointCloud2</code> projected filtered line segments"},{"location":"localization/yabloc/yabloc_image_processing/#parameters_1","title":"Parameters","text":"Name Type Description Default Range min_segment_length float min length threshold (if it is negative, it is unlimited) 1.5 N/A max_segment_distance float max distance threshold (if it is negative, it is unlimited) 30 N/A max_lateral_distance float max lateral distance threshold (if it is negative, it is unlimited) 10 N/A publish_image_with_segment_for_debug boolean toggle whether to publish the filtered line segment for debug 1 N/A max_range float range of debug projection visualization 20 N/A image_size float image size of debug projection visualization 800 N/A"},{"location":"localization/yabloc/yabloc_image_processing/#undistort","title":"undistort","text":""},{"location":"localization/yabloc/yabloc_image_processing/#purpose_3","title":"Purpose","text":"<p>This node performs image resizing and undistortion at the same time.</p>"},{"location":"localization/yabloc/yabloc_image_processing/#inputs-outputs_3","title":"Inputs / Outputs","text":""},{"location":"localization/yabloc/yabloc_image_processing/#input_3","title":"Input","text":"Name Type Description <code>input/camera_info</code> <code>sensor_msgs::msg::CameraInfo</code> camera info <code>input/image_raw</code> <code>sensor_msgs::msg::Image</code> raw camera image <code>input/image_raw/compressed</code> <code>sensor_msgs::msg::CompressedImage</code> compressed camera image <p>This node subscribes to both compressed image and raw image topics. If raw image is subscribed to even once, compressed image will no longer be subscribed to. This is to avoid redundant decompression within Autoware.</p>"},{"location":"localization/yabloc/yabloc_image_processing/#output_3","title":"Output","text":"Name Type Description <code>output/camera_info</code> <code>sensor_msgs::msg::CameraInfo</code> resized camera info <code>output/image_raw</code> <code>sensor_msgs::msg::CompressedImage</code> undistorted and resized image"},{"location":"localization/yabloc/yabloc_image_processing/#parameters_2","title":"Parameters","text":"Name Type Description Default Range use_sensor_qos boolean whether to use sensor qos or not True N/A width float resized image width size 800 N/A override_frame_id string value for overriding the camera's frame_id. if blank, frame_id of static_tf is not overwritten N/A"},{"location":"localization/yabloc/yabloc_image_processing/#about-tf_static-overriding","title":"about tf_static overriding","text":"click to open  Some nodes requires `/tf_static` from `/base_link` to the frame_id of `/sensing/camera/traffic_light/image_raw/compressed` (e.g. `/traffic_light_left_camera/camera_optical_link`). You can verify that the tf_static is correct with the following command.  <pre><code>ros2 run tf2_ros tf2_echo base_link traffic_light_left_camera/camera_optical_link\n</code></pre>  If the wrong `/tf_static` are broadcasted due to using a prototype vehicle, not having accurate calibration data, or some other unavoidable reason, it is useful to give the frame_id in `override_camera_frame_id`. If you give it a non-empty string, `/image_processing/undistort_node` will rewrite the frame_id in camera_info. For example, you can give a different tf_static as follows.  <pre><code>ros2 launch yabloc_launch sample_launch.xml override_camera_frame_id:=fake_camera_optical_link\nros2 run tf2_ros static_transform_publisher \\\n  --frame-id base_link \\\n  --child-frame-id fake_camera_optical_link \\\n  --roll -1.57 \\\n  --yaw -1.570\n</code></pre>"},{"location":"localization/yabloc/yabloc_image_processing/#lanelet2_overlay","title":"lanelet2_overlay","text":""},{"location":"localization/yabloc/yabloc_image_processing/#purpose_4","title":"Purpose","text":"<p>This node overlays lanelet2 on the camera image based on the estimated self-position.</p>"},{"location":"localization/yabloc/yabloc_image_processing/#inputs-outputs_4","title":"Inputs / Outputs","text":""},{"location":"localization/yabloc/yabloc_image_processing/#input_4","title":"Input","text":"Name Type Description <code>input/pose</code> <code>geometry_msgs::msg::PoseStamped</code> estimated self pose <code>input/projected_line_segments_cloud</code> <code>sensor_msgs::msg::PointCloud2</code> projected line segments including non-road markings <code>input/camera_info</code> <code>sensor_msgs::msg::CameraInfo</code> undistorted camera info <code>input/image_raw</code> <code>sensor_msgs::msg::Image</code> undistorted camera image <code>input/ground</code> <code>std_msgs::msg::Float32MultiArray</code> ground tilt <code>input/ll2_road_marking</code> <code>sensor_msgs::msg::PointCloud2</code> lanelet2 elements regarding road surface markings <code>input/ll2_sign_board</code> <code>sensor_msgs::msg::PointCloud2</code> lanelet2 elements regarding traffic sign boards"},{"location":"localization/yabloc/yabloc_image_processing/#output_4","title":"Output","text":"Name Type Description <code>output/lanelet2_overlay_image</code> <code>sensor_msgs::msg::Image</code> lanelet2 overlaid image <code>output/projected_marker</code> <code>visualization_msgs::msg::Marker</code> 3d projected line segments including non-road markings"},{"location":"localization/yabloc/yabloc_image_processing/#line_segments_overlay","title":"line_segments_overlay","text":""},{"location":"localization/yabloc/yabloc_image_processing/#purpose_5","title":"Purpose","text":"<p>This node visualize classified line segments on the camera image</p>"},{"location":"localization/yabloc/yabloc_image_processing/#inputs-outputs_5","title":"Inputs / Outputs","text":""},{"location":"localization/yabloc/yabloc_image_processing/#input_5","title":"Input","text":"Name Type Description <code>input/line_segments_cloud</code> <code>sensor_msgs::msg::PointCloud2</code> classified line segments <code>input/image_raw</code> <code>sensor_msgs::msg::Image</code> undistorted camera image"},{"location":"localization/yabloc/yabloc_image_processing/#output_5","title":"Output","text":"Name Type Description <code>output/image_with_colored_line_segments</code> <code>sensor_msgs::msg::Image</code> image with highlighted line segments"},{"location":"localization/yabloc/yabloc_monitor/","title":"yabloc_monitor","text":""},{"location":"localization/yabloc/yabloc_monitor/#yabloc_monitor","title":"yabloc_monitor","text":"<p>YabLoc monitor is a node that monitors the status of the YabLoc localization system. It is a wrapper node that monitors the status of the YabLoc localization system and publishes the status as diagnostics.</p>"},{"location":"localization/yabloc/yabloc_monitor/#feature","title":"Feature","text":""},{"location":"localization/yabloc/yabloc_monitor/#availability","title":"Availability","text":"<p>The node monitors the final output pose of YabLoc to verify the availability of YabLoc.</p>"},{"location":"localization/yabloc/yabloc_monitor/#others","title":"Others","text":"<p>To be added,</p>"},{"location":"localization/yabloc/yabloc_monitor/#interfaces","title":"Interfaces","text":""},{"location":"localization/yabloc/yabloc_monitor/#input","title":"Input","text":"Name Type Description <code>~/input/yabloc_pose</code> <code>geometry_msgs/PoseStamped</code> The final output pose of YabLoc"},{"location":"localization/yabloc/yabloc_monitor/#output","title":"Output","text":"Name Type Description <code>/diagnostics</code> <code>diagnostic_msgs/DiagnosticArray</code> Diagnostics outputs"},{"location":"localization/yabloc/yabloc_monitor/#parameters","title":"Parameters","text":"Name Type Description Default Range availability/timestamp_tolerance float tolerable time difference between current time and latest estimated pose 1 N/A"},{"location":"localization/yabloc/yabloc_particle_filter/","title":"yabLoc_particle_filter","text":""},{"location":"localization/yabloc/yabloc_particle_filter/#yabloc_particle_filter","title":"yabLoc_particle_filter","text":"<p>This package contains some executable nodes related to particle filter.</p> <ul> <li>particle_predictor</li> <li>gnss_particle_corrector</li> <li>camera_particle_corrector</li> </ul>"},{"location":"localization/yabloc/yabloc_particle_filter/#particle_predictor","title":"particle_predictor","text":""},{"location":"localization/yabloc/yabloc_particle_filter/#purpose","title":"Purpose","text":"<ul> <li>This node performs predictive updating and resampling of particles.</li> <li>It retroactively reflects the particle weights determined by the corrector node.</li> </ul>"},{"location":"localization/yabloc/yabloc_particle_filter/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"localization/yabloc/yabloc_particle_filter/#input","title":"Input","text":"Name Type Description <code>input/initialpose</code> <code>geometry_msgs::msg::PoseWithCovarianceStamped</code> to specify the initial position of particles <code>input/twist_with_covariance</code> <code>geometry_msgs::msg::TwistWithCovarianceStamped</code> linear velocity and angular velocity of prediction update <code>input/height</code> <code>std_msgs::msg::Float32</code> ground height <code>input/weighted_particles</code> <code>yabloc_particle_filter::msg::ParticleArray</code> particles weighted by corrector nodes"},{"location":"localization/yabloc/yabloc_particle_filter/#output","title":"Output","text":"Name Type Description <code>output/pose_with_covariance</code> <code>geometry_msgs::msg::PoseWithCovarianceStamped</code> particle centroid with covariance <code>output/pose</code> <code>geometry_msgs::msg::PoseStamped</code> particle centroid with covariance <code>output/predicted_particles</code> <code>yabloc_particle_filter::msg::ParticleArray</code> particles weighted by predictor nodes <code>debug/init_marker</code> <code>visualization_msgs::msg::Marker</code> debug visualization of initial position <code>debug/particles_marker_array</code> <code>visualization_msgs::msg::MarkerArray</code> particles visualization. published if <code>visualize</code> is true"},{"location":"localization/yabloc/yabloc_particle_filter/#parameters","title":"Parameters","text":"Name Type Description Default Range visualize boolean whether particles are also published in visualization_msgs or not True N/A static_linear_covariance float overriding covariance of <code>/twist_with_covariance</code> 0.04 N/A static_angular_covariance float overriding covariance of <code>/twist_with_covariance</code> 0.006 N/A resampling_interval_seconds float the interval of particle resampling 1.0 N/A num_of_particles float the number of particles 500 N/A prediction_rate float frequency of forecast updates, in Hz 50.0 N/A cov_xx_yy array the covariance of initial pose [2.0, 0.25] N/A"},{"location":"localization/yabloc/yabloc_particle_filter/#services","title":"Services","text":"Name Type Description <code>yabloc_trigger_srv</code> <code>std_srvs::srv::SetBool</code> activation and deactivation of yabloc estimation"},{"location":"localization/yabloc/yabloc_particle_filter/#gnss_particle_corrector","title":"gnss_particle_corrector","text":""},{"location":"localization/yabloc/yabloc_particle_filter/#purpose_1","title":"Purpose","text":"<ul> <li>This node estimated particles weight using GNSS.</li> <li>It supports two types of input: <code>ublox_msgs::msg::NavPVT</code> and <code>geometry_msgs::msg::PoseWithCovarianceStamped</code>.</li> </ul>"},{"location":"localization/yabloc/yabloc_particle_filter/#inputs-outputs_1","title":"Inputs / Outputs","text":""},{"location":"localization/yabloc/yabloc_particle_filter/#input_1","title":"Input","text":"Name Type Description <code>input/height</code> <code>std_msgs::msg::Float32</code> ground height <code>input/predicted_particles</code> <code>yabloc_particle_filter::msg::ParticleArray</code> predicted particles <code>input/pose_with_covariance</code> <code>geometry_msgs::msg::PoseWithCovarianceStamped</code> gnss measurement. used if <code>use_ublox_msg</code> is false <code>input/navpvt</code> <code>ublox_msgs::msg::NavPVT</code> gnss measurement. used if <code>use_ublox_msg</code> is true"},{"location":"localization/yabloc/yabloc_particle_filter/#output_1","title":"Output","text":"Name Type Description <code>output/weighted_particles</code> <code>yabloc_particle_filter::msg::ParticleArray</code> weighted particles <code>debug/gnss_range_marker</code> <code>visualization_msgs::msg::MarkerArray</code> gnss weight distribution <code>debug/particles_marker_array</code> <code>visualization_msgs::msg::MarkerArray</code> particles visualization. published if <code>visualize</code> is true"},{"location":"localization/yabloc/yabloc_particle_filter/#parameters_1","title":"Parameters","text":"Name Type Description Default Range acceptable_max_delay float how long to hold the predicted particles 1 N/A visualize boolean whether publish particles as marker_array or not 0 N/A mahalanobis_distance_threshold float if the Mahalanobis distance to the GNSS for particle exceeds this, the correction skips. 30 N/A for_fixed/max_weight float gnss weight distribution used when observation is fixed 5 N/A for_fixed/flat_radius float gnss weight distribution used when observation is fixed 0.5 N/A for_fixed/max_radius float gnss weight distribution used when observation is fixed 10 N/A for_fixed/min_weight float gnss weight distribution used when observation is fixed 0.5 N/A for_not_fixed/max_weight float gnss weight distribution used when observation is not fixed 1 N/A for_not_fixed/flat_radius float gnss weight distribution used when observation is not fixed 5 N/A for_not_fixed/max_radius float gnss weight distribution used when observation is not fixed 20 N/A for_not_fixed/min_weight float gnss weight distribution used when observation is not fixed 0.5 N/A"},{"location":"localization/yabloc/yabloc_particle_filter/#camera_particle_corrector","title":"camera_particle_corrector","text":""},{"location":"localization/yabloc/yabloc_particle_filter/#purpose_2","title":"Purpose","text":"<ul> <li>This node estimated particles weight using GNSS.</li> </ul>"},{"location":"localization/yabloc/yabloc_particle_filter/#inputs-outputs_2","title":"Inputs / Outputs","text":""},{"location":"localization/yabloc/yabloc_particle_filter/#input_2","title":"Input","text":"Name Type Description <code>input/predicted_particles</code> <code>yabloc_particle_filter::msg::ParticleArray</code> predicted particles <code>input/ll2_bounding_box</code> <code>sensor_msgs::msg::PointCloud2</code> road surface markings converted to line segments <code>input/ll2_road_marking</code> <code>sensor_msgs::msg::PointCloud2</code> road surface markings converted to line segments <code>input/projected_line_segments_cloud</code> <code>sensor_msgs::msg::PointCloud2</code> projected line segments <code>input/pose</code> <code>geometry_msgs::msg::PoseStamped</code> reference to retrieve the area map around the self location"},{"location":"localization/yabloc/yabloc_particle_filter/#output_2","title":"Output","text":"Name Type Description <code>output/weighted_particles</code> <code>yabloc_particle_filter::msg::ParticleArray</code> weighted particles <code>debug/cost_map_image</code> <code>sensor_msgs::msg::Image</code> cost map created from lanelet2 <code>debug/cost_map_range</code> <code>visualization_msgs::msg::MarkerArray</code> cost map boundary <code>debug/match_image</code> <code>sensor_msgs::msg::Image</code> projected line segments image <code>debug/scored_cloud</code> <code>sensor_msgs::msg::PointCloud2</code> weighted 3d line segments <code>debug/scored_post_cloud</code> <code>sensor_msgs::msg::PointCloud2</code> weighted 3d line segments which are iffy <code>debug/state_string</code> <code>std_msgs::msg::String</code> string describing the node state <code>debug/particles_marker_array</code> <code>visualization_msgs::msg::MarkerArray</code> particles visualization. published if <code>visualize</code> is true"},{"location":"localization/yabloc/yabloc_particle_filter/#parameters_2","title":"Parameters","text":"Name Type Description Default Range acceptable_max_delay float how long to hold the predicted particles 1 N/A visualize boolean whether publish particles as marker_array or not 0 N/A image_size float image size of debug/cost_map_image 800 N/A max_range float width of hierarchical cost map 40 N/A gamma float gamma value of the intensity gradient of the cost map 5 N/A min_prob float minimum particle weight the corrector node gives 0.1 N/A far_weight_gain float <code>exp(-far_weight_gain_ * squared_distance_from_camera)</code> is weight gain. if this is large, the nearby road markings will be more important 0.001 N/A enabled_at_first boolean if it is false, this node is not activated at first. you can activate by service call 1 N/A"},{"location":"localization/yabloc/yabloc_particle_filter/#services_1","title":"Services","text":"Name Type Description <code>switch_srv</code> <code>std_srvs::srv::SetBool</code> activation and deactivation of correction"},{"location":"localization/yabloc/yabloc_pose_initializer/","title":"yabloc_pose_initializer","text":""},{"location":"localization/yabloc/yabloc_pose_initializer/#yabloc_pose_initializer","title":"yabloc_pose_initializer","text":"<p>This package contains a node related to initial pose estimation.</p> <ul> <li>camera_pose_initializer</li> </ul> <p>This package requires the pre-trained semantic segmentation model for runtime. This model is usually downloaded by <code>ansible</code> during env preparation phase of the installation. It is also possible to download it manually. Even if the model is not downloaded, initialization will still complete, but the accuracy may be compromised.</p> <p>To download and extract the model manually:</p> <pre><code>$ mkdir -p ~/autoware_data/yabloc_pose_initializer/\n$ wget -P ~/autoware_data/yabloc_pose_initializer/ \\\n       https://s3.ap-northeast-2.wasabisys.com/pinto-model-zoo/136_road-segmentation-adas-0001/resources.tar.gz\n$ tar xzf ~/autoware_data/yabloc_pose_initializer/resources.tar.gz -C ~/autoware_data/yabloc_pose_initializer/\n</code></pre>"},{"location":"localization/yabloc/yabloc_pose_initializer/#note","title":"Note","text":"<p>This package makes use of external code. The trained files are provided by apollo. The trained files are automatically downloaded during env preparation.</p> <p>Original model URL</p> <p>https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/intel/road-segmentation-adas-0001</p> <p>Open Model Zoo is licensed under Apache License Version 2.0.</p> <p>Converted model URL</p> <p>https://github.com/PINTO0309/PINTO_model_zoo/tree/main/136_road-segmentation-adas-0001</p> <p>model conversion scripts are released under the MIT license</p>"},{"location":"localization/yabloc/yabloc_pose_initializer/#special-thanks","title":"Special thanks","text":"<ul> <li>openvinotoolkit/open_model_zoo</li> <li>PINTO0309</li> </ul>"},{"location":"localization/yabloc/yabloc_pose_initializer/#camera_pose_initializer","title":"camera_pose_initializer","text":""},{"location":"localization/yabloc/yabloc_pose_initializer/#purpose","title":"Purpose","text":"<ul> <li>This node estimates the initial position using the camera at the request of ADAPI.</li> </ul>"},{"location":"localization/yabloc/yabloc_pose_initializer/#input","title":"Input","text":"Name Type Description <code>input/camera_info</code> <code>sensor_msgs::msg::CameraInfo</code> undistorted camera info <code>input/image_raw</code> <code>sensor_msgs::msg::Image</code> undistorted camera image <code>input/vector_map</code> <code>autoware_map_msgs::msg::LaneletMapBin</code> vector map"},{"location":"localization/yabloc/yabloc_pose_initializer/#output","title":"Output","text":"Name Type Description <code>output/candidates</code> <code>visualization_msgs::msg::MarkerArray</code> initial pose candidates"},{"location":"localization/yabloc/yabloc_pose_initializer/#parameters","title":"Parameters","text":"Name Type Description Default Range angle_resolution float how many divisions of 1 sigma angle range 30 N/A"},{"location":"localization/yabloc/yabloc_pose_initializer/#services","title":"Services","text":"Name Type Description <code>yabloc_align_srv</code> <code>autoware_internal_localization_msgs::srv::PoseWithCovarianceStamped</code> initial pose estimation request"},{"location":"map/autoware_map_tf_generator/","title":"autoware_map_tf_generator","text":""},{"location":"map/autoware_map_tf_generator/#autoware_map_tf_generator","title":"autoware_map_tf_generator","text":""},{"location":"map/autoware_map_tf_generator/#purpose","title":"Purpose","text":"<p>The nodes in this package broadcast the <code>viewer</code> frame for visualization of the map in RViz.</p> <p>Note that there is no module to need the <code>viewer</code> frame and this is used only for visualization.</p> <p>The following are the supported methods to calculate the position of the <code>viewer</code> frame:</p> <ul> <li><code>pcd_map_tf_generator_node</code> outputs the geometric center of all points in the PCD.</li> <li><code>vector_map_tf_generator_node</code> outputs the geometric center of all points in the point layer.</li> </ul>"},{"location":"map/autoware_map_tf_generator/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"map/autoware_map_tf_generator/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"map/autoware_map_tf_generator/#input","title":"Input","text":""},{"location":"map/autoware_map_tf_generator/#autoware_pcd_map_tf_generator","title":"autoware_pcd_map_tf_generator","text":"Name Type Description <code>/map/pointcloud_map</code> <code>sensor_msgs::msg::PointCloud2</code> Subscribe pointcloud map to calculate position of <code>viewer</code> frames"},{"location":"map/autoware_map_tf_generator/#autoware_vector_map_tf_generator","title":"autoware_vector_map_tf_generator","text":"Name Type Description <code>/map/vector_map</code> <code>autoware_map_msgs::msg::LaneletMapBin</code> Subscribe vector map to calculate position of <code>viewer</code> frames"},{"location":"map/autoware_map_tf_generator/#output","title":"Output","text":"Name Type Description <code>/tf_static</code> <code>tf2_msgs/msg/TFMessage</code> Broadcast <code>viewer</code> frames"},{"location":"map/autoware_map_tf_generator/#parameters","title":"Parameters","text":""},{"location":"map/autoware_map_tf_generator/#node-parameters","title":"Node Parameters","text":"<p>None</p>"},{"location":"map/autoware_map_tf_generator/#core-parameters","title":"Core Parameters","text":"Name Type Description Default Range map_frame string The parent frame name of viewer frame map N/A viewer_frame string Name of <code>viewer</code> frame viewer N/A"},{"location":"map/autoware_map_tf_generator/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"map/util/lanelet2_map_preprocessor/","title":"Index","text":"<p>This package has been moved to https://github.com/autowarefoundation/autoware_tools/tree/main/map/autoware_lanelet2_map_utils .</p>"},{"location":"perception/autoware_bevfusion/","title":"autoware_bevfusion","text":""},{"location":"perception/autoware_bevfusion/#autoware_bevfusion","title":"autoware_bevfusion","text":""},{"location":"perception/autoware_bevfusion/#purpose","title":"Purpose","text":"<p>The <code>autoware_bevfusion</code> package is used for 3D object detection based on lidar or camera-lidar fusion.</p>"},{"location":"perception/autoware_bevfusion/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>This package implements a TensorRT powered inference node for BEVFusion [1]. The sparse convolution backend corresponds to spconv. Autoware installs it automatically in its setup script. If needed, the user can also build it and install it following the following instructions.</p>"},{"location":"perception/autoware_bevfusion/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_bevfusion/#input","title":"Input","text":"Name Type Description <code>~/input/pointcloud</code> <code>sensor_msgs::msg::PointCloud2</code> Input pointcloud topics. <code>~/input/image*</code> <code>sensor_msgs::msg::Image</code> Input image topics. <code>~/input/camera_info*</code> <code>sensor_msgs::msg::CameraInfo</code> Input camera info topics."},{"location":"perception/autoware_bevfusion/#output","title":"Output","text":"Name Type Description <code>~/output/objects</code> <code>autoware_perception_msgs::msg::DetectedObjects</code> Detected objects. <code>debug/cyclic_time_ms</code> <code>tier4_debug_msgs::msg::Float64Stamped</code> Cyclic time (ms). <code>debug/pipeline_latency_ms</code> <code>tier4_debug_msgs::msg::Float64Stamped</code> Pipeline latency time (ms). <code>debug/processing_time/preprocess_ms</code> <code>tier4_debug_msgs::msg::Float64Stamped</code> Preprocess (ms). <code>debug/processing_time/inference_ms</code> <code>tier4_debug_msgs::msg::Float64Stamped</code> Inference time (ms). <code>debug/processing_time/postprocess_ms</code> <code>tier4_debug_msgs::msg::Float64Stamped</code> Postprocess time (ms). <code>debug/processing_time/total_ms</code> <code>tier4_debug_msgs::msg::Float64Stamped</code> Total processing time (ms)."},{"location":"perception/autoware_bevfusion/#parameters","title":"Parameters","text":""},{"location":"perception/autoware_bevfusion/#bevfusion-node","title":"BEVFusion node","text":"Name Type Description Default Range max_camera_lidar_delay float Maximum delay between the lidar and the latest camera in seconds. 0.0 \u22650.0 plugins_path string A path to the TensorRT plugins. $(find-pkg-share autoware_bevfusion)/plugins/libautoware_tensorrt_plugins.so N/A trt_precision string A precision of TensorRT engine. fp16 ['fp16', 'fp32'] cloud_capacity integer Capacity of the point cloud buffer (should be set to at least the maximum theoretical number of points). 2000000 \u22651 onnx_path string A path to ONNX model file. $(var model_path)/bevfusion_camera_lidar.onnx N/A engine_path string A path to TensorRT engine file. $(var model_path)/bevfusion_camera_lidar.engine N/A image_backbone_onnx_path string A path to ONNX model file for image backbone. For fusion mode, all three image_backbone_* parameters must be non-empty. For lidar-only mode, all three must be empty strings. $(var model_path)/bevfusion_image_backbone.onnx N/A image_backbone_engine_path string A path to TensorRT engine file for image backbone. For fusion mode, all three image_backbone_* parameters must be non-empty. For lidar-only mode, all three must be empty strings. $(var model_path)/bevfusion_image_backbone.engine N/A image_backbone_trt_precision string A precision of TensorRT engine for image backbone. For fusion mode, all three image_backbone_* parameters must be non-empty. For lidar-only mode, all three must be empty strings. fp16 ['fp16', 'fp32', ''] densification_num_past_frames integer A number of past frames to be considered as same input frame. 1 \u22650 densification_world_frame_id string A name of frame id where world coordinates system is defined with respect to. map N/A circle_nms_dist_threshold float A distance threshold between detections in NMS. 0.5 \u22650.0 iou_nms_search_distance_2d float A maximum distance value to search the nearest objects. 10.0 \u22650.0 iou_nms_threshold float A threshold value of NMS using IoU score. 0.1 \u22650.0\u22641.0 yaw_norm_thresholds array A thresholds array of direction vectors norm, all of objects with vector norm less than this threshold are ignored. [0.3, 0.3, 0.3, 0.3, 0.0] N/A score_threshold float A threshold value of confidence score, all of objects with score less than this threshold are ignored. 0.2 \u22650.0"},{"location":"perception/autoware_bevfusion/#bevfusion-model","title":"BEVFusion model","text":"Name Type Description Default Range class_names array Predicted classes' names. ['CAR', 'TRUCK', 'BUS', 'BICYCLE', 'PEDESTRIAN'] N/A voxels_num array Voxel ranges used during inference [min, opt, max]. [5000, 30000, 60000] N/A point_cloud_range array Range in meters of the pointcloud in meters [min_x, min_y, min_z, max_x, max_y, max_z]. [-76.8, -76.8, -3.0, 76.8, 76.8, 5.0] N/A voxel_size array Voxels size [x, y, z] in meters. [0.3, 0.3, 8.0] N/A num_proposals integer Number of object proposals. 500 \u22651 out_size_factor integer Output size factor using in the network. 8 \u22651 max_points_per_voxel integer Maximum number of points that a voxel can hold. 10 \u22651 d_bound array Distance bounds used in the view transform in meters (min, max, and step). [1.0, 166.2, 1.4] N/A x_bound array x-axis bounds used in the view transform in meters (min, max, and step). [-122.4, 122.4, 0.68] N/A y_bound array y-axis bounds used in the view transform in meters (min, max, and step). [-122.4, 122.4, 0.68] N/A z_bound array z-axis bounds used in the view transform in meters (min, max, and step). [-10.0, 10.0, 20.0] N/A num_cameras integer Number of cameras to use. 6 \u22650 raw_image_height integer Raw image height in pixels. 1080 \u22650 raw_image_width integer Raw image width in pixels. 1440 \u22650 img_aug_scale_x float Raw image scaling before ROI extraction. 0.489 \u22650.0\u22641.0 img_aug_scale_y float Raw image scaling before ROI extraction. 0.489 \u22650.0\u22641.0 roi_height integer ROI image height (input to the network) in pixels. 384 \u22650 roi_width integer ROI image width (input to the network) in pixels. 704 \u22650 features_height integer Image features height (output of the image backbone) in pixels. 48 \u22650 features_width integer Image features width (output of the image backbone) in pixels. 88 \u22650 num_depth_features integer Number of depth features used in the view transform. 118 \u22650 image_feature_channel integer Image feature dimension (output channels of the image backbone). 256 \u22650 use_intensity boolean Whether to use intensity feature in point cloud processing. True N/A"},{"location":"perception/autoware_bevfusion/#detection-class-remapper","title":"Detection class remapper","text":"Name Type Description Default Range allow_remapping_by_area_matrix array Whether to allow remapping of classes. The order of 8x8 matrix classes comes from ObjectClassification msg. [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] N/A min_area_matrix array Minimum area for specific class to consider class remapping. [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.1, 0.0, 36.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 36.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 36.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] N/A max_area_matrix array Maximum area for specific class to consider class remapping. [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 36.0, 0.0, 999.999, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 999.999, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 999.999, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] N/A"},{"location":"perception/autoware_bevfusion/#the-build_only-option","title":"The <code>build_only</code> option","text":"<p>The <code>autoware_bevfusion</code> node has a <code>build_only</code> option to build the TensorRT engine file from the specified ONNX file, after which the program exits.</p> <pre><code>ros2 launch autoware_bevfusion bevfusion.launch.xml build_only:=true\n</code></pre>"},{"location":"perception/autoware_bevfusion/#the-log_level-option","title":"The <code>log_level</code> option","text":"<p>The default logging severity level for <code>autoware_bevfusion</code> is <code>info</code>. For debugging purposes, the developer may decrease severity level using <code>log_level</code> parameter:</p> <pre><code>ros2 launch autoware_bevfusion bevfusion.launch.xml log_level:=debug\n</code></pre>"},{"location":"perception/autoware_bevfusion/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>This node assumes that the input pointcloud follows the <code>PointXYZIRC</code> layout defined in <code>autoware_point_types</code>.</p>"},{"location":"perception/autoware_bevfusion/#trained-models","title":"Trained Models","text":"<p>You can download the onnx and config files in the following links. The files need to be placed inside <code>$(env HOME)/autoware_data/bevfusion</code></p> <ul> <li>lidar-only model:<ul> <li>onnx</li> <li>config</li> </ul> </li> <li>camera-lidar model:<ul> <li>onnx</li> <li>config</li> </ul> </li> <li>class remapper</li> </ul> <p>The model was trained in TIER IV's internal database (~35k lidar frames) for 30 epochs.</p>"},{"location":"perception/autoware_bevfusion/#changelog","title":"Changelog","text":""},{"location":"perception/autoware_bevfusion/#referencesexternal-links","title":"References/External links","text":"<p>[1] Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang, Huizi Mao, Daniela Rus, and Song Han. \"BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View Representation.\" 2023 International Conference on Robotics and Automation. </p>"},{"location":"perception/autoware_bevfusion/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":"<p>Although this node can perform camera-lidar fusion, as it is the first method in autoware to actually use images and lidars for inference, the package structure and its full integration in the autoware pipeline are left for future work. In the current structure, it can be employed without any changes as a lidar-based detector.</p>"},{"location":"perception/autoware_bytetrack/","title":"bytetrack","text":""},{"location":"perception/autoware_bytetrack/#bytetrack","title":"bytetrack","text":""},{"location":"perception/autoware_bytetrack/#purpose","title":"Purpose","text":"<p>The core algorithm, named <code>ByteTrack</code>, mainly aims to perform multi-object tracking. Because the algorithm associates almost every detection box including ones with low detection scores, the number of false negatives is expected to decrease by using it.</p> <p>demo video</p>"},{"location":"perception/autoware_bytetrack/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"perception/autoware_bytetrack/#cite","title":"Cite","text":"<ul> <li>Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Fucheng Weng, Zehuan Yuan, Ping Luo, Wenyu Liu, and Xinggang Wang,   \"ByteTrack: Multi-Object Tracking by Associating Every Detection Box\", in the proc. of the ECCV   2022, [ref]</li> <li>This package is ported version toward Autoware from this repository   (The C++ implementation by the ByteTrack's authors)</li> </ul>"},{"location":"perception/autoware_bytetrack/#2d-tracking-modification-from-original-codes","title":"2d tracking modification from original codes","text":"<p>The paper just says that the 2d tracking algorithm is a simple Kalman filter. Original codes use the <code>top-left-corner</code> and <code>aspect ratio</code> and <code>size</code> as the state vector.</p> <p>This is sometimes unstable because the aspect ratio can be changed by the occlusion. So, we use the <code>top-left</code> and <code>size</code> as the state vector.</p> <p>Kalman filter settings can be controlled by the parameters in <code>config/bytetrack_node.param.yaml</code>.</p>"},{"location":"perception/autoware_bytetrack/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_bytetrack/#bytetrack_node","title":"bytetrack_node","text":""},{"location":"perception/autoware_bytetrack/#input","title":"Input","text":"Name Type Description <code>in/rect</code> <code>tier4_perception_msgs/DetectedObjectsWithFeature</code> The detected objects with 2D bounding boxes"},{"location":"perception/autoware_bytetrack/#output","title":"Output","text":"Name Type Description <code>out/objects</code> <code>tier4_perception_msgs/DetectedObjectsWithFeature</code> The detected objects with 2D bounding boxes <code>out/objects/debug/uuid</code> <code>tier4_perception_msgs/DynamicObjectArray</code> The universally unique identifiers (UUID) for each object"},{"location":"perception/autoware_bytetrack/#bytetrack_visualizer","title":"bytetrack_visualizer","text":""},{"location":"perception/autoware_bytetrack/#input_1","title":"Input","text":"Name Type Description <code>in/image</code> <code>sensor_msgs/Image</code> or <code>sensor_msgs/CompressedImage</code> The input image on which object detection is performed <code>in/rect</code> <code>tier4_perception_msgs/DetectedObjectsWithFeature</code> The detected objects with 2D bounding boxes <code>in/uuid</code> <code>tier4_perception_msgs/DynamicObjectArray</code> The universally unique identifiers (UUID) for each object"},{"location":"perception/autoware_bytetrack/#output_1","title":"Output","text":"Name Type Description <code>out/image</code> <code>sensor_msgs/Image</code> The image that detection bounding boxes and their UUIDs are drawn"},{"location":"perception/autoware_bytetrack/#parameters","title":"Parameters","text":""},{"location":"perception/autoware_bytetrack/#bytetrack_node_1","title":"bytetrack_node","text":"Name Type Default Value Description <code>track_buffer_length</code> int 30 The frame count that a tracklet is considered to be lost"},{"location":"perception/autoware_bytetrack/#bytetrack_visualizer_1","title":"bytetrack_visualizer","text":"Name Type Default Value Description <code>use_raw</code> bool false The flag for the node to switch <code>sensor_msgs/Image</code> or <code>sensor_msgs/CompressedImage</code> as input"},{"location":"perception/autoware_bytetrack/#assumptionsknown-limits","title":"Assumptions/Known limits","text":""},{"location":"perception/autoware_bytetrack/#reference-repositories","title":"Reference repositories","text":"<ul> <li>https://github.com/ifzhang/ByteTrack</li> </ul>"},{"location":"perception/autoware_bytetrack/#license","title":"License","text":"<p>The codes under the <code>lib</code> directory are copied from the original codes and modified. The original codes belong to the MIT license stated as follows, while this ported packages are provided with Apache License 2.0:</p> <p>MIT License</p> <p>Copyright (c) 2021 Yifu Zhang</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"perception/autoware_camera_streampetr/","title":"autoware_camera_streampetr","text":""},{"location":"perception/autoware_camera_streampetr/#autoware_camera_streampetr","title":"autoware_camera_streampetr","text":""},{"location":"perception/autoware_camera_streampetr/#purpose","title":"Purpose","text":"<p>The <code>autoware_camera_streampetr</code> package is used for 3D object detection based on images only.</p>"},{"location":"perception/autoware_camera_streampetr/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>This package implements a TensorRT powered inference node for StreamPETR [1]. This is the first camera-only 3D object detection node in autoware.</p> <p>This node has been optimized for multi-camera systems where the camera topics are published in a sequential manner, not all at once. The node takes advantage of this by preprocessing (resize, crop, normalize) the images and storing them appropriately on GPU, so that delay due to preprocessing can be minimized.</p> <pre><code>Topic for image_i arrived                                     -------------------------\n  |                                                                                   |\n  |                                                                                   |\n  |                                                                                   |\n  v                                                                                   |\nIs image distorted?                                                                   |\n  |              \\                                                                    |\n  |               \\                                                                   |\nYes               No                                                                  |Image Updates\n  |                |                                                                  |done in parallel, if multitheading is on\n  v                |                                                                  |otherwise done sequentially in FIFO order\nUndistort          |                                                                  |\n  |                |                                                                  |\n  v                v                                                                  |\nLoad image into GPU memory                                                            |\n  |                                                                                   |\n  v                                                                                   |\nPreprocess image (scale &amp; crop ROI &amp; normalize)                                       |\n  |                                                                                   |\n  v                                                                                   |\nStore in GPU memory binding location for model input                                  |\n  |                                                          -------------------------|\n  v                                                                                   |\nIs image the `anchor_image`?                                                          |\n  |                \\                                                                  |\n  |                 \\                                                                 |\nNo                  Yes                                                               |\n  |                  |                                                                |\n  v                  v                                                                | If multithreading is on\n(Wait)     Are all images synced within `max_time_difference`?                        | image Updates are temporarily frozen\n                      |                           \\                                   | until this part completes.\n                      |                            \\                                  |\n                    Yes                             No                                |\n                      |                             |                                 |\n                      v                             v                                 |\n         Perform model forward pass            (Sync failed! Skip prediction)         |\n                      |                                                               |\n                      v                                                               |\n         Postprocess (NMS + ROS2 format)                                              |\n                      |                                                               |\n                      v                                                               |\n             Publish predictions                             -------------------------|\n</code></pre>"},{"location":"perception/autoware_camera_streampetr/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_camera_streampetr/#input","title":"Input","text":"Name Type Description <code>~/input/camera*/image</code> <code>sensor_msgs::msg::Image</code> or <code>sensor_msgs::msg::CompressedImage</code> Input image topics (supports both compressed and uncompressed). <code>~/input/camera*/camera_info</code> <code>sensor_msgs::msg::CameraInfo</code> Input camera info topics, for camera parameters."},{"location":"perception/autoware_camera_streampetr/#output","title":"Output","text":"Name Type Description RTX 3090 Latency (ms) <code>~/output/objects</code> <code>autoware_perception_msgs::msg::DetectedObjects</code> Detected objects. \u2014 <code>latency/preprocess</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> Preprocessing time per image(ms). 3.25 <code>latency/total</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> Total processing time (ms): preprocessing + inference + postprocessing. 26.04 <code>latency/inference</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> Total inference time (ms). 22.13 <code>latency/inference/backbone</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> Backbone inference time (ms). 16.21 <code>latency/inference/ptshead</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> Points head inference time (ms). 5.45 <code>latency/inference/pos_embed</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> Position embedding inference time (ms). 0.40 <code>latency/inference/postprocess</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> nms + filtering + converting network predictions to autoware format (ms). 0.40 <code>latency/cycle_time_ms</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> Time between two consecutive predictions (ms). 110.65"},{"location":"perception/autoware_camera_streampetr/#parameters","title":"Parameters","text":""},{"location":"perception/autoware_camera_streampetr/#streampetr-node","title":"StreamPETR node","text":"<p>The <code>autoware_camera_streampetr</code> node has various parameters for configuration:</p>"},{"location":"perception/autoware_camera_streampetr/#model-parameters","title":"Model Parameters","text":"<ul> <li><code>model_params.backbone_path</code>: Path to the backbone ONNX model</li> <li><code>model_params.head_path</code>: Path to the head ONNX model</li> <li><code>model_params.position_embedding_path</code>: Path to the position embedding ONNX model</li> <li><code>model_params.fp16_mode</code>: Enable FP16 inference mode</li> <li><code>model_params.use_temporal</code>: Enable temporal modeling</li> <li><code>model_params.input_image_height</code>: Input image height for preprocessing</li> <li><code>model_params.input_image_width</code>: Input image width for preprocessing</li> <li><code>model_params.class_names</code>: List of detection class names</li> <li><code>model_params.num_proposals</code>: Number of object proposals</li> <li><code>model_params.detection_range</code>: Detection range for filtering objects</li> </ul>"},{"location":"perception/autoware_camera_streampetr/#post-processing-parameters","title":"Post-processing Parameters","text":"<ul> <li><code>post_process_params.iou_nms_search_distance_2d</code>: 2D search distance for IoU NMS</li> <li><code>post_process_params.circle_nms_dist_threshold</code>: Distance threshold for circle NMS</li> <li><code>post_process_params.iou_nms_threshold</code>: IoU threshold for NMS</li> <li><code>post_process_params.confidence_threshold</code>: Confidence threshold for detections</li> <li><code>post_process_params.yaw_norm_thresholds</code>: Yaw normalization thresholds</li> </ul>"},{"location":"perception/autoware_camera_streampetr/#node-parameters","title":"Node Parameters","text":"<ul> <li><code>max_camera_time_diff</code>: Maximum allowed time difference between cameras (seconds)</li> <li><code>rois_number</code>: Number of camera ROIs/cameras (default: 6)</li> <li><code>is_compressed_image</code>: Whether input images are compressed</li> <li><code>is_distorted_image</code>: Whether input images are distorted</li> <li><code>multithreading</code>: Whether to use multithreading for handling image callbacks</li> <li><code>anchor_camera_id</code>: ID of the anchor camera for synchronization (default: 0)</li> <li><code>debug_mode</code>: Enable debug mode for timing measurements</li> <li><code>build_only</code>: Build TensorRT engines and exit without running inference</li> </ul>"},{"location":"perception/autoware_camera_streampetr/#the-build_only-option","title":"The <code>build_only</code> option","text":"<p>The <code>autoware_camera_streampetr</code> node has a <code>build_only</code> option to build the TensorRT engine files from the specified ONNX files, after which the program exits.</p> <pre><code>ros2 launch autoware_camera_streampetr tensorrt_stream_petr.launch.xml build_only:=true\n</code></pre>"},{"location":"perception/autoware_camera_streampetr/#the-log_level-option","title":"The <code>log_level</code> option","text":"<p>The default logging severity level for <code>autoware_camera_streampetr</code> is <code>info</code>. For debugging purposes, the developer may decrease severity level using <code>log_level</code> parameter:</p> <pre><code>ros2 launch autoware_camera_streampetr tensorrt_stream_petr.launch.xml log_level:=debug\n</code></pre>"},{"location":"perception/autoware_camera_streampetr/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>This node is camera-only and does not require pointcloud input. It assumes:</p> <ul> <li>All cameras are synchronized within the specified <code>max_camera_time_diff</code></li> <li>Camera calibration information is available and accurate</li> <li>The anchor camera (specified by <code>anchor_camera_id</code>) triggers the inference cycle</li> <li>Transform information between camera frames and base_link is available via tf</li> <li>Transform information between map and base_link is available via tf for ego motion compensation</li> <li>The input images are undistorted</li> </ul>"},{"location":"perception/autoware_camera_streampetr/#trained-models","title":"Trained Models","text":"<p>You can download the ONNX model files for StreamPETR. The files should be placed in the appropriate model directory as specified in the launch configuration.</p> <p>Required model files:</p> <ul> <li>Backbone ONNX model: TODO</li> <li>Head ONNX model: TODO</li> <li>Position embedding ONNX model: TODO</li> </ul> <p>If you want to train and deploy your own model, you can find the source code for that in AWML.</p>"},{"location":"perception/autoware_camera_streampetr/#changelog","title":"Changelog","text":""},{"location":"perception/autoware_camera_streampetr/#referencesexternal-links","title":"References/External links","text":"<p>[1] Wang, Shihao and Liu, Yingfei and Wang, Tiancai and Li, Ying and Zhang, Xiangyu. \"Exploring Object-Centric Temporal Modeling for Efficient Multi-View 3D Object Detection.\" 2023 </p>"},{"location":"perception/autoware_camera_streampetr/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":"<ul> <li>Enable 2d object detection. Because 2d object detection is used as an auxiliary loss during training, the same node can easily support 2d object detection with minor updates.</li> <li>Implement int8 quantization for the backbone to further reduce inference latency</li> <li>Execute the image backbone for each image as they arrive, to further reduce latency.</li> <li>Add velocity to predictions.</li> </ul>"},{"location":"perception/autoware_cluster_merger/","title":"autoware cluster merger","text":""},{"location":"perception/autoware_cluster_merger/#autoware-cluster-merger","title":"autoware cluster merger","text":""},{"location":"perception/autoware_cluster_merger/#purpose","title":"Purpose","text":"<p>autoware_cluster_merger is a package for merging pointcloud clusters as detected objects with feature type.</p>"},{"location":"perception/autoware_cluster_merger/#inner-working-algorithms","title":"Inner-working / Algorithms","text":"<p>The clusters of merged topics are simply concatenated from clusters of input topics.</p>"},{"location":"perception/autoware_cluster_merger/#input-output","title":"Input / Output","text":""},{"location":"perception/autoware_cluster_merger/#input","title":"Input","text":"Name Type Description <code>input/cluster0</code> <code>tier4_perception_msgs::msg::DetectedObjectsWithFeature</code> pointcloud clusters <code>input/cluster1</code> <code>tier4_perception_msgs::msg::DetectedObjectsWithFeature</code> pointcloud clusters"},{"location":"perception/autoware_cluster_merger/#output","title":"Output","text":"Name Type Description <code>output/clusters</code> <code>tier4_perception_msgs::msg::DetectedObjectsWithFeature</code> merged clusters"},{"location":"perception/autoware_cluster_merger/#parameters","title":"Parameters","text":"Name Type Description Default Range output_frame_id string The header frame_id of output topic. base_link N/A"},{"location":"perception/autoware_cluster_merger/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"perception/autoware_cluster_merger/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/autoware_cluster_merger/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/autoware_cluster_merger/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"perception/autoware_cluster_merger/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/autoware_compare_map_segmentation/","title":"autoware_compare_map_segmentation","text":""},{"location":"perception/autoware_compare_map_segmentation/#autoware_compare_map_segmentation","title":"autoware_compare_map_segmentation","text":""},{"location":"perception/autoware_compare_map_segmentation/#purpose","title":"Purpose","text":"<p>The <code>autoware_compare_map_segmentation</code> is a package that filters the ground points from the input pointcloud by using map info (e.g. pcd, elevation map or split map pointcloud from map_loader interface).</p>"},{"location":"perception/autoware_compare_map_segmentation/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"perception/autoware_compare_map_segmentation/#compare-elevation-map-filter","title":"Compare Elevation Map Filter","text":"<p>Compare the z of the input points with the value of elevation_map. The height difference is calculated by the binary integration of neighboring cells. Remove points whose height difference is below the <code>height_diff_thresh</code>.</p> <p> </p>"},{"location":"perception/autoware_compare_map_segmentation/#distance-based-compare-map-filter","title":"Distance Based Compare Map Filter","text":"<p>This filter compares the input pointcloud with the map pointcloud using the <code>nearestKSearch</code> function of <code>kdtree</code> and removes points that are close to the map point cloud. The map pointcloud can be loaded statically at once at the beginning or dynamically as the vehicle moves.</p>"},{"location":"perception/autoware_compare_map_segmentation/#voxel-based-approximate-compare-map-filter","title":"Voxel Based Approximate Compare Map Filter","text":"<p>The filter loads the map point cloud, which can be loaded statically at the beginning or dynamically during vehicle movement, and creates a voxel grid of the map point cloud. The filter uses the getCentroidIndexAt function in combination with the getGridCoordinates function from the VoxelGrid class to find input points that are inside the voxel grid and removes them.</p>"},{"location":"perception/autoware_compare_map_segmentation/#voxel-based-compare-map-filter","title":"Voxel Based Compare Map Filter","text":"<p>The filter loads the map pointcloud (static loading whole map at once at beginning or dynamic loading during vehicle moving) and utilizes VoxelGrid to downsample map pointcloud.</p> <p>For each point of input pointcloud, the filter use <code>getCentroidIndexAt</code> combine with <code>getGridCoordinates</code> function from VoxelGrid class to check if the downsampled map point existing surrounding input points. Remove the input point which has downsampled map point in voxels containing or being close to the point.</p>"},{"location":"perception/autoware_compare_map_segmentation/#voxel-distance-based-compare-map-filter","title":"Voxel Distance based Compare Map Filter","text":"<p>This filter is a combination of the distance_based_compare_map_filter and voxel_based_approximate_compare_map_filter. The filter loads the map point cloud, which can be loaded statically at the beginning or dynamically during vehicle movement, and creates a voxel grid and a k-d tree of the map point cloud. The filter uses the getCentroidIndexAt function in combination with the getGridCoordinates function from the VoxelGrid class to find input points that are inside the voxel grid and removes them. For points that do not belong to any voxel grid, they are compared again with the map point cloud using the radiusSearch function of the k-d tree and are removed if they are close enough to the map.</p>"},{"location":"perception/autoware_compare_map_segmentation/#lanelet-elevation-filter","title":"Lanelet Elevation Filter","text":"<p>The Lanelet Elevation Filter filters point clouds based on lanelet elevation information. It creates a grid-based elevation map from lanelet data and filters out points that deviate significantly from the expected road surface height. This filter is useful for removing floating objects, overpass structures, and other non-road elements that should not be considered for ground-level navigation.</p> <p>The filter processes lanelet maps to extract elevation information at regular grid intervals and uses this information to validate incoming point cloud data. Points that are too far above or below the expected lanelet surface elevation are filtered out.</p> <p>If incoming point cloud frame differs from target_frame, points will be transformed to target_frame before elevation check.</p>"},{"location":"perception/autoware_compare_map_segmentation/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_compare_map_segmentation/#compare-elevation-map-filter_1","title":"Compare Elevation Map Filter","text":""},{"location":"perception/autoware_compare_map_segmentation/#input","title":"Input","text":"Name Type Description <code>~/input/points</code> <code>sensor_msgs::msg::PointCloud2</code> reference points <code>~/input/elevation_map</code> <code>grid_map::msg::GridMap</code> elevation map"},{"location":"perception/autoware_compare_map_segmentation/#output","title":"Output","text":"Name Type Description <code>~/output/points</code> <code>sensor_msgs::msg::PointCloud2</code> filtered points"},{"location":"perception/autoware_compare_map_segmentation/#parameters","title":"Parameters","text":"Name Type Description Default value <code>map_layer_name</code> string elevation map layer name elevation <code>map_frame</code> float frame_id of the map that is temporarily used before elevation_map is subscribed map <code>height_diff_thresh</code> float Remove points whose height difference is below this value [m] 0.15"},{"location":"perception/autoware_compare_map_segmentation/#lanelet-elevation-filter_1","title":"Lanelet Elevation Filter","text":""},{"location":"perception/autoware_compare_map_segmentation/#input_1","title":"Input","text":"Name Type Description <code>~/input/pointcloud</code> <code>sensor_msgs::msg::PointCloud2</code> input point cloud <code>~/input/lanelet_map</code> <code>autoware_map_msgs::msg::LaneletMapBin</code> lanelet map"},{"location":"perception/autoware_compare_map_segmentation/#output_1","title":"Output","text":"Name Type Description <code>~/output/pointcloud</code> <code>sensor_msgs::msg::PointCloud2</code> filtered point cloud <code>~/debug/elevation_markers</code> <code>visualization_msgs::msg::MarkerArray</code> elevation grid visualization"},{"location":"perception/autoware_compare_map_segmentation/#parameters_1","title":"Parameters","text":"Name Type Description Default value <code>grid_resolution</code> double Grid cell size in meters for elevation processing 1.0 <code>height_threshold</code> double Maximum height difference from lanelet elevation (meters) 2.0 <code>sampling_distance</code> double Distance between sampled points along lanelet boundaries (meters) 0.5 <code>extension_count</code> int Number of cells to extend around original lanelet points 5 <code>target_frame</code> string Target coordinate frame for processing map <code>cache_directory</code> string Directory for cached grid files $(find-pkg-share autoware_compare_map_segmentation)/data/lanelet_grid_cache <code>require_map_coverage</code> bool If true, only keep points with direct map coverage; reject points requiring interpolation true <code>enable_debug</code> bool Enable debug mode (includes elevation markers and processing time publisher) false"},{"location":"perception/autoware_compare_map_segmentation/#other-filters","title":"Other Filters","text":""},{"location":"perception/autoware_compare_map_segmentation/#input_2","title":"Input","text":"Name Type Description <code>~/input/points</code> <code>sensor_msgs::msg::PointCloud2</code> reference points <code>~/input/map</code> <code>sensor_msgs::msg::PointCloud2</code> map (in case static map loading) <code>/localization/kinematic_state</code> <code>nav_msgs::msg::Odometry</code> current ego-vehicle pose (in case dynamic map loading)"},{"location":"perception/autoware_compare_map_segmentation/#output_2","title":"Output","text":"Name Type Description <code>~/output/points</code> <code>sensor_msgs::msg::PointCloud2</code> filtered points"},{"location":"perception/autoware_compare_map_segmentation/#parameters_2","title":"Parameters","text":"Name Type Description Default value <code>use_dynamic_map_loading</code> bool map loading mode selection, <code>true</code> for dynamic map loading, <code>false</code> for static map loading, recommended for no-split map pointcloud true <code>distance_threshold</code> float Threshold distance to compare input points with map points [m] 0.5 <code>map_update_distance_threshold</code> float Threshold of vehicle movement distance when map update is necessary (in dynamic map loading) [m] 10.0 <code>map_loader_radius</code> float Radius of map need to be loaded (in dynamic map loading) [m] 150.0 <code>timer_interval_ms</code> int Timer interval to check if the map update is necessary (in dynamic map loading) [ms] 100 <code>publish_debug_pcd</code> bool Enable to publish voxelized updated map in <code>debug/downsampled_map/pointcloud</code> for debugging. It might cause additional computation cost false <code>downsize_ratio_z_axis</code> double Positive ratio to reduce voxel_leaf_size and neighbor point distance threshold in z axis 0.5"},{"location":"perception/autoware_compare_map_segmentation/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"perception/autoware_compare_map_segmentation/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/autoware_compare_map_segmentation/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/autoware_compare_map_segmentation/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"perception/autoware_compare_map_segmentation/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/autoware_crosswalk_traffic_light_estimator/","title":"autoware_crosswalk_traffic_light_estimator","text":""},{"location":"perception/autoware_crosswalk_traffic_light_estimator/#autoware_crosswalk_traffic_light_estimator","title":"autoware_crosswalk_traffic_light_estimator","text":""},{"location":"perception/autoware_crosswalk_traffic_light_estimator/#purpose","title":"Purpose","text":"<p><code>autoware_crosswalk_traffic_light_estimator</code> estimates pedestrian traffic signals which can be summarized as the following two tasks:</p> <ul> <li>Estimate pedestrian traffic signals that are not subject to be detected by perception pipeline.</li> <li>Estimate whether pedestrian traffic signals are flashing and modify the result.</li> </ul> <p>This module works without <code>~/input/route</code>, but its behavior is outputting the subscribed results as is.</p>"},{"location":"perception/autoware_crosswalk_traffic_light_estimator/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_crosswalk_traffic_light_estimator/#input","title":"Input","text":"Name Type Description <code>~/input/vector_map</code> autoware_map_msgs::msg::LaneletMapBin vector map <code>~/input/route</code> autoware_planning_msgs::msg::LaneletRoute optional: route <code>~/input/classified/traffic_signals</code> autoware_perception_msgs::msg::TrafficLightGroupArray classified signals"},{"location":"perception/autoware_crosswalk_traffic_light_estimator/#output","title":"Output","text":"Name Type Description <code>~/output/traffic_signals</code> autoware_perception_msgs::msg::TrafficLightGroupArray output that contains estimated pedestrian traffic signals <code>~/debug/processing_time_ms</code> autoware_internal_debug_msgs::msg::Float64Stamped pipeline latency time (ms)"},{"location":"perception/autoware_crosswalk_traffic_light_estimator/#parameters","title":"Parameters","text":"Name Type Description Default value <code>use_last_detect_color</code> bool If this parameter is <code>true</code>, this module estimates pedestrian's traffic signal as RED not only when vehicle's traffic signal is detected as GREEN/AMBER but also when detection results change GREEN/AMBER to UNKNOWN. (If detection results change RED or AMBER to UNKNOWN, this module estimates pedestrian's traffic signal as UNKNOWN.) If this parameter is <code>false</code>, this module use only latest detection results for estimation. (Only when the detection result is GREEN/AMBER, this module estimates pedestrian's traffic signal as RED.) true <code>use_pedestrian_signal_detect</code> bool If this parameter is <code>true</code>, use the pedestrian's traffic signal estimated by the perception pipeline. If <code>false</code>, overwrite it with pedestrian's signals estimated from vehicle traffic signals, HDMap, and route. true <code>last_detect_color_hold_time</code> double The time threshold to hold for last detect color. The unit is second. 2.0 <code>last_colors_hold_time</code> double The time threshold to hold for history detected pedestrian traffic light color. The unit is second. 1.0"},{"location":"perception/autoware_crosswalk_traffic_light_estimator/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>When the pedestrian traffic signals are detected by perception pipeline</p> <ul> <li>If estimates the pedestrian traffic signals are flashing, overwrite the results</li> <li>Prefer the output from perception pipeline, but overwrite it if the pedestrian traffic signals are invalid(<code>no detection</code>, <code>backlight</code>, or <code>occlusion</code>)</li> </ul> <p>When the pedestrian traffic signals are NOT detected by perception pipeline</p> <ul> <li>Estimate the color of pedestrian traffic signals based on detected vehicle traffic signals, HDMap, and route</li> </ul> <p>Override rules specific to some traffic lights can also be defined in the lanelet map. In that case, the crosswalk traffic light estimation is overridden by these rules.</p>"},{"location":"perception/autoware_crosswalk_traffic_light_estimator/#estimate-whether-pedestrian-traffic-signals-are-flashing","title":"Estimate whether pedestrian traffic signals are flashing","text":"<pre><code>start\nif (the pedestrian traffic light classification result exists)then\n    : update the flashing flag according to the classification result(in_signal) and last_signals\n    if (the traffic light is flashing?)then(yes)\n      : update the traffic light state\n    else(no)\n      : the traffic light state is the same with the classification result\nif (the classification result not exists)\n    : the traffic light state is the same with the estimation\n : output the current traffic light state\nend\n</code></pre>"},{"location":"perception/autoware_crosswalk_traffic_light_estimator/#update-flashing-flag","title":"Update flashing flag","text":""},{"location":"perception/autoware_crosswalk_traffic_light_estimator/#update-traffic-light-status","title":"Update traffic light status","text":""},{"location":"perception/autoware_crosswalk_traffic_light_estimator/#estimate-the-color-of-pedestrian-traffic-signals","title":"Estimate the color of pedestrian traffic signals","text":"<p>If traffic between pedestrians and vehicles is controlled by traffic signals, the crosswalk traffic signal maybe RED in order to prevent pedestrian from crossing when the following conditions are satisfied.</p>"},{"location":"perception/autoware_crosswalk_traffic_light_estimator/#situation1","title":"Situation1","text":"<ul> <li>crosswalk conflicts STRAIGHT lanelet</li> <li>the lanelet refers GREEN or AMBER traffic signal (The following pictures show only GREEN case)</li> </ul>"},{"location":"perception/autoware_crosswalk_traffic_light_estimator/#situation2","title":"Situation2","text":"<ul> <li>crosswalk conflicts different turn direction lanelets (STRAIGHT and LEFT, LEFT and RIGHT, RIGHT and STRAIGHT)</li> <li>the lanelets refer GREEN or AMBER traffic signal (The following pictures show only GREEN case)</li> </ul>"},{"location":"perception/autoware_crosswalk_traffic_light_estimator/#map-based-estimation-rules","title":"Map-based estimation rules","text":"<p>Rules can be defined in the lanelet map to override the normal estimation. These rules define the value of a crosswalk traffic light based on the value of a vehicle traffic light.</p> <p>For example, a rule to consider the crosswalk traffic light with id X to be <code>green</code> when the vehicle traffic light with id Y is <code>red</code> can be expressed in the lanelet map as follows:</p> <pre><code>  &lt;relation id=\"Y\"&gt;\n    ...\n    &lt;tag k=\"signal_color_relation:red:green\" v=\"X\"/&gt;\n</code></pre> <p>Colors <code>green</code>, <code>amber</code>, <code>red</code>, and <code>white</code> are currently supported. Multiple crosswalk ids can be listed separated by a comma and without any whitespace (e.g., <code>v=\"1,2,3\"</code>).</p>"},{"location":"perception/autoware_crosswalk_traffic_light_estimator/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"perception/autoware_crosswalk_traffic_light_estimator/#future-extensions-unimplemented-parts","title":"Future extensions / Unimplemented parts","text":""},{"location":"perception/autoware_detected_object_feature_remover/","title":"autoware_detected_object_feature_remover","text":""},{"location":"perception/autoware_detected_object_feature_remover/#autoware_detected_object_feature_remover","title":"autoware_detected_object_feature_remover","text":""},{"location":"perception/autoware_detected_object_feature_remover/#purpose","title":"Purpose","text":"<p>The <code>autoware_detected_object_feature_remover</code> is a package to convert topic-type from <code>DetectedObjectWithFeatureArray</code> to <code>DetectedObjects</code>.</p>"},{"location":"perception/autoware_detected_object_feature_remover/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"perception/autoware_detected_object_feature_remover/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_detected_object_feature_remover/#input","title":"Input","text":"Name Type Description <code>~/input</code> <code>tier4_perception_msgs::msg::DetectedObjectWithFeatureArray</code> detected objects with feature field"},{"location":"perception/autoware_detected_object_feature_remover/#output","title":"Output","text":"Name Type Description <code>~/output</code> <code>autoware_perception_msgs::msg::DetectedObjects</code> detected objects"},{"location":"perception/autoware_detected_object_feature_remover/#parameters","title":"Parameters","text":"<p>None</p>"},{"location":"perception/autoware_detected_object_feature_remover/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"perception/autoware_detected_object_validation/","title":"detected_object_validation","text":""},{"location":"perception/autoware_detected_object_validation/#detected_object_validation","title":"detected_object_validation","text":""},{"location":"perception/autoware_detected_object_validation/#purpose","title":"Purpose","text":"<p>The purpose of this package is to eliminate obvious false positives of DetectedObjects.</p>"},{"location":"perception/autoware_detected_object_validation/#referencesexternal-links","title":"References/External links","text":"<ul> <li>Obstacle pointcloud based validator</li> <li>Occupancy grid based validator</li> <li>Object lanelet filter</li> <li>Object position filter</li> </ul>"},{"location":"perception/autoware_detected_object_validation/#node-parameters","title":"Node Parameters","text":""},{"location":"perception/autoware_detected_object_validation/#object_lanelet_filter","title":"object_lanelet_filter","text":""},{"location":"perception/autoware_detected_object_validation/#detected-objects","title":"Detected Objects","text":"Name Type Description Default Range filter_target_label.UNKNOWN boolean If true, unknown objects are filtered. 1 N/A filter_target_label.CAR boolean If true, car objects are filtered. 0 N/A filter_target_label.TRUCK boolean If true, truck objects are filtered. 0 N/A filter_target_label.BUS boolean If true, bus objects are filtered. 0 N/A filter_target_label.TRAILER boolean If true, trailer objects are filtered. 0 N/A filter_target_label.MOTORCYCLE boolean If true, motorcycle objects are filtered. 0 N/A filter_target_label.BICYCLE boolean If true, bicycle objects are filtered. 0 N/A filter_target_label.PEDESTRIAN boolean If true, pedestrian objects are filtered. 0 N/A lanelet_xy_overlap_filter.enabled boolean If true, objects that are not in the lanelet polygon are filtered. 1 N/A lanelet_direction_filter.enabled boolean If true, objects that are not in the same direction as the lanelet are filtered. 0 N/A lanelet_direction_filter.velocity_yaw_threshold float If the yaw difference between the object and the lanelet is greater than this value, the object is filtered. 0.785398 N/A lanelet_direction_filter.object_speed_threshold float If the object speed is greater than this value, the object is filtered. 3 N/A lanelet_object_elevation_filter.enabled boolean If true, the objects not within the elevation range relative to the lanelet surface will be filtered. 0 N/A lanelet_object_elevation_filter.max_elevation_threshold float Maximum elevation threshold for filtering objects (in meters). 5 N/A lanelet_object_elevation_filter.min_elevation_threshold float Minimum elevation threshold for filtering objects (in meters). 0 N/A filter_settings.lanelet_extra_margin float Extra margin added to the lanelet boundaries. 0 N/A filter_settings.debug boolean If true, debug information is enabled. 0 N/A"},{"location":"perception/autoware_detected_object_validation/#tracked-objects","title":"Tracked Objects","text":"Name Type Description Default Range filter_target_label.UNKNOWN boolean If true, unknown objects are filtered. 1 N/A filter_target_label.CAR boolean If true, car objects are filtered. 0 N/A filter_target_label.TRUCK boolean If true, truck objects are filtered. 0 N/A filter_target_label.BUS boolean If true, bus objects are filtered. 0 N/A filter_target_label.TRAILER boolean If true, trailer objects are filtered. 0 N/A filter_target_label.MOTORCYCLE boolean If true, motorcycle objects are filtered. 0 N/A filter_target_label.BICYCLE boolean If true, bicycle objects are filtered. 0 N/A filter_target_label.PEDESTRIAN boolean If true, pedestrian objects are filtered. 0 N/A lanelet_xy_overlap_filter.enabled boolean If true, objects that are not in the lanelet polygon are filtered. 1 N/A lanelet_direction_filter.enabled boolean If true, objects that are not in the same direction as the lanelet are filtered. 0 N/A lanelet_direction_filter.velocity_yaw_threshold float If the yaw difference between the object and the lanelet is greater than this value, the object is filtered. 0.785398 N/A lanelet_direction_filter.object_speed_threshold float If the object speed is greater than this value, the object is filtered. 3 N/A lanelet_object_elevation_filter.enabled boolean If true, the objects not within the elevation range relative to the lanelet surface will be filtered. 0 N/A lanelet_object_elevation_filter.max_elevation_threshold float Maximum elevation threshold for filtering objects (in meters). 5 N/A lanelet_object_elevation_filter.min_elevation_threshold float Minimum elevation threshold for filtering objects (in meters). 0 N/A filter_settings.lanelet_extra_margin float Extra margin added to the lanelet boundaries. 0 N/A filter_settings.debug boolean If true, debug information is enabled. 0 N/A"},{"location":"perception/autoware_detected_object_validation/#object_position_filter","title":"object_position_filter","text":"Name Type Description Default Range filter_target_label.UNKNOWN boolean Filter for UNKNOWN label 1 N/A filter_target_label.CAR boolean Filter for CAR label 0 N/A filter_target_label.TRUCK boolean Filter for TRUCK label 0 N/A filter_target_label.BUS boolean Filter for BUS label 0 N/A filter_target_label.TRAILER boolean Filter for TRAILER label 0 N/A filter_target_label.MOTORCYCLE boolean Filter for MOTORCYCLE label 0 N/A filter_target_label.BICYCLE boolean Filter for BICYCLE label 0 N/A filter_target_label.PEDESTRIAN boolean Filter for PEDESTRIAN label 0 N/A upper_bound_x float Upper bound for X coordinate 100 N/A lower_bound_x float Lower bound for X coordinate 0 N/A upper_bound_y float Upper bound for Y coordinate 10 N/A lower_bound_y float Lower bound for Y coordinate -10 N/A"},{"location":"perception/autoware_detected_object_validation/#obstacle_pointcloud_based_validator","title":"obstacle_pointcloud_based_validator","text":"Name Type Description Default Range min_points_num array The minimum number of obstacle point clouds in DetectedObjects [10, 10, 10, 10, 10, 10, 10, 10] N/A max_points_num array The max number of obstacle point clouds in DetectedObjects [10, 10, 10, 10, 10, 10, 10, 10] N/A min_points_and_distance_ratio array Threshold value of the number of point clouds per object when the distance from baselink is 1m, because the number of point clouds varies with the distance from baselink. [800, 800, 800, 800, 800, 800, 800, 800] N/A validate_max_distance_m float The maximum distance from the baselink to the object to be validated 70.0 N/A using_2d_validator boolean The xy-plane projected (2D) obstacle point clouds will be used for validation False N/A enable_debugger boolean Whether to create debug topics or not? False N/A"},{"location":"perception/autoware_detected_object_validation/#occupancy_grid_based_validator","title":"occupancy_grid_based_validator","text":"Name Type Description Default Range mean_threshold float The percentage threshold of allowed non-freespace. 0.6 N/A enable_debug boolean Whether to display debug images or not? 0 N/A"},{"location":"perception/autoware_detected_object_validation/object-lanelet-filter/","title":"object_lanelet_filter","text":""},{"location":"perception/autoware_detected_object_validation/object-lanelet-filter/#object_lanelet_filter","title":"object_lanelet_filter","text":""},{"location":"perception/autoware_detected_object_validation/object-lanelet-filter/#purpose","title":"Purpose","text":"<p>The <code>object_lanelet_filter</code> is a node that filters detected object by using vector map. The objects only inside of the vector map will be published.</p>"},{"location":"perception/autoware_detected_object_validation/object-lanelet-filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"perception/autoware_detected_object_validation/object-lanelet-filter/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_detected_object_validation/object-lanelet-filter/#input","title":"Input","text":"Name Type Description <code>input/vector_map</code> <code>autoware_map_msgs::msg::LaneletMapBin</code> vector map <code>input/object</code> <code>autoware_perception_msgs::msg::DetectedObjects</code> input detected objects"},{"location":"perception/autoware_detected_object_validation/object-lanelet-filter/#output","title":"Output","text":"Name Type Description <code>output/object</code> <code>autoware_perception_msgs::msg::DetectedObjects</code> filtered detected objects"},{"location":"perception/autoware_detected_object_validation/object-lanelet-filter/#parameters","title":"Parameters","text":"<p>Description of the <code>filter_settings</code> in the parameters of the <code>object_lanelet_filter</code> node.</p> Name Type Description <code>debug</code> <code>bool</code> If <code>true</code>, publishes additional debug information, including visualization markers on the <code>~/debug/marker</code> topic for tools like RViz. <code>lanelet_extra_margin</code> <code>double</code> If <code>&gt; 0</code>, expands the lanelet polygons used for overlap checks by this margin (in meters). If <code>&lt;= 0</code>, polygon expansion is disabled. <code>lanelet_xy_overlap_filter.enabled</code> <code>bool</code> If <code>true</code>, enables filtering of objects based on their overlap with lanelet polygons. <code>lanelet_direction_filter.enabled</code> <code>bool</code> If <code>true</code>, enables filtering of objects based on their velocity direction relative to the lanelet. <code>lanelet_direction_filter.velocity_yaw_threshold</code> <code>double</code> The yaw angle difference threshold (in radians) between the object\u2019s velocity vector and the lanelet direction. <code>lanelet_direction_filter.object_speed_threshold</code> <code>double</code> The minimum speed (in m/s) of an object required for the direction filter to be applied. <code>lanelet_object_elevation_filter.enabled</code> <code>bool</code> If <code>true</code>, enables filtering of objects based on their elevation relative to the nearest lanelet surface. <code>max_elevation_threshold</code> <code>double</code> The maximum allowable elevation (in meters) of an object relative to the nearest lanelet surface. <code>min_elevation_threshold</code> <code>double</code> The minimum allowable elevation (in meters) of an object relative to the nearest lanelet surface. <code>lanelet_extra_margin</code> <code>double</code> The margin value that will be added to the lanelet boundaries."},{"location":"perception/autoware_detected_object_validation/object-lanelet-filter/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Description <code>filter_target_label.UNKNOWN</code> bool false If true, unknown objects are filtered. <code>filter_target_label.CAR</code> bool false If true, car objects are filtered. <code>filter_target_label.TRUCK</code> bool false If true, truck objects are filtered. <code>filter_target_label.BUS</code> bool false If true, bus objects are filtered. <code>filter_target_label.TRAILER</code> bool false If true, trailer objects are filtered. <code>filter_target_label.MOTORCYCLE</code> bool false If true, motorcycle objects are filtered. <code>filter_target_label.BICYCLE</code> bool false If true, bicycle objects are filtered. <code>filter_target_label.PEDESTRIAN</code> bool false If true, pedestrian objects are filtered."},{"location":"perception/autoware_detected_object_validation/object-lanelet-filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>The lanelet filter is performed based on the shape polygon and bounding box of the objects.</p>"},{"location":"perception/autoware_detected_object_validation/object-lanelet-filter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/autoware_detected_object_validation/object-lanelet-filter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/autoware_detected_object_validation/object-lanelet-filter/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"perception/autoware_detected_object_validation/object-lanelet-filter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/autoware_detected_object_validation/object-position-filter/","title":"object_position_filter","text":""},{"location":"perception/autoware_detected_object_validation/object-position-filter/#object_position_filter","title":"object_position_filter","text":""},{"location":"perception/autoware_detected_object_validation/object-position-filter/#purpose","title":"Purpose","text":"<p>The <code>object_position_filter</code> is a node that filters detected object based on x,y values. The objects only inside of the x, y bound will be published.</p>"},{"location":"perception/autoware_detected_object_validation/object-position-filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"perception/autoware_detected_object_validation/object-position-filter/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_detected_object_validation/object-position-filter/#input","title":"Input","text":"Name Type Description <code>input/object</code> <code>autoware_perception_msgs::msg::DetectedObjects</code> input detected objects"},{"location":"perception/autoware_detected_object_validation/object-position-filter/#output","title":"Output","text":"Name Type Description <code>output/object</code> <code>autoware_perception_msgs::msg::DetectedObjects</code> filtered detected objects"},{"location":"perception/autoware_detected_object_validation/object-position-filter/#parameters","title":"Parameters","text":""},{"location":"perception/autoware_detected_object_validation/object-position-filter/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Description <code>filter_target_label.UNKNOWN</code> bool false If true, unknown objects are filtered. <code>filter_target_label.CAR</code> bool false If true, car objects are filtered. <code>filter_target_label.TRUCK</code> bool false If true, truck objects are filtered. <code>filter_target_label.BUS</code> bool false If true, bus objects are filtered. <code>filter_target_label.TRAILER</code> bool false If true, trailer objects are filtered. <code>filter_target_label.MOTORCYCLE</code> bool false If true, motorcycle objects are filtered. <code>filter_target_label.BICYCLE</code> bool false If true, bicycle objects are filtered. <code>filter_target_label.PEDESTRIAN</code> bool false If true, pedestrian objects are filtered. <code>upper_bound_x</code> float 100.00 Bound for filtering. Only used if filter_by_xy_position is true <code>lower_bound_x</code> float 0.00 Bound for filtering. Only used if filter_by_xy_position is true <code>upper_bound_y</code> float 50.00 Bound for filtering. Only used if filter_by_xy_position is true <code>lower_bound_y</code> float -50.00 Bound for filtering. Only used if filter_by_xy_position is true"},{"location":"perception/autoware_detected_object_validation/object-position-filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>Filtering is performed based on the center position of the object.</p>"},{"location":"perception/autoware_detected_object_validation/object-position-filter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/autoware_detected_object_validation/object-position-filter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/autoware_detected_object_validation/object-position-filter/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"perception/autoware_detected_object_validation/object-position-filter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/autoware_detected_object_validation/obstacle-pointcloud-based-validator/","title":"obstacle pointcloud based validator","text":""},{"location":"perception/autoware_detected_object_validation/obstacle-pointcloud-based-validator/#obstacle-pointcloud-based-validator","title":"obstacle pointcloud based validator","text":""},{"location":"perception/autoware_detected_object_validation/obstacle-pointcloud-based-validator/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>If the number of obstacle point groups in the DetectedObjects is small, it is considered a false positive and removed. The obstacle point cloud can be a point cloud after compare map filtering or a ground filtered point cloud.</p> <p></p> <p>In the debug image above, the red DetectedObject is the validated object. The blue object is the deleted object.</p>"},{"location":"perception/autoware_detected_object_validation/obstacle-pointcloud-based-validator/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_detected_object_validation/obstacle-pointcloud-based-validator/#input","title":"Input","text":"Name Type Description <code>~/input/detected_objects</code> <code>autoware_perception_msgs::msg::DetectedObjects</code> DetectedObjects <code>~/input/obstacle_pointcloud</code> <code>sensor_msgs::msg::PointCloud2</code> Obstacle point cloud of dynamic objects"},{"location":"perception/autoware_detected_object_validation/obstacle-pointcloud-based-validator/#output","title":"Output","text":"Name Type Description <code>~/output/objects</code> <code>autoware_perception_msgs::msg::DetectedObjects</code> validated DetectedObjects"},{"location":"perception/autoware_detected_object_validation/obstacle-pointcloud-based-validator/#parameters","title":"Parameters","text":"Name Type Description <code>using_2d_validator</code> bool The xy-plane projected (2D) obstacle point clouds will be used for validation <code>min_points_num</code> int The minimum number of obstacle point clouds in DetectedObjects <code>max_points_num</code> int The max number of obstacle point clouds in DetectedObjects <code>min_points_and_distance_ratio</code> float Threshold value of the number of point clouds per object when the distance from baselink is 1m, because the number of point clouds varies with the distance from baselink. <code>enable_debugger</code> bool Whether to create debug topics or not?"},{"location":"perception/autoware_detected_object_validation/obstacle-pointcloud-based-validator/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>Currently, only represented objects as BoundingBox or Cylinder are supported.</p>"},{"location":"perception/autoware_detected_object_validation/occupancy-grid-based-validator/","title":"occupancy grid based validator","text":""},{"location":"perception/autoware_detected_object_validation/occupancy-grid-based-validator/#occupancy-grid-based-validator","title":"occupancy grid based validator","text":""},{"location":"perception/autoware_detected_object_validation/occupancy-grid-based-validator/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>Compare the occupancy grid map with the DetectedObject, and if a larger percentage of obstacles are in freespace, delete them.</p> <p></p> <p>Basically, it takes an occupancy grid map as input and generates a binary image of freespace or other.</p> <p>A mask image is generated for each DetectedObject and the average value (percentage) in the mask image is calculated. If the percentage is low, it is deleted.</p>"},{"location":"perception/autoware_detected_object_validation/occupancy-grid-based-validator/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_detected_object_validation/occupancy-grid-based-validator/#input","title":"Input","text":"Name Type Description <code>~/input/detected_objects</code> <code>autoware_perception_msgs::msg::DetectedObjects</code> DetectedObjects <code>~/input/occupancy_grid_map</code> <code>nav_msgs::msg::OccupancyGrid</code> OccupancyGrid with no time series calculation is preferred."},{"location":"perception/autoware_detected_object_validation/occupancy-grid-based-validator/#output","title":"Output","text":"Name Type Description <code>~/output/objects</code> <code>autoware_perception_msgs::msg::DetectedObjects</code> validated DetectedObjects"},{"location":"perception/autoware_detected_object_validation/occupancy-grid-based-validator/#parameters","title":"Parameters","text":"Name Type Description <code>mean_threshold</code> float The percentage threshold of allowed non-freespace. <code>enable_debug</code> bool Whether to display debug images or not?"},{"location":"perception/autoware_detected_object_validation/occupancy-grid-based-validator/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>Currently, only vehicle represented as BoundingBox are supported.</p>"},{"location":"perception/autoware_detection_by_tracker/","title":"autoware_detection_by_tracker","text":""},{"location":"perception/autoware_detection_by_tracker/#autoware_detection_by_tracker","title":"autoware_detection_by_tracker","text":""},{"location":"perception/autoware_detection_by_tracker/#purpose","title":"Purpose","text":"<p>This package feeds back the tracked objects to the detection module to keep it stable and keep detecting objects. </p> <p>The autoware detection by tracker takes as input an unknown object containing a cluster of points and a tracker. The unknown object is optimized to fit the size of the tracker so that it can continue to be detected.</p>"},{"location":"perception/autoware_detection_by_tracker/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The autoware detection by tracker receives an unknown object containing a point cloud and a tracker, where the unknown object is mainly shape-fitted using euclidean clustering. Shape fitting using euclidean clustering and other methods has a problem called under segmentation and over segmentation.</p> <p> Adapted from [3]</p> <p>Simply looking at the overlap between the unknown object and the tracker does not work. We need to take measures for under segmentation and over segmentation.</p>"},{"location":"perception/autoware_detection_by_tracker/#policy-for-dealing-with-over-segmentation","title":"Policy for dealing with over segmentation","text":"<ol> <li>Merge the unknown objects in the tracker as a single object.</li> <li>Shape fitting using the tracker information such as angle and size as reference information.</li> </ol>"},{"location":"perception/autoware_detection_by_tracker/#policy-for-dealing-with-under-segmentation","title":"Policy for dealing with under segmentation","text":"<ol> <li>Compare the tracker and unknown objects, and determine that those with large recall and small precision are under segmented objects.</li> <li>In order to divide the cluster of under segmented objects, it iterate the parameters to make small clusters.</li> <li>Adjust the parameters several times and adopt the one with the highest IoU.</li> </ol>"},{"location":"perception/autoware_detection_by_tracker/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_detection_by_tracker/#input","title":"Input","text":"Name Type Description <code>~/input/initial_objects</code> <code>tier4_perception_msgs::msg::DetectedObjectsWithFeature</code> unknown objects <code>~/input/tracked_objects</code> <code>tier4_perception_msgs::msg::TrackedObjects</code> trackers"},{"location":"perception/autoware_detection_by_tracker/#output","title":"Output","text":"Name Type Description <code>~/output</code> <code>autoware_perception_msgs::msg::DetectedObjects</code> objects"},{"location":"perception/autoware_detection_by_tracker/#parameters","title":"Parameters","text":"Name Type Description Default value <code>tracker_ignore_label.UNKNOWN</code> <code>bool</code> If true, the node will ignore the tracker if its label is unknown. <code>true</code> <code>tracker_ignore_label.CAR</code> <code>bool</code> If true, the node will ignore the tracker if its label is CAR. <code>false</code> <code>tracker_ignore_label.PEDESTRIAN</code> <code>bool</code> If true, the node will ignore the tracker if its label is pedestrian. <code>false</code> <code>tracker_ignore_label.BICYCLE</code> <code>bool</code> If true, the node will ignore the tracker if its label is bicycle. <code>false</code> <code>tracker_ignore_label.MOTORCYCLE</code> <code>bool</code> If true, the node will ignore the tracker if its label is MOTORCYCLE. <code>false</code> <code>tracker_ignore_label.BUS</code> <code>bool</code> If true, the node will ignore the tracker if its label is bus. <code>false</code> <code>tracker_ignore_label.TRUCK</code> <code>bool</code> If true, the node will ignore the tracker if its label is truck. <code>false</code> <code>tracker_ignore_label.TRAILER</code> <code>bool</code> If true, the node will ignore the tracker if its label is TRAILER. <code>false</code>"},{"location":"perception/autoware_detection_by_tracker/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"perception/autoware_detection_by_tracker/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/autoware_detection_by_tracker/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/autoware_detection_by_tracker/#optional-referencesexternal-links","title":"(Optional) References/External links","text":"<p>[1] M. Himmelsbach, et al. \"Tracking and classification of arbitrary objects with bottom-up/top-down detection.\" (2012).</p> <p>[2] Arya Senna Abdul Rachman, Arya. \"3D-LIDAR Multi Object Tracking for Autonomous Driving: Multi-target Detection and Tracking under Urban Road Uncertainties.\" (2017).</p> <p>[3] David Held, et al. \"A Probabilistic Framework for Real-time 3D Segmentation using Spatial, Temporal, and Semantic Cues.\" (2016).</p>"},{"location":"perception/autoware_detection_by_tracker/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/autoware_elevation_map_loader/","title":"autoware_elevation_map_loader","text":""},{"location":"perception/autoware_elevation_map_loader/#autoware_elevation_map_loader","title":"autoware_elevation_map_loader","text":""},{"location":"perception/autoware_elevation_map_loader/#purpose","title":"Purpose","text":"<p>This package provides elevation map for autoware_compare_map_segmentation.</p>"},{"location":"perception/autoware_elevation_map_loader/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>Generate elevation_map from subscribed pointcloud_map and vector_map and publish it. Save the generated elevation_map locally and load it from next time.</p> <p>The elevation value of each cell is the average value of z of the points of the lowest cluster. Cells with No elevation value can be inpainted using the values of neighboring cells.</p> <p> </p>"},{"location":"perception/autoware_elevation_map_loader/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_elevation_map_loader/#input","title":"Input","text":"Name Type Description <code>input/pointcloud_map</code> <code>sensor_msgs::msg::PointCloud2</code> The point cloud map <code>input/vector_map</code> <code>autoware_map_msgs::msg::LaneletMapBin</code> (Optional) The binary data of lanelet2 map <code>input/pointcloud_map_metadata</code> <code>autoware_map_msgs::msg::PointCloudMapMetaData</code> (Optional) The metadata of point cloud map"},{"location":"perception/autoware_elevation_map_loader/#output","title":"Output","text":"Name Type Description <code>output/elevation_map</code> <code>grid_map_msgs::msg::GridMap</code> The elevation map <code>output/elevation_map_cloud</code> <code>sensor_msgs::msg::PointCloud2</code> (Optional) The point cloud generated from the value of elevation map"},{"location":"perception/autoware_elevation_map_loader/#service","title":"Service","text":"Name Type Description <code>service/get_selected_pcd_map</code> <code>autoware_map_msgs::srv::GetSelectedPointCloudMap</code> (Optional) service to request point cloud map. If pointcloud_map_loader uses selected pointcloud map loading via ROS 2 service, use this."},{"location":"perception/autoware_elevation_map_loader/#parameters","title":"Parameters","text":""},{"location":"perception/autoware_elevation_map_loader/#node-parameters","title":"Node parameters","text":"Name Type Description Default value map_layer_name std::string elevation_map layer name elevation param_file_path std::string GridMap parameters config path_default elevation_map_directory std::string elevation_map file (bag2) path_default map_frame std::string map_frame when loading elevation_map file map use_inpaint bool Whether to inpaint empty cells true inpaint_radius float Radius of a circular neighborhood of each point inpainted that is considered by the algorithm [m] 0.3 use_elevation_map_cloud_publisher bool Whether to publish <code>output/elevation_map_cloud</code> false use_lane_filter bool Whether to filter elevation_map with vector_map false lane_margin float Margin distance from the lane polygon of the area to be included in the inpainting mask [m]. Used only when use_lane_filter=True. 0.0 use_sequential_load bool Whether to get point cloud map by service false sequential_map_load_num int The number of point cloud maps to load at once (only used when use_sequential_load is set true). This should not be larger than number of all point cloud map cells. 1"},{"location":"perception/autoware_elevation_map_loader/#gridmap-parameters","title":"GridMap parameters","text":"<p>The parameters are described on <code>config/elevation_map_parameters.yaml</code>.</p>"},{"location":"perception/autoware_elevation_map_loader/#general-parameters","title":"General parameters","text":"Name Type Description Default value pcl_grid_map_extraction/num_processing_threads int Number of threads for processing grid map cells. Filtering of the raw input point cloud is not parallelized. 12"},{"location":"perception/autoware_elevation_map_loader/#grid-map-parameters","title":"Grid map parameters","text":"<p>See: https://github.com/ANYbotics/grid_map/tree/ros2/grid_map_pcl</p> <p>Resulting grid map parameters.</p> Name Type Description Default value pcl_grid_map_extraction/grid_map/min_num_points_per_cell int Minimum number of points in the point cloud that have to fall within any of the grid map cells. Otherwise the cell elevation will be set to NaN. 3 pcl_grid_map_extraction/grid_map/resolution float Resolution of the grid map. Width and length are computed automatically. 0.3 pcl_grid_map_extraction/grid_map/height_type int The parameter that determine the elevation of a cell <code>0: Smallest value among the average values of each cluster</code>, <code>1: Mean value of the cluster with the most points</code> 1 pcl_grid_map_extraction/grid_map/height_thresh float Height range from the smallest cluster (Only for height_type 1) 1.0"},{"location":"perception/autoware_elevation_map_loader/#point-cloud-pre-processing-parameters","title":"Point Cloud Pre-processing Parameters","text":""},{"location":"perception/autoware_elevation_map_loader/#rigid-body-transform-parameters","title":"Rigid body transform parameters","text":"<p>Rigid body transform that is applied to the point cloud before computing elevation.</p> Name Type Description Default value pcl_grid_map_extraction/cloud_transform/translation float Translation (xyz) that is applied to the input point cloud before computing elevation. 0.0 pcl_grid_map_extraction/cloud_transform/rotation float Rotation (intrinsic rotation, convention X-Y'-Z'') that is applied to the input point cloud before computing elevation. 0.0"},{"location":"perception/autoware_elevation_map_loader/#cluster-extraction-parameters","title":"Cluster extraction parameters","text":"<p>Cluster extraction is based on pcl algorithms. See https://pointclouds.org/documentation/tutorials/cluster_extraction.html for more details.</p> Name Type Description Default value pcl_grid_map_extraction/cluster_extraction/cluster_tolerance float Distance between points below which they will still be considered part of one cluster. 0.2 pcl_grid_map_extraction/cluster_extraction/min_num_points int Min number of points that a cluster needs to have (otherwise it will be discarded). 3 pcl_grid_map_extraction/cluster_extraction/max_num_points int Max number of points that a cluster can have (otherwise it will be discarded). 1000000"},{"location":"perception/autoware_elevation_map_loader/#outlier-removal-parameters","title":"Outlier removal parameters","text":"<p>See https://pointclouds.org/documentation/tutorials/statistical_outlier.html for more explanation on outlier removal.</p> Name Type Description Default value pcl_grid_map_extraction/outlier_removal/is_remove_outliers float Whether to perform statistical outlier removal. false pcl_grid_map_extraction/outlier_removal/mean_K float Number of neighbors to analyze for estimating statistics of a point. 10 pcl_grid_map_extraction/outlier_removal/stddev_threshold float Number of standard deviations under which points are considered to be inliers. 1.0"},{"location":"perception/autoware_elevation_map_loader/#subsampling-parameters","title":"Subsampling parameters","text":"<p>See https://pointclouds.org/documentation/tutorials/voxel_grid.html for more explanation on point cloud downsampling.</p> Name Type Description Default value pcl_grid_map_extraction/downsampling/is_downsample_cloud bool Whether to perform downsampling or not. false pcl_grid_map_extraction/downsampling/voxel_size float Voxel sizes (xyz) in meters. 0.02"},{"location":"perception/autoware_euclidean_cluster/","title":"autoware_euclidean_cluster","text":""},{"location":"perception/autoware_euclidean_cluster/#autoware_euclidean_cluster","title":"autoware_euclidean_cluster","text":""},{"location":"perception/autoware_euclidean_cluster/#purpose","title":"Purpose","text":"<p>autoware_euclidean_cluster is a package for clustering points into smaller parts to classify objects.</p> <p>This package has two clustering methods: <code>euclidean_cluster</code> and <code>voxel_grid_based_euclidean_cluster</code>.</p>"},{"location":"perception/autoware_euclidean_cluster/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"perception/autoware_euclidean_cluster/#euclidean_cluster","title":"euclidean_cluster","text":"<p><code>pcl::EuclideanClusterExtraction</code> is applied to points. See official document for details.</p>"},{"location":"perception/autoware_euclidean_cluster/#voxel_grid_based_euclidean_cluster","title":"voxel_grid_based_euclidean_cluster","text":"<ol> <li>A centroid in each voxel is calculated by <code>pcl::VoxelGrid</code>.</li> <li>The centroids are clustered by <code>pcl::EuclideanClusterExtraction</code>.</li> <li>The input points are clustered based on the clustered centroids.</li> </ol>"},{"location":"perception/autoware_euclidean_cluster/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_euclidean_cluster/#input","title":"Input","text":"Name Type Description <code>input</code> <code>sensor_msgs::msg::PointCloud2</code> input pointcloud"},{"location":"perception/autoware_euclidean_cluster/#output","title":"Output","text":"Name Type Description <code>output</code> <code>tier4_perception_msgs::msg::DetectedObjectsWithFeature</code> cluster pointcloud <code>debug/clusters</code> <code>sensor_msgs::msg::PointCloud2</code> colored cluster pointcloud for visualization"},{"location":"perception/autoware_euclidean_cluster/#parameters","title":"Parameters","text":""},{"location":"perception/autoware_euclidean_cluster/#core-parameters","title":"Core Parameters","text":""},{"location":"perception/autoware_euclidean_cluster/#euclidean_cluster_1","title":"euclidean_cluster","text":"Name Type Description Default Range max_cluster_size integer The maximum number of points that a cluster needs to contain in order to be considered valid. 1000 N/A min_cluster_size integer The minimum number of points that a cluster needs to contain in order to be considered valid. 10 N/A tolerance float The spatial cluster tolerance as a measure in the L2 Euclidean space. 0.7 N/A use_height boolean Whether to use point.z for clustering. 0 N/A"},{"location":"perception/autoware_euclidean_cluster/#voxel_grid_based_euclidean_cluster_1","title":"voxel_grid_based_euclidean_cluster","text":"Name Type Description Default Range tolerance float The spatial cluster tolerance as a measure in the L2 Euclidean space. 0.7 N/A voxel_leaf_size float The voxel leaf size of x and y. 0.3 N/A min_points_number_per_voxel integer The minimum number of points required per voxel. 1 N/A min_cluster_size integer The minimum number of points that a cluster needs to contain in order to be considered valid. 10 N/A max_cluster_size integer The maximum number of points that a cluster needs to contain in order to be considered valid. 3000 N/A max_voxel_cluster_for_output integer Maximum number of voxels in a cluster when outputted. 800 N/A min_voxel_cluster_size_for_filtering integer The minimum voxel cluster size for a cluster to be checked for being a large cluster. 65 N/A max_points_per_voxel_in_large_cluster integer The maximum points per voxel allowed in large clusters (used for filtering dense clusters). 10 N/A use_height boolean Use point.z for clustering. 0 N/A"},{"location":"perception/autoware_euclidean_cluster/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"perception/autoware_euclidean_cluster/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/autoware_euclidean_cluster/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/autoware_euclidean_cluster/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"perception/autoware_euclidean_cluster/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":"<p>The <code>use_height</code> option of <code>voxel_grid_based_euclidean_cluster</code> isn't implemented yet.</p>"},{"location":"perception/autoware_ground_segmentation/","title":"autoware_ground_segmentation","text":""},{"location":"perception/autoware_ground_segmentation/#autoware_ground_segmentation","title":"autoware_ground_segmentation","text":""},{"location":"perception/autoware_ground_segmentation/#purpose","title":"Purpose","text":"<p>The <code>autoware_ground_segmentation</code> is a node that remove the ground points from the input pointcloud.</p>"},{"location":"perception/autoware_ground_segmentation/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>Detail description of each ground segmentation algorithm is in the following links.</p> Filter Name Description Detail ray_ground_filter A method of removing the ground based on the geometrical relationship between points lined up on radiation link scan_ground_filter Almost the same method as <code>ray_ground_filter</code>, but with slightly improved performance link ransac_ground_filter A method of removing the ground by approximating the ground to a plane link"},{"location":"perception/autoware_ground_segmentation/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_ground_segmentation/#input","title":"Input","text":"Name Type Description <code>~/input/points</code> <code>sensor_msgs::msg::PointCloud2</code> reference points <code>~/input/indices</code> <code>pcl_msgs::msg::Indices</code> reference indices"},{"location":"perception/autoware_ground_segmentation/#output","title":"Output","text":"Name Type Description <code>~/output/points</code> <code>sensor_msgs::msg::PointCloud2</code> filtered points"},{"location":"perception/autoware_ground_segmentation/#parameters","title":"Parameters","text":""},{"location":"perception/autoware_ground_segmentation/#node-parameters","title":"Node Parameters","text":"Name Type Default Value Description <code>input_frame</code> string \" \" input frame id <code>output_frame</code> string \" \" output frame id <code>max_queue_size</code> int 5 max queue size of input/output topics <code>use_indices</code> bool false flag to use pointcloud indices <code>latched_indices</code> bool false flag to latch pointcloud indices <code>approximate_sync</code> bool false flag to use approximate sync option"},{"location":"perception/autoware_ground_segmentation/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p><code>autoware::pointcloud_preprocessor::Filter</code> is implemented based on pcl_perception [1] because of this issue.</p>"},{"location":"perception/autoware_ground_segmentation/#referencesexternal-links","title":"References/External links","text":"<p>[1] https://github.com/ros-perception/perception_pcl/blob/ros2/pcl_ros/src/pcl_ros/filters/filter.cpp</p>"},{"location":"perception/autoware_ground_segmentation/docs/ransac-ground-filter/","title":"RANSAC Ground Filter","text":""},{"location":"perception/autoware_ground_segmentation/docs/ransac-ground-filter/#ransac-ground-filter","title":"RANSAC Ground Filter","text":""},{"location":"perception/autoware_ground_segmentation/docs/ransac-ground-filter/#purpose","title":"Purpose","text":"<p>The purpose of this node is that remove the ground points from the input pointcloud.</p>"},{"location":"perception/autoware_ground_segmentation/docs/ransac-ground-filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>Apply the input points to the plane, and set the points at a certain distance from the plane as points other than the ground. Normally, whn using this method, the input points is filtered so that it is almost flat before use. Since the drivable area is often flat, there are methods such as filtering by lane.</p>"},{"location":"perception/autoware_ground_segmentation/docs/ransac-ground-filter/#inputs-outputs","title":"Inputs / Outputs","text":"<p>This implementation inherits <code>autoware::pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"perception/autoware_ground_segmentation/docs/ransac-ground-filter/#parameters","title":"Parameters","text":""},{"location":"perception/autoware_ground_segmentation/docs/ransac-ground-filter/#node-parameters","title":"Node Parameters","text":"<p>This implementation inherits <code>autoware::pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"perception/autoware_ground_segmentation/docs/ransac-ground-filter/#core-parameters","title":"Core Parameters","text":"Name Type Description Default Range base_frame string base_link frame base_link N/A unit_axis string The axis which we need to search ground plane z N/A max_iterations integer The maximum number of iterations 1000 N/A min_trial integer min_trial 5000 N/A min_points integer min_points 1000 N/A outlier_threshold float The distance threshold to the model [m] 0.01 N/A plane_slope_threshold float The slope threshold to prevent mis-fitting [deg] 10.0 N/A voxel_size_x float voxel size x [m] 0.04 N/A voxel_size_y float voxel size y [m] 0.04 N/A voxel_size_z float voxel size z [m] 0.04 N/A height_threshold float The height threshold from ground plane for no ground points [m] 0.01 N/A debug boolean whether to output debug information false N/A publish_processing_time_detail boolean publish_processing_time_detail false N/A"},{"location":"perception/autoware_ground_segmentation/docs/ransac-ground-filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<ul> <li>This method can't handle slopes.</li> <li>The input points is filtered so that it is almost flat.</li> </ul>"},{"location":"perception/autoware_ground_segmentation/docs/ransac-ground-filter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/autoware_ground_segmentation/docs/ransac-ground-filter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/autoware_ground_segmentation/docs/ransac-ground-filter/#referencesexternal-links","title":"References/External links","text":"<p>https://pcl.readthedocs.io/projects/tutorials/en/latest/planar_segmentation.html</p>"},{"location":"perception/autoware_ground_segmentation/docs/ransac-ground-filter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/autoware_ground_segmentation/docs/ray-ground-filter/","title":"Ray Ground Filter","text":""},{"location":"perception/autoware_ground_segmentation/docs/ray-ground-filter/#ray-ground-filter","title":"Ray Ground Filter","text":""},{"location":"perception/autoware_ground_segmentation/docs/ray-ground-filter/#purpose","title":"Purpose","text":"<p>The purpose of this node is that remove the ground points from the input pointcloud.</p>"},{"location":"perception/autoware_ground_segmentation/docs/ray-ground-filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The points is separated radially (Ray), and the ground is classified for each Ray sequentially from the point close to ego-vehicle based on the geometric information such as the distance and angle between the points.</p> <p></p>"},{"location":"perception/autoware_ground_segmentation/docs/ray-ground-filter/#inputs-outputs","title":"Inputs / Outputs","text":"<p>This implementation inherits <code>autoware::pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"perception/autoware_ground_segmentation/docs/ray-ground-filter/#parameters","title":"Parameters","text":""},{"location":"perception/autoware_ground_segmentation/docs/ray-ground-filter/#node-parameters","title":"Node Parameters","text":"<p>This implementation inherits <code>autoware::pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"perception/autoware_ground_segmentation/docs/ray-ground-filter/#core-parameters","title":"Core Parameters","text":"Name Type Description Default Range min_x float The parameter to set vehicle footprint manually -0.01 N/A max_x float The parameter to set vehicle footprint manually 0.01 N/A min_y float The parameter to set vehicle footprint manually -0.01 N/A max_y float The parameter to set vehicle footprint manually 0.01 N/A use_vehicle_footprint boolean use_vehicle_footprint false N/A general_max_slope float The triangle created by general_max_slope is called the global cone. If the point is outside the global cone, it is judged to be a point that is not on the ground 8.0 N/A local_max_slope float The triangle created by local_max_slope is called the local cone. This parameter is used for classifying points based on their continuity 6.0 N/A initial_max_slope float Generally, the point where the object first hits is far from ego-vehicle because of sensor blind spot, so resolution is different from that point and thereafter, so this parameter exists to set a separate local_max_slope 3.0 N/A radial_divider_angle float The angle of ray 1.0 N/A min_height_threshold float This parameter is used instead of height_threshold because it's difficult to determine continuity in the local cone when the points are too close to each other 0.15 N/A concentric_divider_distance float Only check points which radius is larger than concentric_divider_distance 0.0 N/A reclass_distance_threshold float To check if point is to far from previous one, if so classify again 0.1 N/A publish_processing_time_detail boolean publish_processing_time_detail false N/A"},{"location":"perception/autoware_ground_segmentation/docs/ray-ground-filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>The input_frame is set as parameter but it must be fixed as base_link for the current algorithm.</p>"},{"location":"perception/autoware_ground_segmentation/docs/ray-ground-filter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/autoware_ground_segmentation/docs/ray-ground-filter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/autoware_ground_segmentation/docs/ray-ground-filter/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"perception/autoware_ground_segmentation/docs/ray-ground-filter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/autoware_ground_segmentation/docs/scan-ground-filter/","title":"Scan Ground Filter","text":""},{"location":"perception/autoware_ground_segmentation/docs/scan-ground-filter/#scan-ground-filter","title":"Scan Ground Filter","text":""},{"location":"perception/autoware_ground_segmentation/docs/scan-ground-filter/#purpose","title":"Purpose","text":"<p>The purpose of this node is that remove the ground points from the input pointcloud.</p>"},{"location":"perception/autoware_ground_segmentation/docs/scan-ground-filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>This algorithm works by following steps,</p> <ol> <li>Divide whole pointclouds into groups by azimuth angle (so-called ray)</li> <li>Sort points by radial distance (xy-distance), on each ray.</li> <li>Divide pointcloud into grids, on each ray.</li> <li>Classify the point<ol> <li>Check radial distance to previous pointcloud, if the distance is large and previous pointcloud is \"no ground\" and the height level of current point greater than previous point, the current pointcloud is classified as no ground.</li> <li>Check vertical angle of the point compared with previous ground grid</li> <li>Check the height of the point compared with predicted ground level</li> <li>If vertical angle is greater than local_slope_max and related height to predicted ground level is greater than \"non ground height threshold\", the point is classified as \"non ground\"</li> <li>If the vertical angle is in range of [-local_slope_max, local_slope_max] or related height to predicted ground level is smaller than non_ground_height_threshold, the point is classified as \"ground\"</li> <li>If the vertical angle is lower than -local_slope_max or the related height to ground level is greater than detection_range_z_max, the point will be classified as out of range</li> </ol> </li> </ol>"},{"location":"perception/autoware_ground_segmentation/docs/scan-ground-filter/#inputs-outputs","title":"Inputs / Outputs","text":"<p>This implementation inherits <code>autoware::pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"perception/autoware_ground_segmentation/docs/scan-ground-filter/#parameters","title":"Parameters","text":""},{"location":"perception/autoware_ground_segmentation/docs/scan-ground-filter/#node-parameters","title":"Node Parameters","text":"<p>This implementation inherits <code>autoware::pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"perception/autoware_ground_segmentation/docs/scan-ground-filter/#core-parameters","title":"Core Parameters","text":"Name Type Description Default Range global_slope_max_angle_deg float The global angle to classify as the ground or object [deg]. A large threshold may reduce false positive of high slope road classification but it may lead to increase false negative of non-ground classification, particularly for small objects. 10.0 N/A local_slope_max_angle_deg float The local angle to classify as the ground or object [deg] when comparing with adjacent point. A small value enhance accuracy classification of object with inclined surface. This should be considered together with split_points_distance_tolerance value. 13.0 N/A split_points_distance_tolerance float The xy-distance threshold to distinguish far and near [m] 0.2 N/A use_virtual_ground_point boolean Whether to use the ground center of front wheels as the virtual ground point true N/A split_height_distance float The height threshold to distinguish ground and non-ground pointcloud when comparing with adjacent points [m]. A small threshold improves classification of non-ground point, especially for high elevation resolution pointcloud lidar. However, it might cause false positive for small step-like road surface or misaligned multiple lidar configuration. 0.2 N/A non_ground_height_threshold float Height threshold of non ground objects [m] as split_height_distance and applied only for elevation_grid_mode 0.2 N/A grid_size_m float The first grid size [m], applied only for elevation_grid_mode. A large value enhances the prediction stability for ground surface. suitable for rough surface or multiple lidar configuration. 0.1 N/A grid_mode_switch_radius float The distance where grid division mode change from by distance to by vertical angle [m], applied only for elevation_grid_mode 20.0 N/A gnd_grid_buffer_size integer Number of grids using to estimate local ground slope, applied only for elevation_grid_mode 4 N/A detection_range_z_max float Maximum height of detection range [m], applied only for elevation_grid_mode 2.5 N/A elevation_grid_mode boolean Elevation grid scan mode option true N/A low_priority_region_x float The non-zero x threshold in back side from which small objects detection is low priority [m] -20.0 N/A center_pcl_shift float The x-axis offset of addition LiDARs from vehicle center of mass [m], recommended to use only for additional LiDARs in elevation_grid_mode 0.0 N/A radial_divider_angle_deg float The angle which divide the whole pointcloud to sliced group [deg] 1.0 N/A use_recheck_ground_cluster boolean Enable recheck ground cluster true N/A recheck_start_distance float The distance to start rechecking ground cluster [m] 20.0 \u22650.0 use_lowest_point boolean To select lowest point for reference in recheck ground cluster, otherwise select middle point true N/A publish_processing_time_detail boolean publish_processing_time_detail false N/A"},{"location":"perception/autoware_ground_segmentation/docs/scan-ground-filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>The input_frame is set as parameter but it must be fixed as base_link for the current algorithm.</p>"},{"location":"perception/autoware_ground_segmentation/docs/scan-ground-filter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/autoware_ground_segmentation/docs/scan-ground-filter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/autoware_ground_segmentation/docs/scan-ground-filter/#optional-referencesexternal-links","title":"(Optional) References/External links","text":"<p>The elevation grid idea is referred from \"Shen Z, Liang H, Lin L, Wang Z, Huang W, Yu J. Fast Ground Segmentation for 3D LiDAR Point Cloud Based on Jump-Convolution-Process. Remote Sensing. 2021; 13(16):3239. https://doi.org/10.3390/rs13163239\"</p>"},{"location":"perception/autoware_ground_segmentation/docs/scan-ground-filter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":"<ul> <li>Horizontal check for classification is not implemented yet.</li> <li>Output ground visibility for diagnostic is not implemented yet.</li> </ul>"},{"location":"perception/autoware_image_object_locator/","title":"autoware_image_object_locator","text":""},{"location":"perception/autoware_image_object_locator/#autoware_image_object_locator","title":"autoware_image_object_locator","text":""},{"location":"perception/autoware_image_object_locator/#purpose","title":"Purpose","text":"<p>The <code>autoware_image_object_locator</code> package is designed to use objects detected from 2D images to generate 3D object detections for more robust perception.</p> Locator Name Description Detail bbox_object_locator Produces 3D object detections from bounding boxes of an image. link"},{"location":"perception/autoware_image_object_locator/docs/bbox-object-locator/","title":"bbox_object_locator","text":""},{"location":"perception/autoware_image_object_locator/docs/bbox-object-locator/#bbox_object_locator","title":"bbox_object_locator","text":""},{"location":"perception/autoware_image_object_locator/docs/bbox-object-locator/#inner-workings-algorithms","title":"Inner Workings / Algorithms","text":"<p>This node produces 3D object detection from a 2D image bounding box (bbox) through the following steps:</p> <ol> <li>Back-project the bottom-center pixel of a bbox onto the ground plane (<code>z=0</code> in the target coordinate system). This point is used as the object's bottom center.</li> <li>Back-project the top-center pixel of the bbox to a point near the one computed in step 1 for height estimation.</li> <li>Back-project the bottom-left and bottom-right pixels of the bbox to estimate the object\u2019s width (diameter).</li> <li>Use these four points to determine the object\u2019s position and size.</li> </ol>"},{"location":"perception/autoware_image_object_locator/docs/bbox-object-locator/#parameters","title":"Parameters","text":"Name Type Description Default Range target_frame string Transform target frame. base_link N/A detection_target_class.UNKNOWN boolean If true, objects with the 'UNKNOWN' label will be ignored during processing. True N/A detection_target_class.CAR boolean If true, objects with the 'CAR' label will be ignored during processing. True N/A detection_target_class.TRUCK boolean If true, objects with the 'TRUCK' label will be ignored during processing. True N/A detection_target_class.BUS boolean If true, objects with the 'BUS' label will be ignored during processing. True N/A detection_target_class.TRAILER boolean If true, objects with the 'TRAILER' label will be ignored during processing. True N/A detection_target_class.MOTORCYCLE boolean If true, objects with the 'MOTORCYCLE' label will be ignored during processing. True N/A detection_target_class.BICYCLE boolean If true, objects with the 'BICYCLE' label will be ignored during processing. True N/A detection_target_class.PEDESTRIAN boolean If true, objects with the 'PEDESTRIAN' label will be ignored during processing. False N/A roi_confidence_threshold float Threshold value for whether to trust and use the ROI. If the confidence of an ROI is below this value, it will not be used. 0.8 \u22650.0\u22641.0 roi_truncation_validation.enabled array Flags to enable validation for each ROI. Must have the same number as the input ROIs. [False, False, False, False, False, False, False, False] N/A roi_truncation_validation.remove_truncated array If this is true, the object will be removed when its ROI is evaluated as truncated. Must have the same number as the input ROIs. [False, False, False, False, False, False, False, False] N/A roi_truncation_validation.image_border_truncation_horizontal_margin_ratio float Margin ratio used to detect whether the ROI is truncated based on the image width. 0.0 \u22650.0\u22640.5 roi_truncation_validation.image_border_truncation_vertical_margin_ratio float Margin ratio used to detect whether the ROI is truncated based on the image height. 0.0 \u22650.0\u22640.5 detection_max_range float Maximum detection range. Objects beyond this distance will not be included in the output [m]. 30.0 \u22650.0 pseudo_height float If height was not able to compute, this value will used as pseudo height of a object [m]. 1.5 \u22650.1 pedestrian_detection_config.width_min float If the estimated width is smaller than this value, it will be clamped to this value [m]. 0.5 \u22650.3 pedestrian_detection_config.width_max float If the estimated width is larger than this value, it will be clamped to this value [m]. 0.8 \u22650.3"},{"location":"perception/autoware_image_object_locator/docs/bbox-object-locator/#input","title":"Input","text":"Name Type Description <code>input/rois&lt;id&gt;</code> tier4_perception_msgs::msg::DetectedObjectsWithFeature 's input ROI <code>input/camera_info&lt;id&gt;</code> sensor_msgs::msg::CameraInfo 's camera info"},{"location":"perception/autoware_image_object_locator/docs/bbox-object-locator/#output","title":"Output","text":"Name Type Description <code>output/rois&lt;id&gt;/objects</code> autoware_perception_msgs::msg::DetectedObjects The object generated from 's 2D ROI"},{"location":"perception/autoware_image_projection_based_fusion/","title":"autoware_image_projection_based_fusion","text":""},{"location":"perception/autoware_image_projection_based_fusion/#autoware_image_projection_based_fusion","title":"autoware_image_projection_based_fusion","text":""},{"location":"perception/autoware_image_projection_based_fusion/#purpose","title":"Purpose","text":"<p>The <code>autoware_image_projection_based_fusion</code> package is designed to enhance obstacle detection accuracy by integrating information from both image-based and LiDAR-based perception. It fuses detected obstacles \u2014 such as bounding boxes or segmentation \u2014 from 2D images with 3D point clouds or other obstacle representations, including bounding boxes, clusters, or segmentation. This fusion helps to refine obstacle classification and detection in autonomous driving applications.</p>"},{"location":"perception/autoware_image_projection_based_fusion/#fusion-algorithms","title":"Fusion algorithms","text":"<p>The package provides multiple fusion algorithms, each designed for specific use cases. Below are the different fusion methods along with their descriptions and detailed documentation links:</p> Fusion Name Description Detail roi_cluster_fusion Assigns classification labels to LiDAR-detected clusters by matching them with Regions of Interest (ROIs) from a 2D object detector. link roi_detected_object_fusion Updates classification labels of detected objects using ROI information from a 2D object detector. link pointpainting_fusion Augments the point cloud by painting each point with additional information from ROIs of a 2D object detector. The enriched point cloud is then processed by a 3D object detector for improved accuracy. link roi_pointcloud_fusion Matching pointcloud with ROIs from a 2D object detector to detect unknown-labeled objects. link segmentation_pointcloud_fusion Filtering pointcloud that are belong to less interesting region which is defined by semantic or instance segmentation by 2D image segmentation. link"},{"location":"perception/autoware_image_projection_based_fusion/#inner-workings-algorithms","title":"Inner Workings / Algorithms","text":"<p>The fusion process operates on two primary types of input data:</p> <ul> <li>Msg3d: This includes 3D data such as point clouds, bounding boxes, or clusters from LiDAR.</li> <li>RoIs (Regions of Interest): These are 2D detections or proposals from camera-based perception modules, such as object detection bounding boxes.</li> </ul> <p>Both inputs come with timestamps, which are crucial for synchronization and fusion. Since sensors operate at different frequencies and may experience network delays, a systematic approach is needed to handle their arrival, align their timestamps, and ensure reliable fusion.</p> <p>The following steps describe how the node processes these inputs, synchronizes them, and performs multi-sensor fusion.</p>"},{"location":"perception/autoware_image_projection_based_fusion/#step-1-matching-and-creating-a-collector","title":"Step 1: Matching and Creating a Collector","text":"<p>When a Msg3d or a set of RoIs arrives, its timestamp is checked, and an offset is subtracted to determine the reference timestamp. The node then searches for an existing collector with the same reference timestamp.</p> <ul> <li>If a matching collector is found, the incoming data is added to it.</li> <li>If no matching collector exists, a new collector is created and initialized with the reference timestamp.</li> </ul>"},{"location":"perception/autoware_image_projection_based_fusion/#step-2-triggering-the-timer","title":"Step 2: Triggering the Timer","text":"<p>Once a collector is created, a countdown timer is started. The timeout duration depends on which message type arrived first and is defined by either <code>msg3d_timeout_sec</code> for msg3d or <code>rois_timeout_sec</code> for RoIs.</p> <p>The collector will attempt to fuse the collected 3D and 2D data either:</p> <ul> <li>When both Msg3d and RoI data are available, or</li> <li>When the timer expires.</li> </ul> <p>If no Msg3d is received before the timer expires, the collector will discard the data without performing fusion.</p>"},{"location":"perception/autoware_image_projection_based_fusion/#step-3-fusion-process","title":"Step 3: Fusion Process","text":"<p>The fusion process consists of three main stages:</p> <ol> <li>Preprocessing \u2013 Preparing the input data for fusion.</li> <li>Fusion \u2013 Aligning and merging RoIs with the 3D point cloud.</li> <li>Postprocessing \u2013 Refining the fused output based on the algorithm's requirements.</li> </ol> <p>The specific operations performed during these stages may vary depending on the type of fusion being applied.</p>"},{"location":"perception/autoware_image_projection_based_fusion/#step-4-publishing-the-fused-result","title":"Step 4: Publishing the Fused Result","text":"<p>After the fusion process is completed, the fused output is published. The collector is then reset to an idle state, ready to process the next incoming message.</p> <p>The figure below shows how the input data is fused in different scenarios. </p>"},{"location":"perception/autoware_image_projection_based_fusion/#parameters","title":"Parameters","text":"<p>All of the fusion nodes have the common parameters described in the following</p> Name Type Description Default Range rois_timestamp_offsets array The timestamp offset between each RoIs and the msg3d in seconds. [0.059, 0.01, 0.026, 0.042, 0.076, 0.093] N/A rois_timeout_sec float Timer's timeout duration in seconds when the collector created by RoIs msg. 0.5 \u22650.001 msg3d_timeout_sec float Timer's timeout duration in seconds when the collector received msg3d. 0.05 \u22650.001 image_buffer_size integer The number of image buffer size for debug. 15 \u22651 point_project_to_unrectified_image array An array of options indicating whether to project point to unrectified image or not. [False, False, False, False, False, False] N/A filter_scope_min_x float Minimum x position to be considered for debug [m]. -100.0 N/A filter_scope_min_y float Minimum y position to be considered for debug [m]. -100.0 N/A filter_scope_min_z float Minimum z position to be considered for debug [m]. -100.0 N/A filter_scope_max_x float Maximum x position to be considered for debug [m]. 100.0 N/A filter_scope_max_y float Maximum y position to be considered for debug [m]. 100.0 N/A filter_scope_max_z float Maximum z position [m]. 100.0 N/A approximate_camera_projection array An array of options indicating whether to use approximated projection for each camera. [True, True, True, True, True, True] N/A approximation_grid_cell_width float The width of grid cell used in approximated projection [pixel]. 1.0 N/A approximation_grid_cell_height float The height of grid cell used in approximated projection [pixel]. 1.0 N/A debug_mode boolean Flag to enable or disable debug message output. False N/A collector_debug_mode boolean Flag to enable or disable collector's debug message output. False N/A publish_processing_time_detail boolean Flag to publish detail message for processing time. False N/A publish_previous_but_late_output_msg boolean Flag to indicate if the current fusion result should be published if its timestamp is earlier than the previous published fusion result. False N/A rosbag_length float This value determine if the rosbag has started from the beginning again. The value should be set smaller than the actual length of the bag. 10.0 N/A matching_strategy.type string Set it to <code>advanced</code> if you want to use more accurate and complicated logic for matching LiDAR and camera; otherwise, set it to <code>naive</code>. advanced ['naive', 'advanced'] matching_strategy.threshold float A maximum threshold value to synchronize RoIs from multiple cameras in seconds. 0.05 \u22650.0\u22640.1 matching_strategy.msg3d_noise_window float msg3d noise window in seconds. 0.001 \u22650.0 matching_strategy.rois_timestamp_noise_window array List of camera timestamp noise windows in seconds. The noise values should be specified in the same order as the input_topics. [0.005, 0.005, 0.005, 0.005, 0.005, 0.005] N/A"},{"location":"perception/autoware_image_projection_based_fusion/#parameter-settings","title":"Parameter Settings","text":""},{"location":"perception/autoware_image_projection_based_fusion/#timeout","title":"Timeout","text":"<p>The order in which <code>RoIs</code> or the <code>msg3d</code> message arrives at the fusion node depends on your system and sensor configuration. Since the primary goal is to fuse <code>2D RoIs</code> with <code>msg3d</code> data, <code>msg3d</code> is essential for processing.</p> <p>If <code>RoIs</code> arrive earlier, they must wait until <code>msg3d</code> is received. You can adjust the waiting time using the <code>rois_timeout_sec</code> parameter.</p> <p>If <code>msg3d</code> arrives first, the fusion process should proceed as quickly as possible, so the waiting time for <code>msg3d</code> (<code>msg3d_timeout_sec</code>) should be kept minimal.</p>"},{"location":"perception/autoware_image_projection_based_fusion/#rois-offsets","title":"RoIs Offsets","text":"<p>The offset between each camera and the LiDAR is determined by their shutter timing. To ensure accurate fusion, users must understand the timing offset between the <code>RoIs</code> and <code>msg3d</code>. Once this offset is known, it should be specified in the parameter <code>rois_timestamp_offsets</code>.</p> <p>In the figure below, the LiDAR completes a full scan from the rear in 100 milliseconds. When the LiDAR scan reaches the area where the camera is facing, the camera is triggered, capturing an image with a corresponding timestamp. The <code>rois_timestamp_offsets</code> can then be calculated by subtracting the LiDAR header timestamp from the camera header timestamp. As a result, the <code>rois_timestamp_offsets</code> would be <code>[0.059, 0.010, 0.026, 0.042, 0.076, 0.093]</code>.</p> <p></p> <p>To check the header timestamp of the msg3d and RoIs, user can easily run</p> <pre><code>ros2 echo [topic] --header field\n</code></pre>"},{"location":"perception/autoware_image_projection_based_fusion/#matching-strategies","title":"Matching Strategies","text":"<p>We provide two matching strategies for different scenarios:</p>"},{"location":"perception/autoware_image_projection_based_fusion/#naive-mode","title":"Naive Mode","text":"<p>User should use this mode if the concatenation node from the Autoware point cloud preprocessor is not being used. User should also set an appropriate <code>threshold</code> value to determine whether the time interval between the <code>msg3d</code> and <code>RoI</code> messages is within an acceptable range. If the interval is less than the match threshold, the messages are considered matched. </p> <ul> <li> <p>Example usage:</p> <pre><code>matching_strategy:\n  type: naive\n  threshold: 0.05\n</code></pre> </li> </ul>"},{"location":"perception/autoware_image_projection_based_fusion/#advanced-mode","title":"Advanced Mode","text":"<p>If the concatenation node from the Autoware point cloud preprocessor is being used, enable this mode. The advanced mode parses diagnostics from the concatenation node to verify whether all point clouds have been successfully concatenated. If concatenation is incomplete, it dynamically adjusts <code>rois_timestamp_offsets</code> based on diagnostic messages. Instead of using a fixed threshold, this mode requires setting two parameters:</p> <ul> <li><code>msg3d_noise_window</code> (a single value)</li> <li><code>rois_timestamp_noise_window</code> (a vector)</li> </ul> <p>These parameters enforce stricter matching between the <code>RoI</code> messages and <code>msg3d</code> input.</p> <ul> <li> <p>Example usage:</p> <pre><code>  matching_strategy:\n    type: advanced\n    msg3d_noise_window: 0.02\n    rois_timestamp_noise_window: [0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n</code></pre> </li> </ul>"},{"location":"perception/autoware_image_projection_based_fusion/#approximate-camera-projection","title":"Approximate camera projection","text":"<p>For algorithms like <code>pointpainting_fusion</code>, the computation required to project points onto an unrectified (raw) image can be substantial. To address this, an option is provided to reduce the computational load. Set the <code>approximate_camera_projection parameter</code> to <code>true</code> for each camera (ROIs). If the corresponding <code>point_project_to_unrectified_image</code> parameter is also set to true, the projections will be pre-cached.</p> <p>The cached projections are calculated using a grid, with the grid size specified by the <code>approximation_grid_width_size</code> and <code>approximation_grid_height_size</code> parameters in the configuration file. The centers of the grid are used as the projected points.</p>"},{"location":"perception/autoware_image_projection_based_fusion/#debug-and-diagnostics","title":"Debug and Diagnostics","text":"<p>To verify whether the node has successfully fuse the msg3d or rois, the user can examine rqt or the <code>/diagnostics</code> topic using the following command:</p> <pre><code>ros2 topic echo /diagnostics\n</code></pre> <p>Below is an example output when the fusion is success:</p> <ul> <li>msg3d has a value of <code>True</code>.</li> <li>Each rois has a value of <code>True</code>.</li> <li>The <code>fusion_success</code> is <code>True</code>.</li> <li>The <code>level</code> value is <code>\\0</code>. (diagnostic_msgs::msg::DiagnosticStatus::OK)</li> </ul> <pre><code>header:\n  stamp:\n    sec: 1722492015\n    nanosec: 848508777\n  frame_id: ''\nstatus:\n- level: \"\\0\"\n  name: 'pointpainting: pointpainting_fusion_status'\n  message: Fused output is published and include all rois and msg3d\n  hardware_id: pointpainting_fusion_checker\n  values:\n  - key: msg3d/is_fused\n    value: 'True'\n  - key: fused_timestamp\n    value: '1738725170.860273600'\n  - key: reference_timestamp_min\n    value: '1738725170.850771904'\n  - key: reference_timestamp_max\n    value: '1738725170.870771885'\n  - key: /rois0/timestamp\n    value: '1738725170.903310537'\n  - key: /rois0/is_fused\n    value: 'True'\n  - key: /rois1/timestamp\n    value: '1738725170.934378386'\n  - key: /rois1/is_fused\n    value: 'True'\n  - key: /rois2/timestamp\n    value: '1738725170.917550087'\n  - key: /rois2/is_fused\n    value: 'True'\n  - key: fusion_success\n    value: 'True'\n</code></pre> <p>Below is an example output when the fusion is failed:</p> <ul> <li>msg3d has a value of <code>True</code>.</li> <li>Each rois has a value of <code>False</code>.</li> <li>The <code>fusion_success</code> is <code>False</code>.</li> <li>The <code>level</code> value is <code>\\x02</code>. (diagnostic_msgs::msg::DiagnosticStatus::ERROR)</li> </ul> <pre><code>header:\n  stamp:\n    sec: 1722492015\n    nanosec: 848508777\n  frame_id: ''\nstatus:\n- level: \"\\x02\"\n  name: 'pointpainting: pointpainting_fusion_status'\n  message: Fused output msg is published but misses some ROIs\n  hardware_id: pointpainting_fusion_checker\n  values:\n  - key: msg3d/is_fused\n    value: 'True'\n  - key: fused_timestamp\n    value: '1738725170.860273600'\n  - key: reference_timestamp_min\n    value: '1738725170.850771904'\n  - key: reference_timestamp_max\n    value: '1738725170.870771885'\n  - key: /rois0/is_fused\n    value: 'False'\n  - key: /rois1/timestamp\n    value: '1738725170.934378386'\n  - key: /rois1/is_fused\n    value: 'True'\n  - key: /rois2/timestamp\n    value: '1738725170.917550087'\n  - key: /rois2/is_fused\n    value: 'True'\n  - key: fusion_success\n    value: 'False'\n</code></pre>"},{"location":"perception/autoware_image_projection_based_fusion/#the-build_only-option","title":"The <code>build_only</code> option","text":"<p>The <code>pointpainting_fusion</code> node has <code>build_only</code> option to build the TensorRT engine file from the ONNX file. Although it is preferred to move all the ROS parameters in <code>.param.yaml</code> file in Autoware Universe, the <code>build_only</code> option is not moved to the <code>.param.yaml</code> file for now, because it may be used as a flag to execute the build as a pre-task. You can execute with the following command:</p> <pre><code>ros2 launch autoware_image_projection_based_fusion pointpainting_fusion.launch.xml model_name:=pointpainting model_path:=/home/autoware/autoware_data/image_projection_based_fusion model_param_path:=$(ros2 pkg prefix autoware_image_projection_based_fusion --share)/config/pointpainting.param.yaml build_only:=true\n</code></pre>"},{"location":"perception/autoware_image_projection_based_fusion/docs/pointpainting-fusion/","title":"pointpainting_fusion","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/pointpainting-fusion/#pointpainting_fusion","title":"pointpainting_fusion","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/pointpainting-fusion/#purpose","title":"Purpose","text":"<p>The <code>pointpainting_fusion</code> is a package for utilizing the class information detected by a 2D object detection in 3D object detection.</p>"},{"location":"perception/autoware_image_projection_based_fusion/docs/pointpainting-fusion/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The lidar points are projected onto the output of an image-only 2d object detection network and the class scores are appended to each point. The painted point cloud can then be fed to the centerpoint network.</p> <p></p>"},{"location":"perception/autoware_image_projection_based_fusion/docs/pointpainting-fusion/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/pointpainting-fusion/#input","title":"Input","text":"Name Type Description <code>input/pointcloud</code> <code>sensor_msgs::msg::PointCloud2</code> pointcloud <code>input/camera_info[0-7]</code> <code>sensor_msgs::msg::CameraInfo</code> camera information to project 3d points onto image planes <code>input/rois[0-7]</code> <code>tier4_perception_msgs::msg::DetectedObjectsWithFeature</code> ROIs from each image <code>input/image_raw[0-7]</code> <code>sensor_msgs::msg::Image</code> images for visualization"},{"location":"perception/autoware_image_projection_based_fusion/docs/pointpainting-fusion/#output","title":"Output","text":"Name Type Description <code>output/objects</code> <code>autoware_perception_msgs::msg::DetectedObjects</code> detected objects <code>debug/painted_pointcloud</code> <code>sensor_msgs::msg::PointCloud2</code> painted pointcloud <code>debug/image_raw[0-7]</code> <code>sensor_msgs::msg::Image</code> images for visualization"},{"location":"perception/autoware_image_projection_based_fusion/docs/pointpainting-fusion/#parameters","title":"Parameters","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/pointpainting-fusion/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Description <code>encoder_onnx_path</code> string <code>\"\"</code> path to VoxelFeatureEncoder ONNX file <code>encoder_engine_path</code> string <code>\"\"</code> path to VoxelFeatureEncoder TensorRT Engine file <code>head_onnx_path</code> string <code>\"\"</code> path to DetectionHead ONNX file <code>head_engine_path</code> string <code>\"\"</code> path to DetectionHead TensorRT Engine file <code>build_only</code> bool <code>false</code> shutdown the node after TensorRT engine file is built <code>trt_precision</code> string <code>fp16</code> TensorRT inference precision: <code>fp32</code> or <code>fp16</code> <code>post_process_params.score_thresholds</code> list[double] [0.4, 0.4, 0.4, 0.4, 0.4] detected objects with score less than their label threshold are ignored. <code>post_process_params.yaw_norm_thresholds</code> list[double] [0.3, 0.3, 0.3, 0.3, 0.0] An array of distance threshold values of norm of yaw [rad]. <code>post_process_params.iou_nms_search_distance_2d</code> double 10.0 A maximum distance value to search the nearest objects. <code>post_process_params.iou_nms_threshold</code> double 0.1 A threshold value of NMS using IoU score. <code>post_process_params.has_twist</code> boolean false Indicates whether the model outputs twist value. <code>densification_params.world_frame_id</code> string <code>map</code> the world frame id to fuse multi-frame pointcloud <code>densification_params.num_past_frames</code> int <code>1</code> the number of past frames to fuse with the current frame"},{"location":"perception/autoware_image_projection_based_fusion/docs/pointpainting-fusion/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<ul> <li>The multi-frame painting is not implemented yet.</li> </ul>"},{"location":"perception/autoware_image_projection_based_fusion/docs/pointpainting-fusion/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/pointpainting-fusion/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/pointpainting-fusion/#referencesexternal-links","title":"References/External links","text":"<p>[1] Vora, Sourabh, et al. \"PointPainting: Sequential fusion for 3d object detection.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.</p> <p>[2] CVPR'20 Workshop on Scalability in Autonomous Driving] Waymo Open Dataset Challenge: https://youtu.be/9g9GsI33ol8?t=535 Ding, Zhuangzhuang, et al. \"1st Place Solution for Waymo Open Dataset Challenge--3D Detection and Domain Adaptation.\" arXiv preprint arXiv:2006.15505 (2020).</p>"},{"location":"perception/autoware_image_projection_based_fusion/docs/pointpainting-fusion/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-cluster-fusion/","title":"roi_cluster_fusion","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-cluster-fusion/#roi_cluster_fusion","title":"roi_cluster_fusion","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-cluster-fusion/#purpose","title":"Purpose","text":"<p>The <code>roi_cluster_fusion</code> is a package for filtering clusters that are less likely to be objects and overwriting labels of clusters with that of Region Of Interests (ROIs) by a 2D object detector.</p>"},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-cluster-fusion/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The clusters are projected onto image planes, and then if the ROIs of clusters and ROIs by a detector are overlapped, the labels of clusters are overwritten with that of ROIs by detector. Intersection over Union (IoU) is used to determine if there are overlaps between them.</p> <p></p>"},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-cluster-fusion/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-cluster-fusion/#input","title":"Input","text":"Name Type Description <code>input</code> <code>tier4_perception_msgs::msg::DetectedObjectsWithFeature</code> clustered pointcloud <code>input/camera_info[0-7]</code> <code>sensor_msgs::msg::CameraInfo</code> camera information to project 3d points onto image planes <code>input/rois[0-7]</code> <code>tier4_perception_msgs::msg::DetectedObjectsWithFeature</code> ROIs from each image <code>input/image_raw[0-7]</code> <code>sensor_msgs::msg::Image</code> images for visualization"},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-cluster-fusion/#output","title":"Output","text":"Name Type Description <code>output</code> <code>tier4_perception_msgs::msg::DetectedObjectsWithFeature</code> labeled cluster pointcloud <code>debug/image_raw[0-7]</code> <code>sensor_msgs::msg::Image</code> images for visualization"},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-cluster-fusion/#parameters","title":"Parameters","text":"<p>The following figure is an inner pipeline overview of RoI cluster fusion node. Please refer to it for your parameter settings.</p> <p></p>"},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-cluster-fusion/#core-parameters","title":"Core Parameters","text":"Name Type Description <code>fusion_distance</code> double If the detected object's distance to frame_id is less than the threshold, the fusion will be processed <code>strict_iou_fusion_distance</code> double if the detected object's distance is less than the <code>strict_iou_fusion_distance</code>, <code>strict_iou_match_mode</code> will be used, otherwise <code>rough_iou_match_mode</code> will be used <code>strict_iou_match_mode</code> string select mode from 3 options {<code>iou</code>, <code>iou_x</code>, <code>iou_y</code>} to calculate IoU in range of [<code>0</code>, <code>strict_iou_fusion_distance</code>].  <code>iou</code>: IoU along x-axis and y-axis  <code>iou_x</code>: IoU along x-axis  <code>iou_y</code>: IoU along y-axis <code>rough_iou_match_mode</code> string the IOU mode using in range of [<code>strict_iou_fusion_distance</code>, <code>fusion_distance</code>] if <code>strict_iou_fusion_distance</code> &lt; <code>fusion_distance</code> <code>use_cluster_semantic_type</code> bool if <code>false</code>, the labels of clusters are overwritten by <code>UNKNOWN</code> before fusion <code>only_allow_inside_cluster</code> bool if <code>true</code>, the only clusters contained inside RoIs by a detector <code>roi_scale_factor</code> double the scale factor for offset of detector RoIs if <code>only_allow_inside_cluster=true</code> <code>iou_threshold</code> double the IoU threshold to overwrite a label of clusters with a label of roi <code>unknown_iou_threshold</code> double the IoU threshold to fuse cluster with unknown label of roi <code>remove_unknown</code> bool if <code>true</code>, remove all <code>UNKNOWN</code> labeled objects from output <code>rois_number</code> int the number of input rois <code>debug_mode</code> bool If <code>true</code>, subscribe and publish images for visualization."},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-cluster-fusion/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-cluster-fusion/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-cluster-fusion/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-cluster-fusion/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-cluster-fusion/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-detected-object-fusion/","title":"roi_detected_object_fusion","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-detected-object-fusion/#roi_detected_object_fusion","title":"roi_detected_object_fusion","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-detected-object-fusion/#purpose","title":"Purpose","text":"<p>The <code>roi_detected_object_fusion</code> is a package to overwrite labels of detected objects with that of Region Of Interests (ROIs) by a 2D object detector.</p>"},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-detected-object-fusion/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>In what follows, we describe the algorithm utilized by <code>roi_detected_object_fusion</code> (the meaning of each parameter can be found in the <code>Parameters</code> section):</p> <ol> <li>If the <code>existence_probability</code> of a detected object is greater than the threshold, it is accepted without any further processing and published in <code>output</code>.</li> <li>The remaining detected objects are projected onto image planes, and if the resulting ROIs overlap with the ones from the 2D detector, they are published as fused objects in <code>output</code>. The Intersection over Union (IoU) is used to determine if there are overlaps between the detections from <code>input</code> and the ROIs from <code>input/rois</code>.</li> </ol> <p>The DetectedObject has three possible shape choices/implementations, where the polygon's vertices for each case are defined as follows:</p> <ul> <li><code>BOUNDING_BOX</code>: The 8 corners of a bounding box.</li> <li><code>CYLINDER</code>: The circle is approximated by a hexagon.</li> <li><code>POLYGON</code>: Not implemented yet.</li> </ul>"},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-detected-object-fusion/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-detected-object-fusion/#input","title":"Input","text":"Name Type Description <code>input</code> <code>autoware_perception_msgs::msg::DetectedObjects</code> input detected objects <code>input/camera_info[0-7]</code> <code>sensor_msgs::msg::CameraInfo</code> camera information to project 3d points onto image planes. <code>input/rois[0-7]</code> <code>tier4_perception_msgs::msg::DetectedObjectsWithFeature</code> ROIs from each image. <code>input/image_raw[0-7]</code> <code>sensor_msgs::msg::Image</code> images for visualization."},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-detected-object-fusion/#output","title":"Output","text":"Name Type Description <code>output</code> <code>autoware_perception_msgs::msg::DetectedObjects</code> detected objects <code>debug/image_raw[0-7]</code> <code>sensor_msgs::msg::Image</code> images for visualization, <code>debug/fused_objects</code> <code>autoware_perception_msgs::msg::DetectedObjects</code> fused detected objects <code>debug/ignored_objects</code> <code>autoware_perception_msgs::msg::DetectedObjects</code> not fused detected objects"},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-detected-object-fusion/#parameters","title":"Parameters","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-detected-object-fusion/#core-parameters","title":"Core Parameters","text":"Name Type Description <code>rois_number</code> int the number of input rois <code>debug_mode</code> bool If set to <code>true</code>, the node subscribes to the image topic and publishes an image with debug drawings. <code>passthrough_lower_bound_probability_thresholds</code> vector[double] If the <code>existence_probability</code> of a detected object is greater than the threshold, it is published in output. <code>trust_distances</code> vector[double] If the distance of a detected object from the origin of frame_id is greater than the threshold, it is published in output. <code>min_iou_threshold</code> double If the iou between detected objects and rois is greater than <code>min_iou_threshold</code>, the objects are classified as fused. <code>use_roi_probability</code> float If set to <code>true</code>, the algorithm uses <code>existence_probability</code> of ROIs to match with the that of detected objects. <code>roi_probability_threshold</code> double If the <code>existence_probability</code> of ROIs is greater than the threshold, matched detected objects are published in <code>output</code>. <code>can_assign_matrix</code> vector[int] association matrix between rois and detected_objects to check that two rois on images can be match"},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-detected-object-fusion/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p><code>POLYGON</code>, which is a shape of a detected object, isn't supported yet.</p>"},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-pointcloud-fusion/","title":"roi_pointcloud_fusion","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-pointcloud-fusion/#roi_pointcloud_fusion","title":"roi_pointcloud_fusion","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-pointcloud-fusion/#purpose","title":"Purpose","text":"<p>The node <code>roi_pointcloud_fusion</code> is to cluster the pointcloud based on Region Of Interests (ROIs) detected by a 2D object detector, specific for unknown labeled ROI.</p>"},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-pointcloud-fusion/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<ul> <li>The pointclouds are projected onto image planes and extracted as cluster if they are inside the unknown labeled ROIs.</li> <li>Since the cluster might contain unrelated points from background, then a refinement step is added to filter the background pointcloud by distance to camera.</li> </ul>"},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-pointcloud-fusion/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-pointcloud-fusion/#input","title":"Input","text":"Name Type Description <code>input</code> <code>sensor_msgs::msg::PointCloud2</code> input pointcloud <code>input/camera_info[0-7]</code> <code>sensor_msgs::msg::CameraInfo</code> camera information to project 3d points onto image planes <code>input/rois[0-7]</code> <code>tier4_perception_msgs::msg::DetectedObjectsWithFeature</code> ROIs from each image <code>input/image_raw[0-7]</code> <code>sensor_msgs::msg::Image</code> images for visualization"},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-pointcloud-fusion/#output","title":"Output","text":"Name Type Description <code>output</code> <code>tier4_perception_msgs::msg::DetectedObjectsWithFeature</code> output clusters <code>debug/clusters</code> <code>sensor_msgs/msg/PointCloud2</code> colored cluster pointcloud for visualization"},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-pointcloud-fusion/#parameters","title":"Parameters","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-pointcloud-fusion/#core-parameters","title":"Core Parameters","text":"Name Type Description Default Range fuse_unknown_only boolean Whether to fuse only UNKNOWN clusters. 1 N/A override_class_with_unknown boolean Whether to override class of output cluster with UNKNOWN. It is effective only when fuse_unknown_only is false. 0 N/A min_cluster_size integer The minimum number of points that a cluster must contain to be considered as valid. 2 \u22652 max_cluster_size integer The maximum number of points that a cluster must contain to be considered as valid. 20 \u22652 cluster_2d_tolerance float A cluster tolerance measured in radial direction [m] 0.5 &gt;0.0 roi_scale_factor float A scale factor for resizing RoI while checking if points are inside the RoI. 1 &gt;0.0 max_object_size float The maximum size of the object to be considered as valid. 2 &gt;0.0"},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-pointcloud-fusion/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<ul> <li>Currently, the refinement is only based on distance to camera, the roi based clustering is supposed to work well with small object ROIs. Others criteria for refinement might needed in the future.</li> </ul>"},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-pointcloud-fusion/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-pointcloud-fusion/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-pointcloud-fusion/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/roi-pointcloud-fusion/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/segmentation-pointcloud-fusion/","title":"segmentation_pointcloud_fusion","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/segmentation-pointcloud-fusion/#segmentation_pointcloud_fusion","title":"segmentation_pointcloud_fusion","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/segmentation-pointcloud-fusion/#purpose","title":"Purpose","text":"<p>The node <code>segmentation_pointcloud_fusion</code> is a package for filtering pointcloud that are belong to less interesting region which is defined by semantic or instance segmentation by 2D image segmentation model.</p>"},{"location":"perception/autoware_image_projection_based_fusion/docs/segmentation-pointcloud-fusion/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<ul> <li>The pointclouds are projected onto the semantic/instance segmentation mask image which is the output from 2D segmentation neural network.</li> <li>The pointclouds are belong to segment such as road, sidewalk, building, vegetation, sky ... are considered as less important points for autonomous driving and could be removed.</li> </ul>"},{"location":"perception/autoware_image_projection_based_fusion/docs/segmentation-pointcloud-fusion/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/segmentation-pointcloud-fusion/#input","title":"Input","text":"Name Type Description <code>input</code> <code>sensor_msgs::msg::PointCloud2</code> input pointcloud <code>input/camera_info[0-7]</code> <code>sensor_msgs::msg::CameraInfo</code> camera information to project 3d points onto image planes <code>input/rois[0-7]</code> <code>sensor_msgs::msg::Image</code> A gray-scale image of semantic segmentation mask, the pixel value is semantic class id <code>input/image_raw[0-7]</code> <code>sensor_msgs::msg::Image</code> images for visualization"},{"location":"perception/autoware_image_projection_based_fusion/docs/segmentation-pointcloud-fusion/#output","title":"Output","text":"Name Type Description <code>output</code> <code>sensor_msgs::msg::PointCloud2</code> output filtered pointcloud"},{"location":"perception/autoware_image_projection_based_fusion/docs/segmentation-pointcloud-fusion/#parameters","title":"Parameters","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/segmentation-pointcloud-fusion/#core-parameters","title":"Core Parameters","text":"Name Type Description Default Range filter_semantic_label_target.UNKNOWN boolean If true, UNKNOWN class of semantic will be filtered. 0 N/A filter_semantic_label_target.BUILDING boolean If true, BUILDING class of semantic will be filtered. 1 N/A filter_semantic_label_target.WALL boolean If true, WALL class of semantic will be filtered. 1 N/A filter_semantic_label_target.OBSTACLE boolean If true, OBSTACLE class of semantic will be filtered. 0 N/A filter_semantic_label_target.TRAFFIC_LIGHT boolean If true, TRAFFIC_LIGHT class of semantic will be filtered. 0 N/A filter_semantic_label_target.TRAFFIC_SIGN boolean If true, TRAFFIC_SIGN class of semantic will be filtered. 0 N/A filter_semantic_label_target.PERSON boolean If true, PERSON class of semantic will be filtered. 0 N/A filter_semantic_label_target.VEHICLE boolean If true, VEHICLE class of semantic will be filtered. 0 N/A filter_semantic_label_target.BIKE boolean If true, BIKE class of semantic will be filtered. 0 N/A filter_semantic_label_target.ROAD boolean If true, ROAD class of semantic will be filtered. 1 N/A filter_semantic_label_target.SIDEWALK boolean If true, SIDEWALK class of semantic will be filtered. 0 N/A filter_semantic_label_target.ROAD_PAINT boolean If true, ROAD_PAINT class of semantic will be filtered. 0 N/A filter_semantic_label_target.CURBSTONE boolean If true, CURBSTONE class of semantic will be filtered. 0 N/A filter_semantic_label_target.CROSSWALK boolean If true, CROSSWALK class of semantic will be filtered. 0 N/A filter_semantic_label_target.VEGETATION boolean If true, VEGETATION class of semantic will be filtered. 1 N/A filter_semantic_label_target.SKY boolean If true, SKY class of semantic will be filtered. 0 N/A filter_distance_threshold float A maximum distance of pointcloud to apply filter [m]. 60 \u22650.0 is_publish_debug_mask boolean If true, debug mask image will be published. 0 N/A"},{"location":"perception/autoware_image_projection_based_fusion/docs/segmentation-pointcloud-fusion/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/segmentation-pointcloud-fusion/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/segmentation-pointcloud-fusion/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/segmentation-pointcloud-fusion/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"perception/autoware_image_projection_based_fusion/docs/segmentation-pointcloud-fusion/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/autoware_lidar_apollo_instance_segmentation/","title":"autoware_lidar_apollo_instance_segmentation","text":""},{"location":"perception/autoware_lidar_apollo_instance_segmentation/#autoware_lidar_apollo_instance_segmentation","title":"autoware_lidar_apollo_instance_segmentation","text":""},{"location":"perception/autoware_lidar_apollo_instance_segmentation/#purpose","title":"Purpose","text":"<p>This node segments 3D pointcloud data from lidar sensors into obstacles, e.g., cars, trucks, bicycles, and pedestrians based on CNN based model and obstacle clustering method.</p>"},{"location":"perception/autoware_lidar_apollo_instance_segmentation/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>See the original design by Apollo.</p>"},{"location":"perception/autoware_lidar_apollo_instance_segmentation/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_lidar_apollo_instance_segmentation/#input","title":"Input","text":"Name Type Description <code>input/pointcloud</code> <code>sensor_msgs/PointCloud2</code> Pointcloud data from lidar sensors"},{"location":"perception/autoware_lidar_apollo_instance_segmentation/#output","title":"Output","text":"Name Type Description <code>output/labeled_clusters</code> <code>tier4_perception_msgs/DetectedObjectsWithFeature</code> Detected objects with labeled pointcloud cluster. <code>debug/instance_pointcloud</code> <code>sensor_msgs/PointCloud2</code> Segmented pointcloud for visualization."},{"location":"perception/autoware_lidar_apollo_instance_segmentation/#parameters","title":"Parameters","text":"Name Type Description Default Range score_threshold float If the score of a detected object is lower than this value, the object is ignored. 0.1 N/A range integer Half of the length of feature map sides. [m] 70 N/A width integer The grid width of the feature map. 672 N/A height integer The grid height of the feature map. 672 N/A use_intensity_feature boolean The flag to use intensity feature of pointcloud. 1 N/A use_constant_feature boolean The flag to use direction and distance feature of pointcloud. 0 N/A z_offset float z offset from target frame. [m] -2 N/A Name Type Description Default Range score_threshold float If the score of a detected object is lower than this value, the object is ignored. 0.1 N/A range integer Half of the length of feature map sides. [m] 70 N/A width integer The grid width of the feature map. 672 N/A height integer The grid height of the feature map. 672 N/A use_intensity_feature boolean The flag to use intensity feature of pointcloud. 1 N/A use_constant_feature boolean The flag to use direction and distance feature of pointcloud. 0 N/A z_offset float z offset from target frame. [m] -2 N/A Name Type Description Default Range score_threshold float If the score of a detected object is lower than this value, the object is ignored. 0.1 N/A range integer Half of the length of feature map sides. [m] 90 N/A width integer The grid width of the feature map. 864 N/A height integer The grid height of the feature map. 864 N/A use_intensity_feature boolean The flag to use intensity feature of pointcloud. 0 N/A use_constant_feature boolean The flag to use direction and distance feature of pointcloud. 0 N/A z_offset float z offset from target frame. [m] -2 N/A"},{"location":"perception/autoware_lidar_apollo_instance_segmentation/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>There is no training code for CNN model.</p>"},{"location":"perception/autoware_lidar_apollo_instance_segmentation/#note","title":"Note","text":"<p>This package makes use of three external codes. The trained files are provided by apollo. The trained files are automatically downloaded when you build.</p> <p>Original URL</p> <ul> <li>VLP-16 :   https://github.com/ApolloAuto/apollo/raw/88bfa5a1acbd20092963d6057f3a922f3939a183/modules/perception/production/data/perception/lidar/models/cnnseg/velodyne16/deploy.caffemodel</li> <li>HDL-64 :   https://github.com/ApolloAuto/apollo/raw/88bfa5a1acbd20092963d6057f3a922f3939a183/modules/perception/production/data/perception/lidar/models/cnnseg/velodyne64/deploy.caffemodel</li> <li>VLS-128 :   https://github.com/ApolloAuto/apollo/raw/91844c80ee4bd0cc838b4de4c625852363c258b5/modules/perception/production/data/perception/lidar/models/cnnseg/velodyne128/deploy.caffemodel</li> </ul> <p>Supported lidars are velodyne 16, 64 and 128, but you can also use velodyne 32 and other lidars with good accuracy.</p> <ol> <li> <p>apollo 3D Obstacle Perception description</p> <pre><code>/******************************************************************************\n* Copyright 2017 The Apollo Authors. All Rights Reserved.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n* http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*****************************************************************************/\n</code></pre> </li> <li> <p>tensorRTWrapper :    It is used under the lib directory.</p> <pre><code>MIT License\n\nCopyright (c) 2018 lewes6369\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre> </li> <li> <p>autoware_perception description</p> <pre><code>/*\n* Copyright 2018-2019 Autoware Foundation. All rights reserved.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*     http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n</code></pre> </li> </ol>"},{"location":"perception/autoware_lidar_apollo_instance_segmentation/#special-thanks","title":"Special thanks","text":"<ul> <li>Apollo Project</li> <li>lewes6369</li> <li>Autoware Foundation</li> <li>Kosuke Takeuchi (TIER IV)</li> </ul>"},{"location":"perception/autoware_lidar_centerpoint/","title":"autoware_lidar_centerpoint","text":""},{"location":"perception/autoware_lidar_centerpoint/#autoware_lidar_centerpoint","title":"autoware_lidar_centerpoint","text":""},{"location":"perception/autoware_lidar_centerpoint/#purpose","title":"Purpose","text":"<p>autoware_lidar_centerpoint is a package for detecting dynamic 3D objects.</p>"},{"location":"perception/autoware_lidar_centerpoint/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>In this implementation, CenterPoint [1] uses a PointPillars-based [2] network to inference with TensorRT.</p> <p>We trained the models using https://github.com/open-mmlab/mmdetection3d.</p>"},{"location":"perception/autoware_lidar_centerpoint/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_lidar_centerpoint/#input","title":"Input","text":"Name Type Description <code>~/input/pointcloud</code> <code>sensor_msgs::msg::PointCloud2</code> input pointcloud"},{"location":"perception/autoware_lidar_centerpoint/#output","title":"Output","text":"Name Type Description <code>~/output/objects</code> <code>autoware_perception_msgs::msg::DetectedObjects</code> detected objects <code>debug/cyclic_time_ms</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> cyclic time (msg) <code>debug/processing_time_ms</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> processing time (ms)"},{"location":"perception/autoware_lidar_centerpoint/#parameters","title":"Parameters","text":""},{"location":"perception/autoware_lidar_centerpoint/#ml-model-parameters","title":"ML Model Parameters","text":"<p>Note that these parameters are associated with ONNX file, predefined during the training phase. Be careful to change ONNX file as well when changing this parameter. Also, whenever you update the ONNX file, do NOT forget to check these values.</p> Name Type Default Value Description <code>model_params.class_names</code> list[string] [\"CAR\", \"TRUCK\", \"BUS\", \"BICYCLE\", \"PEDESTRIAN\"] list of class names for model outputs <code>model_params.point_feature_size</code> int <code>4</code> number of features per point in the point cloud <code>model_params.max_voxel_size</code> int <code>40000</code> maximum number of voxels <code>model_params.point_cloud_range</code> list[double] [-76.8, -76.8, -4.0, 76.8, 76.8, 6.0] detection range [min_x, min_y, min_z, max_x, max_y, max_z] [m] <code>model_params.voxel_size</code> list[double] [0.32, 0.32, 10.0] size of each voxel [x, y, z] [m] <code>model_params.downsample_factor</code> int <code>1</code> downsample factor for coordinates <code>model_params.encoder_in_feature_size</code> int <code>9</code> number of input features to the encoder <code>model_params.has_variance</code> bool <code>false</code> true if the model outputs pose variance as well as pose for each bbox <code>model_params.has_twist</code> bool <code>false</code> true if the model outputs velocity as well as pose for each bbox"},{"location":"perception/autoware_lidar_centerpoint/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Description <code>encoder_onnx_path</code> string <code>\"\"</code> path to VoxelFeatureEncoder ONNX file <code>encoder_engine_path</code> string <code>\"\"</code> path to VoxelFeatureEncoder TensorRT Engine file <code>head_onnx_path</code> string <code>\"\"</code> path to DetectionHead ONNX file <code>head_engine_path</code> string <code>\"\"</code> path to DetectionHead TensorRT Engine file <code>build_only</code> bool <code>false</code> shutdown the node after TensorRT engine file is built <code>trt_precision</code> string <code>fp16</code> TensorRT inference precision: <code>fp32</code> or <code>fp16</code> <code>post_process_params.score_thresholds</code> list[double] [0.35, 0.35, 0.35, 0.35, 0.35] detected objects with score less than their label threshold are ignored. <code>post_process_params.yaw_norm_thresholds</code> list[double] [0.3, 0.3, 0.3, 0.3, 0.0] An array of distance threshold values of norm of yaw [rad]. <code>post_process_params.iou_nms_search_distance_2d</code> double - If two objects are farther than the value, NMS isn't applied. <code>post_process_params.iou_nms_threshold</code> double - IoU threshold for the IoU-based Non Maximum Suppression <code>post_process_params.has_twist</code> boolean false Indicates whether the model outputs twist value. <code>densification_params.world_frame_id</code> string <code>map</code> the world frame id to fuse multi-frame pointcloud <code>densification_params.num_past_frames</code> int <code>1</code> the number of past frames to fuse with the current frame"},{"location":"perception/autoware_lidar_centerpoint/#the-build_only-option","title":"The <code>build_only</code> option","text":"<p>The <code>autoware_lidar_centerpoint</code> node has <code>build_only</code> option to build the TensorRT engine file from the ONNX file. Although it is preferred to move all the ROS parameters in <code>.param.yaml</code> file in Autoware Universe, the <code>build_only</code> option is not moved to the <code>.param.yaml</code> file for now, because it may be used as a flag to execute the build as a pre-task. You can execute with the following command:</p> <pre><code>ros2 launch autoware_lidar_centerpoint lidar_centerpoint.launch.xml model_name:=centerpoint_tiny model_path:=/home/autoware/autoware_data/lidar_centerpoint model_param_path:=$(ros2 pkg prefix autoware_lidar_centerpoint --share)/config/centerpoint_tiny.param.yaml build_only:=true\n</code></pre>"},{"location":"perception/autoware_lidar_centerpoint/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<ul> <li>The <code>object.existence_probability</code> is stored the value of classification confidence of a DNN, not probability.</li> </ul>"},{"location":"perception/autoware_lidar_centerpoint/#trained-models","title":"Trained Models","text":"<p>You can download the onnx format of trained models by clicking on the links below.</p> <ul> <li>Centerpoint: pts_voxel_encoder_centerpoint.onnx, pts_backbone_neck_head_centerpoint.onnx</li> <li>Centerpoint tiny: pts_voxel_encoder_centerpoint_tiny.onnx, pts_backbone_neck_head_centerpoint_tiny.onnx</li> </ul> <p><code>Centerpoint</code> was trained in <code>nuScenes</code> (~28k lidar frames) [8] and TIER IV's internal database (~11k lidar frames) for 60 epochs. <code>Centerpoint tiny</code> was trained in <code>Argoverse 2</code> (~110k lidar frames) [9] and TIER IV's internal database (~11k lidar frames) for 20 epochs.</p>"},{"location":"perception/autoware_lidar_centerpoint/#training-centerpoint-model-and-deploying-to-the-autoware","title":"Training CenterPoint Model and Deploying to the Autoware","text":""},{"location":"perception/autoware_lidar_centerpoint/#overview","title":"Overview","text":"<p>This guide provides instructions on training a CenterPoint model using the mmdetection3d repository and seamlessly deploying it within Autoware.</p>"},{"location":"perception/autoware_lidar_centerpoint/#installation","title":"Installation","text":""},{"location":"perception/autoware_lidar_centerpoint/#install-prerequisites","title":"Install prerequisites","text":"<p>Step 1. Download and install Miniconda from the official website.</p> <p>Step 2. Create a conda virtual environment and activate it</p> <pre><code>conda create --name train-centerpoint python=3.8 -y\nconda activate train-centerpoint\n</code></pre> <p>Step 3. Install PyTorch</p> <p>Please ensure you have PyTorch installed, and compatible with CUDA 11.6, as it is a requirement for current Autoware.</p> <pre><code>conda install pytorch==1.13.1 torchvision==0.14.1 pytorch-cuda=11.6 -c pytorch -c nvidia\n</code></pre>"},{"location":"perception/autoware_lidar_centerpoint/#install-mmdetection3d","title":"Install mmdetection3d","text":"<p>Step 1. Install MMEngine, MMCV, and MMDetection using MIM</p> <pre><code>pip install -U openmim\nmim install mmengine\nmim install 'mmcv&gt;=2.0.0rc4'\nmim install 'mmdet&gt;=3.0.0rc5, &lt;3.3.0'\n</code></pre> <p>Step 2. Install mmdetection3d forked repository</p> <p>Introduced several valuable enhancements in our fork of the mmdetection3d repository. Notably, we've made the PointPillar z voxel feature input optional to maintain compatibility with the original paper. In addition, we've integrated a PyTorch to ONNX converter and a T4 format reader for added functionality.</p> <pre><code>git clone https://github.com/autowarefoundation/mmdetection3d.git\ncd mmdetection3d\npip install -v -e .\n</code></pre>"},{"location":"perception/autoware_lidar_centerpoint/#use-training-repository-with-docker","title":"Use Training Repository with Docker","text":"<p>Alternatively, you can use Docker to run the mmdetection3d repository. We provide a Dockerfile to build a Docker image with the mmdetection3d repository and its dependencies.</p> <p>Clone fork of the mmdetection3d repository</p> <pre><code>git clone https://github.com/autowarefoundation/mmdetection3d.git\n</code></pre> <p>Build the Docker image by running the following command:</p> <pre><code>cd mmdetection3d\ndocker build -t mmdetection3d -f docker/Dockerfile .\n</code></pre> <p>Run the Docker container:</p> <pre><code>docker run --gpus all --shm-size=8g -it -v {DATA_DIR}:/mmdetection3d/data mmdetection3d\n</code></pre>"},{"location":"perception/autoware_lidar_centerpoint/#preparing-nuscenes-dataset-for-training","title":"Preparing NuScenes dataset for training","text":"<p>Step 1. Download the NuScenes dataset from the official website and extract the dataset to a folder of your choice.</p> <p>Note: The NuScenes dataset is large and requires significant disk space. Ensure you have enough storage available before proceeding.</p> <p>Step 2. Create a symbolic link to the dataset folder</p> <pre><code>ln -s /path/to/nuscenes/dataset/ /path/to/mmdetection3d/data/nuscenes/\n</code></pre> <p>Step 3. Prepare the NuScenes data by running:</p> <pre><code>cd mmdetection3d\npython tools/create_data.py nuscenes --root-path ./data/nuscenes --out-dir ./data/nuscenes --extra-tag nuscenes\n</code></pre>"},{"location":"perception/autoware_lidar_centerpoint/#training-centerpoint-with-nuscenes-dataset","title":"Training CenterPoint with NuScenes Dataset","text":""},{"location":"perception/autoware_lidar_centerpoint/#prepare-the-config-file","title":"Prepare the config file","text":"<p>The configuration file that illustrates how to train the CenterPoint model with the NuScenes dataset is located at <code>mmdetection3d/projects/AutowareCenterPoint/configs</code>. This configuration file is a derived version of this centerpoint configuration file from mmdetection3D. In this custom configuration, the use_voxel_center_z parameter is set as False to deactivate the z coordinate of the voxel center, aligning with the original paper's specifications and making the model compatible with Autoware. Additionally, the filter size is set as [32, 32].</p> <p>The CenterPoint model can be tailored to your specific requirements by modifying various parameters within the configuration file. This includes adjustments related to preprocessing operations, training, testing, model architecture, dataset, optimizer, learning rate scheduler, and more.</p>"},{"location":"perception/autoware_lidar_centerpoint/#start-training","title":"Start training","text":"<pre><code>python tools/train.py projects/AutowareCenterPoint/configs/centerpoint_custom.py --work-dir ./work_dirs/centerpoint_custom\n</code></pre>"},{"location":"perception/autoware_lidar_centerpoint/#evaluation-of-the-trained-model","title":"Evaluation of the trained model","text":"<p>For evaluation purposes, we have included a sample dataset captured from the vehicle which consists of the following LiDAR sensors: 1 x Velodyne VLS128, 4 x Velodyne VLP16, and 1 x Robosense RS Bpearl. This dataset comprises 600 LiDAR frames and encompasses 5 distinct classes, 6905 cars, 3951 pedestrians, 75 cyclists, 162 buses, and 326 trucks 3D annotation. In the sample dataset, frames are annotated as 2 frames for each second. You can employ this dataset for a wide range of purposes, including training, evaluation, and fine-tuning of models. It is organized in the T4 format.</p>"},{"location":"perception/autoware_lidar_centerpoint/#download-the-sample-dataset","title":"Download the sample dataset","text":"<pre><code>wget https://autoware-files.s3.us-west-2.amazonaws.com/dataset/lidar_detection_sample_dataset.tar.gz\n#Extract the dataset to a folder of your choice\ntar -xvf lidar_detection_sample_dataset.tar.gz\n#Create a symbolic link to the dataset folder\nln -s /PATH/TO/DATASET/ /PATH/TO/mmdetection3d/data/tier4_dataset/\n</code></pre>"},{"location":"perception/autoware_lidar_centerpoint/#prepare-dataset-and-evaluate-trained-model","title":"Prepare dataset and evaluate trained model","text":"<p>Create <code>.pkl</code> files for training, evaluation, and testing.</p> <p>The dataset was formatted according to T4Dataset specifications, with 'sample_dataset' designated as one of its versions.</p> <pre><code>python tools/create_data.py T4Dataset --root-path data/sample_dataset/ --out-dir data/sample_dataset/ --extra-tag T4Dataset --version sample_dataset --annotation-hz 2\n</code></pre> <p>Run evaluation</p> <pre><code>python tools/test.py projects/AutowareCenterPoint/configs/centerpoint_custom_test.py /PATH/OF/THE/CHECKPOINT  --task lidar_det\n</code></pre> <p>Evaluation results could be relatively low because of the e to variations in sensor modalities between the sample dataset and the training dataset. The model's training parameters are originally tailored to the NuScenes dataset, which employs a single lidar sensor positioned atop the vehicle. In contrast, the provided sample dataset comprises concatenated point clouds positioned at the base link location of the vehicle.</p>"},{"location":"perception/autoware_lidar_centerpoint/#deploying-centerpoint-model-to-autoware","title":"Deploying CenterPoint model to Autoware","text":""},{"location":"perception/autoware_lidar_centerpoint/#convert-centerpoint-pytorch-model-to-onnx-format","title":"Convert CenterPoint PyTorch model to ONNX Format","text":"<p>The autoware_lidar_centerpoint implementation requires two ONNX models as input the voxel encoder and the backbone-neck-head of the CenterPoint model, other aspects of the network, such as preprocessing operations, are implemented externally. Under the fork of the mmdetection3d repository, we have included a script that converts the CenterPoint model to Autoware compatible ONNX format. You can find it in <code>mmdetection3d/projects/AutowareCenterPoint</code> file.</p> <pre><code>python projects/AutowareCenterPoint/centerpoint_onnx_converter.py --cfg projects/AutowareCenterPoint/configs/centerpoint_custom.py --ckpt work_dirs/centerpoint_custom/YOUR_BEST_MODEL.pth --work-dir ./work_dirs/onnx_models\n</code></pre>"},{"location":"perception/autoware_lidar_centerpoint/#create-the-config-file-for-the-custom-model","title":"Create the config file for the custom model","text":"<p>Create a new config file named centerpoint_custom.param.yaml under the config file directory of the autoware_lidar_centerpoint node. Sets the parameters of the config file like point_cloud_range, point_feature_size, voxel_size, etc. according to the training config file.</p> <pre><code>/**:\n  ros__parameters:\n    class_names: [\"CAR\", \"TRUCK\", \"BUS\", \"BICYCLE\", \"PEDESTRIAN\"]\n    point_feature_size: 4\n    max_voxel_size: 40000\n    point_cloud_range: [-51.2, -51.2, -3.0, 51.2, 51.2, 5.0]\n    voxel_size: [0.2, 0.2, 8.0]\n    downsample_factor: 1\n    encoder_in_feature_size: 9\n    # post-process params\n    circle_nms_dist_threshold: 0.5\n    iou_nms_search_distance_2d: 10.0\n    iou_nms_threshold: 0.1\n    yaw_norm_thresholds: [0.3, 0.3, 0.3, 0.3, 0.0]\n</code></pre>"},{"location":"perception/autoware_lidar_centerpoint/#launch-the-lidar_centerpoint-node","title":"Launch the lidar_centerpoint node","text":"<pre><code>cd /YOUR/AUTOWARE/PATH/Autoware\nsource install/setup.bash\nros2 launch autoware_lidar_centerpoint lidar_centerpoint.launch.xml  model_name:=centerpoint_custom  model_path:=/PATH/TO/ONNX/FILE/\n</code></pre>"},{"location":"perception/autoware_lidar_centerpoint/#launch-the-lidar_short_range_centerpoint-node","title":"Launch the lidar_short_range_centerpoint node","text":"<p>It also provides short_range detections using CenterPoint:</p> <pre><code>cd /YOUR/AUTOWARE/PATH/Autoware\nsource install/setup.bash\nros2 launch autoware_lidar_centerpoint lidar_centerpoint.launch.xml model_name:=centerpoint_short_range model_path:=/PATH/TO/ONNX/FILE/\n</code></pre>"},{"location":"perception/autoware_lidar_centerpoint/#changelog","title":"Changelog","text":""},{"location":"perception/autoware_lidar_centerpoint/#v1-20220706","title":"v1 (2022/07/06)","text":"Name URLs Description <code>centerpoint</code> pts_voxel_encoder pts_backbone_neck_head There is a single change due to the limitation in the implementation of this package. <code>num_filters=[32, 32]</code> of <code>PillarFeatureNet</code> <code>centerpoint_tiny</code> pts_voxel_encoder pts_backbone_neck_head The same model as <code>default</code> of <code>v0</code>. <p>These changes are compared with this configuration.</p>"},{"location":"perception/autoware_lidar_centerpoint/#v0-20211203","title":"v0 (2021/12/03)","text":"Name URLs Description <code>default</code> pts_voxel_encoder pts_backbone_neck_head There are two changes from the original CenterPoint architecture. <code>num_filters=[32]</code> of <code>PillarFeatureNet</code> and <code>ds_layer_strides=[2, 2, 2]</code> of <code>RPN</code>"},{"location":"perception/autoware_lidar_centerpoint/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/autoware_lidar_centerpoint/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/autoware_lidar_centerpoint/#referencesexternal-links","title":"References/External links","text":"<p>[1] Yin, Tianwei, Xingyi Zhou, and Philipp Kr\u00e4henb\u00fchl. \"Center-based 3d object detection and tracking.\" arXiv preprint arXiv:2006.11275 (2020).</p> <p>[2] Lang, Alex H., et al. \"PointPillars: Fast encoders for object detection from point clouds.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.</p> <p>[3] https://github.com/tianweiy/CenterPoint</p> <p>[4] https://github.com/open-mmlab/mmdetection3d</p> <p>[5] https://github.com/open-mmlab/OpenPCDet</p> <p>[6] https://github.com/yukkysaito/autoware_perception</p> <p>[7] https://github.com/NVIDIA-AI-IOT/CUDA-PointPillars</p> <p>[8] https://www.nuscenes.org/nuscenes</p> <p>[9] https://www.argoverse.org/av2.html</p>"},{"location":"perception/autoware_lidar_centerpoint/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/autoware_lidar_centerpoint/#acknowledgment-deepenais-3d-annotation-tools-contribution","title":"Acknowledgment: deepen.ai's 3D Annotation Tools Contribution","text":"<p>Special thanks to Deepen AI for providing their 3D Annotation tools, which have been instrumental in creating our sample dataset.</p>"},{"location":"perception/autoware_lidar_centerpoint/#legal-notice","title":"Legal Notice","text":"<p>The nuScenes dataset is released publicly for non-commercial use under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License. Additional Terms of Use can be found at https://www.nuscenes.org/terms-of-use. To inquire about a commercial license please contact nuscenes@motional.com.</p>"},{"location":"perception/autoware_lidar_frnet/","title":"autoware_lidar_frnet","text":""},{"location":"perception/autoware_lidar_frnet/#autoware_lidar_frnet","title":"autoware_lidar_frnet","text":""},{"location":"perception/autoware_lidar_frnet/#purpose","title":"Purpose","text":"<p>The <code>autoware_lidar_frnet</code> package is used for 3D semantic segmentation based on LiDAR data (x, y, z, intensity).</p>"},{"location":"perception/autoware_lidar_frnet/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The implementation is based on the FRNet [1] project. It uses TensorRT library for data processing and network inference.</p> <p>We trained the models using AWML [2].</p>"},{"location":"perception/autoware_lidar_frnet/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_lidar_frnet/#input","title":"Input","text":"Name Type Description <code>~/input/pointcloud</code> <code>sensor_msgs::msg::PointCloud2</code> Input pointcloud."},{"location":"perception/autoware_lidar_frnet/#output","title":"Output","text":"Name Type Description <code>~/output/pointcloud/segmentation</code> <code>sensor_msgs::msg::PointCloud2</code> XYZ cloud with class ID field. <code>~/output/pointcloud/visualization</code> <code>sensor_msgs::msg::PointCloud2</code> XYZ cloud with RGB field. <code>~/output/pointcloud/filtered</code> <code>sensor_msgs::msg::PointCloud2</code> Input format cloud after removing specified point's class. <code>debug/cyclic_time_ms</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> Cyclic time (ms). <code>debug/pipeline_latency_ms</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> Pipeline latency time (ms). <code>debug/processing_time/preprocess_ms</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> Preprocess (ms). <code>debug/processing_time/inference_ms</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> Inference time (ms). <code>debug/processing_time/postprocess_ms</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> Postprocess time (ms). <code>debug/processing_time/total_ms</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> Total processing time (ms). <code>/diagnostics</code> <code>diagnostic_msgs::msg::DiagnosticArray</code> Node diagnostics with respect to processing time constraints"},{"location":"perception/autoware_lidar_frnet/#parameters","title":"Parameters","text":""},{"location":"perception/autoware_lidar_frnet/#frnet-node","title":"FRNet node","text":"Name Type Description Default Range onnx_path string A path to ONNX model file. $(var model_path)/frnet.onnx N/A trt_precision string A precision of TensorRT engine. fp16 ['fp16', 'fp32'] fov_up_deg float LiDAR's upper elevation angle in degrees. 10.0 \u2265-90.0\u226490.0 fov_down_deg float LiDAR's lower elevation angle in degrees. -30.0 \u2265-90.0\u226490.0 frustum_width integer Width of the FRNet frustum in pixels. 480 \u22651 frustum_height integer Height of the FRNet frustum in pixels. 480 \u22651 interpolation_width integer Width of the FRNet LiDAR's points to 2D plane interpolation in pixels. 480 \u22651 interpolation_height integer Height of the FRNet LiDAR's points to 2D plane interpolation in pixels. 480 \u22651 score_threshold float A threshold value for the score model segmentation result. If the score is lower than this value, the point will obtain the 'unknown' class ID. 0.0 N/A excluded_class_names array A list of class names to be excluded in filtered output cloud. ['driveable_surface'] N/A palette array A sequence of RGB values for each class name. The length of the array must be 3 times the number of class names. [255, 120, 50, 255, 192, 203, 255, 255, 0, 0, 150, 245, 0, 255, 255, 255, 127, 0, 255, 0, 0, 255, 240, 150, 135, 60, 0, 160, 32, 240, 255, 0, 255, 139, 137, 137, 75, 0, 75, 150, 240, 80, 230, 230, 250, 0, 175, 0, 0, 0, 0] N/A"},{"location":"perception/autoware_lidar_frnet/#frnet-model","title":"FRNet model","text":"Name Type Description Default Range class_names array An array of class names which will be predicted. ['barrier', 'bicycle', 'bus', 'car', 'construction_vehicle', 'motorcycle', 'pedestrian', 'traffic_cone', 'trailer', 'truck', 'driveable_surface', 'other_flat', 'sidewalk', 'terrain', 'manmade', 'vegetation', 'unknown'] N/A num_points array TensorRT optimization profile for number of points [min, opt, max]. [30000, 150000, 300000] N/A num_unique_coors array TensorRT optimization profile for number of unique coordinates [min, opt, max]. [5000, 10000, 30000] N/A"},{"location":"perception/autoware_lidar_frnet/#frnet-diagnostics","title":"FRNet diagnostics","text":"Name Type Description Default Range max_allowed_processing_time_ms float A threshold value for the allowed processing time. If the processing time exceeds this value, it will be considered a warning state. [ms] 200 \u22650.0 max_acceptable_consecutive_delay_ms float A threshold value for the error state. If the duration since the last processing timestamp, which ended within max_allowed_processing_time_ms, exceeds this value, it will be considered an error state. [ms] 1000 \u22650.0 validation_callback_interval_ms float An interval value for a timer callback that checks whether the current state meets the max_acceptable_consecutive_delay_ms condition. [ms] 100 \u22651.0"},{"location":"perception/autoware_lidar_frnet/#the-build_only-option","title":"The <code>build_only</code> option","text":"<p>The <code>autoware_lidar_frnet</code> node has <code>build_only</code> option to build the TensorRT engine file from the ONNX file.</p> <pre><code>ros2 launch autoware_lidar_frnet lidar_frnet.launch.xml build_only:=true\n</code></pre>"},{"location":"perception/autoware_lidar_frnet/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>This library operates on raw cloud data (bytes). It is assumed that the input pointcloud message has XYZIRC format:</p> <pre><code>[\n  sensor_msgs.msg.PointField(name='x', offset=0, datatype=7, count=1),\n  sensor_msgs.msg.PointField(name='y', offset=4, datatype=7, count=1),\n  sensor_msgs.msg.PointField(name='z', offset=8, datatype=7, count=1),\n  sensor_msgs.msg.PointField(name='intensity', offset=12, datatype=2, count=1),\n  sensor_msgs.msg.PointField(name='ring', offset=13, datatype=2, count=1),\n  sensor_msgs.msg.PointField(name='channel', offset=14, datatype=4, count=1)\n]\n</code></pre> <p>This input may consist of other fields as well - shown format is required minimum. For debug purposes, you can validate your pointcloud topic using simple command:</p> <pre><code>ros2 topic echo &lt;input_topic&gt; --field fields\n</code></pre>"},{"location":"perception/autoware_lidar_frnet/#trained-models","title":"Trained Models","text":"<p>The model was trained on the NuScenes dataset and is available in the Autoware artifacts.</p>"},{"location":"perception/autoware_lidar_frnet/#referencesexternal-links","title":"References/External links","text":"<p>[1] X. Xu, L. Kong, H. Shuai and Q. Liu, \"FRNet: Frustum-Range Networks for Scalable LiDAR Segmentation\" in IEEE Transactions on Image Processing, vol. 34, pp. 2173-2186, 2025, doi: 10.1109/TIP.2025.3550011. </p> <p>[2] https://github.com/tier4/AWML.git</p> <p>[3] https://xiangxu-0103.github.io/FRNet</p>"},{"location":"perception/autoware_lidar_transfusion/","title":"autoware_lidar_transfusion","text":""},{"location":"perception/autoware_lidar_transfusion/#autoware_lidar_transfusion","title":"autoware_lidar_transfusion","text":""},{"location":"perception/autoware_lidar_transfusion/#purpose","title":"Purpose","text":"<p>The <code>autoware_lidar_transfusion</code> package is used for 3D object detection based on lidar data (x, y, z, intensity).</p>"},{"location":"perception/autoware_lidar_transfusion/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The implementation bases on TransFusion [1] work. It uses TensorRT library for data process and network inference.</p> <p>We trained the models using https://github.com/open-mmlab/mmdetection3d.</p>"},{"location":"perception/autoware_lidar_transfusion/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_lidar_transfusion/#input","title":"Input","text":"Name Type Description <code>~/input/pointcloud</code> <code>sensor_msgs::msg::PointCloud2</code> Input pointcloud."},{"location":"perception/autoware_lidar_transfusion/#output","title":"Output","text":"Name Type Description <code>~/output/objects</code> <code>autoware_perception_msgs::msg::DetectedObjects</code> Detected objects. <code>debug/cyclic_time_ms</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> Cyclic time (ms). <code>debug/pipeline_latency_ms</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> Pipeline latency time (ms). <code>debug/processing_time/preprocess_ms</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> Preprocess (ms). <code>debug/processing_time/inference_ms</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> Inference time (ms). <code>debug/processing_time/postprocess_ms</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> Postprocess time (ms). <code>debug/processing_time/total_ms</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> Total processing time (ms)."},{"location":"perception/autoware_lidar_transfusion/#parameters","title":"Parameters","text":""},{"location":"perception/autoware_lidar_transfusion/#transfusion-node","title":"TransFusion node","text":"Name Type Description Default Range trt_precision string A precision of TensorRT engine. fp16 ['fp16', 'fp32'] cloud_capacity integer Capacity of the point cloud buffer (should be set to at least the maximum theoretical number of points). 2000000 \u22651 onnx_path string A path to ONNX model file. $(var model_path)/transfusion.onnx N/A engine_path string A path to TensorRT engine file. $(var model_path)/transfusion.engine N/A densification_num_past_frames integer A number of past frames to be considered as same input frame. 1 \u22650 densification_world_frame_id string A name of frame id where world coordinates system is defined with respect to. map N/A circle_nms_dist_threshold float A distance threshold between detections in NMS. 0.5 \u22650.0 iou_nms_search_distance_2d float A maximum distance value to search the nearest objects. 10.0 \u22650.0 iou_nms_threshold float A threshold value of NMS using IoU score. 0.1 \u22650.0\u22641.0 yaw_norm_thresholds array A thresholds array of direction vectors norm, all of objects with vector norm less than this threshold are ignored. [0.3, 0.3, 0.3, 0.3, 0.0] N/A score_threshold float A threshold value of confidence score, all of objects with score less than this threshold are ignored. 0.2 \u22650.0"},{"location":"perception/autoware_lidar_transfusion/#transfusion-model","title":"TransFusion model","text":"Name Type Description Default Range class_names array An array of class names will be predicted. ['CAR', 'TRUCK', 'BUS', 'BICYCLE', 'PEDESTRIAN'] N/A voxels_num array A maximum number of voxels [min, opt, max]. [5000, 30000, 60000] N/A point_cloud_range array An array of distance ranges of each class. [-76.8, -76.8, -3.0, 76.8, 76.8, 5.0] N/A voxel_size array Voxels size [x, y, z]. [0.3, 0.3, 8.0] N/A num_proposals integer Number of proposals at TransHead. 500 \u22651"},{"location":"perception/autoware_lidar_transfusion/#detection-class-remapper","title":"Detection class remapper","text":"Name Type Description Default Range allow_remapping_by_area_matrix array Whether to allow remapping of classes. The order of 8x8 matrix classes comes from ObjectClassification msg. [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] N/A min_area_matrix array Minimum area for specific class to consider class remapping. [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.1, 0.0, 36.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 36.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 36.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] N/A max_area_matrix array Maximum area for specific class to consider class remapping. [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 36.0, 0.0, 999.999, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 999.999, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 999.999, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] N/A"},{"location":"perception/autoware_lidar_transfusion/#the-build_only-option","title":"The <code>build_only</code> option","text":"<p>The <code>autoware_lidar_transfusion</code> node has <code>build_only</code> option to build the TensorRT engine file from the ONNX file. Although it is preferred to move all the ROS parameters in <code>.param.yaml</code> file in Autoware Universe, the <code>build_only</code> option is not moved to the <code>.param.yaml</code> file for now, because it may be used as a flag to execute the build as a pre-task. You can execute with the following command:</p> <pre><code>ros2 launch autoware_lidar_transfusion lidar_transfusion.launch.xml build_only:=true\n</code></pre>"},{"location":"perception/autoware_lidar_transfusion/#the-log_level-option","title":"The <code>log_level</code> option","text":"<p>The default logging severity level for <code>autoware_lidar_transfusion</code> is <code>info</code>. For debugging purposes, the developer may decrease severity level using <code>log_level</code> parameter:</p> <pre><code>ros2 launch autoware_lidar_transfusion lidar_transfusion.launch.xml log_level:=debug\n</code></pre>"},{"location":"perception/autoware_lidar_transfusion/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>This library operates on raw cloud data (bytes). It is assumed that the input pointcloud message has following format:</p> <pre><code>[\n  sensor_msgs.msg.PointField(name='x', offset=0, datatype=7, count=1),\n  sensor_msgs.msg.PointField(name='y', offset=4, datatype=7, count=1),\n  sensor_msgs.msg.PointField(name='z', offset=8, datatype=7, count=1),\n  sensor_msgs.msg.PointField(name='intensity', offset=12, datatype=2, count=1)\n]\n</code></pre> <p>This input may consist of other fields as well - shown format is required minimum. For debug purposes, you can validate your pointcloud topic using simple command:</p> <pre><code>ros2 topic echo &lt;input_topic&gt; --field fields\n</code></pre>"},{"location":"perception/autoware_lidar_transfusion/#trained-models","title":"Trained Models","text":"<p>You can download the onnx format of trained models by clicking on the links below.</p> <ul> <li>TransFusion: transfusion.onnx</li> </ul> <p>The model was trained in TIER IV's internal database (~11k lidar frames) for 50 epochs.</p>"},{"location":"perception/autoware_lidar_transfusion/#changelog","title":"Changelog","text":""},{"location":"perception/autoware_lidar_transfusion/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/autoware_lidar_transfusion/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/autoware_lidar_transfusion/#referencesexternal-links","title":"References/External links","text":"<p>[1] Xuyang Bai, Zeyu Hu, Xinge Zhu, Qingqiu Huang, Yilun Chen, Hongbo Fu and Chiew-Lan Tai. \"TransFusion: Robust LiDAR-Camera Fusion for 3D Object Detection with Transformers.\" arXiv preprint arXiv:2203.11496 (2022). </p> <p>[2] https://github.com/wep21/CUDA-TransFusion</p> <p>[3] https://github.com/open-mmlab/mmdetection3d</p> <p>[4] https://github.com/open-mmlab/OpenPCDet</p> <p>[5] https://www.nuscenes.org/nuscenes</p>"},{"location":"perception/autoware_lidar_transfusion/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/autoware_map_based_prediction/","title":"map_based_prediction","text":""},{"location":"perception/autoware_map_based_prediction/#map_based_prediction","title":"map_based_prediction","text":""},{"location":"perception/autoware_map_based_prediction/#role","title":"Role","text":"<p><code>map_based_prediction</code> is a module to predict the future paths (and their probabilities) of other vehicles and pedestrians according to the shape of the map and the surrounding environment.</p>"},{"location":"perception/autoware_map_based_prediction/#assumptions","title":"Assumptions","text":"<ul> <li>The following information about the target obstacle is needed<ul> <li>Label (type of person, car, etc.)</li> <li>The object position in the current time and predicted position in the future time.</li> </ul> </li> <li>The following information about the surrounding environment is needed<ul> <li>Road network information with Lanelet2 format</li> </ul> </li> </ul>"},{"location":"perception/autoware_map_based_prediction/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"perception/autoware_map_based_prediction/#flow-chart","title":"Flow chart","text":""},{"location":"perception/autoware_map_based_prediction/#path-prediction-for-road-users","title":"Path prediction for road users","text":""},{"location":"perception/autoware_map_based_prediction/#remove-old-object-history","title":"Remove old object history","text":"<p>Store time-series data of objects to determine the vehicle's route and to detect lane change for several duration. Object Data contains the object's position, speed, and time information.</p>"},{"location":"perception/autoware_map_based_prediction/#get-current-lanelet-and-update-object-history","title":"Get current lanelet and update Object history","text":"<p>Search one or more lanelets satisfying the following conditions for each target object and store them in the ObjectData.</p> <ul> <li>The CoG of the object must be inside the lanelet.</li> <li>The centerline of the lanelet must have two or more points.</li> <li>The angle difference between the lanelet and the direction of the object must be within the threshold given by the parameters.<ul> <li>The angle flip is allowed, the condition is <code>diff_yaw &lt; threshold or diff_yaw &gt; pi - threshold</code>.</li> </ul> </li> <li>The lanelet must be reachable from the lanelet recorded in the past history.</li> </ul>"},{"location":"perception/autoware_map_based_prediction/#get-predicted-reference-path","title":"Get predicted reference path","text":"<ul> <li>Get reference path:<ul> <li>Create a reference path for the object from the associated lanelet.</li> </ul> </li> <li>Predict object maneuver:<ul> <li>Generate predicted paths for the object.</li> <li>Assign probability to each maneuver of <code>Lane Follow</code>, <code>Left Lane Change</code>, and <code>Right Lane Change</code> based on the object history and the reference path obtained in the first step.</li> <li>Lane change decision is based on two domains:<ul> <li>Geometric domain: the lateral distance between the center of gravity of the object and left/right boundaries of the lane.</li> <li>Time domain: estimated time margin for the object to reach the left/right bound.</li> </ul> </li> </ul> </li> </ul> <p>The conditions for left lane change detection are:</p> <ul> <li>Check if the distance to the left lane boundary is less than the distance to the right lane boundary.</li> <li>Check if the distance to the left lane boundary is less than a <code>dist_threshold_to_bound_</code>.</li> <li>Check if the lateral velocity direction is towards the left lane boundary.</li> <li>Check if the time to reach the left lane boundary is less than <code>time_threshold_to_bound_</code>.</li> </ul> <p>Lane change logics is illustrated in the figure below.An example of how to tune the parameters is described later.</p> <p></p> <ul> <li>Calculate object probability:<ul> <li>The path probability obtained above is calculated based on the current position and angle of the object.</li> </ul> </li> <li>Refine predicted paths for smooth movement:<ul> <li>The generated predicted paths are recomputed to take the vehicle dynamics into account.</li> <li>The path is calculated with minimum jerk trajectory implemented by 4th/5th order spline for lateral/longitudinal motion.</li> </ul> </li> </ul>"},{"location":"perception/autoware_map_based_prediction/#tuning-lane-change-detection-logic","title":"Tuning lane change detection logic","text":"<p>Currently we provide three parameters to tune lane change detection:</p> <ul> <li><code>dist_threshold_to_bound_</code>: maximum distance from lane boundary allowed for lane changing vehicle</li> <li><code>time_threshold_to_bound_</code>: maximum time allowed for lane change vehicle to reach the boundary</li> <li><code>cutoff_freq_of_velocity_lpf_</code>: cutoff frequency of low pass filter for lateral velocity</li> </ul> <p>You can change these parameters in rosparam in the table below.</p> param name default value <code>dist_threshold_for_lane_change_detection</code> <code>1.0</code> [m] <code>time_threshold_for_lane_change_detection</code> <code>5.0</code> [s] <code>cutoff_freq_of_velocity_for_lane_change_detection</code> <code>0.1</code> [Hz]"},{"location":"perception/autoware_map_based_prediction/#tuning-threshold-parameters","title":"Tuning threshold parameters","text":"<p>Increasing these two parameters will slow down and stabilize the lane change estimation.</p> <p>Normally, we recommend tuning only <code>time_threshold_for_lane_change_detection</code> because it is the more important factor for lane change decision.</p>"},{"location":"perception/autoware_map_based_prediction/#tuning-lateral-velocity-calculation","title":"Tuning lateral velocity calculation","text":"<p>Lateral velocity calculation is also a very important factor for lane change decision because it is used in the time domain decision.</p> <p>The predicted time to reach the lane boundary is calculated by</p> \\[ t_{predicted} = \\dfrac{d_{lat}}{v_{lat}} \\] <p>where \\(d_{lat}\\) and \\(v_{lat}\\) represent the lateral distance to the lane boundary and the lateral velocity, respectively.</p> <p>Lowering the cutoff frequency of the low-pass filter for lateral velocity will make the lane change decision more stable but slower. Our setting is very conservative, so you may increase this parameter if you want to make the lane change decision faster.</p> <p>For the additional information, here we show how we calculate lateral velocity.</p> lateral velocity calculation method equation description [applied] time derivative of lateral distance \\(\\dfrac{\\Delta d_{lat}}{\\Delta t}\\) Currently, we use this method to deal with winding roads. Since this time differentiation easily becomes noisy, we also use a low-pass filter to get smoothed velocity. [not applied] Object Velocity Projection to Lateral Direction \\(v_{obj} \\sin(\\theta)\\) Normally, object velocities are less noisy than the time derivative of lateral distance. But the yaw difference \\(\\theta\\) between the lane and object directions sometimes becomes discontinuous, so we did not adopt this method. <p>Currently, we use the upper method with a low-pass filter to calculate lateral velocity.</p>"},{"location":"perception/autoware_map_based_prediction/#path-generation","title":"Path generation","text":"<p>Path generation is generated on the frenet frame. The path is generated by the following steps:</p> <ol> <li>Get the frenet frame of the reference path.</li> <li>Generate the frenet frame of the current position of the object and the finite position of the object.</li> <li>Optimize the path in each longitudinal and lateral coordinate of the frenet frame. (Use the quintic polynomial to fit start and end conditions.)</li> <li>Convert the path to the global coordinate.</li> </ol> <p>See paper [2] for more details.</p>"},{"location":"perception/autoware_map_based_prediction/#tuning-lateral-path-shape","title":"Tuning lateral path shape","text":"<p><code>lateral_control_time_horizon</code> parameter supports the tuning of the lateral path shape. This parameter is used to calculate the time to reach the reference path. The smaller the value, the more the path will be generated to reach the reference path quickly. (Mostly the center of the lane.)</p>"},{"location":"perception/autoware_map_based_prediction/#pruning-predicted-paths-with-lateral-acceleration-constraint-for-vehicle-obstacles","title":"Pruning predicted paths with lateral acceleration constraint (for vehicle obstacles)","text":"<p>It is possible to apply a maximum lateral acceleration constraint to generated vehicle paths. This check verifies if it is possible for the vehicle to perform the predicted path without surpassing a lateral acceleration threshold <code>max_lateral_accel</code> when taking a curve. If it is not possible, it checks if the vehicle can slow down on time to take the curve with a deceleration of <code>min_acceleration_before_curve</code> and comply with the constraint. If that is also not possible, the path is eliminated.</p> <p>Currently we provide three parameters to tune the lateral acceleration constraint:</p> <ul> <li><code>check_lateral_acceleration_constraints_</code>: to enable the constraint check.</li> <li><code>max_lateral_accel_</code>: max acceptable lateral acceleration for predicted paths (absolute value).</li> <li><code>min_acceleration_before_curve_</code>: the minimum acceleration the vehicle would theoretically use to slow down before a curve is taken (must be negative).</li> </ul> <p>You can change these parameters in rosparam in the table below.</p> param name default value <code>check_lateral_acceleration_constraints</code> <code>false</code> [bool] <code>max_lateral_accel</code> <code>2.0</code> [m/s^2] <code>min_acceleration_before_curve</code> <code>-2.0</code> [m/s^2]"},{"location":"perception/autoware_map_based_prediction/#using-vehicle-acceleration-for-path-prediction-for-vehicle-obstacles","title":"Using Vehicle Acceleration for Path Prediction (for Vehicle Obstacles)","text":"<p>By default, the <code>map_based_prediction</code> module uses the current obstacle's velocity to compute its predicted path length. However, it is possible to use the obstacle's current acceleration to calculate its predicted path's length.</p>"},{"location":"perception/autoware_map_based_prediction/#decaying-acceleration-model","title":"Decaying Acceleration Model","text":"<p>Since this module tries to predict the vehicle's path several seconds into the future, it is not practical to consider the current vehicle's acceleration as constant (it is not assumed the vehicle will be accelerating for <code>prediction_time_horizon</code> seconds after detection). Instead, a decaying acceleration model is used. With the decaying acceleration model, a vehicle's acceleration is modeled as:</p> <p>$\\ a(t) = a_{t0} \\cdot e^{-\\lambda \\cdot t} $</p> <p>where $\\ a_{t0} $ is the vehicle acceleration at the time of detection $\\ t0 $, and $\\ \\lambda $ is the decay constant $\\ \\lambda = \\ln(2) / hl $ and $\\ hl $ is the exponential's half life.</p> <p>Furthermore, the integration of $\\ a(t) $ over time gives us equations for velocity, $\\ v(t) $ and distance $\\ x(t) $ as:</p> <p>$\\ v(t) = v{t0} + a * (1/\\lambda) \\cdot (1 - e^{-\\lambda \\cdot t}) $</p> <p>and</p> <p>$\\ x(t) = x{t0} + (v + a{t0} * (1/\\lambda)) \\cdot t + a(1/\u03bb^2)(e^{-\\lambda \\cdot t} - 1) $</p> <p>With this model, the influence of the vehicle's detected instantaneous acceleration on the predicted path's length is diminished but still considered. This feature also considers that the obstacle might not accelerate past its road's speed limit (multiplied by a tunable factor).</p> <p>Currently, we provide three parameters to tune the use of obstacle acceleration for path prediction:</p> <ul> <li><code>use_vehicle_acceleration</code>: to enable the feature.</li> <li><code>acceleration_exponential_half_life</code>: The decaying acceleration model considers that the current vehicle acceleration will be halved after this many seconds.</li> <li><code>speed_limit_multiplier</code>: Set the vehicle type obstacle's maximum predicted speed as the legal speed limit in that lanelet times this value. This value should be at least equal or greater than 1.0.</li> </ul> <p>You can change these parameters in <code>rosparam</code> in the table below.</p> Param Name Default Value <code>use_vehicle_acceleration</code> <code>false</code> [bool] <code>acceleration_exponential_half_life</code> <code>2.5</code> [s] <code>speed_limit_multiplier</code> <code>1.5</code> []"},{"location":"perception/autoware_map_based_prediction/#path-prediction-for-crosswalk-users","title":"Path prediction for crosswalk users","text":"<p>This module treats Pedestrians and Bicycles as objects using the crosswalk, and outputs prediction path based on map and estimated object's velocity, assuming the object has intention to cross the crosswalk, if the objects satisfies at least one of the following conditions:</p> <ul> <li>move toward the crosswalk</li> <li>stop near the crosswalk</li> </ul> <p>If there are a reachable crosswalk entry points within the <code>prediction_time_horizon</code> and the objects satisfies above condition, this module outputs additional predicted path to cross the opposite side via the crosswalk entry point.</p> <p>To prevent the predicted crossing path from chattering due to noise in the estimated pose or velocity of a pedestrian who is judged to have crossing intention, the module holds the crossing intention state for a certain duration based on the following parameters:</p> Parameter Unit Type Description <code>crossing_intention_duration</code> [s] double Minimum duration that crossing intention must continuously persist to be judged as true <code>no_crossing_intention_duration</code> [s] double Minimum duration that lack of crossing intention must continuously persist to be judged as false <p>Note</p> <p>Increasing <code>crossing_intention_duration</code> can reduce the frequency of false positives caused by noise that mistakenly indicates crossing intention. However, it also delays the system\u2019s response to pedestrians who actually intend to cross. Therefore, setting this parameter to a large value is not recommended.</p> <p>This module takes into account the corresponding traffic light information. When RED signal is indicated, we assume the target object will not walk across. In addition, if the target object is stopping (not moving) against GREEN signal, we assume the target object will not walk across either. This prediction comes from the assumption that the object should move if the traffic light is green and the object is intended to cross.</p> <p>If the target object is inside the road or crosswalk, this module outputs one or two additional prediction path(s) to reach exit point of the crosswalk. The number of prediction paths are depend on whether object is moving or not. If the object is moving, this module outputs one prediction path toward an exit point that existed in the direction of object's movement. One the other hand, if the object has stopped, it is impossible to infer which exit points the object want to go, so this module outputs two prediction paths toward both side exit point.</p> <p>In the case where the target object is inside the road, the additional path(s) to reach the nearest crosswalk are only generated if the distance between the object and the crosswalk is not higher than parameter <code>max_crosswalk_user_on_road_distance</code>.</p>"},{"location":"perception/autoware_map_based_prediction/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_map_based_prediction/#input","title":"Input","text":"Name Type Description <code>~/perception/object_recognition/tracking/objects</code> <code>autoware_perception_msgs::msg::TrackedObjects</code> tracking objects without predicted path. <code>~/vector_map</code> <code>autoware_map_msgs::msg::LaneletMapBin</code> binary data of Lanelet2 Map. <code>~/perception/traffic_light_recognition/traffic_signals</code> <code>autoware_perception_msgs::msg::TrafficLightGroupArray</code> rearranged information on the corresponding traffic lights"},{"location":"perception/autoware_map_based_prediction/#output","title":"Output","text":"Name Type Description <code>~/input/objects</code> <code>autoware_perception_msgs::msg::TrackedObjects</code> tracking objects. Default is set to <code>/perception/object_recognition/tracking/objects</code> <code>~/output/objects</code> <code>autoware_perception_msgs::msg::PredictedObjects</code> tracking objects with predicted path. <code>~/objects_path_markers</code> <code>visualization_msgs::msg::MarkerArray</code> marker for visualization. <code>~/debug/processing_time_ms</code> <code>std_msgs::msg::Float64</code> processing time of this module. <code>~/debug/cyclic_time_ms</code> <code>std_msgs::msg::Float64</code> cyclic time of this module."},{"location":"perception/autoware_map_based_prediction/#parameters","title":"Parameters","text":"Parameter Unit Type Description <code>prediction_time_horizon</code> [s] double predict time duration for predicted path <code>lateral_control_time_horizon</code> [s] double time duration for predicted path will reach the reference path (mostly center of the lane) <code>prediction_sampling_delta_time</code> [s] double sampling time for points in predicted path <code>min_velocity_for_map_based_prediction</code> [m/s] double apply map-based prediction to the objects with higher velocity than this value <code>min_crosswalk_user_velocity</code> [m/s] double minimum velocity used when crosswalk user's velocity is calculated <code>max_crosswalk_user_delta_yaw_threshold_for_lanelet</code> [rad] double maximum yaw difference between crosswalk user and lanelet to use in path prediction for crosswalk users <code>max_crosswalk_user_on_road_distance</code> [m/s] double crosswalk users on road or shoulder lanelets are only predicted to cross the crosswalk if they are within this distance <code>dist_threshold_for_searching_lanelet</code> [m] double The threshold of the angle used when searching for the lane to which the object belongs <code>delta_yaw_threshold_for_searching_lanelet</code> [rad] double The threshold of the angle used when searching for the lane to which the object belongs <code>sigma_lateral_offset</code> [m] double Standard deviation for lateral position of objects <code>sigma_yaw_angle_deg</code> [deg] double Standard deviation yaw angle of objects <code>object_buffer_time_length</code> [s] double Time span of object history to store the information <code>history_time_length</code> [s] double Time span of object information used for prediction <code>prediction_time_horizon_rate_for_validate_shoulder_lane_length</code> [-] double prediction path will disabled when the estimated path length exceeds lanelet length. This parameter control the estimated path length"},{"location":"perception/autoware_map_based_prediction/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<ul> <li>For object types of passenger car, bus, and truck<ul> <li>The predicted path of the object follows the road structure.</li> <li>For the object not being on any roads, the predicted path is generated by just a straight line prediction.</li> <li>For the object on a lanelet but moving in a different direction of the road, the predicted path is just straight.</li> <li>Vehicle dynamics may not be properly considered in the predicted path.</li> </ul> </li> <li>For object types of person and motorcycle<ul> <li>The predicted path is generated by just a straight line in all situations except for \"around crosswalk\".</li> </ul> </li> <li>For all obstacles<ul> <li>In the prediction, the vehicle motion is assumed to be a constant velocity due to a lack of acceleration information.</li> </ul> </li> </ul>"},{"location":"perception/autoware_map_based_prediction/#reference","title":"Reference","text":"<ol> <li>M. Werling, J. Ziegler, S. Kammel, and S. Thrun, \u201cOptimal trajectory generation for dynamic street scenario in a frenet frame,\u201d IEEE International Conference on Robotics and Automation, Anchorage, Alaska, USA, May 2010.</li> <li>A. Houenou, P. Bonnifait, V. Cherfaoui, and Wen Yao, \u201cVehicle trajectory prediction based on motion model and maneuver recognition,\u201d in 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, nov 2013, pp. 4363-4369.</li> </ol>"},{"location":"perception/autoware_multi_object_tracker/","title":"multi_object_tracker","text":""},{"location":"perception/autoware_multi_object_tracker/#multi_object_tracker","title":"multi_object_tracker","text":""},{"location":"perception/autoware_multi_object_tracker/#purpose","title":"Purpose","text":"<p>The results of the detection are processed by a time series. The main purpose is to give ID and estimate velocity.</p>"},{"location":"perception/autoware_multi_object_tracker/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>This multi object tracker consists of data association and EKF.</p> <p></p>"},{"location":"perception/autoware_multi_object_tracker/#data-association","title":"Data association","text":"<p>The data association performs maximum score matching, called min cost max flow problem. In this package, mussp[1] is used as solver. In addition, when associating observations to tracers, data association have gates such as the area of the object from the BEV, Mahalanobis distance, and maximum distance, depending on the class label.</p>"},{"location":"perception/autoware_multi_object_tracker/#ekf-tracker","title":"EKF Tracker","text":"<p>Models for pedestrians, bicycles (motorcycles), cars and unknown are available. The pedestrian or bicycle tracker is running at the same time as the respective EKF model in order to enable the transition between pedestrian and bicycle tracking. For big vehicles such as trucks and buses, we have separate models for passenger cars and large vehicles because they are difficult to distinguish from passenger cars and are not stable. Therefore, separate models are prepared for passenger cars and big vehicles, and these models are run at the same time as the respective EKF models to ensure stability.</p>"},{"location":"perception/autoware_multi_object_tracker/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_multi_object_tracker/#input","title":"Input","text":"<p>Multiple inputs are pre-defined in the input channel parameters (described below) and the inputs can be configured</p> Name Type Description <code>input/detection**/objects</code> <code>std::string</code> input topic <code>input/detection**/channel</code> <code>std::string</code> input channel configuration <p>rule of the channel configuration</p> <ul> <li>'none' or empty : Indicates that this detection input channel is not used/disabled</li> <li>Any other string : Specifies a custom channel name to be used for the detection input, configured in <code>schema/input_channels.schema.json</code></li> </ul> <p>Up to 12 detection inputs can be configured (detection01 through detection12). Each input consists of an objects topic and its corresponding channel configuration.</p> <p>Example configurations:</p> <ul> <li>Single detection input:</li> </ul> <pre><code>input/detection01/objects: /perception/object_recognition/detection/objects\ninput/detection01/channel: detected_objects # general input channel type\ninput/detection02/objects: input/objects02\ninput/detection02/channel: none # Disabled\n</code></pre> <ul> <li>Multiple detection inputs:</li> </ul> <pre><code># lidar centerpoint\ninput/detection01/objects: /perception/object_recognition/detection/lidar_centerpoint/objects\ninput/detection01/channel: lidar_centerpoint\n\n# lidar short_range centerpoint\ninput/detection02/channel: /perception/object_recognition/detection/centerpoint_short_range/objects\ninput/detection02/objects: lidar_centerpoint_short_range\n\n# camera lidar fusion\ninput/detection03/objects: /perception/object_recognition/detection/clustering/camera_lidar_fusion/objects\ninput/detection03/channel: camera_lidar_fusion\n\n# camera lidar fusion based irregular object detection\ninput/detection04/objects: /perception/object_recognition/detection/irregular_object/objects\ninput/detection04/channel: camera_lidar_fusion_irregular\n\n# detection by tracker\ninput/detection05/objects: /perception/object_recognition/detection/detection_by_tracker/objects\ninput/detection05/channel: detection_by_tracker\n\n# radar\ninput/detection06/objects: /perception/object_recognition/detection/radar/objects\ninput/detection06/channel: radar\n\n# disable\ninput/detection07/objects: input/objects07\ninput/detection07/channel: none # Disabled\n</code></pre> <p>Up to 12 detection inputs can be configured (detection01 through detection12). Each input consists of an objects topic and its corresponding channel configuration.</p>"},{"location":"perception/autoware_multi_object_tracker/#output","title":"Output","text":"Name Type Description <code>~/output</code> <code>autoware_perception_msgs::msg::TrackedObjects</code> tracked objects"},{"location":"perception/autoware_multi_object_tracker/#parameters","title":"Parameters","text":""},{"location":"perception/autoware_multi_object_tracker/#input-channel-parameters","title":"Input Channel parameters","text":"Name Type Description Default Range flags.can_spawn_new_tracker boolean Indicates if the input channel can spawn new trackers. True N/A flags.can_trust_existence_probability boolean Indicates if the input channel can trust the existence probability. True N/A flags.can_trust_extension boolean Indicates if the input channel can trust the object size(extension). True N/A flags.can_trust_classification boolean Indicates if the input channel can trust the object classification. True N/A flags.can_trust_orientation boolean Indicates if the input channel can trust the object orientation. True N/A optional.name string The name of the input channel. detected_objects N/A optional.short_name string The short name of the input channel. all N/A"},{"location":"perception/autoware_multi_object_tracker/#core-parameters","title":"Core Parameters","text":"<ul> <li>Node</li> </ul> Name Type Description Default Range car_tracker string Tracker model for car class. multi_vehicle_tracker N/A truck_tracker string Tracker model for truck class. multi_vehicle_tracker N/A bus_tracker string Tracker model for bus class. multi_vehicle_tracker N/A trailer_tracker string Tracker model for trailer class. multi_vehicle_tracker N/A pedestrian_tracker string Tracker model for pedestrian class. pedestrian_and_bicycle_tracker N/A bicycle_tracker string Tracker model for bicycle class. pedestrian_and_bicycle_tracker N/A motorcycle_tracker string Tracker model for motorcycle class. pedestrian_and_bicycle_tracker N/A publish_rate float Timer frequency to output with delay compensation. 10.0 N/A world_frame_id string Object kinematics definition frame. map N/A ego_frame_id string Vehicle's ego frame. base_link N/A enable_delay_compensation boolean If True, tracker use timers to schedule publishers and use prediction step to extrapolate object state at desired timestamp. False N/A consider_odometry_uncertainty boolean If True, consider odometry uncertainty in tracking. False N/A enable_unknown_object_velocity_estimation boolean If True, enable velocity estimation for unknown objects. True N/A enable_unknown_object_motion_output boolean If True, export unknown object velocity. False N/A tracker_lifetime float Lifetime of the tracker in seconds. 1.0 N/A min_known_object_removal_iou float Minimum IOU between associated objects with known label to remove younger tracker 0.1 N/A min_unknown_object_removal_iou float Minimum IOU between associated objects with unknown label to remove unknown tracker 0.001 N/A pruning_generalized_iou_thresholds array Generalized IoU threshold for each class. [-0.3, -0.4, -0.6, -0.6, -0.6, -0.1, -0.1, -0.1] N/A pruning_static_object_speed float Speed threshold to consider an object as static. Below this speed, nearby unknown objects should always be detected. 1.38 N/A pruning_moving_object_speed float Speed threshold to consider an object as moving. Above this speed, nearby unknown objects should always be merged to avoid false unknown detections. 5.5 N/A pruning_static_iou_threshold float Generalized IoU threshold for a static object. 0.0 N/A pruning_distance_thresholds array Overlap distance threshold for each class. [9.0, 5.0, 9.0, 9.0, 9.0, 4.0, 3.0, 2.0] N/A publish_processing_time boolean Enable to publish debug message of process time information. False N/A publish_processing_time_detail boolean Enable to publish debug message of detailed process time information. False N/A publish_tentative_objects boolean Enable to publish tentative tracked objects, which have lower confidence. False N/A publish_debug_markers boolean Enable to publish debug markers, which indicates association of multi-inputs, existence probability of each detection. False N/A diagnostics_warn_delay float Delay threshold for warning diagnostics in seconds. 0.5 N/A diagnostics_error_delay float Delay threshold for error diagnostics in seconds. 1.0 N/A diagnostics_warn_extrapolation float Delay extrapolation threshold for warning diagnostics in seconds. 0.5 N/A diagnostics_error_extrapolation float Delay extrapolation threshold for error diagnostics in seconds. 1.0 N/A <ul> <li>Association</li> </ul> Name Type Description Default Range can_assign_matrix array Assignment table for data association. [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1] N/A max_dist_matrix array Maximum distance table for data association. [4.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 4.0, 2.0, 5.0, 5.0, 5.0, 1.0, 1.0, 1.0, 4.0, 2.0, 5.0, 5.0, 5.0, 1.0, 1.0, 1.0, 4.0, 2.0, 5.0, 5.0, 5.0, 1.0, 1.0, 1.0, 4.0, 2.0, 5.0, 5.0, 5.0, 1.0, 1.0, 1.0, 3.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 2.0, 3.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 2.0] N/A max_area_matrix array Maximum area table for data association. [100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 12.1, 12.1, 36.0, 60.0, 60.0, 10000.0, 10000.0, 10000.0, 36.0, 12.1, 36.0, 60.0, 60.0, 10000.0, 10000.0, 10000.0, 60.0, 12.1, 36.0, 60.0, 60.0, 10000.0, 10000.0, 10000.0, 60.0, 12.1, 36.0, 60.0, 60.0, 10000.0, 10000.0, 10000.0, 2.5, 10000.0, 10000.0, 10000.0, 10000.0, 2.5, 2.5, 2.5, 2.5, 10000.0, 10000.0, 10000.0, 10000.0, 2.5, 2.5, 2.5, 2.0, 10000.0, 10000.0, 10000.0, 10000.0, 2.0, 2.0, 2.0] N/A min_area_matrix array Minimum area table for data association. [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.6, 3.6, 6.0, 10.0, 10.0, 0.0, 0.0, 0.0, 6.0, 3.6, 6.0, 10.0, 10.0, 0.0, 0.0, 0.0, 10.0, 3.6, 6.0, 10.0, 10.0, 0.0, 0.0, 0.0, 10.0, 3.6, 6.0, 10.0, 10.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.1, 0.1, 0.1, 0.001, 0.0, 0.0, 0.0, 0.0, 0.1, 0.1, 0.1, 0.001, 0.0, 0.0, 0.0, 0.0, 0.1, 0.1, 0.1] N/A max_rad_matrix array Maximum angle table for data association. [3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 1.047, 1.047, 1.047, 1.047, 3.15, 3.15, 3.15, 3.15, 1.047, 1.047, 1.047, 1.047, 3.15, 3.15, 3.15, 3.15, 1.047, 1.047, 1.047, 1.047, 3.15, 3.15, 3.15, 3.15, 1.047, 1.047, 1.047, 1.047, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15] N/A min_iou_matrix array A matrix that represents the minimum Intersection over Union (IoU) limit allowed for assignment. [0.0001, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.2, 0.2, 0.1, 0.1, 0.1, 0.1, 0.2, 0.3, 0.3, 0.3, 0.1, 0.1, 0.1, 0.1, 0.2, 0.3, 0.3, 0.3, 0.1, 0.1, 0.1, 0.1, 0.2, 0.3, 0.3, 0.3, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0001] N/A unknown_association_giou_threshold float GIoU threshold for unknown-unknown association. -0.8 N/A"},{"location":"perception/autoware_multi_object_tracker/#simulation-parameters","title":"Simulation parameters","text":"Name Type Description Default Range car_tracker string Tracker model for car class. pass_through_tracker N/A truck_tracker string Tracker model for truck class. pass_through_tracker N/A bus_tracker string Tracker model for bus class. pass_through_tracker N/A pedestrian_tracker string Tracker model for pedestrian class. pass_through_tracker N/A bicycle_tracker string Tracker model for bicycle class. pass_through_tracker N/A motorcycle_tracker string Tracker model for motorcycle class. pass_through_tracker N/A"},{"location":"perception/autoware_multi_object_tracker/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>See the model explanations.</p>"},{"location":"perception/autoware_multi_object_tracker/#performance-benchmark-unit-testing","title":"Performance Benchmark &amp; Unit Testing","text":""},{"location":"perception/autoware_multi_object_tracker/#overview","title":"Overview","text":"<p>Unit tests and benchmarks are included to evaluate tracker performance under varying detection loads and object types.</p>"},{"location":"perception/autoware_multi_object_tracker/#how-to-run-locally","title":"How to Run Locally","text":""},{"location":"perception/autoware_multi_object_tracker/#1-build-with-tests","title":"1. Build with Tests","text":"<pre><code>colcon build --packages-select autoware_multi_object_tracker \\\n  --cmake-args -DCMAKE_BUILD_TYPE=RelWithDebInfo\n</code></pre>"},{"location":"perception/autoware_multi_object_tracker/#2-run-the-performance-benchmark","title":"2. Run the Performance Benchmark","text":"<pre><code>source install/setup.bash\n./build/autoware_multi_object_tracker/test_multi_object_tracker\n</code></pre> <p>This runs the default test (<code>SimulatedDataPerformanceTest</code> and <code>RealDataRosbagPerformanceTest</code>) and outputs timing data.</p>"},{"location":"perception/autoware_multi_object_tracker/#3-run-optional-profiling-modestests","title":"3. Run Optional Profiling Modes/Tests","text":"<ul> <li><code>PerformanceVsCarCount()</code></li> <li><code>PerformanceVsPedestrianCount()</code></li> <li><code>PerformanceVsUnknownObjectCount()</code></li> <li><code>AssociationTest()</code></li> </ul> <p>These optional profiling tests are compiled as disabled and can be run directly using GoogleTest options:</p> <p>To run a specific option profiling test:</p> <pre><code>./build/autoware_multi_object_tracker/test_multi_object_tracker \\\n  --gtest_also_run_disabled_tests \\\n  --gtest_filter=\"*.*AssociationTest\"\n</code></pre> <p>To run multiple profiling tests together (separate with <code>:</code>):</p> <pre><code>./build/autoware_multi_object_tracker/test_multi_object_tracker \\\n  --gtest_also_run_disabled_tests \\\n  --gtest_filter=\"*.*AssociationTest:*.*PerformanceVsPedestrianCount\"\n</code></pre> <p>This allows you to evaluate scalability with object count and other scenarios without modifying the source code.</p>"},{"location":"perception/autoware_multi_object_tracker/#rosbag-replay-visualization","title":"Rosbag Replay &amp; Visualization","text":""},{"location":"perception/autoware_multi_object_tracker/#simulated-rosbag-output","title":"Simulated Rosbag Output","text":"<p>To record benchmark results for visualization in RViz:</p> <ol> <li>Enable <code>write_bag = true</code> in <code>runIterations()</code></li> <li>Run the test; the output <code>.db3</code> path is printed</li> <li>Play rosbag and visualize using rviz(suggested to use a product one):</li> </ol> <pre><code>ros2 bag play &lt;output_file&gt;.db3\nrviz2 -d &lt;your_rviz_config&gt;.rviz\n</code></pre>"},{"location":"perception/autoware_multi_object_tracker/#real-rosbag-input","title":"Real Rosbag Input","text":"<ol> <li>Set the path in <code>runPerformanceTestWithRosbag()</code> to a real <code>.db3</code> file</li> <li>Run the test</li> <li>Visualize the tracking result in RViz</li> </ol>"},{"location":"perception/autoware_multi_object_tracker/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/autoware_multi_object_tracker/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/autoware_multi_object_tracker/#evaluation-of-mussp","title":"Evaluation of muSSP","text":"<p>According to our evaluation, muSSP is faster than normal SSP when the matrix size is more than 100.</p> <p>Execution time for varying matrix size at 95% sparsity. In real data, the sparsity was often around 95%. </p> <p>Execution time for varying the sparsity with matrix size 100. </p>"},{"location":"perception/autoware_multi_object_tracker/#optional-referencesexternal-links","title":"(Optional) References/External links","text":"<p>This package makes use of external code.</p> Name License Original Repository muSSP Apache-2.0 https://github.com/yu-lab-vt/muSSP <p>[1] C. Wang, Y. Wang, Y. Wang, C.-t. Wu, and G. Yu, \"muSSP: Efficient Min-cost Flow Algorithm for Multi-object Tracking,\" NeurIPS, 2019</p>"},{"location":"perception/autoware_multi_object_tracker/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/autoware_multi_object_tracker/models/","title":"Models used in this module","text":""},{"location":"perception/autoware_multi_object_tracker/models/#models-used-in-this-module","title":"Models used in this module","text":""},{"location":"perception/autoware_multi_object_tracker/models/#tracking-model","title":"Tracking model","text":""},{"location":"perception/autoware_multi_object_tracker/models/#ctrv-model-1","title":"CTRV model [1]","text":"<p>CTRV model is a model that assumes constant turn rate and velocity magnitude.</p> <ul> <li>state transition equation</li> </ul> \\[ \\begin{aligned} x_{k+1} &amp; = x_{k} + v_{k} \\cos(\\psi_k) \\cdot {d t} \\\\ y_{k+1} &amp; = y_{k} + v_{k} \\sin(\\psi_k) \\cdot {d t} \\\\ \\psi_{k+1} &amp; = \\psi_k + \\dot\\psi_{k} \\cdot {d t} \\\\ v_{k+1} &amp; = v_{k} \\\\ \\dot\\psi_{k+1} &amp; = \\dot\\psi_{k} \\\\ \\end{aligned} \\] <ul> <li>jacobian</li> </ul> \\[ A = \\begin{bmatrix} 1 &amp; 0 &amp; -v \\sin(\\psi) \\cdot dt &amp; \\cos(\\psi) \\cdot dt &amp; 0 \\\\ 0 &amp; 1 &amp; v \\cos(\\psi) \\cdot dt &amp; \\sin(\\psi) \\cdot dt &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; dt \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ \\end{bmatrix} \\]"},{"location":"perception/autoware_multi_object_tracker/models/#kinematic-bicycle-model-2","title":"Kinematic bicycle model [2]","text":"<p>Static bicycle model uses two wheel positions (front and rear) with longitudinal and lateral velocities to represent vehicle motion. The merit of using this model is that it can handle both longitudinal and lateral motion while maintaining vehicle orientation through wheel base geometry.</p> <p></p> <ul> <li>state variable<ul> <li>rear wheel position( \\(x_1=x-l_r\\cos\\psi\\), \\(y_1=y-l\\sin\\psi\\) ), front wheel position( \\(x_2=x+l_f\\cos\\psi\\), \\(y_2=y+l_f\\sin\\psi\\) ), longitudinal velocity( \\(v_{long} = v\\cos\\beta\\) ), and lateral velocity of the front wheel ( \\(v_{lat}=\\frac{l_r+l_f}{l_r}v\\sin\\beta\\) )</li> <li>\\([x_{1k}, y_{1k}, x_{2k}, y_{2k}, v_{long,k}, v_{lat,k} ]^\\mathrm{T}\\)</li> </ul> </li> <li>Prediction Equation<ul> <li>\\(dt\\): sampling time</li> <li>\\(l_{wheelbase} = l_r + l_f = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}\\) : distance between front and rear wheels</li> <li>\\(\\cos\\psi = \\frac{x_2 - x_1}{l_{wheelbase}}\\), \\(\\sin\\psi = \\frac{y_2 - y_1}{l_{wheelbase}}\\) : vehicle orientation components</li> <li>\\(\\gamma = \\frac{\\ln(2)}{t_{halflife}}\\) : decay constant for lateral velocity</li> </ul> </li> </ul> \\[ \\begin{aligned} x_{1,k+1} &amp; = x_{1,k} + v_{long,k} \\cos\\psi \\cdot dt \\\\ y_{1,k+1} &amp; = y_{1,k} + v_{long,k} \\sin\\psi \\cdot dt \\\\ x_{2,k+1} &amp; = x_{2,k} + v_{long,k} \\cos\\psi \\cdot dt - v_{lat,k} \\sin\\psi \\cdot dt \\\\ y_{2,k+1} &amp; = y_{2,k} + v_{long,k} \\sin\\psi \\cdot dt + v_{lat,k} \\cos\\psi \\cdot dt \\\\ v_{long,k+1} &amp; = v_{long,k} \\\\ v_{lat,k+1} &amp; = v_{lat,k} \\cdot e^{-\\gamma \\cdot dt} \\end{aligned} \\] <ul> <li>Jacobian Matrix</li> </ul> \\[ A = \\begin{bmatrix} 1 - \\frac{v_{long} }{l_{wheelbase}}\\cdot dt &amp; 0 &amp; \\frac{v_{long}}{l_{wheelbase}}\\cdot dt &amp; 0 &amp; \\cos\\psi \\cdot dt &amp; 0 \\\\ 0 &amp; 1 - \\frac{v_{long}}{l_{wheelbase}}\\cdot dt &amp; 0 &amp; \\frac{v_{long}}{l_{wheelbase}}\\cdot dt &amp; \\sin\\psi \\cdot dt &amp; 0 \\\\ -\\frac{v_{long}}{l_{wheelbase}}\\cdot dt &amp; \\frac{v_{lat}}{l_{wheelbase}}\\cdot dt &amp; 1 + \\frac{v_{long}}{l_{wheelbase}}\\cdot dt &amp; -\\frac{v_{lat}}{l_{wheelbase}}\\cdot dt &amp; \\cos\\psi \\cdot dt &amp; -\\sin\\psi \\cdot dt \\\\ -\\frac{v_{lat}}{l_{wheelbase}}\\cdot dt &amp; \\frac{v_{long}}{l_{wheelbase}}\\cdot dt &amp; \\frac{v_{lat}}{l_{wheelbase}}\\cdot dt &amp; 1 + \\frac{v_{long}}{l_{wheelbase}}\\cdot dt &amp; \\sin\\psi \\cdot dt &amp; \\cos\\psi \\cdot dt \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; e^{-\\gamma \\cdot dt} \\end{bmatrix} \\]"},{"location":"perception/autoware_multi_object_tracker/models/#remarks-on-the-output","title":"remarks on the output","text":"<p>The output twist in the vehicle coordinate system is calculated from the state variables:</p> <ul> <li>Vehicle center position: \\(x = \\frac{x_1 + x_2}{2}\\), \\(y = \\frac{y_1 + y_2}{2}\\)</li> <li>Vehicle yaw: \\(\\psi = tan^{-1}{(\\frac{y_2 - y_1}{x_2 - x_1})}\\)</li> <li>Longitudinal velocity: \\(v_x = v_{long}\\)</li> <li>Lateral velocity: \\(v_y = v_{lat}\\frac{ {l_{wheelbase}}}{l_{f}}\\)</li> <li>Angular velocity: \\(\\omega_z = \\frac{v_{lat}}{l_{wheelbase}}\\)</li> </ul> <p>The lateral velocity decays exponentially over time to model the natural stabilization of vehicle slip motion.</p>"},{"location":"perception/autoware_multi_object_tracker/models/#references","title":"References","text":"<p>[1] Schubert, Robin &amp; Richter, Eric &amp; Wanielik, Gerd. (2008). Comparison and evaluation of advanced motion models for vehicle tracking. 1 - 6. 10.1109/ICIF.2008.4632283.</p> <p>[2] Kong, Jason &amp; Pfeiffer, Mark &amp; Schildbach, Georg &amp; Borrelli, Francesco. (2015). Kinematic and dynamic vehicle models for autonomous driving control design. 1094-1099. 10.1109/IVS.2015.7225830.</p>"},{"location":"perception/autoware_object_merger/","title":"object_merger","text":""},{"location":"perception/autoware_object_merger/#object_merger","title":"object_merger","text":""},{"location":"perception/autoware_object_merger/#purpose","title":"Purpose","text":"<p>object_merger is a package for merging detected objects from two methods by data association.</p>"},{"location":"perception/autoware_object_merger/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The successive shortest path algorithm is used to solve the data association problem (the minimum-cost flow problem). The cost is calculated by the distance between two objects and gate functions are applied to reset cost, s.t. the maximum distance, the maximum area and the minimum area.</p>"},{"location":"perception/autoware_object_merger/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_object_merger/#input","title":"Input","text":"Name Type Description <code>input/object0</code> <code>autoware_perception_msgs::msg::DetectedObjects</code> detection objects <code>input/object1</code> <code>autoware_perception_msgs::msg::DetectedObjects</code> detection objects"},{"location":"perception/autoware_object_merger/#output","title":"Output","text":"Name Type Description <code>output/object</code> <code>autoware_perception_msgs::msg::DetectedObjects</code> modified Objects"},{"location":"perception/autoware_object_merger/#parameters","title":"Parameters","text":"<ul> <li>object association merger</li> </ul> Name Type Description Default Range sync_queue_size integer The size of the synchronization queue. 20 N/A precision_threshold_to_judge_overlapped float The precision threshold to judge if objects are overlapped. 0.4 N/A recall_threshold_to_judge_overlapped float The recall threshold to judge if objects are overlapped. 0.5 N/A remove_overlapped_unknown_objects boolean Flag to remove overlapped unknown objects. True N/A base_link_frame_id string The frame ID of the association frame. base_link N/A priority_mode integer Index for the priority_mode. 3 [0, 1, 2, 3] class_based_priority_matrix array Class based priority matrix for each object class. [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0] N/A message_timeout_sec float The warning timeout threshold for no synchronized messages received. 1.0 N/A initialization_timeout_sec float The warning timeout threshold for no synchronized messages received since startup. 10.0 N/A <ul> <li>data association matrix</li> </ul> Name Type Description Default Range can_assign_matrix array Assignment table for data association. [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1] N/A max_dist_matrix array Maximum distance table for data association. [4.0, 4.0, 5.0, 5.0, 5.0, 2.0, 2.0, 2.0, 4.0, 2.0, 5.0, 5.0, 5.0, 1.0, 1.0, 1.0, 5.0, 5.0, 5.0, 5.0, 5.0, 1.0, 1.0, 1.0, 5.0, 5.0, 5.0, 5.0, 5.0, 1.0, 1.0, 1.0, 5.0, 5.0, 5.0, 5.0, 5.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, 2.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 3.0, 2.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 2.0] N/A max_rad_matrix array Maximum angle table for data association. If value is greater than pi, it will be ignored. [3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 1.047, 1.047, 1.047, 1.047, 3.15, 3.15, 3.15, 3.15, 1.047, 1.047, 1.047, 1.047, 3.15, 3.15, 3.15, 3.15, 1.047, 1.047, 1.047, 1.047, 3.15, 3.15, 3.15, 3.15, 1.047, 1.047, 1.047, 1.047, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15, 3.15] N/A min_iou_matrix array Minimum IoU threshold matrix for data association. If value is negative, it will be ignored. [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.3, 0.2, 0.2, 0.2, 0.1, 0.1, 0.1, 0.1, 0.2, 0.3, 0.3, 0.3, 0.1, 0.1, 0.1, 0.1, 0.2, 0.3, 0.3, 0.3, 0.1, 0.1, 0.1, 0.1, 0.2, 0.3, 0.3, 0.3, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1] N/A <ul> <li>overlapped judge</li> </ul> Name Type Description Default Range distance_threshold_list array Distance threshold for each class used in judging overlap. [9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0] N/A generalized_iou_threshold array Generalized IoU threshold for each class. [-0.1, -0.1, -0.1, -0.6, -0.6, -0.1, -0.1, -0.1] N/A"},{"location":"perception/autoware_object_merger/#tips","title":"Tips","text":"<ul> <li>False Positive Unknown object detected by clustering method sometimes raises the risk of sudden stop and interferes with Planning module. If ML based detector rarely misses objects, you can tune the parameter of object_merger and make Perception module ignore unknown objects.<ul> <li>If you want to remove unknown object close to large vehicle,<ul> <li>use HIGH <code>distance_threshold_list</code><ul> <li>However, this causes high computational load</li> </ul> </li> <li>use LOW <code>precision_threshold_to_judge_overlapped</code></li> <li>use LOW <code>generalized_iou_threshold</code><ul> <li>However, these 2 params raise the risk of overlooking object close to known object.</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"perception/autoware_object_merger/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"perception/autoware_object_merger/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/autoware_object_merger/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/autoware_object_merger/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"perception/autoware_object_merger/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":"<p>Data association algorithm was the same as that of multi_object_tracker, but the algorithm of multi_object_tracker was already updated.</p>"},{"location":"perception/autoware_object_range_splitter/","title":"autoware_object_range_splitter","text":""},{"location":"perception/autoware_object_range_splitter/#autoware_object_range_splitter","title":"<code>autoware_object_range_splitter</code>","text":""},{"location":"perception/autoware_object_range_splitter/#purpose","title":"Purpose","text":"<p>autoware_object_range_splitter is a package to divide detected objects into two messages by the distance from the origin.</p>"},{"location":"perception/autoware_object_range_splitter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"perception/autoware_object_range_splitter/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_object_range_splitter/#input","title":"Input","text":"Name Type Description <code>input/object</code> <code>autoware_perception_msgs::msg::DetectedObjects</code> detected objects"},{"location":"perception/autoware_object_range_splitter/#output","title":"Output","text":"Name Type Description <code>output/long_range_object</code> <code>autoware_perception_msgs::msg::DetectedObjects</code> long range detected objects <code>output/short_range_object</code> <code>autoware_perception_msgs::msg::DetectedObjects</code> short range detected objects"},{"location":"perception/autoware_object_range_splitter/#parameters","title":"Parameters","text":"Name Type Description Default Range split_range float object_range_splitter is a package to divide detected objects into two messages by the distance from the origin 30 N/A"},{"location":"perception/autoware_object_range_splitter/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"perception/autoware_object_range_splitter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/autoware_object_range_splitter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/autoware_object_range_splitter/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"perception/autoware_object_range_splitter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/autoware_object_sorter/","title":"autoware_object_sorter","text":""},{"location":"perception/autoware_object_sorter/#autoware_object_sorter","title":"autoware_object_sorter","text":"<p>This package contains a object filter module for autoware_perception_msgs/msg/DetectedObject and autoware_perception_msgs/msg/TrackedObject.</p> <p>This package can filter the objects based on range and velocity.</p>"},{"location":"perception/autoware_object_sorter/#interface","title":"Interface","text":""},{"location":"perception/autoware_object_sorter/#input","title":"Input","text":"<ul> <li><code>~/input/objects</code> (<code>autoware_perception_msgs/msg/DetectedObjects.msg</code> or <code>autoware_perception_msgs/msg/TrackedObjects.msg</code>)<ul> <li>3D detected objects</li> </ul> </li> </ul>"},{"location":"perception/autoware_object_sorter/#output","title":"Output","text":"<ul> <li><code>~/output/objects</code> (<code>autoware_perception_msgs/msg/DetectedObjects.msg</code> or <code>autoware_perception_msgs/msg/TrackedObjects.msg</code>)</li> </ul>"},{"location":"perception/autoware_object_sorter/#parameters","title":"Parameters","text":"Name Type Description Default Range range_calc_frame_id string Target frame of the calculations. base_link N/A range_calc_offset.x float An offset value for the x-axis in the target frame when calculating the range [m]. 0.0 N/A range_calc_offset.y float An offset value for the y-axis in the target frame when calculating the range [m]. 0.0 N/A UNKNOWN.publish boolean If true, objects with the 'UNKNOWN' label will be published. True N/A UNKNOWN.min_velocity_threshold float Minimum velocity threshold for retaining objects [m/s]. 0.0 \u22650.0 range_threshold.max_distance float Maximum distance threshold for retaining objects [m]. 100.0 N/A range_threshold.min_distance float Minimum distance threshold for retaining objects [m]. 30.0 N/A range_threshold.max_x float Maximum x-axis distance threshold for retaining objects [m]. 100.0 N/A range_threshold.min_x float Minimum x-axis distance threshold for retaining objects [m]. -100.0 N/A range_threshold.max_y float Maximum y-axis distance threshold for retaining objects [m]. 100.0 N/A range_threshold.min_y float Minimum y-axis distance threshold for retaining objects [m]. -100.0 N/A CAR.publish boolean If true, objects with the 'CAR' label will be published. True N/A CAR.min_velocity_threshold float Minimum velocity threshold for retaining objects [m/s]. 0.0 \u22650.0 range_threshold.max_distance float Maximum distance threshold for retaining objects [m]. 100.0 N/A range_threshold.min_distance float Minimum distance threshold for retaining objects [m]. 30.0 N/A range_threshold.max_x float Maximum x-axis distance threshold for retaining objects [m]. 100.0 N/A range_threshold.min_x float Minimum x-axis distance threshold for retaining objects [m]. -100.0 N/A range_threshold.max_y float Maximum y-axis distance threshold for retaining objects [m]. 100.0 N/A range_threshold.min_y float Minimum y-axis distance threshold for retaining objects [m]. -100.0 N/A TRUCK.publish boolean If true, objects with the 'TRUCK' label will be published. True N/A TRUCK.min_velocity_threshold float Minimum velocity threshold for retaining objects [m/s]. 0.0 \u22650.0 range_threshold.max_distance float Maximum distance threshold for retaining objects [m]. 100.0 N/A range_threshold.min_distance float Minimum distance threshold for retaining objects [m]. 30.0 N/A range_threshold.max_x float Maximum x-axis distance threshold for retaining objects [m]. 100.0 N/A range_threshold.min_x float Minimum x-axis distance threshold for retaining objects [m]. -100.0 N/A range_threshold.max_y float Maximum y-axis distance threshold for retaining objects [m]. 100.0 N/A range_threshold.min_y float Minimum y-axis distance threshold for retaining objects [m]. -100.0 N/A BUS.publish boolean If true, objects with the 'BUS' label will be published. True N/A BUS.min_velocity_threshold float Minimum velocity threshold for retaining objects [m/s]. 0.0 \u22650.0 range_threshold.max_distance float Maximum distance threshold for retaining objects [m]. 100.0 N/A range_threshold.min_distance float Minimum distance threshold for retaining objects [m]. 30.0 N/A range_threshold.max_x float Maximum x-axis distance threshold for retaining objects [m]. 100.0 N/A range_threshold.min_x float Minimum x-axis distance threshold for retaining objects [m]. -100.0 N/A range_threshold.max_y float Maximum y-axis distance threshold for retaining objects [m]. 100.0 N/A range_threshold.min_y float Minimum y-axis distance threshold for retaining objects [m]. -100.0 N/A TRAILER.publish boolean If true, objects with the 'TRAILER' label will be published. True N/A TRAILER.min_velocity_threshold float Minimum velocity threshold for retaining objects [m/s]. 0.0 \u22650.0 range_threshold.max_distance float Maximum distance threshold for retaining objects [m]. 100.0 N/A range_threshold.min_distance float Minimum distance threshold for retaining objects [m]. 30.0 N/A range_threshold.max_x float Maximum x-axis distance threshold for retaining objects [m]. 100.0 N/A range_threshold.min_x float Minimum x-axis distance threshold for retaining objects [m]. -100.0 N/A range_threshold.max_y float Maximum y-axis distance threshold for retaining objects [m]. 100.0 N/A range_threshold.min_y float Minimum y-axis distance threshold for retaining objects [m]. -100.0 N/A MOTORCYCLE.publish boolean If true, objects with the 'MOTORCYCLE' label will be published. True N/A MOTORCYCLE.min_velocity_threshold float Minimum velocity threshold for retaining objects [m/s]. 0.0 \u22650.0 range_threshold.max_distance float Maximum distance threshold for retaining objects [m]. 100.0 N/A range_threshold.min_distance float Minimum distance threshold for retaining objects [m]. 30.0 N/A range_threshold.max_x float Maximum x-axis distance threshold for retaining objects [m]. 100.0 N/A range_threshold.min_x float Minimum x-axis distance threshold for retaining objects [m]. -100.0 N/A range_threshold.max_y float Maximum y-axis distance threshold for retaining objects [m]. 100.0 N/A range_threshold.min_y float Minimum y-axis distance threshold for retaining objects [m]. -100.0 N/A BICYCLE.publish boolean If true, objects with the 'BICYCLE' label will be published. True N/A BICYCLE.min_velocity_threshold float Minimum velocity threshold for retaining objects [m/s]. 0.0 \u22650.0 range_threshold.max_distance float Maximum distance threshold for retaining objects [m]. 100.0 N/A range_threshold.min_distance float Minimum distance threshold for retaining objects [m]. 30.0 N/A range_threshold.max_x float Maximum x-axis distance threshold for retaining objects [m]. 100.0 N/A range_threshold.min_x float Minimum x-axis distance threshold for retaining objects [m]. -100.0 N/A range_threshold.max_y float Maximum y-axis distance threshold for retaining objects [m]. 100.0 N/A range_threshold.min_y float Minimum y-axis distance threshold for retaining objects [m]. -100.0 N/A PEDESTRIAN.publish boolean If true, objects with the 'PEDESTRIAN' label will be published. True N/A PEDESTRIAN.min_velocity_threshold float Minimum velocity threshold for retaining objects [m/s]. 0.0 \u22650.0 range_threshold.max_distance float Maximum distance threshold for retaining objects [m]. 100.0 N/A range_threshold.min_distance float Minimum distance threshold for retaining objects [m]. 30.0 N/A range_threshold.max_x float Maximum x-axis distance threshold for retaining objects [m]. 100.0 N/A range_threshold.min_x float Minimum x-axis distance threshold for retaining objects [m]. -100.0 N/A range_threshold.max_y float Maximum y-axis distance threshold for retaining objects [m]. 100.0 N/A range_threshold.min_y float Minimum y-axis distance threshold for retaining objects [m]. -100.0 N/A"},{"location":"perception/autoware_object_velocity_splitter/","title":"autoware_object_velocity_splitter","text":""},{"location":"perception/autoware_object_velocity_splitter/#autoware_object_velocity_splitter","title":"autoware_object_velocity_splitter","text":"<p>This package contains a object filter module for autoware_perception_msgs/msg/DetectedObject. This package can split DetectedObjects into two messages by object's speed.</p>"},{"location":"perception/autoware_object_velocity_splitter/#interface","title":"Interface","text":""},{"location":"perception/autoware_object_velocity_splitter/#input","title":"Input","text":"<ul> <li><code>~/input/objects</code> (<code>autoware_perception_msgs/msg/DetectedObjects.msg</code>)<ul> <li>3D detected objects</li> </ul> </li> </ul>"},{"location":"perception/autoware_object_velocity_splitter/#output","title":"Output","text":"<ul> <li><code>~/output/low_speed_objects</code> (<code>autoware_perception_msgs/msg/DetectedObjects.msg</code>)<ul> <li>Objects with low speed</li> </ul> </li> <li><code>~/output/high_speed_objects</code> (<code>autoware_perception_msgs/msg/DetectedObjects.msg</code>)<ul> <li>Objects with high speed</li> </ul> </li> </ul>"},{"location":"perception/autoware_object_velocity_splitter/#parameters","title":"Parameters","text":"Name Type Description Default Range velocity_threshold float Velocity threshold (in m/s) used to split objects. 3 N/A"},{"location":"perception/autoware_occupancy_grid_map_outlier_filter/","title":"autoware_occupancy_grid_map_outlier_filter","text":""},{"location":"perception/autoware_occupancy_grid_map_outlier_filter/#autoware_occupancy_grid_map_outlier_filter","title":"autoware_occupancy_grid_map_outlier_filter","text":""},{"location":"perception/autoware_occupancy_grid_map_outlier_filter/#purpose","title":"Purpose","text":"<p>This node is an outlier filter based on a occupancy grid map. Depending on the implementation of occupancy grid map, it can be called an outlier filter in time series, since the occupancy grid map expresses the occupancy probabilities in time series.</p>"},{"location":"perception/autoware_occupancy_grid_map_outlier_filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<ol> <li> <p>Use the occupancy grid map to separate point clouds into those with low occupancy probability and those with high occupancy probability.</p> </li> <li> <p>The point clouds that belong to the low occupancy probability are not necessarily outliers. In particular, the top of the moving object tends to belong to the low occupancy probability. Therefore, if <code>use_radius_search_2d_filter</code> is true, then apply an radius search 2d outlier filter to the point cloud that is determined to have a low occupancy probability.</p> <ol> <li>For each low occupancy probability point, determine the outlier from the radius (<code>radius_search_2d_filter/search_radius</code>) and the number of point clouds. In this case, the point cloud to be referenced is not only low occupancy probability points, but all point cloud including high occupancy probability points.</li> <li>The number of point clouds can be multiplied by <code>radius_search_2d_filter/min_points_and_distance_ratio</code> and distance from base link. However, the minimum and maximum number of point clouds is limited.</li> </ol> </li> </ol> <p>The following video is a sample. Yellow points are high occupancy probability, green points are low occupancy probability which is not an outlier, and red points are outliers. At around 0:15 and 1:16 in the first video, a bird crosses the road, but it is considered as an outlier.</p> <ul> <li>movie1</li> <li>movie2</li> </ul> <p></p>"},{"location":"perception/autoware_occupancy_grid_map_outlier_filter/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_occupancy_grid_map_outlier_filter/#input","title":"Input","text":"Name Type Description <code>~/input/pointcloud</code> <code>sensor_msgs/PointCloud2</code> Obstacle point cloud with ground removed. <code>~/input/occupancy_grid_map</code> <code>nav_msgs/OccupancyGrid</code> A map in which the probability of the presence of an obstacle is occupancy probability map"},{"location":"perception/autoware_occupancy_grid_map_outlier_filter/#output","title":"Output","text":"Name Type Description <code>~/output/pointcloud</code> <code>sensor_msgs/PointCloud2</code> Point cloud with outliers removed. trajectory <code>~/output/debug/outlier/pointcloud</code> <code>sensor_msgs/PointCloud2</code> Point clouds removed as outliers. <code>~/output/debug/low_confidence/pointcloud</code> <code>sensor_msgs/PointCloud2</code> Point clouds that had a low probability of occupancy in the occupancy grid map. However, it is not considered as an outlier. <code>~/output/debug/high_confidence/pointcloud</code> <code>sensor_msgs/PointCloud2</code> Point clouds that had a high probability of occupancy in the occupancy grid map. trajectory"},{"location":"perception/autoware_occupancy_grid_map_outlier_filter/#parameters","title":"Parameters","text":"Name Type Description Default Range radius_search_2d_filter.search_radius float Radius used when calculating the density. 1.0 \u22650.0 radius_search_2d_filter.min_points_and_distance_ratio float Threshold value of the number of point clouds per radius when the distance from the base link is 1m. 400.0 \u22650.0 radius_search_2d_filter.min_points integer Minimum number of point clouds per radius. 4 \u22651 radius_search_2d_filter.max_points integer Maximum number of point clouds per radius. 70 \u22651 radius_search_2d_filter.max_filter_points_nb integer Maximum number of point clouds to be filtered. 15000 \u22651 map_frame string The frame ID for the map. map N/A base_link_frame string The frame ID for the base link. base_link N/A cost_threshold integer Cost threshold for occupancy grid map (0-100), where 100 indicates a high probability of an obstacle. 45 \u22650\u2264100 use_radius_search_2d_filter boolean Enable or disable the 2D radius search filter. True N/A enable_debugger boolean Enable or disable debugging output. False N/A publish_processing_time_detail boolean Enable or disable publishing of processing time details. False N/A"},{"location":"perception/autoware_occupancy_grid_map_outlier_filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"perception/autoware_occupancy_grid_map_outlier_filter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/autoware_occupancy_grid_map_outlier_filter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/autoware_occupancy_grid_map_outlier_filter/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"perception/autoware_occupancy_grid_map_outlier_filter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/autoware_predicted_path_postprocessor/","title":"autoware_predicted_path_postprocessor","text":""},{"location":"perception/autoware_predicted_path_postprocessor/#autoware_predicted_path_postprocessor","title":"autoware_predicted_path_postprocessor","text":""},{"location":"perception/autoware_predicted_path_postprocessor/#purpose","title":"Purpose","text":"<p>The <code>autoware_predicted_path_postprocessor</code> performs post-processing on predicted paths.</p>"},{"location":"perception/autoware_predicted_path_postprocessor/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The following processors are supported:</p> <ul> <li>RefineBySpeed<ul> <li>Refine the paths of objects based on their current speed.</li> </ul> </li> </ul>"},{"location":"perception/autoware_predicted_path_postprocessor/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_predicted_path_postprocessor/#input","title":"Input","text":"Name Type Description <code>~/input/objects</code> <code>autoware_perception_msgs::msg::PredictedObjects</code> Predicted objects <code>~/input/lanelet_map</code> <code>autoware_msgs::msg::LaneletMapBin</code> [OPTIONAL] Lanelet map"},{"location":"perception/autoware_predicted_path_postprocessor/#output","title":"Output","text":"Name Type Description <code>~/output/objects</code> <code>autoware_perception_msgs::msg::PredictedObjects</code> Processed objects"},{"location":"perception/autoware_predicted_path_postprocessor/#quick-start","title":"Quick Start","text":""},{"location":"perception/autoware_predicted_path_postprocessor/#launch-ros-2-node","title":"Launch ROS 2 Node","text":"<p>To use this package, you can launch it with the following command:</p> <pre><code>ros2 launch autoware_predicted_path_postprocessor autoware_predicted_path_postprocessor.launch.xml\n</code></pre>"},{"location":"perception/autoware_predicted_path_postprocessor/#leverage-processor-in-your-codebase","title":"Leverage Processor in Your Codebase","text":"<p>You can leverage the processor in your codebase by including the appropriate headers and using the <code>ComposableProcessor</code> class:</p> <pre><code>#include &lt;autoware/predicted_path_postprocessor/processor/composable.hpp&gt;\n#include &lt;autoware/predicted_path_postprocessor/processor/interface.hpp&gt;\n\nclass SomeNode final : public rclcpp::Node\n{\npublic:\n  explicit SomeNode(const rclcpp::NodeOptions&amp; options)\n    : Node(\"some_node\", options)\n  {\n    // Initialize your node here\n    auto processors = declare_parameter&lt;std::vector&lt;std::string&gt;&gt;(\"processors\");\n    context_ = std::make_unique&lt;autoware::predicted_path_postprocessor::processor::Context&gt;();\n    processor_ = std::make_unique&lt;autoware::predicted_path_postprocessor::processor::ComposableProcessor&gt;(this, processors);\n  }\n\nprivate:\n  void callback(const autoware_perception_msgs::msg::PredictedObjects::ConstSharedPtr &amp; msg)\n  {\n    auto objects = std::make_shared&lt;autoware_perception_msgs::msg::PredictedObjects&gt;(*msg);\n\n    // update the context with the predicted objects\n    context_-&gt;update(objects);\n\n    // process the predicted objects using the processor\n    const auto result = processor_-&gt;process(objects, context_);\n    if (result) {\n      const auto processed_objects = result.ok();\n      // do something with the processed objects\n      // ...\n    } else {\n      RCLCPP_ERROR(get_logger(), \"Failed to process predicted objects\");\n    }\n  }\n\n  std::unique_ptr&lt;autoware::predicted_path_postprocessor::processor::Context&gt; context_;\n  std::unique_ptr&lt;autoware::predicted_path_postprocessor::processor::ComposableProcessor&gt; processor_;\n};\n</code></pre>"},{"location":"perception/autoware_predicted_path_postprocessor/#how-to-add-new-processor","title":"How to Add New Processor","text":"<p>Processors in this package should follow a structured naming convention as below:</p> Class Name String Identifier Roles <code>RefineBy**</code> <code>refine_by_**</code> Modify or improve existing paths based on specific criteria <code>FilterBy**</code> <code>filter_by_**</code> Remove or exclude paths that don't meet specific criteria <p>As an example, let's see how to add a new processor by using a processor called <code>FilterBySomething</code>.</p> <ol> <li> <p>Create a new processor class that inherits from <code>ProcessorInterface</code>:</p> <pre><code>class FilterBySomething final : public ProcessorInterface\n{\n  public:\n    FilterBySomething(rclcpp::Node * node_ptr, const std::string &amp; processor_name)\n    : ProcessorInterface(processor_name)\n    {\n      // Load parameters\n      double_param_ = node_ptr-&gt;declare_parameter&lt;double&gt;(processor_name + \".double_param\");\n      string_param_ = node_ptr-&gt;declare_parameter&lt;std::string&gt;(processor_name + \".string_param\");\n    }\n\n private:\n    result_type check_context(const Context &amp; context)\n    {\n      // ...Check context if it contains the required information\n      return make_ok&lt;error_type&gt;();\n    }\n\n    result_type process(target_type &amp; target, const Context &amp; context) override\n    {\n      // ...Execute processor specific logic\n      return make_ok&lt;error_type&gt;();\n    }\n\n private:\n   double double_param_;\n   std::string string_param_;\n};\n</code></pre> </li> <li> <p>Register the new processor in <code>build_processors(...)</code> function:</p> <pre><code>std::vector&lt;ProcessorInterface::UniquePtr&gt; build_processors(rclcpp::Node * node_ptr, const std::string &amp; processor_name)\n{\n  std::vector&lt;ProcessorInterface::UniquePtr&gt; outputs;\n  for (const auto &amp; name : processor_names) {\n    if ( /* ... */) {\n      // ...\n    } else if (name == \"filter_by_something\") {\n      outputs.push_back(std::make_unique&lt;FilterBySomething&gt;(node_ptr, name));\n    }\n  }\n  return outputs;\n}\n</code></pre> </li> <li> <p>Add parameter to the <code>config/predicted_path_postprocessor.param.yaml</code>:</p> <p>The parameters must be grouped under the processor's string identifier.  The processors specified in the <code>processors</code> array are launched in runtime.</p> <pre><code>/**:\n  ros__parameters:\n    processors: [filter_by_something]\n    # --- FilterBySomething ---\n    filter_by_something:\n      double_param: 100.0\n      string_param: I'm a processor!!\n    # --- Parameters for other processors ---\n    # ...\n</code></pre> </li> </ol>"},{"location":"perception/autoware_predicted_path_postprocessor/docs/refine_by_speed/","title":"RefineBySpeed","text":""},{"location":"perception/autoware_predicted_path_postprocessor/docs/refine_by_speed/#refinebyspeed","title":"RefineBySpeed","text":""},{"location":"perception/autoware_predicted_path_postprocessor/docs/refine_by_speed/#overview","title":"Overview","text":"<p>The <code>RefineBySpeed</code> processor refines the predicted paths of objects based on their current speed. This processor is particularly useful for improving prediction accuracy of slow-moving or stationary objects by adjusting their predicted trajectories to be more consistent with their actual motion.</p>"},{"location":"perception/autoware_predicted_path_postprocessor/docs/refine_by_speed/#purpose","title":"Purpose","text":"<p>Even if objects are moving slowly or are stationary, their predicted paths generated by the perception system might still assume higher speeds or contain unrealistic trajectory shapes.</p> <p>The <code>RefineBySpeed</code> processor addresses this by:</p> <ol> <li>Speed-based filtering: Only processes objects below a configurable speed threshold</li> <li>Path refinement: Recalculates predicted waypoints based on the object's actual current speed</li> <li>Trajectory smoothing: Ensures predicted paths are physically consistent with the object's motion</li> </ol>"},{"location":"perception/autoware_predicted_path_postprocessor/docs/refine_by_speed/#algorithm","title":"Algorithm","text":"<p>The processor implements the following algorithm:</p> <pre><code>ALGORITHM: RefineBySpeed\n\nINPUT: (mutable)object, speed_threshold\n\nBEGIN\n    // Only process slow objects\n    IF object.speed &gt; speed_threshold THEN\n        RETURN\n    END IF\n\n    // Modify each predicted mode\n    FOR EACH mode IN object.predicted_paths DO\n        // Build distance array along original path\n        original_distances = [0, d1, d2, d3, ...]  // cumulative distances\n        original_positions = [p0, p1, p2, p3, ...]  // original waypoints\n        // Calculate new distances based on actual speed\n        FOR EACH i, waypoint in ENUMERATE(waypoints:=mode.path, i:=1) DO\n            new_distance = object.speed \u00d7 i \u00d7 time_step\n            // Interpolate to find a new position along original path shape\n            new_position = INTERPOLATE(original_distances, original_positions, new_distance)\n            waypoints[i].position = new_position\n            waypoints[i].orientation = AZIMUTH_BETWEEN(waypoints[i-1], waypoints[i])\n        END FOR\n    END FOR\nEND\n</code></pre>"},{"location":"perception/autoware_predicted_path_postprocessor/docs/refine_by_speed/#parameters","title":"Parameters","text":"Parameter Type Default Unit Description <code>speed_threshold</code> double 1.0 m/s Speed threshold below which path refinement is applied. Objects moving faster than this threshold are not processed. <code>interpolation</code> string \"linear\" - Interpolation method to use when finding position along original path shape. Options: \"linear\", \"spline\", \"spline_by_akima\""},{"location":"perception/autoware_predicted_path_postprocessor/docs/refine_by_speed/#use-cases","title":"Use Cases","text":""},{"location":"perception/autoware_predicted_path_postprocessor/docs/refine_by_speed/#ideal-scenarios","title":"Ideal Scenarios","text":"<ul> <li>Parking lots: Vehicles moving slowly or maneuvering</li> <li>Traffic jams: Slow-moving or stop-and-go traffic</li> <li>Pedestrian areas: Slow-moving pedestrians and cyclists</li> <li>Construction zones: Reduced speed scenarios</li> </ul>"},{"location":"perception/autoware_predicted_path_postprocessor/docs/refine_by_speed/#not-recommended-for","title":"Not Recommended For","text":"<ul> <li>Highway driving: Fast-moving objects where original predictions are likely accurate</li> <li>Emergency vehicles: Objects that may have unpredictable acceleration patterns</li> <li>Sports scenarios: Objects with rapid speed changes</li> </ul>"},{"location":"perception/autoware_predicted_path_postprocessor/docs/refine_by_speed/#implementation-notes","title":"Implementation Notes","text":""},{"location":"perception/autoware_predicted_path_postprocessor/docs/refine_by_speed/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Conditional processing: Objects above speed threshold skip all computations</li> <li>Memory efficiency: Reuses existing waypoint vectors where possible</li> <li>Numerical stability: Includes checks for zero time steps and minimum path lengths</li> </ul>"},{"location":"perception/autoware_predicted_path_postprocessor/docs/refine_by_speed/#edge-case-handling","title":"Edge Case Handling","text":"<ul> <li>Empty paths: Skips processing for paths with fewer than 2 waypoints</li> <li>Zero time step: Skips processing when <code>time_step \u2264 0</code></li> <li>Zero path length: Skips processing when total path distance is negligible (<code>\u2264 1e-6</code>)</li> <li>Boundary conditions: Clamps interpolation queries to valid path bounds</li> </ul>"},{"location":"perception/autoware_predicted_path_postprocessor/docs/refine_by_speed/#configuration-example","title":"Configuration Example","text":"<p>It should generally be placed early in the pipeline to ensure that subsequent processors work with refined, physically consistent trajectories.</p> <pre><code>/**:\n  ros__parameters:\n    processors: [\"refine_by_speed\"]\n    refine_by_speed:\n      speed_threshold: 1.0 # Process objects moving slower than 1.0 m/s\n      interpolation: \"linear\" # Interpolation method. Options: \"linear\", \"spline\", \"spline_by_akima\"\n</code></pre>"},{"location":"perception/autoware_probabilistic_occupancy_grid_map/","title":"autoware_probabilistic_occupancy_grid_map","text":""},{"location":"perception/autoware_probabilistic_occupancy_grid_map/#autoware_probabilistic_occupancy_grid_map","title":"autoware_probabilistic_occupancy_grid_map","text":""},{"location":"perception/autoware_probabilistic_occupancy_grid_map/#purpose","title":"Purpose","text":"<p>This package outputs the probability of having an obstacle as occupancy grid map. </p>"},{"location":"perception/autoware_probabilistic_occupancy_grid_map/#referencesexternal-links","title":"References/External links","text":"<ul> <li>Pointcloud based occupancy grid map</li> <li>Laserscan based occupancy grid map</li> <li>Grid map fusion</li> </ul>"},{"location":"perception/autoware_probabilistic_occupancy_grid_map/#settings","title":"Settings","text":"<p>Occupancy grid map is generated on <code>map_frame</code>, and grid orientation is fixed.</p> <p>You may need to choose <code>scan_origin_frame</code> and <code>gridmap_origin_frame</code> which means sensor origin and gridmap origin respectively. Especially, set your main LiDAR sensor frame (e.g. <code>velodyne_top</code> in sample_vehicle) as a <code>scan_origin_frame</code> would result in better performance.</p> <p></p>"},{"location":"perception/autoware_probabilistic_occupancy_grid_map/#parameters","title":"Parameters","text":"<ul> <li> <p>binary bayes filter updater</p> Name Type Description Default Range probability_matrix.occupied_to_occupied float Probability of transitioning from occupied to occupied state. 0.95 N/A probability_matrix.occupied_to_free float Probability of transitioning from occupied to free state. 0.05 N/A probability_matrix.free_to_occupied float Probability of transitioning from free to occupied state. 0.2 N/A probability_matrix.free_to_free float Probability of transitioning from free to free state. 0.8 N/A v_ratio float Ratio of the variance used in the filter. 0.1 N/A </li> </ul> <ul> <li> <p>grid map</p> Name Type Description Default Range type string The type of grid map visualization. occupancy_grid N/A params.layer string The layer of the grid map visualization. filled_free_to_farthest N/A params.data_min float The minimum data value for the visualization. 0.0 N/A params.data_max float The maximum data value for the visualization. 100.0 N/A </li> </ul> <ul> <li> <p>laserscan based occupancy grid map</p> Name Type Description Default Range use_height_filter boolean Flag to use height filter. 1 N/A min_height float Minimum height for the height filter. -1 N/A max_height float Maximum height for the height filter. 2 N/A </li> </ul> <ul> <li> <p>multi lidar pointcloud based occupancy grid map</p> Name Type Description Default Range use_height_filter boolean Flag to use height filter. 1 N/A min_height float Minimum height for the height filter. -1 N/A max_height float Maximum height for the height filter. 2 N/A </li> </ul> <ul> <li> <p>pointcloud based occupancy grid map</p> Name Type Description Default Range use_height_filter boolean Flag to use height filter. 1 N/A min_height float Minimum height for the height filter. -1 N/A max_height float Maximum height for the height filter. 2 N/A </li> </ul> <ul> <li> <p>synchronized grid map fusion</p> Name Type Description Default Range fusion_input_ogm_topics array List of fusion input occupancy grid map topics. ['topic1', 'topic2'] N/A input_ogm_reliabilities array Reliability of each sensor for fusion. [0.8, 0.2] N/A fusion_method string Method for occupancy grid map fusion. overwrite ['overwrite', 'log-odds', 'dempster-shafer'] match_threshold_sec float Time threshold for matching in seconds. 0.01 N/A timeout_sec float Timeout for synchronization in seconds. 0.1 N/A input_offset_sec array Offset for each input in seconds. [0.0, 0.0] N/A map_frame_ string The frame ID of the map. map N/A base_link_frame_ string The frame ID of the base link. base_link N/A grid_map_origin_frame_ string The frame ID of the grid map origin. base_link N/A fusion_map_length_x float The length of the fusion map in the x direction. 100.0 N/A fusion_map_length_y float The length of the fusion map in the y direction. 100.0 N/A fusion_map_resolution float The resolution of the fusion map. 0.5 N/A publish_processing_time_detail boolean True for showing detail of publish processing time. False N/A </li> </ul>"},{"location":"perception/autoware_probabilistic_occupancy_grid_map/#downsample-input-pointcloudoptional","title":"Downsample input pointcloud(Optional)","text":"<p>If you set <code>downsample_input_pointcloud</code> to <code>true</code>, the input pointcloud will be downsampled and following topics are also used. This feature is currently only for the pointcloud based occupancy grid map.</p> <ul> <li>pointcloud_based_occupancy_grid_map method</li> </ul> <pre><code># downsampled raw and obstacle pointcloud\n/perception/occupancy_grid_map/obstacle/downsample/pointcloud\n/perception/occupancy_grid_map/raw/downsample/pointcloud\n</code></pre> <ul> <li>multi_lidar_pointcloud_based_point_cloud</li> </ul> <pre><code># downsampled raw and obstacle pointcloud\n/perception/occupancy_grid_map/obstacle/downsample/pointcloud\n/perception/occupancy_grid_map/&lt;sensor_name&gt;/raw/downsample/pointcloud\n</code></pre>"},{"location":"perception/autoware_probabilistic_occupancy_grid_map/#test","title":"Test","text":"<p>This package provides unit tests using <code>gtest</code>. You can run the test by the following command.</p> <pre><code>colcon test --packages-select autoware_probabilistic_occupancy_grid_map --event-handlers console_direct+\n</code></pre> <p>Test contains the following.</p> <ul> <li>Unit test for cost value conversion function</li> <li>Unit test for utility functions</li> <li>Unit test for occupancy grid map fusion functions</li> <li>Input/Output test for pointcloud based occupancy grid map</li> </ul>"},{"location":"perception/autoware_probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/","title":"laserscan based occupancy grid map","text":""},{"location":"perception/autoware_probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#laserscan-based-occupancy-grid-map","title":"laserscan based occupancy grid map","text":""},{"location":"perception/autoware_probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The basic idea is to take a 2D laserscan and ray trace it to create a time-series processed occupancy grid map.</p> <ol> <li>the node take a laserscan and make an occupancy grid map with one frame. ray trace is done by Bresenham's line algorithm.    </li> <li> <p>Optionally, obstacle point clouds and raw point clouds can be received and reflected in the occupancy grid map. The reason is that laserscan only uses the most foreground point in the polar coordinate system, so it throws away a lot of information. As a result, the occupancy grid map is almost an UNKNOWN cell.    Therefore, the obstacle point cloud and the raw point cloud are used to reflect what is judged to be the ground and what is judged to be an obstacle in the occupancy grid map. The black and red dots represent raw point clouds, and the red dots represent obstacle point clouds. In other words, the black points are determined as the ground, and the red point cloud is the points determined as obstacles. The gray cells are represented as UNKNOWN cells.    </p> </li> <li> <p>Using the previous occupancy grid map, update the existence probability using a binary Bayesian filter (1). Also, the unobserved cells are time-decayed like the system noise of the Kalman filter (2).</p> </li> </ol> \\[ \\hat{P_{o}} = \\frac{(P_{o} *P_{z})}{(P_{o}* P_{z} + (1 - P_{o}) * \\bar{P_{z}})} \\tag{1} \\] \\[ \\hat{P_{o}} = \\frac{(P_{o} + 0.5 * \\frac{1}{ratio})}{(\\frac{1}{ratio} + 1)} \\tag{2} \\]"},{"location":"perception/autoware_probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#input","title":"Input","text":"Name Type Description <code>~/input/laserscan</code> <code>sensor_msgs::LaserScan</code> laserscan <code>~/input/obstacle_pointcloud</code> <code>sensor_msgs::PointCloud2</code> obstacle pointcloud <code>~/input/raw_pointcloud</code> <code>sensor_msgs::PointCloud2</code> The overall point cloud used to input the obstacle point cloud"},{"location":"perception/autoware_probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#output","title":"Output","text":"Name Type Description <code>~/output/occupancy_grid_map</code> <code>nav_msgs::OccupancyGrid</code> occupancy grid map"},{"location":"perception/autoware_probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#parameters","title":"Parameters","text":""},{"location":"perception/autoware_probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#node-parameters","title":"Node Parameters","text":"Name Type Description <code>map_frame</code> string map frame <code>base_link_frame</code> string base_link frame <code>input_obstacle_pointcloud</code> bool whether to use the optional obstacle point cloud? If this is true, <code>~/input/obstacle_pointcloud</code> topics will be received. <code>input_obstacle_and_raw_pointcloud</code> bool whether to use the optional obstacle and raw point cloud? If this is true, <code>~/input/obstacle_pointcloud</code> and <code>~/input/raw_pointcloud</code> topics will be received. <code>use_height_filter</code> bool whether to height filter for <code>~/input/obstacle_pointcloud</code> and <code>~/input/raw_pointcloud</code>? By default, the height is set to -1~2m. <code>map_length</code> double The length of the map. -100 if it is 50~50[m] <code>map_resolution</code> double The map cell resolution [m]"},{"location":"perception/autoware_probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>In several places we have modified the external code written in BSD3 license.</p> <ul> <li>occupancy_grid_map.hpp</li> <li>cost_value.hpp</li> <li>occupancy_grid_map.cpp</li> </ul>"},{"location":"perception/autoware_probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/autoware_probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/autoware_probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#optional-referencesexternal-links","title":"(Optional) References/External links","text":"<p>Bresenham's_line_algorithm</p> <ul> <li>https://en.wikipedia.org/wiki/Bresenham%27s_line_algorithm</li> <li>https://ieeexplore.ieee.org/document/5388473</li> </ul>"},{"location":"perception/autoware_probabilistic_occupancy_grid_map/laserscan-based-occupancy-grid-map/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":"<ul> <li>The update probability of the binary Bayesian filter is currently hard-coded and requires a code change to be modified.</li> <li>Since there is no special support for moving objects, the probability of existence is not increased for fast objects.</li> </ul>"},{"location":"perception/autoware_probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/","title":"pointcloud based occupancy grid map","text":""},{"location":"perception/autoware_probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#pointcloud-based-occupancy-grid-map","title":"pointcloud based occupancy grid map","text":""},{"location":"perception/autoware_probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"perception/autoware_probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#1st-step","title":"1st step","text":"<p>First of all, input obstacle/raw pointcloud are transformed into the polar coordinate centered around <code>scan_origin</code> and divided int circular bins per angle_increment respectively. At this time, each point belonging to each bin is stored as range data. In addition, the x,y information in the map coordinate is also stored for ray-tracing on the map coordinate. The bin contains the following information for each point</p> <ul> <li>range data from origin of raytrace</li> <li>x on map coordinate</li> <li>y on map coordinate</li> </ul> <p></p> <p>The following figure shows each of the bins from side view. </p>"},{"location":"perception/autoware_probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#2nd-step","title":"2nd step","text":"<p>The ray trace is performed in three steps for each cell. The ray trace is done by Bresenham's line algorithm. </p> <ol> <li> <p>Initialize freespace to the farthest point of each bin.</p> <p></p> </li> <li> <p>Fill in the unknown cells.    Based on the assumption that <code>UNKNOWN</code> is behind the obstacle, the cells that are more than a distance margin from each obstacle point are filled with <code>UNKNOWN</code></p> <p></p> <p>There are three reasons for setting a distance margin.  - It is unlikely that a point on the ground will be immediately behind an obstacle.  - The obstacle point cloud is processed and may not match the raw pointcloud.  - The input may be inaccurate and obstacle points may not be determined as obstacles.</p> <p>When the parameter <code>grid_map_type</code> is \"OccupancyGridMapProjectiveBlindSpot\" and the <code>scan_origin</code> is a sensor frame like <code>velodyne_top</code> for instance, for each obstacle pointcloud, if there are no visible raw pointclouds that are located above the projected ray from the <code>scan_origin</code> to that obstacle pointcloud, the cells between the obstacle pointcloud and the <code>projected point</code> are filled with <code>UNKNOWN</code>. Note that the <code>scan_origin</code> should not be <code>base_link</code> if this flag is true because otherwise all the cells behind the obstacle point clouds would be filled with <code>UNKNOWN</code>.</p> <p></p> </li> <li> <p>Fill in the occupied cells.    Fill in the point where the obstacle point is located with occupied.    In addition, If the distance between obstacle points is less than or equal to the distance margin, that interval is filled with <code>OCCUPIED</code> because the input may be inaccurate and obstacle points may not be determined as obstacles.</p> <p></p> </li> </ol>"},{"location":"perception/autoware_probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#3rd-step","title":"3rd step","text":"<p>Using the previous occupancy grid map, update the existence probability using a binary Bayesian filter (1). Also, the unobserved cells are time-decayed like the system noise of the Kalman filter (2).</p> \\[ \\hat{P_{o}} = \\frac{(P_{o} *P_{z})}{(P_{o}* P_{z} + (1 - P_{o}) * \\bar{P_{z}})} \\tag{1} \\] \\[ \\hat{P_{o}} = \\frac{(P_{o} + 0.5 * \\frac{1}{ratio})}{(\\frac{1}{ratio} + 1)} \\tag{2} \\]"},{"location":"perception/autoware_probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#input","title":"Input","text":"Name Type Description <code>~/input/obstacle_pointcloud</code> <code>sensor_msgs::PointCloud2</code> obstacle pointcloud <code>~/input/raw_pointcloud</code> <code>sensor_msgs::PointCloud2</code> The overall point cloud used to input the obstacle point cloud"},{"location":"perception/autoware_probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#output","title":"Output","text":"Name Type Description <code>~/output/occupancy_grid_map</code> <code>nav_msgs::OccupancyGrid</code> occupancy grid map"},{"location":"perception/autoware_probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#related-topics","title":"Related topics","text":"<p>If you set <code>downsample_input_pointcloud</code> to <code>true</code>, the input pointcloud will be downsampled and following topics are also used.</p> <ul> <li>pointcloud_based_occupancy_grid_map method</li> </ul> <pre><code># downsampled raw and obstacle pointcloud\n/perception/occupancy_grid_map/obstacle/downsample/pointcloud\n/perception/occupancy_grid_map/raw/downsample/pointcloud\n</code></pre>"},{"location":"perception/autoware_probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#parameters","title":"Parameters","text":""},{"location":"perception/autoware_probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#node-parameters","title":"Node Parameters","text":"Name Type Description <code>map_frame</code> string map frame <code>base_link_frame</code> string base_link frame <code>use_height_filter</code> bool whether to height filter for <code>~/input/obstacle_pointcloud</code> and <code>~/input/raw_pointcloud</code>? By default, the height is set to -1~2m. <code>map_length</code> double The length of the map. -100 if it is 50~50[m] <code>map_resolution</code> double The map cell resolution [m] <code>grid_map_type</code> string The type of grid map for estimating <code>UNKNOWN</code> region behind obstacle point clouds <code>scan_origin</code> string The origin of the scan. It should be a sensor frame. <code>pub_debug_grid</code> bool Whether to publish debug grid maps <code>downsample_input_pointcloud</code> bool Whether to downsample the input pointclouds. The downsampled pointclouds are used for the ray tracing. <code>downsample_voxel_size</code> double The voxel size for the downsampled pointclouds."},{"location":"perception/autoware_probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>In several places we have modified the external code written in BSD3 license.</p> <ul> <li>occupancy_grid_map.hpp</li> <li>cost_value.hpp</li> <li>occupancy_grid_map.cpp</li> </ul>"},{"location":"perception/autoware_probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/autoware_probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/autoware_probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"perception/autoware_probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":"<ul> <li>The update probability of the binary Bayesian filter is currently hard-coded and requires a code change to be modified.</li> <li>Since there is no special support for moving objects, the probability of existence is not increased for fast objects.</li> </ul>"},{"location":"perception/autoware_probabilistic_occupancy_grid_map/pointcloud-based-occupancy-grid-map/#how-to-debug","title":"How to debug","text":"<p>If <code>grid_map_type</code> is \"OccupancyGridMapProjectiveBlindSpot\" and <code>pub_debug_grid</code> is <code>true</code>, it is possible to check the each process of grid map generation by running</p> <pre><code>ros2 launch autoware_probabilistic_occupancy_grid_map debug.launch.xml\n</code></pre> <p>and visualizing the following occupancy grid map topics (which are listed in config/grid_map_param.yaml):</p> <ul> <li><code>/perception/occupancy_grid_map/grid_1st_step</code>: <code>FREE</code> cells are filled</li> <li><code>/perception/occupancy_grid_map/grid_2nd_step</code>: <code>UNKNOWN</code> cells are filled</li> <li><code>/perception/occupancy_grid_map/grid_3rd_step</code>: <code>OCCUPIED</code> cells are filled</li> </ul>"},{"location":"perception/autoware_probabilistic_occupancy_grid_map/synchronized_grid_map_fusion/","title":"synchronized OGM fusion","text":""},{"location":"perception/autoware_probabilistic_occupancy_grid_map/synchronized_grid_map_fusion/#synchronized-ogm-fusion","title":"synchronized OGM fusion","text":"<p>For simplicity, we use OGM as the meaning of the occupancy grid map.</p> <p>This package is used to fuse the OGMs from synchronized sensors. Especially for the lidar.</p> <p>Here shows the example OGM for the this synchronized OGM fusion.</p> left lidar OGM right lidar OGM top lidar OGM <p>OGM fusion with asynchronous sensor outputs is not suitable for this package. Asynchronous OGM fusion is under construction.</p>"},{"location":"perception/autoware_probabilistic_occupancy_grid_map/synchronized_grid_map_fusion/#processing-flow","title":"Processing flow","text":"<p>The processing flow of this package is shown in the following figure.</p> <p></p> <ul> <li>Single Frame Fusion<ul> <li>Single frame fusion means that the OGMs from synchronized sensors are fused in a certain time frame \\(t=t_n\\).</li> </ul> </li> <li>Multi Frame Fusion<ul> <li>In the multi frame fusion process, current fused single frame OGM in \\(t_n\\) is fused with the previous fused single frame OGM in \\(t_{n-1}\\).</li> </ul> </li> </ul>"},{"location":"perception/autoware_probabilistic_occupancy_grid_map/synchronized_grid_map_fusion/#io","title":"I/O","text":"Input topic name Type Description <code>input_ogm_topics</code> list of nav_msgs::msg::OccupancyGrid List of input topics for Occupancy Grid Maps. This parameter is given in list, so Output topic name Type Description <code>~/output/occupancy_grid_map</code> nav_msgs::msg::OccupancyGrid Output topic name of the fused Occupancy Grid Map. <code>~/debug/single_frame_map</code> nav_msgs::msg::OccupancyGrid (debug topic) Output topic name of the single frame fused Occupancy Grid Map."},{"location":"perception/autoware_probabilistic_occupancy_grid_map/synchronized_grid_map_fusion/#parameters","title":"Parameters","text":"<p>Synchronized OGM fusion node parameters are shown in the following table. Main parameters to be considered in the fusion node is shown as bold.</p> Ros param name Sample value Description input_ogm_topics [\"topic1\", \"topic2\"] List of input topics for Occupancy Grid Maps input_ogm_reliabilities [0.8, 0.2] Weights for the reliability of each input topic fusion_method \"overwrite\" Method of fusion (\"overwrite\", \"log-odds\", \"dempster-shafer\") match_threshold_sec 0.01 Matching threshold in milliseconds timeout_sec 0.1 Timeout duration in seconds input_offset_sec [0.0, 0.0] Offset time in seconds for each input topic mapframe \"map\" Frame name for the fused map baselink_frame \"base_link\" Frame name for the base link gridmap_origin_frame \"base_link\" Frame name for the origin of the grid map fusion_map_length_x 100.0 Length of the fused map along the X-axis fusion_map_length_y 100.0 Length of the fused map along the Y-axis fusion_map_resolution 0.5 Resolution of the fused map <p>Since this node assumes that the OGMs from synchronized sensors are generated in the same time, we need to tune the <code>match_threshold_sec</code>, <code>timeout_sec</code> and <code>input_offset_sec</code> parameters to successfully fuse the OGMs.</p>"},{"location":"perception/autoware_probabilistic_occupancy_grid_map/synchronized_grid_map_fusion/#fusion-methods","title":"Fusion methods","text":"<p>For the single frame fusion, the following fusion methods are supported.</p> Fusion Method in parameter Description <code>overwrite</code> The value of the cell in the fused OGM is overwritten by the value of the cell in the OGM with the highest priority.  We set priority as <code>Occupied</code> &gt; <code>Free</code> &gt; <code>Unknown</code>. <code>log-odds</code> The value of the cell in the fused OGM is calculated by the log-odds ratio method, which is known as a Bayesian fusion method.  The log-odds of a probability \\(p\\) can be written as \\(l_p = \\log(\\frac{p}{1-p})\\).  And the fused log-odds is calculated by the sum of log-odds. \\(l_f = \\Sigma l_p\\) <code>dempster-shafer</code> The value of the cell in the fused OGM is calculated by the Dempster-Shafer theory[1]. This is also popular method to handle multiple evidences. This package applied conflict escape logic in [2] for the performance. See references for the algorithm details. <p>For the multi frame fusion, currently only supporting <code>log-odds</code> fusion method.</p>"},{"location":"perception/autoware_probabilistic_occupancy_grid_map/synchronized_grid_map_fusion/#how-to-use","title":"How to use","text":""},{"location":"perception/autoware_probabilistic_occupancy_grid_map/synchronized_grid_map_fusion/#launch-fusion-node","title":"launch fusion node","text":"<p>The minimum node launch will be like the following.</p> <pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;launch&gt;\n&lt;arg name=\"output_topic\" default=\"~/output/occupancy_grid_map\"/&gt;\n&lt;arg name=\"fusion_node_param_path\" default=\"$(find-pkg-share autoware_probabilistic_occupancy_grid_map)/config/synchronized_grid_map_fusion_node.param.yaml\"/&gt;\n\n&lt;node name=\"synchronized_grid_map_fusion_node\" exec=\"synchronized_grid_map_fusion_node\" pkg=\"autoware_probabilistic_occupancy_grid_map\" output=\"screen\"&gt;\n  &lt;remap from=\"~/output/occupancy_grid_map\" to=\"$(var output_topic)\"/&gt;\n  &lt;param from=\"$(var fusion_node_param_path)\"/&gt;\n&lt;/node&gt;\n&lt;/launch&gt;\n</code></pre>"},{"location":"perception/autoware_probabilistic_occupancy_grid_map/synchronized_grid_map_fusion/#optional-generate-ogms-in-each-sensor-frame","title":"(Optional) Generate OGMs in each sensor frame","text":"<p>You need to generate OGMs in each sensor frame before achieving grid map fusion.</p> <p><code>autoware_probabilistic_occupancy_grid_map</code> package supports to generate OGMs for the each from the point cloud data.</p>  Example launch.xml (click to expand)  <pre><code>&lt;include file=\"$(find-pkg-share tier4_perception_launch)/launch/occupancy_grid_map/probabilistic_occupancy_grid_map.launch.xml\"&gt;\n    &lt;arg name=\"input/obstacle_pointcloud\" value=\"/perception/obstacle_segmentation/single_frame/pointcloud\"/&gt;\n    &lt;arg name=\"input/raw_pointcloud\" value=\"/sensing/lidar/right/outlier_filtered/pointcloud_synchronized\"/&gt;\n    &lt;arg name=\"output\" value=\"/perception/occupancy_grid_map/right_lidar/map\"/&gt;\n    &lt;arg name=\"map_frame\" value=\"base_link\"/&gt;\n    &lt;arg name=\"scan_origin\" value=\"velodyne_right\"/&gt;\n    &lt;arg name=\"use_intra_process\" value=\"true\"/&gt;\n    &lt;arg name=\"use_multithread\" value=\"true\"/&gt;\n    &lt;arg name=\"use_pointcloud_container\" value=\"$(var use_pointcloud_container)\"/&gt;\n    &lt;arg name=\"pointcloud_container_name\" value=\"$(var pointcloud_container_name)\"/&gt;\n    &lt;arg name=\"method\" value=\"pointcloud_based_occupancy_grid_map\"/&gt;\n    &lt;arg name=\"param_file\" value=\"$(find-pkg-share autoware_probabilistic_occupancy_grid_map)/config/pointcloud_based_occupancy_grid_map_fusion.param.yaml\"/&gt;\n&lt;/include&gt;\n\n\nThe minimum parameter for the OGM generation in each frame is shown in the following table.\n\n|Parameter|Description|\n|--|--|\n|`input/obstacle_pointcloud`| The input point cloud data for the OGM generation. This point cloud data should be the point cloud data which is segmented as the obstacle.|\n|`input/raw_pointcloud`| The input point cloud data for the OGM generation. This point cloud data should be the point cloud data which is not segmented as the obstacle. |\n|`output`| The output topic of the OGM. |\n|`map_frame`| The tf frame for the OGM center origin. |\n|`scan_origin`| The tf frame for the sensor origin. |\n|`method`| The method for the OGM generation. Currently we support `pointcloud_based_occupancy_grid_map` and `laser_scan_based_occupancy_grid_map`. The pointcloud based method is recommended. |\n|`param_file`| The parameter file for the OGM generation. See [example parameter file](config/pointcloud_based_occupancy_grid_map_for_fusion.param.yaml) |\n</code></pre> <p></p> <p>We recommend to use same <code>map_frame</code>, size and resolutions for the OGMs from synchronized sensors. Also, remember to set <code>enable_single_frame_mode</code> and <code>filter_obstacle_pointcloud_by_raw_pointcloud</code> to <code>true</code> in the <code>autoware_probabilistic_occupancy_grid_map</code> package (you do not need to set these parameters if you use the above example config file).</p> <p></p>"},{"location":"perception/autoware_probabilistic_occupancy_grid_map/synchronized_grid_map_fusion/#run-both-ogm-generation-node-and-fusion-node","title":"Run both OGM generation node and fusion node","text":"<p>We prepared the launch file to run both OGM generation node and fusion node in <code>grid_map_fusion_with_synchronized_pointclouds.launch.py</code></p> <p>You can include this launch file like the following.</p> <pre><code>&lt;include file=\"$(find-pkg-share autoware_probabilistic_occupancy_grid_map)/launch/grid_map_fusion_with_synchronized_pointclouds.launch.py\"&gt;\n  &lt;arg name=\"output\" value=\"/perception/occupancy_grid_map/fusion/map\"/&gt;\n  &lt;arg name=\"use_intra_process\" value=\"true\"/&gt;\n  &lt;arg name=\"use_multithread\" value=\"true\"/&gt;\n  &lt;arg name=\"use_pointcloud_container\" value=\"$(var use_pointcloud_container)\"/&gt;\n  &lt;arg name=\"pointcloud_container_name\" value=\"$(var pointcloud_container_name)\"/&gt;\n  &lt;arg name=\"method\" value=\"pointcloud_based_occupancy_grid_map\"/&gt;\n  &lt;arg name=\"fusion_config_file\" value=\"$(var fusion_config_file)\"/&gt;\n  &lt;arg name=\"ogm_config_file\" value=\"$(var ogm_config_file)\"/&gt;\n&lt;/include&gt;\n</code></pre> <p>The minimum parameter for the launch file is shown in the following table.</p> Parameter Description <code>output</code> The output topic of the finally fused OGM. <code>method</code> The method for the OGM generation. Currently we support <code>pointcloud_based_occupancy_grid_map</code> and <code>laser_scan_based_occupancy_grid_map</code>. The pointcloud based method is recommended. <code>fusion_config_file</code> The parameter file for the grid map fusion. See example parameter file <code>ogm_config_file</code> The parameter file for the OGM generation. See example parameter file"},{"location":"perception/autoware_probabilistic_occupancy_grid_map/synchronized_grid_map_fusion/#references","title":"References","text":"<ul> <li>[1] Dempster, A. P., Laird, N. M., &amp; Rubin, D. B. (1977). Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39(1), 1-38.</li> <li>[2] https://www.diva-portal.org/smash/get/diva2:852457/FULLTEXT01.pdf</li> </ul>"},{"location":"perception/autoware_ptv3/","title":"autoware_ptv3","text":""},{"location":"perception/autoware_ptv3/#autoware_ptv3","title":"autoware_ptv3","text":""},{"location":"perception/autoware_ptv3/#purpose","title":"Purpose","text":"<p>The <code>autoware_ptv3</code> package is used for 3D lidar segmentation.</p>"},{"location":"perception/autoware_ptv3/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>This package implements a TensorRT powered inference node for Point Transformers V3 (PTv3) [1]. The sparse convolution backend corresponds to spconv. Autoware installs it automatically in its setup script. If needed, the user can also build it and install it following the following instructions.</p>"},{"location":"perception/autoware_ptv3/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_ptv3/#input","title":"Input","text":"Name Type Description <code>~/input/pointcloud</code> <code>sensor_msgs::msg::PointCloud2</code> Input pointcloud topic."},{"location":"perception/autoware_ptv3/#output","title":"Output","text":"Name Type Description <code>~/output/segmented/pointcloud</code> <code>sensor_msgs::msg::PointCloud2</code> RGB segmented pointcloud. <code>~/output/ground_segmented/pointcloud</code> <code>sensor_msgs::msg::PointCloud2</code> Pointcloud with the ground segmented out. <code>~/output/probs/pointcloud</code> <code>sensor_msgs::msg::PointCloud2</code> Class probabilities segmented pointcloud. <code>debug/cyclic_time_ms</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> Cyclic time (ms). <code>debug/pipeline_latency_ms</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> Pipeline latency time (ms). <code>debug/processing_time/preprocess_ms</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> Preprocess (ms). <code>debug/processing_time/inference_ms</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> Inference time (ms). <code>debug/processing_time/postprocess_ms</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> Postprocess time (ms). <code>debug/processing_time/total_ms</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> Total processing time (ms)."},{"location":"perception/autoware_ptv3/#parameters","title":"Parameters","text":""},{"location":"perception/autoware_ptv3/#ptv3node-node","title":"PTv3Node node","text":"Name Type Description Default Range plugins_path string A path to the TensorRT plugins. $(find-pkg-share autoware_ptv3)/plugins/libautoware_tensorrt_plugins.so N/A trt_precision string A precision of TensorRT engine. fp16 ['fp16', 'fp32'] cloud_capacity integer Capacity of the point cloud buffer (should be set to at least the maximum theoretical number of points). 2000000 \u22651 onnx_path string A path to ONNX model file. $(var model_path)/ptv3.onnx N/A engine_path string A path to TensorRT engine file. $(var model_path)/ptv3.engine N/A colors_red array Red component of the class to RGB map for the segmented point cloud. Must have the same number of elements as the number of classes. [255, 0, 255, 64, 0, 255] N/A colors_green array Green component of the class to RGB map for the segmented point cloud. Must have the same number of elements as the number of classes. [0, 0, 0, 64, 255, 255] N/A colors_blue array Blue component of the class to RGB map for the segmented point cloud. Must have the same number of elements as the number of classes. [0, 255, 255, 64, 0, 255] N/A ground_prob_threshold float Probability threshold to classify a point as ground. 0.3 \u22650.0\u22641.0"},{"location":"perception/autoware_ptv3/#ptv3node-model","title":"PTv3Node model","text":"Name Type Description Default Range class_names array Predicted classes' names. ['CAR', 'TRUCK', 'BUS', 'BICYCLE', 'PEDESTRIAN'] N/A voxels_num array Voxel ranges used during inference [min, opt, max]. [5000, 30000, 60000] N/A point_cloud_range array Range in meters of the pointcloud in meters [min_x, min_y, min_z, max_x, max_y, max_z]. [-76.8, -76.8, -3.0, 76.8, 76.8, 5.0] N/A voxel_size array Voxels size [x, y, z] in meters. [0.3, 0.3, 8.0] N/A"},{"location":"perception/autoware_ptv3/#the-build_only-option","title":"The <code>build_only</code> option","text":"<p>The <code>autoware_ptv3</code> node has a <code>build_only</code> option to build the TensorRT engine file from the specified ONNX file, after which the program exits.</p> <pre><code>ros2 launch autoware_ptv3 ptv3.launch.xml build_only:=true\n</code></pre>"},{"location":"perception/autoware_ptv3/#the-log_level-option","title":"The <code>log_level</code> option","text":"<p>The default logging severity level for <code>autoware_ptv3</code> is <code>info</code>. For debugging purposes, the developer may decrease severity level using <code>log_level</code> parameter:</p> <pre><code>ros2 launch autoware_ptv3 ptv3.launch.xml log_level:=debug\n</code></pre>"},{"location":"perception/autoware_ptv3/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>This node assumes that the input pointcloud follows the <code>PointXYZIRC</code> layout defined in <code>autoware_point_types</code>.</p>"},{"location":"perception/autoware_ptv3/#trained-models","title":"Trained Models","text":"<ul> <li>v1 \u2013 First model release trained with PoC pseudo labels for the internal T4 dataset.</li> </ul>"},{"location":"perception/autoware_ptv3/#changelog","title":"Changelog","text":""},{"location":"perception/autoware_ptv3/#referencesexternal-links","title":"References/External links","text":"<p>[1] Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, and Hengshuang Zhao. \"Point Transformer V3: Simpler, Faster, Stronger.\" 2024 Conference on Computer Vision and Pattern Recognition. </p>"},{"location":"perception/autoware_radar_fusion_to_detected_object/","title":"autoware_radar_fusion_to_detected_object","text":""},{"location":"perception/autoware_radar_fusion_to_detected_object/#autoware_radar_fusion_to_detected_object","title":"<code>autoware_radar_fusion_to_detected_object</code>","text":"<p>This package contains a sensor fusion module for radar-detected objects and 3D detected objects.</p> <p>The fusion node can:</p> <ul> <li>Attach velocity to 3D detections when successfully matching radar data. The tracking modules use the velocity information to enhance the tracking results while planning modules use it to execute actions like adaptive cruise control.</li> <li>Improve the low confidence 3D detections when corresponding radar detections are found.</li> </ul> <p></p>"},{"location":"perception/autoware_radar_fusion_to_detected_object/#design","title":"Design","text":""},{"location":"perception/autoware_radar_fusion_to_detected_object/#background","title":"Background","text":"<p>This package is the fusion with LiDAR-based 3D detection output and radar data. LiDAR based 3D detection can estimate position and size of objects with high precision, but it cannot estimate velocity of objects. Radar data can estimate doppler velocity of objects, but it cannot estimate position and size of objects with high precision This fusion package is aim to fuse these characteristic data, and to estimate position, size, velocity of objects with high precision.</p>"},{"location":"perception/autoware_radar_fusion_to_detected_object/#algorithm","title":"Algorithm","text":"<p>The document of core algorithm is here</p>"},{"location":"perception/autoware_radar_fusion_to_detected_object/#interface-for-core-algorithm","title":"Interface for core algorithm","text":"<p>The parameters for core algorithm can be set as <code>core_params</code>.</p>"},{"location":"perception/autoware_radar_fusion_to_detected_object/#parameters-for-sensor-fusion","title":"Parameters for sensor fusion","text":"<ul> <li><code>bounding_box_margin</code> (double) [m]<ul> <li>Default parameter is 2.0.</li> </ul> </li> </ul> <p>This parameter is the distance to extend the 2D bird's-eye view bounding box on each side. This parameter is used as a threshold to find radar centroids falling inside the extended box.</p> <ul> <li><code>split_threshold_velocity</code> (double) [m/s]<ul> <li>Default parameter is 5.0.</li> </ul> </li> </ul> <p>This parameter is the object's velocity threshold to decide to split for two objects from radar information. Note that this feature is not currently implemented.</p> <ul> <li><code>threshold_yaw_diff</code> (double) [rad]<ul> <li>Default parameter is 0.35.</li> </ul> </li> </ul> <p>This parameter is the yaw orientation threshold. If the difference of yaw degree between from a LiDAR-based detection object and radar velocity, radar information is attached to output objects.</p>"},{"location":"perception/autoware_radar_fusion_to_detected_object/#weight-parameters-for-velocity-estimation","title":"Weight parameters for velocity estimation","text":"<p>To tune these weight parameters, please see document in detail.</p> <ul> <li><code>velocity_weight_average</code> (double)</li> <li>Default parameter is 0.0.</li> </ul> <p>This parameter is the twist coefficient of average twist of radar data in velocity estimation.</p> <ul> <li><code>velocity_weight_median</code> (double)</li> <li>Default parameter is 0.0.</li> </ul> <p>This parameter is the twist coefficient of median twist of radar data in velocity estimation.</p> <ul> <li><code>velocity_weight_min_distance</code> (double)</li> <li>Default parameter is 1.0.</li> </ul> <p>This parameter is the twist coefficient of radar data nearest to the center of bounding box in velocity estimation.</p> <ul> <li><code>velocity_weight_target_value_average</code> (double)</li> <li>Default parameter is 0.0.</li> </ul> <p>This parameter is the twist coefficient of target value weighted average in velocity estimation. Target value is amplitude if using radar pointcloud. Target value is probability if using radar objects.</p> <ul> <li><code>velocity_weight_target_value_top</code> (double)</li> <li>Default parameter is 0.0.</li> </ul> <p>This parameter is the twist coefficient of top target value radar data in velocity estimation. Target value is amplitude if using radar pointcloud. Target value is probability if using radar objects.</p>"},{"location":"perception/autoware_radar_fusion_to_detected_object/#parameters-for-fixed-object-information","title":"Parameters for fixed object information","text":"<ul> <li><code>convert_doppler_to_twist</code> (bool)<ul> <li>Default parameter is false.</li> </ul> </li> </ul> <p>This parameter is the flag whether convert doppler velocity to twist using the yaw information of a detected object.</p> <ul> <li><code>threshold_probability</code> (float)<ul> <li>Default parameter is 0.4.</li> </ul> </li> </ul> <p>This parameter is the threshold to filter output objects. If the probability of an output object is lower than this parameter, and the output object does not have radar points/objects, then delete the object.</p> <ul> <li><code>compensate_probability</code> (bool)<ul> <li>Default parameter is false.</li> </ul> </li> </ul> <p>This parameter is the flag to use probability compensation. If this parameter is true, compensate probability of objects to threshold probability.</p>"},{"location":"perception/autoware_radar_fusion_to_detected_object/#interface-for-autoware_radar_object_fusion_to_detected_object","title":"Interface for <code>autoware_radar_object_fusion_to_detected_object</code>","text":"<p>Sensor fusion with radar objects and a detected object.</p> <ul> <li>Calculation cost is O(nm).<ul> <li>n: the number of radar objects.</li> <li>m: the number of objects from 3d detection.</li> </ul> </li> </ul>"},{"location":"perception/autoware_radar_fusion_to_detected_object/#how-to-launch","title":"How to launch","text":"<pre><code>ros2 launch autoware_radar_fusion_to_detected_object radar_object_to_detected_object.launch.xml\n</code></pre>"},{"location":"perception/autoware_radar_fusion_to_detected_object/#input","title":"Input","text":"<ul> <li><code>~/input/objects</code> (<code>autoware_perception_msgs/msg/DetectedObjects.msg</code>)<ul> <li>3D detected objects.</li> </ul> </li> <li><code>~/input/radar_objects</code> (<code>autoware_perception_msgs/msg/DetectedObjects.msg</code>)<ul> <li>Radar objects. Note that frame_id need to be same as <code>~/input/objects</code></li> </ul> </li> </ul>"},{"location":"perception/autoware_radar_fusion_to_detected_object/#output","title":"Output","text":"<ul> <li><code>~/output/objects</code> (<code>autoware_perception_msgs/msg/DetectedObjects.msg</code>)<ul> <li>3D detected object with twist.</li> </ul> </li> <li><code>~/debug/low_confidence_objects</code> (<code>autoware_perception_msgs/msg/DetectedObjects.msg</code>)<ul> <li>3D detected object that doesn't output as <code>~/output/objects</code> because of low confidence</li> </ul> </li> </ul>"},{"location":"perception/autoware_radar_fusion_to_detected_object/#parameters","title":"Parameters","text":"<p>The parameters for core algorithm can be set as <code>node_params</code>.</p> <ul> <li><code>update_rate_hz</code> (double) [hz]<ul> <li>Default parameter is 20.0</li> </ul> </li> </ul> <p>This parameter is update rate for the <code>onTimer</code> function. This parameter should be same as the frame rate of input topics.</p>"},{"location":"perception/autoware_radar_fusion_to_detected_object/#interface-for-radar_scan_fusion_to_detected_object-tbd","title":"Interface for radar_scan_fusion_to_detected_object (TBD)","text":"<p>Under implement</p>"},{"location":"perception/autoware_radar_fusion_to_detected_object/docs/algorithm/","title":"Algorithm","text":""},{"location":"perception/autoware_radar_fusion_to_detected_object/docs/algorithm/#common-algorithm","title":"Common Algorithm","text":""},{"location":"perception/autoware_radar_fusion_to_detected_object/docs/algorithm/#1-link-between-3d-bounding-box-and-radar-data","title":"1. Link between 3d bounding box and radar data","text":"<p>Choose radar pointcloud/objects within 3D bounding box from lidar-base detection with margin space from bird's-eye view.</p> <p></p>"},{"location":"perception/autoware_radar_fusion_to_detected_object/docs/algorithm/#2-feature-support-split-the-object-going-in-a-different-direction","title":"2. [Feature support] Split the object going in a different direction","text":"<ul> <li>Split two object for the low confidence object that can be estimated to derive two object.</li> </ul>"},{"location":"perception/autoware_radar_fusion_to_detected_object/docs/algorithm/#3-estimate-twist-of-object","title":"3. Estimate twist of object","text":"<p>Estimate twist from chosen radar pointcloud/objects using twist and target value (Target value is amplitude if using radar pointcloud. Target value is probability if using radar objects). First, the estimation function calculate</p> <ul> <li>Average twist for radar pointcloud/objects.</li> <li>Median twist for radar pointcloud/objects.</li> <li>Twist for radar pointcloud/objects nearest of the center of bounding box in velocity.</li> <li>Weighted average twist with target value of radar pointcloud/objects.</li> <li>Twist with max target value of radar pointcloud/objects.</li> </ul> <p>Second, the estimation function calculate weighted average of these list. Third, twist information of estimated twist is attached to an object.</p> <p></p>"},{"location":"perception/autoware_radar_fusion_to_detected_object/docs/algorithm/#4-feature-support-option-convert-doppler-velocity-to-twist","title":"4. [Feature support] [Option] Convert doppler velocity to twist","text":"<p>If the twist information of radars is doppler velocity, convert from doppler velocity to twist using yaw angle of DetectedObject. Because radar pointcloud has only doppler velocity information, radar pointcloud fusion should use this feature. On the other hand, because radar objects have twist information, radar object fusion should not use this feature.</p> <p></p>"},{"location":"perception/autoware_radar_fusion_to_detected_object/docs/algorithm/#5-delete-objects-with-low-probability","title":"5. Delete objects with low probability","text":"<ul> <li>Delete low confidence objects that do not have some radar points/objects.</li> </ul>"},{"location":"perception/autoware_radar_object_tracker/","title":"autoware_radar_object_tracker","text":""},{"location":"perception/autoware_radar_object_tracker/#autoware_radar_object_tracker","title":"<code>autoware_radar_object_tracker</code>","text":""},{"location":"perception/autoware_radar_object_tracker/#purpose","title":"Purpose","text":"<p>This package provides a radar object tracking node that processes sequences of detected objects to assign consistent identities to them and estimate their velocities.</p>"},{"location":"perception/autoware_radar_object_tracker/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>This radar object tracker is a combination of data association and tracking algorithms.</p>"},{"location":"perception/autoware_radar_object_tracker/#data-association","title":"Data Association","text":"<p>The data association algorithm matches detected objects to existing tracks.</p>"},{"location":"perception/autoware_radar_object_tracker/#tracker-models","title":"Tracker Models","text":"<p>The tracker models used in this package vary based on the class of the detected object. See more details in the models.md.</p>"},{"location":"perception/autoware_radar_object_tracker/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_radar_object_tracker/#input","title":"Input","text":"Name Type Description <code>~/input</code> <code>autoware_perception_msgs::msg::DetectedObjects</code> Detected objects <code>/vector/map</code> <code>autoware_map_msgs::msg::LaneletMapBin</code> Map data"},{"location":"perception/autoware_radar_object_tracker/#output","title":"Output","text":"Name Type Description <code>~/output</code> <code>autoware_perception_msgs::msg::TrackedObjects</code> Tracked objects"},{"location":"perception/autoware_radar_object_tracker/#parameters","title":"Parameters","text":""},{"location":"perception/autoware_radar_object_tracker/#node-parameters","title":"Node Parameters","text":"Name Type Default Value Description <code>publish_rate</code> double 10.0 The rate at which to publish the output messages <code>world_frame_id</code> string \"map\" The frame ID of the world coordinate system <code>enable_delay_compensation</code> bool false Whether to enable delay compensation. If set to <code>true</code>, output topic is published by timer with <code>publish_rate</code>. <code>tracking_config_directory</code> string \"./config/tracking/\" The directory containing the tracking configuration files <code>enable_logging</code> bool false Whether to enable logging <code>logging_file_path</code> string \"/tmp/association_log.json\" The path to the file where logs should be written <code>tracker_lifetime</code> double 1.0 The lifetime of the tracker in seconds <code>use_distance_based_noise_filtering</code> bool true Whether to use distance based filtering <code>minimum_range_threshold</code> double 70.0 Minimum distance threshold for filtering in meters <code>use_map_based_noise_filtering</code> bool true Whether to use map based filtering <code>max_distance_from_lane</code> double 5.0 Maximum distance from lane for filtering in meters <code>max_angle_diff_from_lane</code> double 0.785398 Maximum angle difference from lane for filtering in radians <code>max_lateral_velocity</code> double 5.0 Maximum lateral velocity for filtering in m/s <code>can_assign_matrix</code> array An array of integers used in the data association algorithm <code>max_dist_matrix</code> array An array of doubles used in the data association algorithm <code>max_area_matrix</code> array An array of doubles used in the data association algorithm <code>min_area_matrix</code> array An array of doubles used in the data association algorithm <code>max_rad_matrix</code> array An array of doubles used in the data association algorithm <code>min_iou_matrix</code> array An array of doubles used in the data association algorithm <p>See more details in the models.md.</p>"},{"location":"perception/autoware_radar_object_tracker/#tracker-parameters","title":"Tracker parameters","text":"<p>Currently, this package supports the following trackers:</p> <ul> <li><code>linear_motion_tracker</code></li> <li><code>constant_turn_rate_motion_tracker</code></li> </ul> <p>Default settings for each tracker are defined in the ./config/tracking/, and described in models.md.</p>"},{"location":"perception/autoware_radar_object_tracker/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"perception/autoware_radar_object_tracker/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/autoware_radar_object_tracker/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/autoware_radar_object_tracker/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"perception/autoware_radar_object_tracker/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/autoware_radar_object_tracker/models/","title":"models","text":""},{"location":"perception/autoware_radar_object_tracker/models/#models","title":"models","text":"<p>Tracking models can be chosen from the ros parameter <code>~tracking_model</code>:</p> <p>Each model has its own parameters, which can be set in the ros parameter server.</p> <ul> <li>model name<ul> <li>parameter name for general</li> <li>override parameter name for each tracking object class</li> </ul> </li> </ul>"},{"location":"perception/autoware_radar_object_tracker/models/#linear-constant-acceleration-model","title":"linear constant acceleration model","text":"<ul> <li>prediction</li> </ul> \\[ \\begin{bmatrix} x_{k+1} \\\\ y_{k+1} \\\\ v_{x_{k+1}} \\\\ v_{y_{k+1}} \\\\ a_{x_{k+1}} \\\\ a_{y_{k+1}} \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 0 &amp; dt &amp; 0 &amp; \\frac{1}{2}dt^2 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; dt &amp; 0 &amp; \\frac{1}{2}dt^2 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; dt &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; dt \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ \\end{bmatrix} \\begin{bmatrix} x_k \\\\ y_k \\\\ v_{x_k} \\\\ v_{y_k} \\\\ a_{x_k} \\\\ a_{y_k} \\end{bmatrix} + noise \\] <ul> <li>noise model<ul> <li>random walk in acc: 2 parameters(currently disabled)</li> <li>random state noise: 6 parameters    </li> </ul> </li> </ul> <ul> <li>observation<ul> <li>observation: x,y,vx,vy</li> <li>observation noise: 4 parameters</li> </ul> </li> </ul>"},{"location":"perception/autoware_radar_object_tracker/models/#constant-turn-rate-and-velocity-model","title":"constant turn rate and velocity model","text":"<p>Just idea, not implemented yet.</p> \\[ \\begin{align} x_{k+1} &amp;= x_k + \\frac{v_k}{\\omega_k} (sin(\\theta_k + \\omega_k dt) - sin(\\theta_k)) \\\\ y_{k+1} &amp;= y_k + \\frac{v_k}{\\omega_k} (cos(\\theta_k) - cos(\\theta_k + \\omega_k dt)) \\\\ v_{k+1} &amp;= v_k \\\\ \\theta_{k+1} &amp;= \\theta_k + \\omega_k dt \\\\ \\omega_{k+1} &amp;= \\omega_k \\end{align} \\]"},{"location":"perception/autoware_radar_object_tracker/models/#noise-filtering","title":"Noise filtering","text":"<p>Radar sensors often have noisy measurement. So we use the following filter to reduce the false positive objects.</p> <p>The figure below shows the current noise filtering process.</p> <p></p>"},{"location":"perception/autoware_radar_object_tracker/models/#minimum-range-filter","title":"minimum range filter","text":"<p>In most cases, Radar sensors are used with other sensors such as LiDAR and Camera, and Radar sensors are used to detect objects far away. So we can filter out objects that are too close to the sensor.</p> <p><code>use_distance_based_noise_filtering</code> parameter is used to enable/disable this filter, and <code>minimum_range_threshold</code> parameter is used to set the threshold.</p>"},{"location":"perception/autoware_radar_object_tracker/models/#lanelet-based-filter","title":"lanelet based filter","text":"<p>With lanelet map information, We can filter out false positive objects that are not likely important obstacles.</p> <p>We filter out objects that satisfy the following conditions:</p> <ul> <li>too large lateral distance from lane</li> <li>velocity direction is too different from lane direction</li> <li>too large lateral velocity</li> </ul> <p>Each condition can be set by the following parameters:</p> <ul> <li><code>max_distance_from_lane</code></li> <li><code>max_angle_diff_from_lane</code></li> <li><code>max_lateral_velocity</code></li> </ul>"},{"location":"perception/autoware_radar_tracks_msgs_converter/","title":"radar_tracks_msgs_converter","text":""},{"location":"perception/autoware_radar_tracks_msgs_converter/#radar_tracks_msgs_converter","title":"radar_tracks_msgs_converter","text":"<p>This package converts from radar_msgs/msg/RadarTracks into autoware_perception_msgs/msg/DetectedObject and autoware_perception_msgs/msg/TrackedObject.</p> <ul> <li>Calculation cost is O(n).<ul> <li>n: The number of radar objects</li> </ul> </li> </ul>"},{"location":"perception/autoware_radar_tracks_msgs_converter/#design","title":"Design","text":""},{"location":"perception/autoware_radar_tracks_msgs_converter/#background","title":"Background","text":"<p>Autoware uses radar_msgs/msg/RadarTracks.msg as radar objects input data. To use radar objects data for Autoware perception module easily, <code>radar_tracks_msgs_converter</code> converts message type from <code>radar_msgs/msg/RadarTracks.msg</code> to <code>autoware_perception_msgs/msg/DetectedObject</code>. In addition, because many detection module have an assumption on base_link frame, <code>radar_tracks_msgs_converter</code> provide the functions of transform frame_id.</p>"},{"location":"perception/autoware_radar_tracks_msgs_converter/#note","title":"Note","text":"<p><code>Radar_tracks_msgs_converter</code> converts the label from <code>radar_msgs/msg/RadarTrack.msg</code> to Autoware label. Label id is defined as below.</p> RadarTrack Autoware UNKNOWN 32000 0 CAR 32001 1 TRUCK 32002 2 BUS 32003 3 TRAILER 32004 4 MOTORCYCLE 32005 5 BICYCLE 32006 6 PEDESTRIAN 32007 7 <p>Additional vendor-specific classifications are permitted starting from 32000 in radar_msgs/msg/RadarTrack.msg. Autoware objects label is defined in ObjectClassification</p>"},{"location":"perception/autoware_radar_tracks_msgs_converter/#interface","title":"Interface","text":""},{"location":"perception/autoware_radar_tracks_msgs_converter/#input","title":"Input","text":"<ul> <li><code>~/input/radar_objects</code> (<code>radar_msgs/msg/RadarTracks.msg</code>)<ul> <li>Input radar topic</li> </ul> </li> <li><code>~/input/odometry</code> (<code>nav_msgs/msg/Odometry.msg</code>)<ul> <li>Ego vehicle odometry topic</li> </ul> </li> </ul>"},{"location":"perception/autoware_radar_tracks_msgs_converter/#output","title":"Output","text":"<ul> <li><code>~/output/radar_detected_objects</code> (<code>autoware_perception_msgs/msg/DetectedObject.idl</code>)<ul> <li>DetectedObject topic converted to Autoware message.</li> <li>This is used for radar sensor fusion detection and radar detection.</li> </ul> </li> <li><code>~/output/radar_tracked_objects</code> (<code>autoware_perception_msgs/msg/TrackedObject.idl</code>)<ul> <li>TrackedObject topic converted to Autoware message.</li> <li>This is used for tracking layer sensor fusion.</li> </ul> </li> </ul>"},{"location":"perception/autoware_radar_tracks_msgs_converter/#parameters","title":"Parameters","text":""},{"location":"perception/autoware_radar_tracks_msgs_converter/#parameter-summary","title":"Parameter Summary","text":"Name Type Description Default Range update_rate_hz float The update rate [hz] of the output topic 20.0 \u22650.0 new_frame_id string The header frame_id of the output topic base_link N/A use_twist_compensation boolean Flag to enable the linear compensation of ego vehicle's twist false N/A use_twist_yaw_compensation boolean Flag to enable the compensation of yaw rotation of ego vehicle's twist false N/A static_object_speed_threshold float Threshold to treat detected objects as static objects 1.0 N/A"},{"location":"perception/autoware_radar_tracks_msgs_converter/#parameter-description","title":"Parameter Description","text":"<ul> <li><code>update_rate_hz</code> (double) [hz]<ul> <li>Default parameter is 20.0</li> </ul> </li> </ul> <p>This parameter is update rate for the <code>onTimer</code> function. This parameter should be same as the frame rate of input topics.</p> <ul> <li><code>new_frame_id</code> (string)<ul> <li>Default parameter is \"base_link\"</li> </ul> </li> </ul> <p>This parameter is the header frame_id of the output topic.</p> <ul> <li><code>use_twist_compensation</code> (bool)<ul> <li>Default parameter is \"true\"</li> </ul> </li> </ul> <p>This parameter is the flag to use the compensation to linear of ego vehicle's twist. If the parameter is true, then the twist of the output objects' topic is compensated by the ego vehicle linear motion.</p> <ul> <li><code>use_twist_yaw_compensation</code> (bool)<ul> <li>Default parameter is \"false\"</li> </ul> </li> </ul> <p>This parameter is the flag to use the compensation to yaw rotation of ego vehicle's twist. If the parameter is true, then the ego motion compensation will also consider yaw motion of the ego vehicle.</p> <ul> <li><code>static_object_speed_threshold</code> (float) [m/s]<ul> <li>Default parameter is 1.0</li> </ul> </li> </ul> <p>This parameter is the threshold to determine the flag <code>is_stationary</code>. If the velocity is lower than this parameter, the flag <code>is_stationary</code> of DetectedObject is set to <code>true</code> and dealt as a static object.</p>"},{"location":"perception/autoware_raindrop_cluster_filter/raindrop_cluster_filter/","title":"low_intensity_cluster_filter","text":""},{"location":"perception/autoware_raindrop_cluster_filter/raindrop_cluster_filter/#low_intensity_cluster_filter","title":"low_intensity_cluster_filter","text":""},{"location":"perception/autoware_raindrop_cluster_filter/raindrop_cluster_filter/#purpose","title":"Purpose","text":"<p>The <code>low_intensity_cluster_filter</code> is a node that filters clusters based on the intensity of their pointcloud.</p> <p>Mainly this focuses on filtering out unknown objects with very low intensity pointcloud, such as fail detection of unknown objects caused by raindrop or water splash from ego or other fast moving vehicles.</p>"},{"location":"perception/autoware_raindrop_cluster_filter/raindrop_cluster_filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"perception/autoware_raindrop_cluster_filter/raindrop_cluster_filter/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_raindrop_cluster_filter/raindrop_cluster_filter/#input","title":"Input","text":"Name Type Description <code>input/object</code> <code>tier4_perception_msgs::msg::DetectedObjectsWithFeature</code> input detected objects"},{"location":"perception/autoware_raindrop_cluster_filter/raindrop_cluster_filter/#output","title":"Output","text":"Name Type Description <code>output/object</code> <code>tier4_perception_msgs::msg::DetectedObjectsWithFeature</code> filtered detected objects"},{"location":"perception/autoware_raindrop_cluster_filter/raindrop_cluster_filter/#parameters","title":"Parameters","text":"Name Type Description Default Range intensity_threshold float The threshold of average intensity for filter. 1.0 N/A existence_probability_threshold float The existence probability threshold to apply the filter. 0.2 N/A max_x float Maximum of x of the filter effective range. 60.0 N/A min_x float Minimum of x of the filter effective range. -20.0 N/A max_y float Maximum of y of the filter effective range. 20.0 N/A min_y float Minimum of y of the filter effective range. -20.0 N/A filter_target_label.UNKNOWN boolean If true, unknown objects are filtered. true N/A filter_target_label.CAR boolean If true, car objects are filtered. false N/A filter_target_label.TRUCK boolean If true, truck objects are filtered. false N/A filter_target_label.BUS boolean If true, bus objects are filtered. false N/A filter_target_label.TRAILER boolean If true, trailer objects are filtered. false N/A filter_target_label.MOTORCYCLE boolean If true, motorcycle objects are filtered. false N/A filter_target_label.BICYCLE boolean If true, bicycle objects are filtered. false N/A filter_target_label.PEDESTRIAN boolean If true, pedestrian objects are filtered. false N/A"},{"location":"perception/autoware_raindrop_cluster_filter/raindrop_cluster_filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"perception/autoware_raindrop_cluster_filter/raindrop_cluster_filter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/autoware_raindrop_cluster_filter/raindrop_cluster_filter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/autoware_raindrop_cluster_filter/raindrop_cluster_filter/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"perception/autoware_raindrop_cluster_filter/raindrop_cluster_filter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/autoware_shape_estimation/","title":"autoware_shape_estimation","text":""},{"location":"perception/autoware_shape_estimation/#autoware_shape_estimation","title":"autoware_shape_estimation","text":""},{"location":"perception/autoware_shape_estimation/#purpose","title":"Purpose","text":"<p>This node estimates refined 3D object shapes from point cloud clusters using object labels. It supports both rule-based algorithms (L-shape fitting, cylinder, convex hull with filtering and correction) and ML-based estimation (PointNet) for vehicles, incorporating reference information from prior detections to improve shape accuracy and orientation estimation.</p>"},{"location":"perception/autoware_shape_estimation/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_shape_estimation/#input","title":"Input","text":"Name Type Description <code>input</code> <code>tier4_perception_msgs::msg::DetectedObjectsWithFeature</code> detected objects with labeled cluster"},{"location":"perception/autoware_shape_estimation/#output","title":"Output","text":"Name Type Description <code>output/objects</code> <code>autoware_perception_msgs::msg::DetectedObjects</code> detected objects with refined shape"},{"location":"perception/autoware_shape_estimation/#parameters","title":"Parameters","text":"Name Type Description Default Range use_corrector boolean The flag to apply rule-based corrector. true N/A use_filter boolean The flag to apply rule-based filter true N/A use_vehicle_reference_yaw boolean The flag to use vehicle reference yaw for corrector false N/A use_vehicle_reference_shape_size boolean The flag to use vehicle reference shape size false N/A use_boost_bbox_optimizer boolean The flag to use boost bbox optimizer false N/A model_params.use_ml_shape_estimator boolean The flag to apply use ml bounding box estimator. true N/A model_params.minimum_points integer The minimum number of points to fit a bounding box. 16 N/A model_params.precision string The precision of the model. fp32 N/A model_params.batch_size integer The batch size of the model. 32 N/A model_params.build_only boolean The flag to build the model only. false N/A"},{"location":"perception/autoware_shape_estimation/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"perception/autoware_shape_estimation/#rule-based-algorithms","title":"Rule-based algorithms","text":"<p>This rule-based geometric algorithms applies object-type-specific shape fitting (L-shape for vehicles, cylinder for pedestrians, convex hull for unknown objects), followed by filtering and correction stages that incorporate reference information from prior detections to ensure geometric consistency and improve orientation accuracy.</p> <p>The shape fitting algorithm pipeline consists of following three stages.</p> <ol> <li> <p>Shape Estimation</p> <ul> <li>Vehicle Objects (CAR, TRUCK, BUS, TRAILER, MOTORCYCLE, BICYCLE):<ul> <li> <p>L-shape Fitting Algorithm (<code>fitLShape</code> function):</p> <ul> <li>Implements search-based rectangle fitting from IV2017 paper by Zhang et al.</li> </ul> <ul> <li>Angle Optimization:<ul> <li>Default search range: 0 to 90 degrees for full angular sweep</li> <li>Reference yaw constraint: +/-search_angle_range around reference when available</li> <li>Two optimization methods: Standard iterative search or Boost-based Brent optimization</li> </ul> </li> </ul> <ul> <li>Closeness Criterion: Evaluates fitting quality using Algorithm 4 from referenced paper<ul> <li>Distance thresholds: d_min (0.01m squared), d_max (0.16m squared)</li> <li>Point-to-boundary distance calculation for quality assessment</li> </ul> </li> </ul> <ul> <li>3D Bounding Box Construction:<ul> <li>Projects points onto orthogonal axes e1 and e2</li> <li>Calculates intersection points to determine center and dimensions</li> <li>Height derived from point cloud Z-range with minimum epsilon (0.001m)</li> </ul> </li> </ul> <ul> <li>Output Validation: Ensures minimum dimensions to prevent degenerate boxes</li> </ul> </li> </ul> </li> </ul> <ul> <li>Pedestrian (PEDESTRIAN):<ul> <li>Cylinder shape estimation using cv::minEnclosingCircle</li> </ul> </li> </ul> <ul> <li>Other/Unknown Objects:<ul> <li>Convex hull shape estimation using cv::convexHull</li> </ul> </li> </ul> </li> <li> <p>Filtering</p> <ul> <li>Vehicle Type-specific Filtering:<ul> <li>Car Filter: Vehicle size validity verification</li> <li>Truck Filter: Truck-specific shape constraints</li> <li>Bus Filter: Bus-specific dimension checks</li> <li>Trailer Filter: Trailer shape validation</li> </ul> </li> </ul> <ul> <li>Physical validity checks of estimated shapes</li> </ul> <ul> <li>Exclusion of invalid estimation results</li> </ul> </li> <li> <p>Corrector</p> <ul> <li>Reference Information-based Correction:<ul> <li>Orientation correction using reference yaw information</li> <li>Dimension correction using reference shape size (minimum/fixed value modes)</li> </ul> </li> </ul> <ul> <li> <p>Shape Correction Algorithm (<code>correctWithDefaultValue</code> function):</p> <ul> <li>Purpose: Rule-based bounding box correction using default vehicle dimensions when estimated shapes violate physical constraints</li> </ul> <ul> <li> <p>Correction Vector Application:</p> <ul> <li>Computes correction vector based on conditions by correctWithDefaultValue Function<ul> <li></li> </ul> </li> </ul> <ul> <li>Updates shape dimensions: <code>shape.dimensions += correction_vector * 2.0</code></li> <li>Adjusts pose position: <code>pose.position += rotation_matrix * correction_vector</code></li> </ul> </li> </ul> <ul> <li>Orientation Normalization: Ensures longest dimension aligns with x-axis (90 degree rotation if needed)</li> </ul> </li> </ul> <ul> <li>Vehicle Type-specific Correctors:<ul> <li>Vehicle Corrector: General vehicle correction</li> <li>Dedicated correction logic for each vehicle type</li> </ul> </li> </ul> <ul> <li>Geometric consistency assurance</li> </ul> </li> <li> <p>Fallback Mechanism</p> <ul> <li>Automatic fallback to UNKNOWN label with convex hull estimation when any stage fails</li> </ul> </li> </ol>"},{"location":"perception/autoware_shape_estimation/#ml-based-shape-implementation","title":"ML Based Shape Implementation","text":"<p>The model takes a point cloud and object label(provided by camera detections/Apollo instance segmentation) as an input and outputs the 3D bounding box of the object.</p> <p>ML based shape estimation algorithm uses a PointNet model as a backbone to estimate the 3D bounding box of the object. The model is trained on the NuScenes dataset with vehicle labels (Car, Truck, Bus, Trailer).</p> <p>The implemented model is concatenated with STN (Spatial Transformer Network) to learn the transformation of the input point cloud to the canonical space and PointNet to predict the 3D bounding box of the object. Bounding box estimation part of Frustum PointNets for 3D Object Detection from RGB-D Data paper used as a reference.</p> <p>The model predicts the following outputs for each object:</p> <ul> <li>x,y,z coordinates of the object center</li> <li>object heading angle classification result(Uses 12 bins for angle classification - 30 degrees each)</li> <li>object heading angle residuals</li> <li>object size classification result</li> <li>object size residuals</li> </ul>"},{"location":"perception/autoware_shape_estimation/#training-ml-based-shape-estimation-model","title":"Training ML Based Shape Estimation Model","text":"<p>To train the model, you need ground truth 3D bounding box annotations. When using the mmdetection3d repository for training a 3D object detection algorithm, these ground truth annotations are saved and utilized for data augmentation. These annotations are used as an essential dataset for training the shape estimation model effectively.</p>"},{"location":"perception/autoware_shape_estimation/#preparing-the-dataset","title":"Preparing the Dataset","text":""},{"location":"perception/autoware_shape_estimation/#install-mmdetection3d-prerequisites","title":"Install MMDetection3D prerequisites","text":"<p>Step 1. Download and install Miniconda from the official website.</p> <p>Step 2. Create a conda virtual environment and activate it</p> <pre><code>conda create --name train-shape-estimation python=3.8 -y\nconda activate train-shape-estimation\n</code></pre> <p>Step 3. Install PyTorch</p> <pre><code>conda install pytorch torchvision -c pytorch\n</code></pre>"},{"location":"perception/autoware_shape_estimation/#install-mmdetection3d","title":"Install mmdetection3d","text":"<p>Step 1. Install MMEngine, MMCV, and MMDetection using MIM</p> <pre><code>pip install -U openmim\nmim install mmengine\nmim install 'mmcv&gt;=2.0.0rc4'\nmim install 'mmdet&gt;=3.0.0rc5, &lt;3.3.0'\n</code></pre> <p>Step 2. Install Autoware's MMDetection3D fork</p> <pre><code>git clone https://github.com/autowarefoundation/mmdetection3d.git\ncd mmdetection3d\npip install -v -e .\n</code></pre>"},{"location":"perception/autoware_shape_estimation/#preparing-nuscenes-dataset-for-training","title":"Preparing NuScenes dataset for training","text":"<p>Step 1. Download the NuScenes dataset from the official website and extract the dataset to a folder of your choice.</p> <p>Note: The NuScenes dataset is large and requires significant disk space. Ensure you have enough storage available before proceeding.</p> <p>Step 2. Create a symbolic link to the dataset folder</p> <pre><code>ln -s /path/to/nuscenes/dataset/ /path/to/mmdetection3d/data/nuscenes/\n</code></pre> <p>Step 3. Prepare the NuScenes data by running:</p> <pre><code>cd mmdetection3d\npython tools/create_data.py nuscenes --root-path ./data/nuscenes --out-dir ./data/nuscenes --extra-tag nuscenes --only-gt-database True\n</code></pre>"},{"location":"perception/autoware_shape_estimation/#clone-bounding-box-estimator-model","title":"Clone Bounding Box Estimator model","text":"<pre><code>git clone https://github.com/autowarefoundation/bbox_estimator.git\n</code></pre>"},{"location":"perception/autoware_shape_estimation/#split-the-dataset-into-training-and-validation-sets","title":"Split the dataset into training and validation sets","text":"<pre><code>cd bbox_estimator\npython3 utils/split_dbinfos.py --dataset_path /path/to/mmdetection3d/data/nuscenes --classes 'car' 'truck' 'bus' 'trailer'  --train_ratio 0.8\n</code></pre>"},{"location":"perception/autoware_shape_estimation/#training-and-deploying-the-model","title":"Training and Deploying the model","text":""},{"location":"perception/autoware_shape_estimation/#training-the-model","title":"Training the model","text":"<pre><code># Detailed training options can be found in the training script\n# For more details, run `python3 train.py --help`\npython3 train.py --dataset_path /path/to/mmdetection3d/data/nuscenes\n</code></pre>"},{"location":"perception/autoware_shape_estimation/#deploying-the-model","title":"Deploying the model","text":"<pre><code># Convert the trained model to ONNX format\npython3 onnx_converter.py --weight_path /path/to/best_checkpoint.pth --output_path /path/to/output.onnx\n</code></pre> <p>Give the output path of the ONNX model to the <code>model_path</code> parameter in the <code>shape_estimation</code> node launch file.</p>"},{"location":"perception/autoware_shape_estimation/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD</p>"},{"location":"perception/autoware_shape_estimation/#referencesexternal-links","title":"References/External links","text":"<p>L-shape fitting implementation of the paper:</p> <pre><code>@conference{Zhang-2017-26536,\nauthor = {Xiao Zhang and Wenda Xu and Chiyu Dong and John M. Dolan},\ntitle = {Efficient L-Shape Fitting for Vehicle Detection Using Laser Scanners},\nbooktitle = {2017 IEEE Intelligent Vehicles Symposium},\nyear = {2017},\nmonth = {June},\nkeywords = {autonomous driving, laser scanner, perception, segmentation},\n}\n</code></pre> <p>Frustum PointNets for 3D Object Detection from RGB-D Data:</p> <pre><code>@inproceedings{qi2018frustum,\ntitle={Frustum pointnets for 3d object detection from rgb-d data},\nauthor={Qi, Charles R and Liu, Wei and Wu, Chenxia and Su, Hao and Guibas, Leonidas J},\nbooktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},\npages={918--927},\nyear={2018}\n}```\n</code></pre>"},{"location":"perception/autoware_simpl_prediction/","title":"autoware_simpl_prediction","text":""},{"location":"perception/autoware_simpl_prediction/#autoware_simpl_prediction","title":"autoware_simpl_prediction","text":""},{"location":"perception/autoware_simpl_prediction/#purpose","title":"Purpose","text":"<p>The <code>autoware_simpl_prediction</code> is used for 3D object motion prediction based on ML-based model called SIMPL.</p>"},{"location":"perception/autoware_simpl_prediction/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The implementation bases on SIMPL [1] [2] work. It uses TensorRT library for data process and network interface.</p> <p>Workflow overview of this node is as follows:</p> <pre><code>flowchart TD\n    In1@{ shape: card, label: \"~/input/objects\" } -- autoware_perception_msgs::msg::TrackedObjects --&gt; Callback1@{ shape: rect, label: \"SimplNode::callback(...)\" }\n    Callback1 --&gt; B@{ shape: stadium, label: \"Start measuring Processing Time\" }\n\n    In2@{ shape: card, label: \"/localization/kinematic_state\" } -- nav_msgs::msg::Odometry --&gt; C\n    B --&gt; C@{ shape: subproc, label: \"SimplNode::subscribe_ego(...)\" }\n\n    In3@{ shape: card, label: \"~/input/vector_map\" } -- \"autoware_map_msgs::msg::LaneletMapBin\" --&gt; Callback2@{ shape: rect, label: \"SimplNode::on_map(...)\" }\n    Callback2 --&gt; X@{ shape: subproc, label: \"LaneletConverter::convert(...)\" }\n\n    C --&gt;|\u2705| D@{ shape: subproc, label: \"LaneletConverter::polylines()\" }\n    C --&gt;|\u274c| Z1@{ shape: curv-trap, label: \"\u26a0\ufe0fWARNING: Failed to subscribe ego\" } --&gt; END@{ shape: stadium }\n\n    D --&gt;|\u2705| E@{ shape: subproc, label: \"SimplNode::update_history(...)\" }\n    D --&gt;|\u274c| Z2@{ shape: curv-trap, label: \"\u26a0\ufe0fWARNING: No map points\" } --&gt; END\n\n    E --&gt; F@{ shape: subproc, label: \"PreProcessor::process(...)\" }\n    F --&gt; G@{ shape: subproc, label: \"TrtSimpl::do_inference(...)\" }\n\n    G --&gt;|\u2705| I@{ shape: subproc, label: \"PostProcessor::process(...)\" } --&gt; J@{ shape: stadium, label: \"Publish Predicted Objects\" }\n    G --&gt;|\u274c| Z3@{ shape: curv-trap, label: \"\u26a0\ufe0fERROR: Inference failed\" } --&gt; H\n\n    J -- autoware_perception_msgs::msg::PredictedObjects --&gt; Out1@{ shape: card, label: \"~/output/objects\" }\n    J --&gt; H@{ shape: stadium, label: \"Publish Processing &amp; Cyclic Time\" }\n\n    H -- autoware_internal_debug_msgs::msg::Float64Stamped --&gt; Out3@{ shape: card, label: \"~/debug/cyclic_time_ms\" }\n    H -- autoware_internal_debug_msgs::msg::Float64Stamped --&gt; Out2@{ shape: card, label: \"~/debug/processing_time_ms\" }\n\n    H --&gt; END</code></pre>"},{"location":"perception/autoware_simpl_prediction/#inputs-representation","title":"Inputs Representation","text":"<ul> <li>\\(X_A\\in R^{N\\times D_{agent}\\times T_{past}}\\): Agent histories input.</li> <li>\\(X_M\\in R^{K\\times P\\times D_{map}}\\): Map points input.</li> <li>\\(X_{RPE}\\in R^{(N+K)\\times (N+K)\\times D_{rpe}}\\): Relative pose encoding input.</li> </ul>"},{"location":"perception/autoware_simpl_prediction/#outputs-representation","title":"Outputs Representation","text":"<ul> <li>\\(P_{score}\\in R^{N\\times M}\\): Predicted scores for each agent and mode.</li> <li>\\(P_{trajectory}\\in R^{N\\times M\\times T_{future}\\times D_{trajectory}}\\): Predicted trajectories for each agent and mode.<ul> <li>Where, \\(D_{trajectory}\\) is \\((x, y, v_x, v_y)\\) in the agent local coordinate frame.</li> </ul> </li> </ul>"},{"location":"perception/autoware_simpl_prediction/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_simpl_prediction/#inputs","title":"Inputs","text":"Name Type Description <code>~/input/objects</code> <code>autoware_perception_msgs::msg::TrackedObjects</code> Input tracked agents. <code>~/input/vector_map</code> <code>autoware_map_msgs::msg::LaneletMapBin</code> Input vector map. <code>/localization/kinematic_state</code> <code>nav_msgs::msg::Odometry</code> Ego vehicle odometry."},{"location":"perception/autoware_simpl_prediction/#outputs","title":"Outputs","text":"Name Type Description <code>~/output/objects</code> <code>autoware_perception_msgs::msg::PredictedObjects</code> Predicted agents' motion."},{"location":"perception/autoware_simpl_prediction/#debug-outputs","title":"Debug Outputs","text":"Name Type Description <code>~/debug/cyclic_time_ms</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> Cyclic time [ms]. <code>~/debug/processing_time_ms</code> <code>autoware_internal_debug_msgs::msg::Float64Stamped</code> Processing time [ms]."},{"location":"perception/autoware_simpl_prediction/#parameters","title":"Parameters","text":"Name Type Description Default Range detector.onnx_path string Filepath to ONNX. $(var data_path)/simpl.onnx N/A detector.engine_path string Filepath to TensorRT engine. $(var data_path)/simpl.engine N/A detector.precision string Precision for inference. fp16 ['fp32', 'fp16', 'int8'] preprocess.labels array An array of label names to be predicted. All of elements must be included in <code>archetype::AgentLabel</code>. ['VEHICLE', 'PEDESTRIAN', 'MOTORCYCLIST', 'CYCLIST', 'LARGE_VEHICLE'] N/A preprocess.max_num_agent integer Maximum number of agents to be predicted (\\(N\\)). 50 \u22651 preprocess.num_past integer Number of past history for model input (\\(T_{past}\\)). 48 \u22651 preprocess.max_num_polyline integer Maximum number of polylines (\\(K\\)). 300 \u22651 preprocess.max_num_point integer Maximum number of points included in a single polyline (\\(P\\)). 20 \u22651 preprocess.polyline_range_threshold float Distance threshold from the ego vehicle to filter out map polylines [m]. 100.0 \u22651.0 preprocess.polyline_break_distance float If the distance of two points is greater than this value they are separated into two polylines [m]. 5.0 \u22651.0 postprocess.num_mode integer Number of predicted modes for a single agent (\\(M\\)). 6 \u22651 postprocess.num_future integer Number of predicted future time steps (\\(T_{future}\\)). 80 \u22651 postprocess.score_threshold float Score threshold to filter predictions. 0.15 \u22650.0\u22641.0"},{"location":"perception/autoware_simpl_prediction/#wip-model-training-deployment","title":"[WIP] Model Training / Deployment","text":"<p>Now we are preparing a library to train and deploy SIMPL and other ML models featuring motion prediction tasks.</p>"},{"location":"perception/autoware_simpl_prediction/#testing","title":"Testing","text":"<p>Unit tests are provided and can be run with:</p> <pre><code>colcon test --packages-select autoware_simpl\ncolcon test-result --all\n</code></pre> <p>To print the test's details with while the tests are being run, use the <code>--event-handlers console_cohesion+</code> option to print the details directly to the console:</p> <pre><code>colcon test --event-handlers console_cohesion+ --packages-select autoware_simpl\n</code></pre>"},{"location":"perception/autoware_simpl_prediction/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"perception/autoware_simpl_prediction/#agent-to-be-published","title":"Agent to be Published","text":"<p>Note that only agents whose label is contained in <code>preprocess.labels</code> will be published.</p> <p>For example, if <code>preprocess.labels</code> is set to <code>[\"VEHICLE\", \"PEDESTRIAN\"]</code>, only agents with labels \"ObjectClassification.CAR\" and \"ObjectClassification.PEDESTRIAN\" will be published.</p> <p>Here is the table of Autoware labels and their corresponding labels:</p> Autoware Label Correspondence <code>ObjectClassification::CAR</code> <code>VEHICLE</code> <code>ObjectClassification::PEDESTRIAN</code> <code>PEDESTRIAN</code> <code>ObjectClassification::BICYCLE</code> <code>CYCLIST</code> <code>ObjectClassification::MOTORCYCLE</code> <code>MOTORCYCLIST</code> <code>ObjectClassification::TRUCK</code> <code>LARGE_VEHICLE</code> <code>ObjectClassification::TRAILER</code> <code>LARGE_VEHICLE</code> <code>ObjectClassification::BUS</code> <code>LARGE_VEHICLE</code> <code>ObjectClassification::UNKNOWN</code> <code>UNKNOWN</code>"},{"location":"perception/autoware_simpl_prediction/#maximum-number-of-predictable-agents","title":"Maximum Number of Predictable Agents","text":"<p>We have not supported the dynamic shape inference yet. Therefore, the number of predicted agents must be fixed as <code>preprocess.max_num_agent</code> (\\(N\\)). This value is determined when exporting ONNX.</p> <p>Note that the following parameters are also determined when exporting ONNX:</p> <ul> <li><code>preprocess.num_past</code>: \\(T_{past}\\)</li> <li><code>preprocess.max_num_polyline</code>: \\(K\\)</li> <li><code>preprocess.max_num_point</code>: \\(P\\)</li> <li><code>postprocess.num_mode</code>: \\(M\\)</li> <li><code>postprocess.num_future</code>: \\(T_{future}\\)</li> </ul>"},{"location":"perception/autoware_simpl_prediction/#agent-history-lifetime","title":"Agent History Lifetime","text":"<p>Under the hood, <code>SimplNode</code> stores and accumulates agent history in every callback, but removes the history that is not observed in callbacks.</p>"},{"location":"perception/autoware_simpl_prediction/#reliability-of-predicted-paths","title":"Reliability of Predicted Paths","text":"<p>Predicted paths with a confidence score lower than <code>postprocess.score_threshold</code> are filtered out and not published. If all predicted modes for an object are filtered out, the published object will contain no path.</p> <p>Note that the total confidence of the remaining predicted paths is not guaranteed to sum to 100% after this filtering process.</p>"},{"location":"perception/autoware_simpl_prediction/#references-external-links","title":"References / External Links","text":"<p>[1] Lu Zhang, Peiliang Li, Sikang Liu, and Shaojie Shen, \"SIMPL: A Simple and Efficient Multi-agent Motion Prediction Baseline for Autonomous Driving\", arXiv preprint arXiv:2402.02519 (2024). </p> <p>[2] https://github.com/HKUST-Aerial-Robotics/SIMPL</p>"},{"location":"perception/autoware_simple_object_merger/","title":"autoware_simple_object_merger","text":""},{"location":"perception/autoware_simple_object_merger/#autoware_simple_object_merger","title":"autoware_simple_object_merger","text":"<p>This package can merge multiple topics of autoware_perception_msgs/msg/DetectedObject with low calculation cost.</p>"},{"location":"perception/autoware_simple_object_merger/#design","title":"Design","text":""},{"location":"perception/autoware_simple_object_merger/#background","title":"Background","text":"<p>Object_merger is mainly used for merge process with DetectedObjects. There are 2 characteristics in <code>Object_merger</code>. First, <code>object_merger</code> solve data association algorithm like Hungarian algorithm for matching problem, but it needs computational cost. Second, <code>object_merger</code> can handle only 2 DetectedObjects topics and cannot handle more than 2 topics in one node. To merge 6 DetectedObjects topics, 6 <code>object_merger</code> nodes need to stand for now.</p> <p>Therefore, <code>autoware_simple_object_merger</code> aim to merge multiple DetectedObjects with low calculation cost. The package do not use data association algorithm to reduce the computational cost, and it can handle more than 2 topics in one node to prevent launching a large number of nodes.</p>"},{"location":"perception/autoware_simple_object_merger/#use-case","title":"Use case","text":"<ul> <li>Multiple radar detection</li> </ul> <p><code>autoware_simple_object_merger</code> can be used for multiple radar detection. By combining them into one topic from multiple radar topics, the pipeline for faraway detection with radar can be simpler.</p>"},{"location":"perception/autoware_simple_object_merger/#limitation","title":"Limitation","text":"<ul> <li>Sensor data drops and delay</li> </ul> <p>Merged objects will not be published until all topic data is received when initializing. In addition, to care sensor data drops and delayed, this package has a parameter to judge timeout. When the latest time of the data of a topic is older than the timeout parameter, it is not merged for output objects. For now specification of this package, if all topic data is received at first and after that the data drops, and the merged objects are published without objects which is judged as timeout.The timeout parameter should be determined by sensor cycle time.</p> <ul> <li>Post-processing</li> </ul> <p>Because this package does not have matching processing, there are overlapping objects depending on the input objects. So output objects can be used only when post-processing is used.</p>"},{"location":"perception/autoware_simple_object_merger/#interface","title":"Interface","text":""},{"location":"perception/autoware_simple_object_merger/#input","title":"Input","text":"<p>Input topics is defined by the parameter of <code>input_topics</code> (List[string]). The type of input topics is <code>std::vector&lt;autoware_perception_msgs/msg/DetectedObjects.msg&gt;</code>.</p>"},{"location":"perception/autoware_simple_object_merger/#output","title":"Output","text":"<ul> <li><code>~/output/objects</code> (<code>autoware_perception_msgs/msg/DetectedObjects.msg</code>)<ul> <li>Merged objects combined from input topics.</li> </ul> </li> </ul>"},{"location":"perception/autoware_simple_object_merger/#parameters","title":"Parameters","text":"Name Type Description Default Range update_rate_hz float This parameter defines the update rate for the <code>onTimer</code> function. This value should match the frame rate of the input topics. 20.0 N/A new_frame_id string This parameter specifies the header frame_id for the output topic. If the output topics are used for a perception module, this should be set to <code>base_link</code>. base_link N/A timeout_threshold float This parameter sets the threshold for timeout judgment. If the time difference between the first topic in <code>input_topics</code> and another input topic exceeds this value, the objects in that topic are not merged into the output objects. 1.0 N/A input_topics array This parameter lists the names of input topics. For example, when this package is used for radar objects, input topics can be specified here. [''] N/A"},{"location":"perception/autoware_tensorrt_bevdet/","title":"tensorrt_bevdet","text":""},{"location":"perception/autoware_tensorrt_bevdet/#tensorrt_bevdet","title":"tensorrt_bevdet","text":""},{"location":"perception/autoware_tensorrt_bevdet/#purpose","title":"Purpose","text":"<p>The core algorithm, named <code>BEVDet</code>, it unifies multi-view images into the perspective of BEV for 3D object detection task.</p>"},{"location":"perception/autoware_tensorrt_bevdet/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"perception/autoware_tensorrt_bevdet/#cite","title":"Cite","text":"<ul> <li>Junjie Huang, Guan Huang, \"BEVPoolv2: A Cutting-edge Implementation of BEVDet Toward Deployment\", [ref]</li> <li>bevdet_vendor package are copied from the original codes (The TensorRT, C++ implementation by LCH1238) and modified.</li> <li>This package is ported version toward Autoware from bevdet_vendor.</li> </ul>"},{"location":"perception/autoware_tensorrt_bevdet/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_tensorrt_bevdet/#inputs","title":"Inputs","text":"Name Type Description <code>~/input/topic_img_front_left</code> <code>sensor_msgs::msg::Image</code> input front_left camera image <code>~/input/topic_img_front</code> <code>sensor_msgs::msg::Image</code> input front camera image <code>~/input/topic_img_front_right</code> <code>sensor_msgs::msg::Image</code> input front_right camera image <code>~/input/topic_img_back_left</code> <code>sensor_msgs::msg::Image</code> input back_left camera image <code>~/input/topic_img_back</code> <code>sensor_msgs::msg::Image</code> input back camera image <code>~/input/topic_img_back_right</code> <code>sensor_msgs::msg::Image</code> input back_right camera image <code>~/input/topic_img_front_left/camera_info</code> <code>sensor_msgs::msg::CameraInfo</code> input front_left camera parameters <code>~/input/topic_img_front/camera_info</code> <code>sensor_msgs::msg::CameraInfo</code> input front camera parameters <code>~/input/topic_img_front_right/camera_info</code> <code>sensor_msgs::msg::CameraInfo</code> input front_right camera parameters <code>~/input/topic_img_back_left/camera_info</code> <code>sensor_msgs::msg::CameraInfo</code> input back_left camera parameters <code>~/input/topic_img_back/camera_info</code> <code>sensor_msgs::msg::CameraInfo</code> input back camera parameters <code>~/input/topic_img_back_right/camera_info</code> <code>sensor_msgs::msg::CameraInfo</code> input back_right camera parameters"},{"location":"perception/autoware_tensorrt_bevdet/#outputs","title":"Outputs","text":"Name Type Description <code>~/output/boxes</code> <code>autoware_perception_msgs::msg::DetectedObjects</code> detected objects <code>~/output_bboxes</code> <code>visualization_msgs::msg::MarkerArray</code> detected objects for nuScenes visualization"},{"location":"perception/autoware_tensorrt_bevdet/#how-to-use-tensorrt-bevdet-node","title":"How to Use Tensorrt BEVDet Node","text":""},{"location":"perception/autoware_tensorrt_bevdet/#prerequisites","title":"Prerequisites","text":"<ul> <li>Tensorrt 10.8.0.43</li> <li>CUDA 12.4</li> <li>cuDNN 8.9.2</li> </ul>"},{"location":"perception/autoware_tensorrt_bevdet/#trained-models","title":"Trained Models","text":"<p>Download the trained models with the instructions in Autoware artifacts.</p> <p>The <code>BEVDet</code> model was trained in <code>NuScenes</code> dataset for 20 epochs.</p>"},{"location":"perception/autoware_tensorrt_bevdet/#test-tensorrt-bevdet-node-with-nuscenes","title":"Test Tensorrt BEVDet Node with Nuscenes","text":"<ol> <li> <p>Integrate this branch changes in your autoware_universe/perception directory</p> </li> <li> <p>Include this bevdet_vendor pr in src/universe/external/bevdet_vendor as this supports fp16 precision and api support for Tensorrt 10.x.x</p> </li> <li> <p>To play ros2 bag of nuScenes data     </p> <pre><code>cd autoware/src\ngit clone https://github.com/Owen-Liuyuxuan/ros2_dataset_bridge\ncd ..\n\n# Open the launch file to configure dataset settings:\n\nnano src/ros2_dataset_bridge/launch/nuscenes_launch.xml\n\n# Update the following lines with the correct NuScenes dataset path and set the publishing frequency to 10 Hz for optimal data streaming:\n\n&lt;arg name=\"NUSCENES_DIR\" default=\"&lt;nuscenes_dataset_path&gt;\"/&gt;\n&lt;arg name=\"NUSCENES_VER\" default=\"v1.0-trainval\"/&gt;\n&lt;arg name=\"UPDATE_FREQUENCY\" default=\"10.0\"/&gt;\n\n# Open the ros_utils script:\n\nnano src/ros2_dataset_bridge/ros2_dataset_bridge/utils/ros_util.py\n\n# Modify the encoding to bgr8 at line 92:\n\nimage_msg = self.cv_bridge.cv2_to_imgmsg(image, encoding=\"bgr8\")\n</code></pre> </li> <li> <p>Build the <code>autoware_tensorrt_bevdet</code> and <code>ros2_dataset_bridge</code> packages</p> <pre><code># Build autoware_tensorrt_bevdet\n\ncolcon build --packages-up-to autoware_tensorrt_bevdet\n\n# Build ros2_dataset_bridge\n\ncolcon build --packages-select=ros2_dataset_bridge\n\n# Source the environment\n\nsource install/setup.bash # install/setup.zsh or install/setup.sh for your own need.\nsource /opt/ros/humble/setup.bash\n</code></pre> </li> <li> <p>Launch <code>ros2_dataset_bridge</code> that publishes nuScenes dataset</p> <pre><code># Launch the data publisher, RViz, and GUI controller:\n\nros2 launch ros2_dataset_bridge nuscenes_launch.xml\n\n# Tip: If NuScenes boxes are not visible in RViz, ensure the \"Stop\" checkbox in the GUI controller is unchecked, then click \"OK\".\n\n# Note: ROS bag playback is limited to 10 Hz, which constrains the BEVDet node to the same rate. However, based on callback execution time, BEVDet can run at up to 35 FPS with FP16 and 17 FPS with FP32.\n</code></pre> </li> <li> <p>Launch <code>tensorrt_bevdet_node</code></p> <pre><code>ros2 launch autoware_tensorrt_bevdet tensorrt_bevdet.launch.xml\n</code></pre> </li> </ol>"},{"location":"perception/autoware_tensorrt_bevdet/#configuration","title":"Configuration","text":"<p>The configuration file in <code>config/bevdet.param.yaml</code> can be modified to suit your needs:</p> <ul> <li>Modify <code>precision</code> to <code>fp16</code> or <code>fp32</code></li> <li>Set <code>debug_mode</code> to <code>true</code> to enable publishing bounding box markers.</li> </ul>"},{"location":"perception/autoware_tensorrt_bevdet/#limitation","title":"Limitation","text":"<p>The model is trained on open-source dataset <code>NuScenes</code> and has poor generalization on its own dataset, If you want to use this model to infer your data, you need to retrain it.</p>"},{"location":"perception/autoware_tensorrt_bevdet/#training-bevdet-model","title":"Training BEVDet Model","text":"<p>If you want to train model using the TIER IV's internal database(~2600 key frames), please refer to the following repositories:BEVDet adapted to TIER IV dataset.</p>"},{"location":"perception/autoware_tensorrt_bevdet/#referencesexternal-links","title":"References/External links","text":"<p>[1] https://github.com/HuangJunJie2017/BEVDet/tree/dev2.1</p> <p>[2] https://github.com/LCH1238/BEVDet/tree/export</p> <p>[3] https://github.com/LCH1238/bevdet-tensorrt-cpp/tree/one</p>"},{"location":"perception/autoware_tensorrt_bevformer/","title":"tensorrt_bevformer","text":""},{"location":"perception/autoware_tensorrt_bevformer/#tensorrt_bevformer","title":"tensorrt_bevformer","text":""},{"location":"perception/autoware_tensorrt_bevformer/#purpose","title":"Purpose","text":"<p>The core algorithm, named <code>BEVFormer</code>, unifies multi-view images into the BEV perspective for 3D object detection tasks with temporal fusion.</p>"},{"location":"perception/autoware_tensorrt_bevformer/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"perception/autoware_tensorrt_bevformer/#cite","title":"Cite","text":"<ul> <li>Zhicheng Wang, et al., \"BEVFormer: Incorporating Transformers for Multi-Camera 3D Detection\" [ref]</li> <li>This node is ported and adapted for Autoware from Multicoreware's BEVFormer ROS2 C++ repository.</li> </ul>"},{"location":"perception/autoware_tensorrt_bevformer/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_tensorrt_bevformer/#inputs","title":"Inputs","text":"Name Type Description <code>~/input/topic_img_front_left</code> <code>sensor_msgs::msg::Image</code> input front_left camera image <code>~/input/topic_img_front</code> <code>sensor_msgs::msg::Image</code> input front camera image <code>~/input/topic_img_front_right</code> <code>sensor_msgs::msg::Image</code> input front_right camera image <code>~/input/topic_img_back_left</code> <code>sensor_msgs::msg::Image</code> input back_left camera image <code>~/input/topic_img_back</code> <code>sensor_msgs::msg::Image</code> input back camera image <code>~/input/topic_img_back_right</code> <code>sensor_msgs::msg::Image</code> input back_right camera image <code>~/input/topic_img_front_left/camera_info</code> <code>sensor_msgs::msg::CameraInfo</code> input front_left camera parameters <code>~/input/topic_img_front/camera_info</code> <code>sensor_msgs::msg::CameraInfo</code> input front camera parameters <code>~/input/topic_img_front_right/camera_info</code> <code>sensor_msgs::msg::CameraInfo</code> input front_right camera parameters <code>~/input/topic_img_back_left/camera_info</code> <code>sensor_msgs::msg::CameraInfo</code> input back_left camera parameters <code>~/input/topic_img_back/camera_info</code> <code>sensor_msgs::msg::CameraInfo</code> input back camera parameters <code>~/input/topic_img_back_right/camera_info</code> <code>sensor_msgs::msg::CameraInfo</code> input back_right camera parameters <code>~/input/can_bus</code> <code>autoware_localization_msgs::msg::KinematicState</code> CAN bus data for ego-motion"},{"location":"perception/autoware_tensorrt_bevformer/#outputs","title":"Outputs","text":"Name Type Description <code>~/output_boxes</code> <code>autoware_perception_msgs::msg::DetectedObjects</code> detected objects <code>~/output_bboxes</code> <code>visualization_msgs::msg::MarkerArray</code> detected objects for nuScenes visualization"},{"location":"perception/autoware_tensorrt_bevformer/#how-to-use-tensorrt-bevformer-node","title":"How to Use Tensorrt BEVFormer Node","text":""},{"location":"perception/autoware_tensorrt_bevformer/#prerequisites","title":"Prerequisites","text":"<ul> <li>TensorRT 10.8.0.43</li> <li>CUDA 12.4</li> <li>cuDNN 8.9.2</li> </ul>"},{"location":"perception/autoware_tensorrt_bevformer/#trained-model","title":"Trained Model","text":"<p>Download the <code>bevformer_small.onnx</code> model to:</p> <pre><code>$HOME/autoware_data/tensorrt_bevformer\n</code></pre> <p>Note: The BEVFormer model was trained on the nuScenes dataset for 24 epochs with temporal fusion enabled.</p>"},{"location":"perception/autoware_tensorrt_bevformer/#test-tensorrt-bevformer-node-with-nuscenes","title":"Test TensorRT BEVFormer Node with nuScenes","text":"<ol> <li> <p>Integrate this package into your autoware_universe/perception directory.</p> </li> <li> <p>To play ROS 2 bag of nuScenes data:</p> <pre><code>cd autoware/src\ngit clone -b feature/bevformer-integration https://github.com/naveen-mcw/ros2_dataset_bridge.git\ncd ..\n</code></pre> <p>Note: The <code>feature/bevformer-integration</code> branch provides required data for the BEVFormer.</p> <p>Download nuScenes dataset and canbus data here.</p> <p>Open and edit the launch file to set dataset paths/configs:</p> <pre><code>nano src/ros2_dataset_bridge/launch/nuscenes_launch.xml\n</code></pre> <p>Update as needed:</p> <pre><code>&lt;arg name=\"NUSCENES_DIR\" default=\"&lt;nuScenes_dataset_path&gt;\"/&gt;\n&lt;arg name=\"NUSCENES_CAN_BUS_DIR\" default=\"&lt;can_bus_path&gt;\"/&gt;\n&lt;arg name=\"NUSCENES_VER\" default=\"v1.0-trainval\"/&gt;\n&lt;arg name=\"UPDATE_FREQUENCY\" default=\"10.0\"/&gt;\n</code></pre> </li> <li> <p>Build the autoware_tensorrt_bevformer and ros2_dataset_bridge packages</p> <pre><code># Build ros2_dataset_bridge\n\ncolcon build --packages-up-to ros2_dataset_bridge\n\n# Build autoware_tensorrt_bevformer\n\ncolcon build --packages-up-to autoware_tensorrt_bevformer\n</code></pre> <p>Source environments:</p> <pre><code>source install/setup.bash\nsource /opt/ros/humble/setup.bash\n</code></pre> </li> <li> <p>Launch dataset publisher and GUI tools:</p> <pre><code>ros2 launch ros2_dataset_bridge nuscenes_launch.xml\n</code></pre> <p>Tip: If nuScenes boxes aren't visible in RViz, uncheck Stop in the GUI controller, then click OK.</p> </li> <li> <p>Launch TensorRT BEVFormer Node</p> <pre><code># Default mode (FP16)\nros2 launch autoware_tensorrt_bevformer bevformer.launch.xml\n\n# FP32 precision\nros2 launch autoware_tensorrt_bevformer bevformer.launch.xml precision:=fp32\n\n# With visualization\nros2 launch autoware_tensorrt_bevformer bevformer.launch.xml debug_mode:=true\n\n# FP32 + visualization\nros2 launch autoware_tensorrt_bevformer bevformer.launch.xml precision:=fp32 debug_mode:=true\n</code></pre> </li> </ol>"},{"location":"perception/autoware_tensorrt_bevformer/#configuration","title":"Configuration","text":"<p>The configuration file in <code>config/bevformer.param.yaml</code> can be modified to suit your needs:</p> <ul> <li>Modify <code>precision</code> to <code>fp16</code> or <code>fp32</code></li> <li>Set <code>debug_mode</code> to <code>true</code> to enable publishing bounding box markers.</li> </ul>"},{"location":"perception/autoware_tensorrt_bevformer/#referencesexternal-links","title":"References/External links","text":"<p>[1] BEVFormer (arXiv) [2] Original Python BEVFormer TensorRT [3] nuScenes Dataset</p>"},{"location":"perception/autoware_tensorrt_bevformer/TensorRT/","title":"TensorRT Plugins","text":""},{"location":"perception/autoware_tensorrt_bevformer/TensorRT/#tensorrt-plugins","title":"TensorRT Plugins","text":""},{"location":"perception/autoware_tensorrt_bevformer/TensorRT/#plugins","title":"Plugins","text":""},{"location":"perception/autoware_tensorrt_bevformer/TensorRT/#grid-sampler","title":"Grid Sampler","text":"OP Name Attributes Inputs Outputs FP32 Speed FP16 Speed INT8 Speed Half Type Tensor Format Test Device GridSampler2DTRT interpolation_mode: intpadding_mode: intalign_corners: int input: Tgrid: T output: T x1 x2.0 x3.8 nv_half kLinear, kCHW4 RTX 2080Ti GridSampler2DTRT2 interpolation_mode: intpadding_mode: intalign_corners: int input: Tgrid: T output: T x1 x3.1 x3.8 nv_half2 kLinear, kCHW2, kCHW4 RTX 2080Ti GridSampler3DTRT interpolation_mode: intpadding_mode: intalign_corners: int input: Tgrid: T output: T x1 x1.3 - nv_half kLinear RTX 2080Ti GridSampler3DTRT2 interpolation_mode: intpadding_mode: intalign_corners: int input: Tgrid: T output: T x1 x2.2 - nv_half2 kLinear RTX 2080Ti"},{"location":"perception/autoware_tensorrt_bevformer/TensorRT/#inputs","title":"Inputs","text":"<ul> <li> <p>input: T[float/half/half2/int8]</p> <p>Tensor shape: <code>[N, C, H_in, W_in]</code> (4D case) or <code>[N, C, D_in, H_in, W_in]</code> (5D case)</p> </li> </ul> <ul> <li> <p>grid: T[float/half/half2/int8]</p> <p>Tensor shape: <code>[N, 2, H_out, W_out]</code> (4D case) or <code>[N, 3, D_out, H_out, W_out]</code> (5D case)</p> <p><code>grid</code> specifies the sampling pixel locations normalized by the <code>input</code> spatial dimensions. Therefore, it should have most values in the range of <code>[-10, 10]</code>. For example, values <code>x = -10, y = -10</code> is the left-top pixel of <code>input</code>, and values <code>x = 10, y = 10</code> is the right-bottom pixel of <code>input</code>.</p> </li> </ul>"},{"location":"perception/autoware_tensorrt_bevformer/TensorRT/#attributes","title":"Attributes","text":"<ul> <li> <p>interpolation_mode: int</p> <p>Interpolation mode to calculate output values. (0: <code>bilinear</code> , 1: <code>nearest</code>, 2: <code>bicubic</code>)</p> <p>Note: <code>bicubic</code> supports only 4-D input.</p> </li> </ul> <ul> <li> <p>padding_mode: int</p> <p>Padding mode for outside grid values. (0: <code>zeros</code>, 1: <code>border</code>, 2: <code>reflection</code>)</p> </li> </ul> <ul> <li> <p>align_corners: int</p> <p>If <code>align_corners=1</code>, the extrema (<code>-1</code> and <code>1</code>) are considered as referring to the center points of the input's corner pixels. If <code>align_corners=0</code>, they are instead considered as referring to the corner points of the input's corner pixels, making the sampling more resolution agnostic.</p> </li> </ul>"},{"location":"perception/autoware_tensorrt_bevformer/TensorRT/#outputs","title":"Outputs","text":"<ul> <li> <p>output: T[float/half/half2/int8]</p> <p>Tensor shape: <code>[N, C, H_out, W_out]</code> (4D case) or <code>[N, C, D_out, H_out, W_out]</code> (5D case)</p> </li> </ul>"},{"location":"perception/autoware_tensorrt_bevformer/TensorRT/#multi-scale-deformable-attention","title":"Multi-scale Deformable Attention","text":"OP Name Attributes Inputs Outputs FP32 Speed FP16 Speed INT8/FP16 Speed Half Type Tensor Format Test Device MultiScaleDeformableAttnTRT - value: Tvalue_spatial_shapes: Tsampling_locations: Tattention_weights: T output: T x1 x1.3 x3.2 nv_half kLinear RTX 2080Ti MultiScaleDeformableAttnTRT2 - value: Tvalue_spatial_shapes: Tvalue_level_start_index: Tsampling_locations: Tattention_weights: T output: T x1 x2.0 x2.7 nv_half2 kLinear RTX 2080Ti"},{"location":"perception/autoware_tensorrt_bevformer/TensorRT/#inputs_1","title":"Inputs","text":"<ul> <li> <p>value: T[float/half/half2/int8]</p> <p>Tensor shape: <code>[N, num_keys, mum_heads, channel]</code></p> </li> </ul> <ul> <li> <p>value_spatial_shapes: T[int32]</p> <p>Spatial shape of each feature map, has shape <code>[num_levels, 2]</code>, last dimension 2 represent (h, w)</p> </li> </ul> <ul> <li> <p>reference_points: T[float/half2]</p> <p>The reference points.</p> <p>Tensor shape: <code>[N, num_queries, 1, points_per_group * 2]</code></p> </li> </ul> <ul> <li> <p>sampling_offsets: T[float/half/half2/int8]</p> <p>The offset of sampling points.</p> <p>Tensor shape: <code>[N, num_queries, num_heads, num_levels * num_points * 2]</code></p> </li> </ul> <ul> <li> <p>attention_weights: T[float/half/int8]</p> <p>The weight of sampling points used when calculate the attention (before softmax), has shape<code>[N ,num_queries, num_heads, num_levels * num_points]</code>.</p> </li> </ul>"},{"location":"perception/autoware_tensorrt_bevformer/TensorRT/#attributes_1","title":"Attributes","text":"<p>\u200b -</p>"},{"location":"perception/autoware_tensorrt_bevformer/TensorRT/#outputs_1","title":"Outputs","text":"<ul> <li> <p>output: T[float/half/int8]</p> <p>Tensor shape: <code>[N, num_queries, mum_heads, channel]</code></p> </li> </ul>"},{"location":"perception/autoware_tensorrt_bevformer/TensorRT/#modulated-deformable-conv2d","title":"Modulated Deformable Conv2d","text":"OP Name Attributes Inputs Outputs FP32 Speed FP16 Speed INT8/FP16 Speed Half Type Tensor Format Test Device ModulatedDeformableConv2dTRT stride: int[2]padding: int[2]dilation: int[2]groups: intdeform_groups: int input: Toffset: Tmask: Tweight: Tbias: T (optional) output: T x1 x2.9 x3.7 nv_half kLinear, kCHW4 RTX 2080Ti ModulatedDeformableConv2dTRT2 stride: int[2]padding: int[2]dilation: int[2]groups: intdeform_groups: int input: Toffset: Tmask: Tweight: Tbias: T (optional) output: T x1 x3.5 x3.7 nv_half2 kLinear, kCHW2, kCHW4 RTX 2080Ti"},{"location":"perception/autoware_tensorrt_bevformer/TensorRT/#inputs_2","title":"Inputs","text":"<ul> <li> <p>input: T[float/half/half2/int8]</p> <p>Tensor shape: <code>[N, C_in, H_in, W_in]</code></p> </li> </ul> <ul> <li> <p>offset: T[float/half/half2/int8]</p> <p>Tensor shape: <code>[N, deform_groups*K_h*K_w*2, H_out, W_out]</code></p> </li> </ul> <ul> <li> <p>mask: T[float/half/half2/int8]</p> <p>Tensor shape: <code>[N, deform_groups*K_h*K_w, H_out, W_out]</code></p> </li> </ul> <ul> <li> <p>weight: T[float/half/half2/int8]</p> <p>Tensor shape: <code>[C_out, C_in/groups, K_h, K_w]</code></p> </li> </ul> <ul> <li> <p>bias: T[float/half/half2] (optional)</p> <p>Tensor shape: <code>[C_out]</code></p> </li> </ul>"},{"location":"perception/autoware_tensorrt_bevformer/TensorRT/#attributes_2","title":"Attributes","text":"<ul> <li> <p>stride: int[2]</p> <p>Same as torch.nn.Conv2d.</p> </li> </ul> <ul> <li> <p>padding: int[2]</p> <p>Same as torch.nn.Conv2d.</p> </li> </ul> <ul> <li> <p>dilation: int[2]</p> <p>Same as torch.nn.Conv2d.</p> </li> </ul> <ul> <li> <p>groups: int</p> <p>Same as torch.nn.Conv2d.</p> </li> </ul> <ul> <li> <p>deform_groups: int</p> <p>Deformable conv2d groups.</p> </li> </ul>"},{"location":"perception/autoware_tensorrt_bevformer/TensorRT/#outputs_2","title":"Outputs","text":"<ul> <li> <p>output: T[float/half/half2/int8]</p> <p>Tensor shape: <code>[N, C_out, H_out, W_out]</code></p> </li> </ul> <p>NOTE: Values (C_in / groups) and (C_in / deform_groups) should be even numbers.</p>"},{"location":"perception/autoware_tensorrt_bevformer/TensorRT/#rotate","title":"Rotate","text":"OP Name Attributes Inputs Outputs FP32 Speed FP16 Speed INT8/FP16 Speed Half Type Tensor Format Test Device RotateTRT interpolation: int img: Tangle: Tcenter: T output: T x1 X1.8 X4.4 nv_half kLinear, kCHW4 RTX 2080Ti RotateTRT2 interpolation: int img: Tangle: Tcenter: T output: T x1 x2.2 x4.4 nv_half2 kLinear, kCHW2, kCHW4 RTX 2080Ti"},{"location":"perception/autoware_tensorrt_bevformer/TensorRT/#inputs_3","title":"Inputs","text":"<ul> <li> <p>img: T[float/half/half2/int8]</p> <p>Tensor shape: <code>[C, H, W]</code></p> </li> </ul> <ul> <li> <p>angle: T[float/half/half2]</p> <p>Tensor shape: <code>[1]</code></p> </li> </ul> <ul> <li> <p>center: T[float/half/half2]</p> <p>Tensor shape: <code>[2]</code></p> </li> </ul>"},{"location":"perception/autoware_tensorrt_bevformer/TensorRT/#attributes_3","title":"Attributes","text":"<ul> <li> <p>interpolation: int</p> <p>Interpolation mode to calculate output values. (0: <code>bilinear</code> , 1: <code>nearest</code>)</p> </li> </ul>"},{"location":"perception/autoware_tensorrt_bevformer/TensorRT/#outputs_3","title":"Outputs","text":"<ul> <li> <p>output: T[float/half/half2/int8]</p> <p>Tensor shape: <code>[C, H, W]</code></p> </li> </ul>"},{"location":"perception/autoware_tensorrt_bevformer/TensorRT/#inverse","title":"Inverse","text":"OP Name Attributes Inputs Outputs Tensor Format Test Device InverseTRT - input: T[float] output: T[float] kLinear RTX 2080Ti"},{"location":"perception/autoware_tensorrt_bevformer/TensorRT/#inputs_4","title":"Inputs","text":"<ul> <li> <p>input: T[float]</p> <p>Tensor shape: <code>[B, C, H, W]</code></p> </li> </ul>"},{"location":"perception/autoware_tensorrt_bevformer/TensorRT/#outputs_4","title":"Outputs","text":"<ul> <li> <p>output: T[float]</p> <p>Tensor shape: <code>[B, C, H, W]</code></p> </li> </ul>"},{"location":"perception/autoware_tensorrt_bevformer/TensorRT/#bev-pool","title":"BEV Pool","text":"OP Name Attributes Inputs Outputs FP32 Speed FP16 Speed INT8 Speed Half Type Tensor Format Test Device BEVPoolV2TRT out_height: intout_width: int depth: Tfeat: Tranks_depth: Tranks_feat: T ranks_bev: T interval_starts: Tinterval_lengths: T output: T x1 X1.1 X2.1 nv_half kLinear RTX 2080Ti BEVPoolV2TRT2 out_height: intout_width: int depth: Tfeat: Tranks_depth: Tranks_feat: T ranks_bev: T interval_starts: Tinterval_lengths: T output: T x1 x1.4 X2.1 nv_half2 kLinear RTX 2080Ti"},{"location":"perception/autoware_tensorrt_bevformer/TensorRT/#inputs_5","title":"Inputs","text":"<ul> <li> <p>depth: T[float/half/half2/int8]</p> <p>Tensor shape: <code>[Cam, D, H, W]</code></p> </li> </ul> <ul> <li> <p>feat: T[float/half/half2/int8]</p> <p>Tensor shape: <code>[Cam, H, W, C]</code></p> </li> </ul> <ul> <li>ranks_depth: T[int32]</li> </ul> <ul> <li>ranks_feat: T[int32]</li> </ul> <ul> <li>ranks_bev: T[int32]</li> </ul> <ul> <li>interval_starts: T[int32]</li> </ul> <ul> <li>interval_lengths: T[int32]</li> </ul>"},{"location":"perception/autoware_tensorrt_bevformer/TensorRT/#attributes_4","title":"Attributes","text":"<ul> <li> <p>out_height: int</p> <p>BEV feature height</p> </li> </ul> <ul> <li> <p>out_width: int</p> <p>BEV feature width</p> </li> </ul>"},{"location":"perception/autoware_tensorrt_bevformer/TensorRT/#outputs_5","title":"Outputs","text":"<ul> <li> <p>output: T[float/half/half2/int8]</p> <p>Tensor shape: <code>[1, out_height, out_width, C]</code></p> </li> </ul>"},{"location":"perception/autoware_tensorrt_bevformer/TensorRT/#multi-head-attention","title":"Multi-Head Attention","text":"OP Name Inputs Outputs FP32 Speed NHMA FP16 Speed NHMA FP32 Speed FHMA FP16 Speed FHMA INT8 Speed FHMA Half Type Test Device QKVTRT query: Tkey: Tvalue: T output: T x1 X2.0 x4.6 x6.1 x8.2 nv_half RTX 2080Ti QKVTRT2 query: Tkey: Tvalue: T output: T x1 X2.1 x4.6 x6.3 x8.2 nv_half2 RTX 2080Ti"},{"location":"perception/autoware_tensorrt_bevformer/TensorRT/#inputs_6","title":"Inputs","text":"<ul> <li> <p>query: T[float/half/half2/int8]</p> <p>Tensor shape: <code>[batch, q_len, channel]</code></p> </li> </ul> <ul> <li> <p>key: T[float/half/half2/int8]</p> <p>Tensor shape: <code>[batch, kv_len, channel]</code></p> </li> </ul> <ul> <li> <p>value: T[float/half/half2/int8]</p> <p>Tensor shape: <code>[batch, kv_len, channel]</code></p> </li> </ul>"},{"location":"perception/autoware_tensorrt_bevformer/TensorRT/#attributes_5","title":"Attributes","text":"<p>\u200b -</p>"},{"location":"perception/autoware_tensorrt_bevformer/TensorRT/#outputs_6","title":"Outputs","text":"<ul> <li> <p>output: T[float/half/half2/int8]</p> <p>Tensor shape: <code>[batch, q_len, channel]</code></p> </li> </ul> <p>NOTE: If <code>q_len</code> and <code>kv_len</code> are both multiples of 64, the plugin will run with Flash Multi-Head Attention (FMHA), else Naive Multi-Head Attention (NMHA).</p>"},{"location":"perception/autoware_tensorrt_classifier/","title":"TensorRT Classification for Efficient Dynamic Batched Inference","text":""},{"location":"perception/autoware_tensorrt_classifier/#tensorrt-classification-for-efficient-dynamic-batched-inference","title":"TensorRT Classification for Efficient Dynamic Batched Inference","text":""},{"location":"perception/autoware_tensorrt_classifier/#purpose","title":"Purpose","text":"<p>This package classifies arbitrary categories using TensorRT for efficient and faster inference. Specifically, this optimizes preprocessing for efficient inference on embedded platform. Moreover, we support dynamic batched inference in GPUs and DLAs.</p>"},{"location":"perception/autoware_tensorrt_common/","title":"autoware_tensorrt_common","text":""},{"location":"perception/autoware_tensorrt_common/#autoware_tensorrt_common","title":"autoware_tensorrt_common","text":"<p>This package provides a high-level API to work with TensorRT. This library simplifies the process of loading, building, and executing TensorRT inference engines using ONNX models. It also includes utilities for profiling and managing TensorRT execution contexts, making it easier to integrate TensorRT-based packages in Autoware.</p>"},{"location":"perception/autoware_tensorrt_common/#usage","title":"Usage","text":"<p>Here is an example usage of the library. For the full API documentation, please refer to the doxygen documentation (see header file).</p> <pre><code>#include &lt;autoware/tensorrt_common/tensorrt_common.hpp&gt;\n\n#include &lt;memory&gt;\n#include &lt;utility&gt;\n#include &lt;vector&gt;\n\nusing autoware::tensorrt_common::TrtCommon;\nusing autoware::tensorrt_common::TrtCommonConfig;\nusing autoware::tensorrt_common::TensorInfo;\nusing autoware::tensorrt_common::NetworkIO;\nusing autoware::tensorrt_common::ProfileDims;\n\nstd::unique_ptr&lt;TrtCommon&gt; trt_common_;\n</code></pre>"},{"location":"perception/autoware_tensorrt_common/#create-a-tensorrt-common-instance-and-setup-engine","title":"Create a tensorrt common instance and setup engine","text":"<ul> <li>With minimal configuration.</li> </ul> <pre><code>trt_common_ = std::make_unique&lt;TrtCommon&gt;(TrtCommonConfig(\"/path/to/onnx/model.onnx\"));\ntrt_common_-&gt;setup();\n</code></pre> <ul> <li>With full configuration.</li> </ul> <pre><code>trt_common_ = std::make_unique&lt;TrtCommon&gt;(TrtCommonConfig(\"/path/to/onnx/model.onnx\", \"fp16\", \"/path/to/engine/model.engine\", (1ULL &lt;&lt; 30U), -1, false));\n\nstd::vector&lt;NetworkIO&gt; network_io{\n  NetworkIO(\"sample_input\", {3, {-1, 64, 512}}), NetworkIO(\"sample_output\", {1, {50}})};\nstd::vector&lt;ProfileDims&gt; profile_dims{\n  ProfileDims(\"sample_input\", {3, {1, 64, 512}}, {3, {3, 64, 512}}, {3, {9, 64, 512}})};\n\nauto network_io_ptr = std::make_unique&lt;std::vector&lt;NetworkIO&gt;&gt;(network_io);\nauto profile_dims_ptr = std::make_unique&lt;std::vector&lt;ProfileDims&gt;&gt;(profile_dims);\n\ntrt_common_-&gt;setup(std::move(profile_dims_ptr), std::move(network_io_ptr));\n</code></pre> <p>By defining network IO names and dimensions, an extra shapes validation will be performed after building / loading engine. This is useful to ensure the model is compatible with current code for preprocessing as it might consists of operations dependent on tensor shapes.</p> <p>Profile dimension is used to specify the min, opt, and max dimensions for dynamic shapes.</p> <p>Network IO or / and profile dimensions can be omitted if not needed.</p>"},{"location":"perception/autoware_tensorrt_common/#setting-input-and-output-tensors","title":"Setting input and output tensors","text":"<pre><code>bool success = true;\nsuccess &amp;= trt_common_-&gt;setTensor(\"sample_input\", sample_input_d_.get(), nvinfer1::Dims{3, {var_size, 64, 512}});\nsuccess &amp;= trt_common_-&gt;setTensor(\"sample_output\", sample_output_d_.get());\nreturn success;\n</code></pre>"},{"location":"perception/autoware_tensorrt_common/#execute-inference","title":"Execute inference","text":"<pre><code>auto success = trt_common_-&gt;enqueueV3(stream_);\nreturn success;\n</code></pre>"},{"location":"perception/autoware_tensorrt_plugins/","title":"autoware_tensorrt_plugins","text":""},{"location":"perception/autoware_tensorrt_plugins/#autoware_tensorrt_plugins","title":"autoware_tensorrt_plugins","text":""},{"location":"perception/autoware_tensorrt_plugins/#purpose","title":"Purpose","text":"<p>The <code>autoware_tensorrt_plugins</code> package extends the operations available in TensorRT via plugins.</p>"},{"location":"perception/autoware_tensorrt_plugins/#algorithms","title":"Algorithms","text":"<p>The following operations are implemented:</p>"},{"location":"perception/autoware_tensorrt_plugins/#sparse-convolutions","title":"Sparse convolutions","text":"<p>We provide a wrapper for spconv (please see the correspondent package for details about the algorithms involved). This requires the installation of <code>spconv_cpp</code> which is automatically installed in autoware's setup script. If needed, the user can also build and install it using the repository's instructions.</p>"},{"location":"perception/autoware_tensorrt_plugins/#argsort","title":"Argsort","text":"<p>We provide an implementation for the <code>Argsort</code> operation as a plugin since the <code>TopK</code> TensorRT implementation has limitations in the size of elements it can handle.</p>"},{"location":"perception/autoware_tensorrt_plugins/#bev-pool","title":"BEV Pool","text":"<p>We provide a wrapper for the <code>bev_pool</code> operation presented in BEVFusion. Please refer to the original paper for specific details.</p>"},{"location":"perception/autoware_tensorrt_plugins/#scatter-operations","title":"Scatter operations","text":"<p>We provide a wrapper for the <code>segment_csr</code> operation presented in torch_scatter. Please refer to the original code for specific details.</p>"},{"location":"perception/autoware_tensorrt_plugins/#unique","title":"Unique","text":"<p>While ONNX supports the unique operation, TensorRT does not provide an implementation. For this reason we implement <code>Unique</code> as <code>CustomUnique</code> to avoid name classes. The implementation mostly follows torch_scatter implementation. Please refer to the original code for specific details.</p>"},{"location":"perception/autoware_tensorrt_plugins/#multi-scale-deformable-attention","title":"Multi-Scale Deformable Attention","text":"<p>The <code>MultiScaleDeformableAttentionPlugin</code> implements the multi-scale deformable attention mechanism introduced in Deformable DETR. This operation is crucial for vision transformers that need to attend to multiple scales and spatial locations efficiently.</p> <p>Key features:</p> <ul> <li>Supports multi-scale feature maps with different resolutions</li> <li>Enables learning of sampling offsets and attention weights</li> <li>Optimized CUDA implementation for efficient GPU execution</li> <li>Supports both FP32 and FP16 precision</li> </ul> <p>Inputs:</p> <ol> <li><code>value</code>: Feature maps at different scales (B, L, M, D)</li> <li><code>spatial_shapes</code>: Spatial dimensions of each scale (N, 2)</li> <li><code>level_start_index</code>: Starting indices for each scale (N,)</li> <li><code>sampling_loc</code>: Learned sampling locations (B, Q, M, L, P, 2)</li> <li><code>attn_weight</code>: Learned attention weights (B, Q, M, L, P)</li> </ol> <p>Output:</p> <ul> <li>Attended features (B, Q, M*D)</li> </ul>"},{"location":"perception/autoware_tensorrt_plugins/#rotate","title":"Rotate","text":"<p>The <code>RotatePlugin</code> provides efficient image rotation functionality with support for different interpolation methods. This is useful for data augmentation and geometric transformations in perception pipelines.</p> <p>Key features:</p> <ul> <li>Supports bilinear and nearest neighbor interpolation</li> <li>Arbitrary rotation angles around a specified center point</li> <li>Optimized CUDA kernels for both FP32 and FP16 precision</li> <li>Handles boundary conditions properly</li> </ul> <p>Inputs:</p> <ol> <li><code>input</code>: Input image tensor (C, H, W)</li> <li><code>angle</code>: Rotation angle in degrees (scalar)</li> <li><code>center</code>: Center of rotation (2,)</li> </ol> <p>Output:</p> <ul> <li>Rotated image with same dimensions as input</li> </ul> <p>Parameters:</p> <ul> <li><code>interpolation</code>: Interpolation mode (0 = bilinear, 1 = nearest)</li> </ul>"},{"location":"perception/autoware_tensorrt_plugins/#select-and-pad","title":"Select and Pad","text":"<p>The <code>SelectAndPadPlugin</code> enables conditional selection and padding of tensor elements based on flags. This is particularly useful for dynamic batching scenarios where sequences have variable lengths.</p> <p>Key features:</p> <ul> <li>Efficiently selects valid elements based on boolean flags</li> <li>Pads output to a fixed size with invalid tokens</li> <li>Uses CUB library for optimized GPU selection operations</li> <li>Supports both FP32 and FP16 precision</li> </ul> <p>Inputs:</p> <ol> <li><code>feat</code>: Input features (B, Q, C)</li> <li><code>flags</code>: Selection flags indicating valid elements (Q,)</li> <li><code>invalid</code>: Padding value for invalid positions (C,)</li> </ol> <p>Output:</p> <ul> <li>Selected and padded features (B, P, C)</li> </ul> <p>Parameters:</p> <ul> <li><code>P</code>: Fixed output size for padding</li> </ul>"},{"location":"perception/autoware_tensorrt_plugins/#licenses","title":"Licenses","text":""},{"location":"perception/autoware_tensorrt_plugins/#multi-scale-deformable-attention_1","title":"Multi-Scale Deformable Attention","text":"<p>The implementation of multi-scale deformable attention is derived from Deformable DETR and modified for TensorRT plugin usage. The original implementation is provided under Apache License 2.0:</p> <p>Copyright (c) 2020 SenseTime. All Rights Reserved.</p> <p>Licensed under the Apache License, Version 2.0</p>"},{"location":"perception/autoware_tensorrt_plugins/#rotate-and-select-and-pad","title":"Rotate and Select and Pad","text":"<p>The rotate and select_and_pad plugin implementations are derived from NVIDIA's reference implementations and are provided under Apache License 2.0:</p> <p>Copyright (c) 1993-2025 NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved.</p> <p>Licensed under the Apache License, Version 2.0</p>"},{"location":"perception/autoware_tensorrt_plugins/#scatter","title":"Scatter","text":"<p>The codes under the <code>src/scatter_ops</code> and <code>include/autoware/scatter_ops</code> directory are copied and modified from the original implementation and TensorRT adaptation. The original codes belong to the MIT license (original implementation) and Apache License 2.0, whereas further modifications are provided under Apache License 2.0.</p> <p>MIT License Copyright (c) 2020 Matthias Fey matthias.fey@tu-dortmund.de</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"perception/autoware_tensorrt_plugins/#unique_1","title":"Unique","text":"<p>The codes under the <code>src/unique_ops</code> and <code>include/autoware/unique_ops</code> directory are copied and modified from Pytorch's implementation. The original codes is provided under Pytorch's license as follows:</p> <p>Pytorch License</p> <p>From PyTorch:</p> <p>Copyright (c) 2016- Facebook, Inc (Adam Paszke) Copyright (c) 2014- Facebook, Inc (Soumith Chintala) Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert) Copyright (c) 2012-2014 Deepmind Technologies (Koray Kavukcuoglu) Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu) Copyright (c) 2011-2013 NYU (Clement Farabet) Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston) Copyright (c) 2006 Idiap Research Institute (Samy Bengio) Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)</p> <p>From Caffe2:</p> <p>Copyright (c) 2016-present, Facebook Inc. All rights reserved.</p> <p>All contributions by Facebook: Copyright (c) 2016 Facebook Inc.</p> <p>All contributions by Google: Copyright (c) 2015 Google Inc. All rights reserved.</p> <p>All contributions by Yangqing Jia: Copyright (c) 2015 Yangqing Jia All rights reserved.</p> <p>All contributions by Kakao Brain: Copyright 2019-2020 Kakao Brain</p> <p>All contributions by Cruise LLC: Copyright (c) 2022 Cruise LLC. All rights reserved.</p> <p>All contributions by Tri Dao: Copyright (c) 2024 Tri Dao. All rights reserved.</p> <p>All contributions by Arm: Copyright (c) 2021, 2023-2024 Arm Limited and/or its affiliates</p> <p>All contributions from Caffe: Copyright(c) 2013, 2014, 2015, the respective contributors All rights reserved.</p> <p>All other contributions: Copyright(c) 2015, 2016 the respective contributors All rights reserved.</p> <p>Caffe2 uses a copyright model similar to Caffe: each contributor holds copyright over their contributions to Caffe2. The project versioning records all such contribution and copyright details. If a contributor wants to further mark their specific copyright on a particular contribution, they should indicate their copyright solely in the commit message of the change when it is committed.</p> <p>All rights reserved.</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:</p> <ol> <li>Redistributions of source code must retain the above copyright    notice, this list of conditions and the following disclaimer.</li> <li>Redistributions in binary form must reproduce the above copyright    notice, this list of conditions and the following disclaimer in the    documentation and/or other materials provided with the distribution.</li> <li>Neither the names of Facebook, Deepmind Technologies, NYU, NEC Laboratories America    and IDIAP Research Institute nor the names of its contributors may be    used to endorse or promote products derived from this software without    specific prior written permission.</li> </ol> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>"},{"location":"perception/autoware_tensorrt_yolox/","title":"autoware_tensorrt_yolox","text":""},{"location":"perception/autoware_tensorrt_yolox/#autoware_tensorrt_yolox","title":"autoware_tensorrt_yolox","text":""},{"location":"perception/autoware_tensorrt_yolox/#purpose","title":"Purpose","text":"<p>This package detects target objects e.g., cars, trucks, bicycles, and pedestrians and segment target objects such as cars, trucks, buses and pedestrian, building, vegetation, road, sidewalk on a image based on YOLOX model with multi-header structure.</p> <p>Additionally, the package also supports traffic light detection by switching onnx file which target classes listed on respective <code>label_file</code>. Currently 0: <code>unknown</code>, 1: <code>car_traffic_light</code> and 2: <code>pedestrian_traffic_light</code>.</p>"},{"location":"perception/autoware_tensorrt_yolox/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"perception/autoware_tensorrt_yolox/#cite","title":"Cite","text":"<p>Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, Jian Sun, \"YOLOX: Exceeding YOLO Series in 2021\", arXiv preprint arXiv:2107.08430, 2021 [ref]</p>"},{"location":"perception/autoware_tensorrt_yolox/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_tensorrt_yolox/#input","title":"Input","text":"Name Type Description <code>in/image</code> <code>sensor_msgs/Image</code> The input image"},{"location":"perception/autoware_tensorrt_yolox/#output","title":"Output","text":"Name Type Description <code>out/objects</code> <code>tier4_perception_msgs/DetectedObjectsWithFeature</code> The detected objects or traffic light with 2D bounding boxes <code>out/image</code> <code>sensor_msgs/Image</code> The image with 2D bounding boxes for visualization <code>out/mask</code> <code>sensor_msgs/Image</code> The semantic segmentation mask (only effective for semseg model) <code>out/color_mask</code> <code>sensor_msgs/Image</code> The colorized image of semantic segmentation mask for visualization (only effective for semseg model)"},{"location":"perception/autoware_tensorrt_yolox/#parameters","title":"Parameters","text":""},{"location":"perception/autoware_tensorrt_yolox/#yolox_s_plus_opt","title":"yolox_s_plus_opt","text":"Name Type Description Default Range score_threshold float A threshold value of existence probability score, all of objects with score less than this threshold are ignored. 0.35 \u22650.0\u22641.0 nms_threshold float A threshold value of NMS. 0.7 \u22650.0\u22641.0 precision string Operation precision to be used on inference. Valid value is one of: [fp32, fp16, int8]. int8 N/A calibration_algorithm string Calibration algorithm to be used for quantization when precision==int8. Valid value is one of: [Entropy, (Legacy Percentile), MinMax]. Entropy dla_core_id float If positive ID value is specified, the node assign inference task to the DLA core. -1 N/A quantize_first_layer boolean If true, set the operating precision for the first (input) layer to be fp16. This option is valid only when precision==int8. False N/A quantize_last_layer boolean If true, set the operating precision for the last (output) layer to be fp16. This option is valid only when precision==int8. False N/A profile_per_layer boolean If true, profiler function will be enabled. Since the profile function may affect execution speed, it is recommended to set this flag true only for development purpose. False N/A clip_value float If positive value is specified, the value of each layer output will be clipped between [0.0, clip_value]. This option is valid only when precision==int8 and used to manually specify the dynamic range instead of using any calibration. 6.0 N/A preprocess_on_gpu boolean If true, pre-processing is performed on GPU. True N/A gpu_id integer GPU ID for selecting CUDA device 0 N/A calibration_image_list_path string Path to a file which contains path to images. Those images will be used for int8 quantization. N/A"},{"location":"perception/autoware_tensorrt_yolox/#yolox_tiny","title":"yolox_tiny","text":"Name Type Description Default Range score_threshold float A threshold value of existence probability score, all of objects with score less than this threshold are ignored. 0.35 \u22650.0\u22641.0 nms_threshold float A threshold value of NMS. 0.7 \u22650.0\u22641.0 precision string Operation precision to be used on inference. Valid value is one of: [fp32, fp16, int8]. fp16 N/A calibration_algorithm string Calibration algorithm to be used for quantization when precision==int8. Valid value is one of: [Entropy, (Legacy Percentile), MinMax]. MinMax dla_core_id float If positive ID value is specified, the node assign inference task to the DLA core. -1 N/A quantize_first_layer boolean If true, set the operating precision for the first (input) layer to be fp16. This option is valid only when precision==int8. False N/A quantize_last_layer boolean If true, set the operating precision for the last (output) layer to be fp16. This option is valid only when precision==int8. False N/A profile_per_layer boolean If true, profiler function will be enabled. Since the profile function may affect execution speed, it is recommended to set this flag true only for development purpose. False N/A clip_value float If positive value is specified, the value of each layer output will be clipped between [0.0, clip_value]. This option is valid only when precision==int8 and used to manually specify the dynamic range instead of using any calibration. 0.0 N/A preprocess_on_gpu boolean If true, pre-processing is performed on GPU. True N/A gpu_id integer GPU ID for selecting CUDA device 0 N/A calibration_image_list_path string Path to a file which contains path to images. Those images will be used for int8 quantization. N/A"},{"location":"perception/autoware_tensorrt_yolox/#yolox_traffic_light_detector","title":"yolox_traffic_light_detector","text":"Name Type Description Default Range score_threshold float A threshold value of existence probability score, all of objects with score less than this threshold are ignored. 0.35 \u22650.0\u22641.0 nms_threshold float A threshold value of NMS. 0.7 \u22650.0\u22641.0 precision string Operation precision to be used on inference. Valid value is one of: [fp32, fp16, int8]. fp16 N/A calibration_algorithm string Calibration algorithm to be used for quantization when precision==int8. Valid value is one of: [Entropy, (Legacy Percentile), MinMax]. MinMax dla_core_id float If positive ID value is specified, the node assign inference task to the DLA core. -1 N/A quantize_first_layer boolean If true, set the operating precision for the first (input) layer to be fp16. This option is valid only when precision==int8. False N/A quantize_last_layer boolean If true, set the operating precision for the last (output) layer to be fp16. This option is valid only when precision==int8. False N/A profile_per_layer boolean If true, profiler function will be enabled. Since the profile function may affect execution speed, it is recommended to set this flag true only for development purpose. False N/A clip_value float If positive value is specified, the value of each layer output will be clipped between [0.0, clip_value]. This option is valid only when precision==int8 and used to manually specify the dynamic range instead of using any calibration. 0.0 N/A preprocess_on_gpu boolean If true, pre-processing is performed on GPU. True N/A gpu_id integer GPU ID for selecting CUDA device 0 N/A calibration_image_list_path string Path to a file which contains path to images. Those images will be used for int8 quantization. N/A"},{"location":"perception/autoware_tensorrt_yolox/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>The label contained in detected 2D bounding boxes (i.e., <code>out/objects</code>) will be either one of the followings:</p> <ul> <li>CAR</li> <li>PEDESTRIAN (\"PERSON\" will also be categorized as \"PEDESTRIAN\")</li> <li>BUS</li> <li>TRUCK</li> <li>BICYCLE</li> <li>MOTORCYCLE</li> </ul> <p>or</p> <ul> <li>UNKNOWN</li> <li>CAR_TRAFFIC_LIGHT</li> <li>PEDESTRIAN_TRAFFIC_LIGHT</li> </ul> <p>for traffic light detector onnx model.</p> <p>If other labels (case insensitive) are contained in the file specified via the <code>label_file</code> parameter, those are labeled as <code>UNKNOWN</code>, while detected rectangles are drawn in the visualization result (<code>out/image</code>).</p> <p>The semantic segmentation mask is a gray image whose each pixel is index of one following class:</p> index semantic name 0 road 1 building 2 wall 3 obstacle 4 traffic_light 5 traffic_sign 6 person 7 vehicle 8 bike 9 road 10 sidewalk 11 roadPaint 12 curbstone 13 crosswalk_others 14 vegetation 15 sky"},{"location":"perception/autoware_tensorrt_yolox/#onnx-model","title":"Onnx model","text":"<p>A sample model (named <code>yolox-tiny.onnx</code>) is downloaded by ansible script on env preparation stage, if not, please, follow Manual downloading of artifacts. To accelerate Non-maximum-suppression (NMS), which is one of the common post-process after object detection inference, <code>EfficientNMS_TRT</code> module is attached after the ordinal YOLOX (tiny) network. The <code>EfficientNMS_TRT</code> module contains fixed values for <code>score_threshold</code> and <code>nms_threshold</code> in it, hence these parameters are ignored when users specify ONNX models including this module.</p> <p>This package accepts both <code>EfficientNMS_TRT</code> attached ONNXs and models published from the official YOLOX repository (we referred to them as \"plain\" models).</p> <p>In addition to <code>yolox-tiny.onnx</code>, a custom model named <code>yolox-sPlus-opt-pseudoV2-T4-960x960-T4-seg16cls</code> is either available. This model is multi-header structure model which is based on YOLOX-s and tuned to perform more accurate detection with almost comparable execution speed with <code>yolox-tiny</code>. To get better results with this model, users are recommended to use some specific running arguments such as <code>precision:=int8</code>, <code>calibration_algorithm:=Entropy</code>, <code>clip_value:=6.0</code>. Users can refer <code>launch/yolox_sPlus_opt.launch.xml</code> to see how this model can be used. Beside detection result, this model also output image semantic segmentation result for pointcloud filtering purpose.</p> <p>All models are automatically converted to TensorRT format. These converted files will be saved in the same directory as specified ONNX files with <code>.engine</code> filename extension and reused from the next run. The conversion process may take a while (typically 10 to 20 minutes) and the inference process is blocked until complete the conversion, so it will take some time until detection results are published (even until appearing in the topic list) on the first run</p>"},{"location":"perception/autoware_tensorrt_yolox/#package-acceptable-model-generation","title":"Package acceptable model generation","text":"<p>To convert users' own model that saved in PyTorch's <code>pth</code> format into ONNX, users can exploit the converter offered by the official repository. For the convenience, only procedures are described below. Please refer the official document for more detail.</p>"},{"location":"perception/autoware_tensorrt_yolox/#for-plain-models","title":"For plain models","text":"<ol> <li> <p>Install dependency</p> <pre><code>git clone git@github.com:Megvii-BaseDetection/YOLOX.git\ncd YOLOX\npython3 setup.py develop --user\n</code></pre> </li> <li> <p>Convert pth into ONNX</p> <pre><code>python3 tools/export_onnx.py \\\n  --output-name YOUR_YOLOX.onnx \\\n  -f YOUR_YOLOX.py \\\n  -c YOUR_YOLOX.pth\n</code></pre> </li> </ol>"},{"location":"perception/autoware_tensorrt_yolox/#for-efficientnms_trt-embedded-models","title":"For EfficientNMS_TRT embedded models","text":"<ol> <li> <p>Install dependency</p> <pre><code>git clone git@github.com:Megvii-BaseDetection/YOLOX.git\ncd YOLOX\npython3 setup.py develop --user\npip3 install git+ssh://git@github.com/wep21/yolox_onnx_modifier.git --user\n</code></pre> </li> <li> <p>Convert pth into ONNX</p> <pre><code>python3 tools/export_onnx.py \\\n  --output-name YOUR_YOLOX.onnx \\\n  -f YOUR_YOLOX.py \\\n  -c YOUR_YOLOX.pth\n  --decode_in_inference\n</code></pre> </li> <li> <p>Embed <code>EfficientNMS_TRT</code> to the end of YOLOX</p> <pre><code>yolox_onnx_modifier YOUR_YOLOX.onnx -o YOUR_YOLOX_WITH_NMS.onnx\n</code></pre> </li> </ol>"},{"location":"perception/autoware_tensorrt_yolox/#label-file","title":"Label file","text":"<p>A sample label file (named <code>label.txt</code>) and semantic segmentation color map file (name <code>semseg_color_map.csv</code>) are also downloaded automatically during env preparation process (NOTE: This file is incompatible with models that output labels for the COCO dataset (e.g., models from the official YOLOX repository)).</p> <p>This file represents the correspondence between class index (integer outputted from YOLOX network) and class label (strings making understanding easier). This package maps class IDs (incremented from 0) with labels according to the order in this file.</p>"},{"location":"perception/autoware_tensorrt_yolox/#reference-repositories","title":"Reference repositories","text":"<ul> <li>https://github.com/Megvii-BaseDetection/YOLOX</li> <li>https://github.com/wep21/yolox_onnx_modifier</li> <li>https://github.com/tier4/trt-yoloXP</li> </ul>"},{"location":"perception/autoware_tracking_object_merger/","title":"Tracking Object Merger","text":""},{"location":"perception/autoware_tracking_object_merger/#tracking-object-merger","title":"Tracking Object Merger","text":""},{"location":"perception/autoware_tracking_object_merger/#purpose","title":"Purpose","text":"<p>This package try to merge two tracking objects from different sensor.</p>"},{"location":"perception/autoware_tracking_object_merger/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>Merging tracking objects from different sensor is a combination of data association and state fusion algorithms.</p> <p>Detailed process depends on the merger policy.</p>"},{"location":"perception/autoware_tracking_object_merger/#decorative_tracker_merger","title":"decorative_tracker_merger","text":"<p>In decorative_tracker_merger, we assume there are dominant tracking objects and sub tracking objects. The name <code>decorative</code> means that sub tracking objects are used to complement the main objects.</p> <p>Usually the dominant tracking objects are from LiDAR and sub tracking objects are from Radar or Camera.</p> <p>Here show the processing pipeline.</p> <p></p>"},{"location":"perception/autoware_tracking_object_merger/#time-sync","title":"time sync","text":"<p>Sub object(Radar or Camera) often has higher frequency than dominant object(LiDAR). So we need to sync the time of sub object to dominant object.</p> <p></p>"},{"location":"perception/autoware_tracking_object_merger/#data-association","title":"data association","text":"<p>In the data association, we use the following rules to determine whether two tracking objects are the same object.</p> <ul> <li>gating<ul> <li><code>distance gate</code>: distance between two tracking objects</li> <li><code>angle gate</code>: angle between two tracking objects</li> <li><code>mahalanobis_distance_gate</code>: Mahalanobis distance between two tracking objects</li> <li><code>min_iou_gate</code>: minimum IoU between two tracking objects</li> <li><code>max_velocity_gate</code>: maximum velocity difference between two tracking objects</li> </ul> </li> <li>score<ul> <li>score used in matching is equivalent to the distance between two tracking objects</li> </ul> </li> </ul>"},{"location":"perception/autoware_tracking_object_merger/#tracklet-update","title":"tracklet update","text":"<p>Sub tracking objects are merged into dominant tracking objects.</p> <p>Depends on the tracklet input sensor state, we update the tracklet state with different rules.</p> state\\priority 1st 2nd 3rd Kinematics except velocity LiDAR Radar Camera Forward velocity Radar LiDAR Camera Object classification Camera LiDAR Radar"},{"location":"perception/autoware_tracking_object_merger/#tracklet-management","title":"tracklet management","text":"<p>We use the <code>existence_probability</code> to manage tracklet.</p> <ul> <li>When we create a new tracklet, we set the <code>existence_probability</code> to \\(p_{sensor}\\) value.</li> <li>In each update with specific sensor, we set the <code>existence_probability</code> to \\(p_{sensor}\\) value.</li> <li>When tracklet does not have update with specific sensor, we reduce the <code>existence_probability</code> by <code>decay_rate</code></li> <li>Object can be published if <code>existence_probability</code> is larger than <code>publish_probability_threshold</code> and time from last update is smaller than <code>max_dt</code></li> <li>Object will be removed if <code>existence_probability</code> is smaller than <code>remove_probability_threshold</code> and time from last update is larger than <code>max_dt</code></li> </ul> <p></p> <p>These parameter can be set in <code>config/decorative_tracker_merger.param.yaml</code>.</p> <pre><code>tracker_state_parameter:\n  remove_probability_threshold: 0.3\n  publish_probability_threshold: 0.6\n  default_lidar_existence_probability: 0.7\n  default_radar_existence_probability: 0.6\n  default_camera_existence_probability: 0.6\n  decay_rate: 0.1\n  max_dt: 1.0\n</code></pre>"},{"location":"perception/autoware_tracking_object_merger/#inputparameters","title":"input/parameters","text":"topic name message type description <code>~input/main_object</code> <code>autoware_perception_msgs::TrackedObjects</code> Dominant tracking objects. Output will be published with this dominant object stamps. <code>~input/sub_object</code> <code>autoware_perception_msgs::TrackedObjects</code> Sub tracking objects. <code>output/object</code> <code>autoware_perception_msgs::TrackedObjects</code> Merged tracking objects. <code>debug/interpolated_sub_object</code> <code>autoware_perception_msgs::TrackedObjects</code> Interpolated sub tracking objects. <p>Default parameters are set in config/decorative_tracker_merger.param.yaml.</p> parameter name description default value <code>base_link_frame_id</code> base link frame id. This is used to transform the tracking object. \"base_link\" <code>time_sync_threshold</code> time sync threshold. If the time difference between two tracking objects is smaller than this value, we consider these two tracking objects are the same object. 0.05 <code>sub_object_timeout_sec</code> sub object timeout. If the sub object is not updated for this time, we consider this object is not exist. 0.5 <code>main_sensor_type</code> main sensor type. This is used to determine the dominant tracking object. \"lidar\" <code>sub_sensor_type</code> sub sensor type. This is used to determine the sub tracking object. \"radar\" <code>tracker_state_parameter</code> tracker state parameter. This is used to manage the tracklet. <ul> <li>the detail of <code>tracker_state_parameter</code> is described in tracklet management</li> </ul>"},{"location":"perception/autoware_tracking_object_merger/#tuning","title":"tuning","text":"<p>As explained in tracklet management, this tracker merger tend to maintain the both input tracking objects.</p> <p>If there are many false positive tracking objects,</p> <ul> <li>decrease <code>default_&lt;sensor&gt;_existence_probability</code> of that sensor</li> <li>increase <code>decay_rate</code></li> <li>increase <code>publish_probability_threshold</code> to publish only reliable tracking objects</li> </ul>"},{"location":"perception/autoware_tracking_object_merger/#equivalent_tracker_merger","title":"equivalent_tracker_merger","text":"<p>This is future work.</p>"},{"location":"perception/autoware_traffic_light_arbiter/","title":"autoware_traffic_light_arbiter","text":""},{"location":"perception/autoware_traffic_light_arbiter/#autoware_traffic_light_arbiter","title":"autoware_traffic_light_arbiter","text":""},{"location":"perception/autoware_traffic_light_arbiter/#purpose","title":"Purpose","text":"<p>This package receives traffic signals from perception and external (e.g., V2X) components and combines them using either a confidence-based or a external-preference based approach.</p>"},{"location":"perception/autoware_traffic_light_arbiter/#trafficlightarbiter","title":"TrafficLightArbiter","text":"<p>A node that merges traffic light/signal state from image recognition and external (e.g., V2X) systems to provide to a planning component. Merging is only applied to <code>elements</code> in the <code>TrafficLightGroup</code> msg, not to <code>predictions</code>.</p>"},{"location":"perception/autoware_traffic_light_arbiter/#signal-match-validator","title":"Signal Match Validator","text":"<p>When <code>enable_signal_matching</code> is set to true, this node validates the match between perception signals and external signals. The table below outlines how the matching process determines the output based on the combination of perception and external signal colors. Each cell represents the outcome when a specific color from a perception signal (columns) intersects with a color from an external signal (rows).</p> External \\ Perception RED AMBER GREEN UNKNOWN Not Received RED RED UNKNOWN UNKNOWN UNKNOWN UNKNOWN AMBER UNKNOWN AMBER UNKNOWN UNKNOWN UNKNOWN GREEN UNKNOWN UNKNOWN GREEN UNKNOWN UNKNOWN UNKNOWN UNKNOWN UNKNOWN UNKNOWN UNKNOWN UNKNOWN Not Received UNKNOWN UNKNOWN UNKNOWN UNKNOWN UNKNOWN"},{"location":"perception/autoware_traffic_light_arbiter/#source-priority","title":"Source priority","text":"<p>As an alternative to the signal matching option, one of the sources can be prioritized. There are three priority modes for signal selection: 'external' prioritizes external signals, 'perception' prioritizes perception signals, 'confidence' uses confidence-based selection.</p> <p>Note, this option will not be effective if signal matching is enabled.</p>"},{"location":"perception/autoware_traffic_light_arbiter/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_traffic_light_arbiter/#input","title":"Input","text":"Name Type Description ~/sub/vector_map autoware_map_msgs::msg::LaneletMapBin The vector map to get valid traffic signal ids. ~/sub/perception_traffic_signals autoware_perception_msgs::msg::TrafficLightGroupArray The traffic signals from the image recognition pipeline. ~/sub/external_traffic_signals autoware_perception_msgs::msg::TrafficLightGroupArray The traffic signals from an external system."},{"location":"perception/autoware_traffic_light_arbiter/#output","title":"Output","text":"Name Type Description ~/pub/traffic_signals autoware_perception_msgs::msg::TrafficLightGroupArray The merged traffic signal state."},{"location":"perception/autoware_traffic_light_arbiter/#parameters","title":"Parameters","text":"Name Type Description Default Range external_delay_tolerance float The duration in seconds an external message is considered valid for merging in comparison with current time. 5.0 N/A external_time_tolerance float The duration in seconds an external message is considered valid for merging. 5.0 N/A perception_time_tolerance float The duration in seconds a perception message is considered valid for merging. 1.0 N/A source_priority string Priority mode for signal selection: 'external' prioritizes external signals, 'perception' prioritizes perception signals, 'confidence' uses confidence-based selection. confidence ['external', 'perception', 'confidence'] enable_signal_matching boolean Decide whether to validate the match between perception signals and external signals. If set to true, verify that the colors match and only publish them if they are identical. False N/A"},{"location":"perception/autoware_traffic_light_category_merger/","title":"Index","text":"<p><code>autoware_traffic_light_category_merger</code></p>"},{"location":"perception/autoware_traffic_light_category_merger/#overview","title":"Overview","text":"<p><code>autoware_traffic_light_category_merger</code> receives the Traffic Light (TL) classification result from Car/Pedestrian classifiers to merge into single classification result. The expect ROIs TL without classification result will be filled as Unknown.</p>"},{"location":"perception/autoware_traffic_light_category_merger/#input-topics","title":"Input topics","text":"Name Type Description <code>input/car_signals</code> tier4_perception_msgs::msg::TrafficLightArray Car TLs classification <code>input/pedestrian_signals</code> tier4_perception_msgs::msg::TrafficLightArray Pedestrian TLs classification"},{"location":"perception/autoware_traffic_light_category_merger/#output-topics","title":"Output topics","text":"Name Type Description <code>output/traffic_signals</code> tier4_perception_msgs::msg::TrafficLightArray Car and Pedestrian TLs classification"},{"location":"perception/autoware_traffic_light_category_merger/#node-parameters","title":"Node parameters","text":"<p>N/A</p>"},{"location":"perception/autoware_traffic_light_classifier/","title":"autoware_traffic_light_classifier","text":""},{"location":"perception/autoware_traffic_light_classifier/#autoware_traffic_light_classifier","title":"autoware_traffic_light_classifier","text":""},{"location":"perception/autoware_traffic_light_classifier/#purpose","title":"Purpose","text":"<p><code>autoware_traffic_light_classifier</code> is a package for classifying traffic light labels using cropped image around a traffic light. This package has two classifier models: <code>cnn_classifier</code> and <code>hsv_classifier</code>.</p>"},{"location":"perception/autoware_traffic_light_classifier/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>If height and width of <code>~/input/rois</code> is <code>0</code>, color, shape, and confidence of <code>~/output/traffic_signals</code> become <code>UNKNOWN</code>, <code>CIRCLE</code>, and <code>0.0</code>. If <code>~/input/rois</code> is judged as backlight, color, shape, and confidence of <code>~/output/traffic_signals</code> become <code>UNKNOWN</code>, <code>UNKNOWN</code>, and <code>0.0</code>.</p>"},{"location":"perception/autoware_traffic_light_classifier/#cnn_classifier","title":"cnn_classifier","text":"<p>Traffic light labels are classified by EfficientNet-b1 or MobileNet-v2. We trained classifiers for vehicular signals and pedestrian signals separately. For vehicular signals, a total of 83400 (58600 for training, 14800 for evaluation and 10000 for test) TIER IV internal images of Japanese traffic lights were used for fine-tuning.</p> Name Input Size Test Accuracy EfficientNet-b1 128 x 128 99.76% MobileNet-v2 224 x 224 99.81% <p>For pedestrian signals, a total of 21199 (17860 for training, 2114 for evaluation and 1225 for test) TIER IV internal images of Japanese traffic lights were used for fine-tuning. The information of the models is listed here:</p> Name Input Size Test Accuracy EfficientNet-b1 128 x 128 97.89% MobileNet-v2 224 x 224 99.10%"},{"location":"perception/autoware_traffic_light_classifier/#hsv_classifier","title":"hsv_classifier","text":"<p>Traffic light colors (green, yellow and red) are classified in HSV model.</p>"},{"location":"perception/autoware_traffic_light_classifier/#about-label","title":"About Label","text":"<p>The message type is designed to comply with the unified road signs proposed at the Vienna Convention. This idea has been also proposed in Autoware.Auto.</p> <p>There are rules for naming labels that nodes receive. One traffic light is represented by the following character string separated by commas. <code>color1-shape1, color2-shape2</code> .</p> <p>For example, the simple red and red cross traffic light label must be expressed as \"red-circle, red-cross\".</p> <p>These colors and shapes are assigned to the message as follows: </p>"},{"location":"perception/autoware_traffic_light_classifier/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_traffic_light_classifier/#input","title":"Input","text":"Name Type Description <code>~/input/image</code> <code>sensor_msgs::msg::Image</code> input image <code>~/input/rois</code> <code>tier4_perception_msgs::msg::TrafficLightRoiArray</code> rois of traffic lights"},{"location":"perception/autoware_traffic_light_classifier/#output","title":"Output","text":"Name Type Description <code>~/output/traffic_signals</code> <code>tier4_perception_msgs::msg::TrafficLightArray</code> classified signals <code>~/output/debug/image</code> <code>sensor_msgs::msg::Image</code> image for debugging"},{"location":"perception/autoware_traffic_light_classifier/#parameters","title":"Parameters","text":""},{"location":"perception/autoware_traffic_light_classifier/#node-parameters","title":"Node Parameters","text":""},{"location":"perception/autoware_traffic_light_classifier/#car_traffic_light_classifier","title":"car_traffic_light_classifier","text":"Name Type Description Default Range approximate_sync boolean Enable or disable approximate synchronization. False N/A precision string Precision used for traffic light classifier inference. Valid values: [fp32, fp16, int8]. fp16 N/A mean array Mean values used for input normalization [R, G, B]. [123.675, 116.28, 103.53] N/A std array Standard deviation values used for input normalization [R, G, B]. [58.395, 57.12, 57.375] N/A backlight_threshold float If the intensity get grater than this overwrite with UNKNOWN in corresponding RoI. Note that, if the value is much higher, the node only overwrites in the harsher backlight situations. Therefore, If you wouldn't like to use this feature set this value to <code>1.0</code>. The value can be <code>[0.0, 1.0]</code>. The confidence of overwritten signal is set to <code>0.0</code>. 0.85 N/A classifier_type integer Type of classifier used. {0: hsv_filter, 1: cnn}. 1 N/A traffic_light_type integer Type of traffic light to classify. {0: car, 1: pedestrian}. 0 N/A"},{"location":"perception/autoware_traffic_light_classifier/#pedestrian_traffic_light_classifier","title":"pedestrian_traffic_light_classifier","text":"Name Type Description Default Range approximate_sync boolean Enable or disable approximate synchronization. False N/A precision string Precision used for traffic light classifier inference. Valid values: [fp32, fp16, int8]. fp16 N/A mean array Mean values used for input normalization [R, G, B]. [123.675, 116.28, 103.53] N/A std array Standard deviation values used for input normalization [R, G, B]. [58.395, 57.12, 57.375] N/A backlight_threshold float If the intensity get grater than this overwrite with UNKNOWN in corresponding RoI. Note that, if the value is much higher, the node only overwrites in the harsher backlight situations. Therefore, If you wouldn't like to use this feature set this value to <code>1.0</code>. The value can be <code>[0.0, 1.0]</code>. The confidence of overwritten signal is set to <code>0.0</code>. 0.85 N/A classifier_type integer Type of classifier used. {0: hsv_filter, 1: cnn}. 1 N/A traffic_light_type integer Type of traffic light to classify. {0: car, 1: pedestrian}. 1 N/A"},{"location":"perception/autoware_traffic_light_classifier/#core-parameters","title":"Core Parameters","text":""},{"location":"perception/autoware_traffic_light_classifier/#cnn_classifier_1","title":"cnn_classifier","text":"<p>Including this section</p>"},{"location":"perception/autoware_traffic_light_classifier/#hsv_classifier_1","title":"hsv_classifier","text":"Name Type Description <code>green_min_h</code> int the minimum hue of green color <code>green_min_s</code> int the minimum saturation of green color <code>green_min_v</code> int the minimum value (brightness) of green color <code>green_max_h</code> int the maximum hue of green color <code>green_max_s</code> int the maximum saturation of green color <code>green_max_v</code> int the maximum value (brightness) of green color <code>yellow_min_h</code> int the minimum hue of yellow color <code>yellow_min_s</code> int the minimum saturation of yellow color <code>yellow_min_v</code> int the minimum value (brightness) of yellow color <code>yellow_max_h</code> int the maximum hue of yellow color <code>yellow_max_s</code> int the maximum saturation of yellow color <code>yellow_max_v</code> int the maximum value (brightness) of yellow color <code>red_min_h</code> int the minimum hue of red color <code>red_min_s</code> int the minimum saturation of red color <code>red_min_v</code> int the minimum value (brightness) of red color <code>red_max_h</code> int the maximum hue of red color <code>red_max_s</code> int the maximum saturation of red color <code>red_max_v</code> int the maximum value (brightness) of red color"},{"location":"perception/autoware_traffic_light_classifier/#training-traffic-light-classifier-model","title":"Training Traffic Light Classifier Model","text":""},{"location":"perception/autoware_traffic_light_classifier/#overview","title":"Overview","text":"<p>This guide provides detailed instructions on training a traffic light classifier model using the mmlab/mmpretrain repository and deploying it using mmlab/mmdeploy. If you wish to create a custom traffic light classifier model with your own dataset, please follow the steps outlined below.</p>"},{"location":"perception/autoware_traffic_light_classifier/#data-preparation","title":"Data Preparation","text":""},{"location":"perception/autoware_traffic_light_classifier/#use-sample-dataset","title":"Use Sample Dataset","text":"<p>Autoware offers a sample dataset that illustrates the training procedures for traffic light classification. This dataset comprises 1045 images categorized into red, green, and yellow labels. To utilize this sample dataset, please download it from link and extract it to a designated folder of your choice.</p>"},{"location":"perception/autoware_traffic_light_classifier/#use-your-custom-dataset","title":"Use Your Custom Dataset","text":"<p>To train a traffic light classifier, adopt a structured subfolder format where each subfolder represents a distinct class. Below is an illustrative dataset structure example;</p> <pre><code>DATASET_ROOT\n    \u251c\u2500\u2500 TRAIN\n    \u2502    \u251c\u2500\u2500 RED\n    \u2502    \u2502   \u251c\u2500\u2500 001.png\n    \u2502    \u2502   \u251c\u2500\u2500 002.png\n    \u2502    \u2502   \u2514\u2500\u2500 ...\n    \u2502    \u2502\n    \u2502    \u251c\u2500\u2500 GREEN\n    \u2502    \u2502    \u251c\u2500\u2500 001.png\n    \u2502    \u2502    \u251c\u2500\u2500 002.png\n    \u2502    \u2502    \u2514\u2500\u2500...\n    \u2502    \u2502\n    \u2502    \u251c\u2500\u2500 YELLOW\n    \u2502    \u2502    \u251c\u2500\u2500 001.png\n    \u2502    \u2502    \u251c\u2500\u2500 002.png\n    \u2502    \u2502    \u2514\u2500\u2500...\n    \u2502    \u2514\u2500\u2500 ...\n    \u2502\n    \u251c\u2500\u2500 VAL\n    \u2502       \u2514\u2500\u2500...\n    \u2502\n    \u2502\n    \u2514\u2500\u2500 TEST\n           \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"perception/autoware_traffic_light_classifier/#installation","title":"Installation","text":""},{"location":"perception/autoware_traffic_light_classifier/#prerequisites","title":"Prerequisites","text":"<p>Step 1. Download and install Miniconda from the official website.</p> <p>Step 2. Create a conda virtual environment and activate it</p> <pre><code>conda create --name tl-classifier python=3.8 -y\nconda activate tl-classifier\n</code></pre> <p>Step 3. Install PyTorch</p> <p>Please ensure you have PyTorch installed, compatible with CUDA 11.6, as it is a requirement for current Autoware</p> <pre><code>conda install pytorch==1.13.1 torchvision==0.14.1 pytorch-cuda=11.6 -c pytorch -c nvidia\n</code></pre>"},{"location":"perception/autoware_traffic_light_classifier/#install-mmlabmmpretrain","title":"Install mmlab/mmpretrain","text":"<p>Step 1. Install mmpretrain from source</p> <pre><code>cd ~/\ngit clone https://github.com/open-mmlab/mmpretrain.git\ncd mmpretrain\npip install -U openmim &amp;&amp; mim install -e .\n</code></pre>"},{"location":"perception/autoware_traffic_light_classifier/#training","title":"Training","text":"<p>MMPretrain offers a training script that is controlled through a configuration file. Leveraging an inheritance design pattern, you can effortlessly tailor the training script using Python files as configuration files.</p> <p>In the example, we demonstrate the training steps on the MobileNetV2 model, but you have the flexibility to employ alternative classification models such as EfficientNetV2, EfficientNetV3, ResNet, and more.</p>"},{"location":"perception/autoware_traffic_light_classifier/#create-a-config-file","title":"Create a config file","text":"<p>Generate a configuration file for your preferred model within the <code>configs</code> folder</p> <pre><code>touch ~/mmpretrain/configs/mobilenet_v2/mobilenet-v2_8xb32_custom.py\n</code></pre> <p>Open the configuration file in your preferred text editor and make a copy of the provided content. Adjust the data_root variable to match the path of your dataset. You are welcome to customize the configuration parameters for the model, dataset, and scheduler to suit your preferences</p> <pre><code># Inherit model, schedule and default_runtime from base model\n_base_ = [\n    '../_base_/models/mobilenet_v2_1x.py',\n    '../_base_/schedules/imagenet_bs256_epochstep.py',\n    '../_base_/default_runtime.py'\n]\n\n# Set the number of classes to the model\n# You can also change other model parameters here\n# For detailed descriptions of model parameters, please refer to link below\n# (Customize model)[https://mmpretrain.readthedocs.io/en/latest/advanced_guides/modules.html]\nmodel = dict(head=dict(num_classes=3, topk=(1, 3)))\n\n# Set max epochs and validation interval\ntrain_cfg = dict(by_epoch=True, max_epochs=50, val_interval=5)\n\n# Set optimizer and lr scheduler\noptim_wrapper = dict(\n    optimizer=dict(type='SGD', lr=0.001, momentum=0.9))\nparam_scheduler = dict(type='StepLR', by_epoch=True, step_size=1, gamma=0.98)\n\ndataset_type = 'CustomDataset'\ndata_root = \"/PATH/OF/YOUR/DATASET\"\n\n# Customize data preprocessing and dataloader pipeline for training set\n# These parameters calculated for the sample dataset\ndata_preprocessor = dict(\n    mean=[0.2888 * 256, 0.2570 * 256, 0.2329 * 256],\n    std=[0.2106 * 256, 0.2037 * 256, 0.1864 * 256],\n    num_classes=3,\n    to_rgb=True,\n)\n\n# Customize data preprocessing and dataloader pipeline for train set\n# For detailed descriptions of data pipeline, please refer to link below\n# (Customize data pipeline)[https://mmpretrain.readthedocs.io/en/latest/advanced_guides/pipeline.html]\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='Resize', scale=224),\n    dict(type='RandomFlip', prob=0.5, direction='horizontal'),\n    dict(type='PackInputs'),\n]\ntrain_dataloader = dict(\n    dataset=dict(\n        type=dataset_type,\n        data_root=data_root,\n        ann_file='',\n        data_prefix='train',\n        with_label=True,\n        pipeline=train_pipeline,\n    ),\n    num_workers=8,\n    batch_size=32,\n    sampler=dict(type='DefaultSampler', shuffle=True)\n)\n\n# Customize data preprocessing and dataloader pipeline for test set\ntest_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='Resize', scale=224),\n    dict(type='PackInputs'),\n]\n\n# Customize data preprocessing and dataloader pipeline for validation set\nval_cfg = dict()\nval_dataloader = dict(\n    dataset=dict(\n        type=dataset_type,\n        data_root=data_root,\n        ann_file='',\n        data_prefix='val',\n        with_label=True,\n        pipeline=test_pipeline,\n    ),\n    num_workers=8,\n    batch_size=32,\n    sampler=dict(type='DefaultSampler', shuffle=True)\n)\n\nval_evaluator = dict(topk=(1, 3,), type='Accuracy')\n\ntest_dataloader = val_dataloader\ntest_evaluator = val_evaluator\n</code></pre>"},{"location":"perception/autoware_traffic_light_classifier/#start-training","title":"Start training","text":"<pre><code>cd ~/mmpretrain\npython tools/train.py configs/mobilenet_v2/mobilenet-v2_8xb32_custom.py\n</code></pre> <p>Training logs and weights will be saved in the <code>work_dirs/mobilenet-v2_8xb32_custom</code> folder.</p>"},{"location":"perception/autoware_traffic_light_classifier/#convert-pytorch-model-to-onnx-model","title":"Convert PyTorch model to ONNX model","text":""},{"location":"perception/autoware_traffic_light_classifier/#install-mmdeploy","title":"Install mmdeploy","text":"<p>The 'mmdeploy' toolset is designed for deploying your trained model onto various target devices. With its capabilities, you can seamlessly convert PyTorch models into the ONNX format.</p> <pre><code># Activate your conda environment\nconda activate tl-classifier\n\n# Install mmenigne and mmcv\nmim install mmengine\nmim install \"mmcv&gt;=2.0.0rc2\"\n\n# Install mmdeploy\npip install mmdeploy==1.2.0\n\n# Support onnxruntime\npip install mmdeploy-runtime==1.2.0\npip install mmdeploy-runtime-gpu==1.2.0\npip install onnxruntime-gpu==1.8.1\n\n#Clone mmdeploy repository\ncd ~/\ngit clone -b main https://github.com/open-mmlab/mmdeploy.git\n</code></pre>"},{"location":"perception/autoware_traffic_light_classifier/#convert-pytorch-model-to-onnx-model_1","title":"Convert PyTorch model to ONNX model","text":"<pre><code>cd ~/mmdeploy\n\n# Run deploy.py script\n# deploy.py script takes 5 main arguments with these order; config file path, train config file path,\n# checkpoint file path, demo image path, and work directory path\npython tools/deploy.py \\\n~/mmdeploy/configs/mmpretrain/classification_onnxruntime_static.py\\\n~/mmpretrain/configs/mobilenet_v2/train_mobilenet_v2.py \\\n~/mmpretrain/work_dirs/train_mobilenet_v2/epoch_300.pth \\\n/SAMPLE/IAMGE/DIRECTORY \\\n--work-dir mmdeploy_model/mobilenet_v2\n</code></pre> <p>Converted ONNX model will be saved in the <code>mmdeploy/mmdeploy_model/mobilenet_v2</code> folder.</p> <p>After obtaining your onnx model, update parameters defined in the launch file (e.g. <code>model_file_path</code>, <code>label_file_path</code>, <code>input_h</code>, <code>input_w</code>...). Note that, we only support labels defined in tier4_perception_msgs::msg::TrafficLightElement.</p>"},{"location":"perception/autoware_traffic_light_classifier/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"perception/autoware_traffic_light_classifier/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/autoware_traffic_light_classifier/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/autoware_traffic_light_classifier/#referencesexternal-links","title":"References/External links","text":"<p>[1] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov and L. Chen, \"MobileNetV2: Inverted Residuals and Linear Bottlenecks,\" 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, 2018, pp. 4510-4520, doi: 10.1109/CVPR.2018.00474.</p> <p>[2] Tan, Mingxing, and Quoc Le. \"EfficientNet: Rethinking model scaling for convolutional neural networks.\" International conference on machine learning. PMLR, 2019.</p>"},{"location":"perception/autoware_traffic_light_classifier/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/autoware_traffic_light_fine_detector/","title":"autoware_traffic_light_fine_detector","text":""},{"location":"perception/autoware_traffic_light_fine_detector/#autoware_traffic_light_fine_detector","title":"autoware_traffic_light_fine_detector","text":""},{"location":"perception/autoware_traffic_light_fine_detector/#purpose","title":"Purpose","text":"<p>It is a package for traffic light detection using YOLOX-s.</p>"},{"location":"perception/autoware_traffic_light_fine_detector/#training-information","title":"Training Information","text":""},{"location":"perception/autoware_traffic_light_fine_detector/#pretrained-model","title":"Pretrained Model","text":"<p>The model is based on YOLOX and the pretrained model could be downloaded from here.</p>"},{"location":"perception/autoware_traffic_light_fine_detector/#training-data","title":"Training Data","text":"<p>The model was fine-tuned on around 17,000 TIER IV internal images of Japanese traffic lights.</p>"},{"location":"perception/autoware_traffic_light_fine_detector/#trained-onnx-model","title":"Trained Onnx model","text":"<p>You can download the ONNX file using these instructions. Please visit autoware-documentation for more information.</p>"},{"location":"perception/autoware_traffic_light_fine_detector/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>Based on the camera image and the global ROI array detected by <code>map_based_detector</code> node, a CNN-based detection method enables highly accurate traffic light detection. If can not detect traffic light, x_offset, y_offset, height and width of output ROI become <code>0</code>. ROIs detected from YOLOX will be selected by a combination of <code>expect/rois</code>. At this time, evaluate the whole as ROIs, not just the ROI alone.</p>"},{"location":"perception/autoware_traffic_light_fine_detector/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_traffic_light_fine_detector/#input","title":"Input","text":"Name Type Description <code>~/input/image</code> <code>sensor_msgs::msg::Image</code> The full size camera image <code>~/input/rois</code> <code>tier4_perception_msgs::msg::TrafficLightRoiArray</code> The array of ROIs detected by map_based_detector <code>~/expect/rois</code> <code>tier4_perception_msgs::msg::TrafficLightRoiArray</code> The array of ROIs detected by map_based_detector without any offset, used to select the best detection results"},{"location":"perception/autoware_traffic_light_fine_detector/#output","title":"Output","text":"Name Type Description <code>~/output/rois</code> <code>tier4_perception_msgs::msg::TrafficLightRoiArray</code> The detected accurate rois <code>~/debug/exe_time_ms</code> <code>autoware_internal_debug_msgs::msg::Float32Stamped</code> The time taken for inference"},{"location":"perception/autoware_traffic_light_fine_detector/#parameters","title":"Parameters","text":"Name Type Description Default Range precision string Precision used for traffic light fine detector inference. Valid values: [fp32, fp16]. fp16 N/A score_thresh float If the objectness score is less than this value, the object is ignored. 0.3 N/A nms_thresh float IoU threshold to perform Non-Maximum Suppression (NMS). 0.65 N/A approximate_sync boolean Flag for whether to use approximate sync policy. False N/A gpu_id integer ID for selecting the CUDA GPU device. 0 N/A"},{"location":"perception/autoware_traffic_light_fine_detector/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"perception/autoware_traffic_light_fine_detector/#reference-repositories","title":"Reference repositories","text":"<p>YOLOX github repository</p> <ul> <li>https://github.com/Megvii-BaseDetection/YOLOX</li> </ul>"},{"location":"perception/autoware_traffic_light_map_based_detector/","title":"autoware_traffic_light_map_based_detector","text":""},{"location":"perception/autoware_traffic_light_map_based_detector/#autoware_traffic_light_map_based_detector","title":"autoware_traffic_light_map_based_detector","text":""},{"location":"perception/autoware_traffic_light_map_based_detector/#overview","title":"Overview","text":"<p><code>autoware_traffic_light_map_based_detector</code> calculates where the traffic lights will appear in the image based on the HD map.</p> <p>Calibration and vibration errors can be entered as parameters, and the size of the detected RegionOfInterest will change according to the error.</p> <p></p> <p>If the node receives route information, it only looks at traffic lights on that route. If the node receives no route information, it looks at them within a radius of <code>max_detection_range</code> and the angle between the traffic light and the camera is less than <code>car_traffic_light_max_angle_range</code> or <code>pedestrian_traffic_light_max_angle_range</code>.</p>"},{"location":"perception/autoware_traffic_light_map_based_detector/#input-topics","title":"Input topics","text":"Name Type Description <code>~/input/vector_map</code> autoware_map_msgs::msg::LaneletMapBin vector map <code>~/input/camera_info</code> sensor_msgs::msg::CameraInfo target camera parameter <code>~/input/route</code> autoware_planning_msgs::msg::LaneletRoute optional: route"},{"location":"perception/autoware_traffic_light_map_based_detector/#output-topics","title":"Output topics","text":"Name Type Description <code>~/output/rois</code> tier4_perception_msgs::msg::TrafficLightRoiArray location of traffic lights in image corresponding to the camera info <code>~/expect/rois</code> tier4_perception_msgs::msg::TrafficLightRoiArray location of traffic lights in image without any offset <code>~/debug/markers</code> visualization_msgs::msg::MarkerArray markers which show a line that combines from camera to each traffic light"},{"location":"perception/autoware_traffic_light_map_based_detector/#node-parameters","title":"Node parameters","text":"Name Type Description Default Range max_vibration_pitch float Maximum error in pitch direction. If -5~+5, it will be 10. 0.0174533 N/A max_vibration_yaw float Maximum error in yaw direction. If -5~+5, it will be 10. 0.0174533 N/A max_vibration_height float Maximum error in height direction. If -5~+5, it will be 10. 0.5 N/A max_vibration_width float Maximum error in width direction. If -5~+5, it will be 10. 0.5 N/A max_vibration_depth float Maximum error in depth direction. If -5~+5, it will be 10. 0.5 N/A max_detection_range float Maximum detection range in meters. Must be positive. 200 N/A min_timestamp_offset float Minimum timestamp offset when searching for corresponding tf. -0.3 N/A max_timestamp_offset float Maximum timestamp offset when searching for corresponding tf. 0 N/A timestamp_sample_len float Sampling length between min_timestamp_offset and max_timestamp_offset. 0.02 N/A car_traffic_light_max_angle_range float Maximum angle range for detecting car traffic lights (degrees). 40 N/A pedestrian_traffic_light_max_angle_range float Maximum angle range for detecting pedestrian traffic lights (degrees). 80 N/A"},{"location":"perception/autoware_traffic_light_multi_camera_fusion/","title":"autoware_traffic_light_multi_camera_fusion","text":""},{"location":"perception/autoware_traffic_light_multi_camera_fusion/#autoware_traffic_light_multi_camera_fusion","title":"autoware_traffic_light_multi_camera_fusion","text":""},{"location":"perception/autoware_traffic_light_multi_camera_fusion/#overview","title":"Overview","text":"<p>This node fuses traffic light recognition results from multiple cameras to produce a single, reliable traffic light state. By integrating information from different viewpoints and ROIs, it ensures robust performance even in challenging scenarios, such as partial occlusions or recognition errors from an individual camera.</p> <pre><code>graph LR\n    subgraph \"Multi Camera Feeds\"\n        direction TB\n        Cam1[\" &lt;br&gt; &lt;b&gt;Camera 1&lt;/b&gt; &lt;br&gt; State: GREEN &lt;br&gt; Confidence: 0.95\"]\n        Cam2[\" &lt;br&gt; &lt;b&gt;Camera 2&lt;/b&gt; &lt;br&gt; State: GREEN &lt;br&gt; Confidence: 0.94\"]\n        Cam3[\" &lt;br&gt; &lt;b&gt;Camera 3&lt;/b&gt; &lt;br&gt; State: RED &lt;br&gt; Confidence: 0.95\"]\n    end\n\n    subgraph \"Processing\"\n        direction TB\n        Fusion[\"&lt;b&gt;Multi-Camera Fusion Node&lt;/b&gt; &lt;br&gt;&lt;i&gt;Fuses evidence using &lt;br&gt; Bayesian updating&lt;/i&gt;\"]\n    end\n\n    subgraph \"Unified &amp; Robust State\"\n        direction TB\n        Result[\" &lt;br&gt; &lt;b&gt;Final State: GREEN&lt;/b&gt;\"]\n    end\n\n    Cam1 --&gt; Fusion\n    Cam2 --&gt; Fusion\n    Cam3 --&gt; Fusion\n    Fusion --&gt; Result\n\n    style Fusion fill:#e0f7fa,stroke:#00796b,stroke-width:2px,color:#004d40\n    style Result fill:#e8f5e9,stroke:#2e7d32,stroke-width:3px,color:#1b5e20</code></pre>"},{"location":"perception/autoware_traffic_light_multi_camera_fusion/#how-it-works","title":"How It Works","text":"<p>The fusion algorithm operates in two main stages.</p> <pre><code>graph TD\n    subgraph \"Input: Multiple Camera Results\"\n        A[\"Camera 1&lt;br&gt;Recognition Result\"]\n        B[\"Camera 2&lt;br&gt;Recognition Result\"]\n        C[\"...\"]\n    end\n\n    subgraph \"Stage 1: Per-Camera Fusion\"\n        D{\"Best ROIs Selection&lt;br&gt;&lt;br&gt;For each ROI,&lt;br&gt;select the single most&lt;br&gt;reliable detection result.\"}\n    end\n\n    E[\"Best Detection per ROIs\"]\n\n    subgraph \"Stage 2: Group Fusion\"\n        F{\"Group Consensus&lt;br&gt;&lt;br&gt;Fuse all 'best detections'&lt;br&gt;into a single state for&lt;br&gt;the entire traffic light group&lt;br&gt;using Bayesian updating.\"}\n    end\n\n    subgraph \"Final Output\"\n        G[\"Final Group State&lt;br&gt;(e.g., GREEN)\"]\n    end\n\n    A --&gt; D\n    B --&gt; D\n    C --&gt; D\n    D --&gt; E\n    E --&gt; F\n    F --&gt; G\n\n    style D fill:#e0f7fa,stroke:#00796b,stroke-width:2px,color:black\n    style F fill:#e0f7fa,stroke:#00796b,stroke-width:2px,color:black\n    style E fill:#fff,stroke:#333,stroke-width:2px,stroke-dasharray: 5 5,color:black\n    style G fill:#e8f5e9,stroke:#2e7d32,stroke-width:3px,color:black</code></pre>"},{"location":"perception/autoware_traffic_light_multi_camera_fusion/#stage-1-best-view-selection-per-camera-fusion","title":"Stage 1: Best View Selection (Per-Camera Fusion)","text":"<p>First, for each individual ROIs, the node selects the single most reliable detection\u2014the \"best shot\"\u2014from all available camera views.</p> <p>This selection is based on a strict priority queue:</p> <ul> <li>Latest Timestamp: Detections with the most recent timestamp are prioritized for the same sensor.</li> <li>Known State: Results with a known color (Red, Green, etc.) are prioritized over 'Unknown'.</li> <li>Full Visibility: Detections from non-truncated ROIs (fully visible ROIs) are prioritized.</li> <li>Highest Confidence: The result with the highest detection confidence score is prioritized.</li> </ul> <p>This process yields the single most plausible recognition for every ROIs.</p>"},{"location":"perception/autoware_traffic_light_multi_camera_fusion/#stage-2-group-consensus-bayesian-fusion","title":"Stage 2: Group Consensus (Bayesian Fusion)","text":"<p>Next, the \"best shot\" detections from Stage 1 are fused to determine a single, coherent state for the entire traffic light group. Instead of simple voting or averaging, this node employs a more principled method: Bayesian updating.</p> <ul> <li>Belief Score: Each color (Red, Green, Yellow) maintains a \"belief score\" represented in log-odds for numerical stability and ease of updating.</li> <li>Evidence Update: Each selected detection from Stage 1 is treated as a piece of \"evidence.\" Its confidence score is converted into a log-odds value representing the strength of that evidence.</li> <li>Score Accumulation: This evidence is added to the corresponding color's belief score.</li> <li>Final Decision: After accumulating all evidence, the color with the highest final score is chosen as the definitive state for the group.</li> </ul>"},{"location":"perception/autoware_traffic_light_multi_camera_fusion/#input-topics","title":"Input topics","text":"<p>For every camera, the following three topics are subscribed:</p> Name Type Description <code>~/&lt;camera_namespace&gt;/camera_info</code> sensor_msgs::msg::CameraInfo camera info from map_based_detector <code>~/&lt;camera_namespace&gt;/detection/rois</code> tier4_perception_msgs::msg::TrafficLightRoiArray detection roi from fine_detector <code>~/&lt;camera_namespace&gt;/classification/traffic_signals</code> tier4_perception_msgs::msg::TrafficLightArray classification result from classifier <p>You don't need to configure these topics manually. Just provide the <code>camera_namespaces</code> parameter and the node will automatically extract the <code>&lt;camera_namespace&gt;</code> and create the subscribers.</p>"},{"location":"perception/autoware_traffic_light_multi_camera_fusion/#output-topics","title":"Output topics","text":"Name Type Description <code>~/output/traffic_signals</code> autoware_perception_msgs::msg::TrafficLightGroupArray traffic light signal fusion result"},{"location":"perception/autoware_traffic_light_multi_camera_fusion/#node-parameters","title":"Node parameters","text":"Name Type Description Default Range message_lifespan float The maximum timestamp span to be fused. 0 N/A approximate_sync boolean Whether to work in Approximate Synchronization Mode. 0 N/A prior_log_odds float The uniform prior log-odds to initialize the belief for any new traffic light state. 0.0 (log(1)) means a neutral (50%) initial belief. Positive values make all states initially more likely, negative values make them less likely. 0 N/A"},{"location":"perception/autoware_traffic_light_occlusion_predictor/","title":"autoware_traffic_light_occlusion_predictor","text":""},{"location":"perception/autoware_traffic_light_occlusion_predictor/#autoware_traffic_light_occlusion_predictor","title":"autoware_traffic_light_occlusion_predictor","text":""},{"location":"perception/autoware_traffic_light_occlusion_predictor/#overview","title":"Overview","text":"<p><code>autoware_traffic_light_occlusion_predictor</code> receives the detected traffic lights rois and calculates the occlusion ratios of each roi with point cloud. If that rois is judged as occlusion, color, shape, and confidence of <code>~/output/traffic_signals</code> become <code>UNKNOWN</code>, <code>UNKNOWN</code>, and <code>0.0</code>. This node publishes when each car and pedestrian <code>traffic_signals</code> is arrived and processed.</p> <p>For each traffic light roi, hundreds of pixels would be selected and projected into the 3D space. Then from the camera point of view, the number of projected pixels that are occluded by the point cloud is counted and used for calculating the occlusion ratio for the roi. As shown in follow image, the red pixels are occluded and the occlusion ratio is the number of red pixels divided by the total pixel numbers.</p> <p></p> <p>If no point cloud is received or all point clouds have very large stamp difference with the camera image, the occlusion ratio of each roi would be set as 0.</p>"},{"location":"perception/autoware_traffic_light_occlusion_predictor/#input-topics","title":"Input topics","text":"Name Type Description <code>~/input/vector_map</code> autoware_map_msgs::msg::LaneletMapBin vector map <code>~/input/car/traffic_signals</code> tier4_perception_msgs::msg::TrafficLightArray vehicular traffic light signals <code>~/input/pedestrian/traffic_signals</code> tier4_perception_msgs::msg::TrafficLightArray pedestrian traffic light signals <code>~/input/rois</code> tier4_perception_msgs::msg::TrafficLightRoiArray traffic light detections <code>~/input/camera_info</code> sensor_msgs::msg::CameraInfo target camera parameter <code>~/input/cloud</code> sensor_msgs::msg::PointCloud2 LiDAR point cloud"},{"location":"perception/autoware_traffic_light_occlusion_predictor/#output-topics","title":"Output topics","text":"Name Type Description <code>~/output/traffic_signals</code> tier4_perception_msgs::msg::TrafficLightArray traffic light signals which occluded image results overwritten"},{"location":"perception/autoware_traffic_light_occlusion_predictor/#node-parameters","title":"Node parameters","text":"Name Type Description Default Range azimuth_occlusion_resolution_deg float Azimuth resolution of LiDAR point cloud (degree). 0.15 N/A elevation_occlusion_resolution_deg float Elevation resolution of LiDAR point cloud (degree). 0.08 N/A max_valid_pt_dist float The points within this distance would be used for calculation. 50 N/A max_image_cloud_delay float The maximum delay between LiDAR point cloud and camera image. 0.5 N/A max_wait_t float The maximum time waiting for the LiDAR point cloud. 0.05 N/A max_occlusion_ratio integer Maximum occlusion ratio for traffic lights. 50 N/A"},{"location":"perception/autoware_traffic_light_selector/","title":"Index","text":"<p>autoware_traffic_light_selector</p>"},{"location":"perception/autoware_traffic_light_selector/#overview","title":"Overview","text":"<p><code>autoware_traffic_light_selector</code> selects the interest traffic light from the list of accurately detected traffic lights by something (e.g. deep learning neural network) based on the expect ROIs and rough ROIs information and then assign traffic_light_id for them.</p>"},{"location":"perception/autoware_traffic_light_selector/#input-topics","title":"Input topics","text":"Name Type Description <code>input/detected_rois</code> tier4_perception_msgs::msg::DetectedObjectsWithFeature accurately detected traffic light <code>input/rough_rois</code> tier4_perception_msgs::msg::TrafficLightRoiArray location of traffic lights in image corresponding to the camera info <code>input/expect_rois</code> tier4_perception_msgs::msg::TrafficLightRoiArray location of traffic lights in image without any offset"},{"location":"perception/autoware_traffic_light_selector/#output-topics","title":"Output topics","text":"Name Type Description <code>output/traffic_rois</code> tier4_perception_msgs::TrafficLightRoiArray detected traffic light of interest with id"},{"location":"perception/autoware_traffic_light_selector/#node-parameters","title":"Node parameters","text":"<p>None</p>"},{"location":"perception/autoware_traffic_light_visualization/","title":"autoware_traffic_light_visualization","text":""},{"location":"perception/autoware_traffic_light_visualization/#autoware_traffic_light_visualization","title":"autoware_traffic_light_visualization","text":""},{"location":"perception/autoware_traffic_light_visualization/#purpose","title":"Purpose","text":"<p>The <code>autoware_traffic_light_visualization</code> is a package that includes two visualizing nodes:</p>"},{"location":"perception/autoware_traffic_light_visualization/#traffic_light_map_visualizer","title":"traffic_light_map_visualizer","text":"<p>The node shows traffic light's color and position on rviz as markers.</p> <p></p> <ul> <li>The estimated traffic light color is visualized using map information like the traffic light at the center of the image.</li> <li>May not be possible to visualize without information on each of the lights (light_bulbs) at the traffic lights.</li> </ul>"},{"location":"perception/autoware_traffic_light_visualization/#traffic_light_roi_visualizer","title":"traffic_light_roi_visualizer","text":"<p>The node draws the result of traffic light recognition on the input image as shown in the following figure and publishes it.</p> <p></p> <ul> <li>The colors <code>~/input/rois</code> and <code>~/input/rough/rois</code> are the same as <code>color</code> whose <code>shape</code> is CIRCLE in <code>~/input/traffic_signals</code> (unknown shows as white).</li> <li>The labels in the upper left of <code>~/input/rois</code> shows <code>shape</code> and <code>confidence</code> in <code>~/input/traffic_signals</code>.</li> <li>The type of <code>shape</code> should be referred to images directory. There are only 3 types of arrows in the image, but they can represent 8 directions.</li> </ul>"},{"location":"perception/autoware_traffic_light_visualization/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"perception/autoware_traffic_light_visualization/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"perception/autoware_traffic_light_visualization/#traffic_light_map_visualizer_1","title":"traffic_light_map_visualizer","text":""},{"location":"perception/autoware_traffic_light_visualization/#input","title":"Input","text":"Name Type Description <code>~/input/tl_state</code> autoware_perception_msgs::msg::TrafficLightGroupArray status of traffic lights <code>~/input/vector_map</code> autoware_map_msgs::msg::LaneletMapBin vector map"},{"location":"perception/autoware_traffic_light_visualization/#output","title":"Output","text":"Name Type Description <code>~/output/traffic_light</code> visualization_msgs::msg::MarkerArray marker array that indicates status of traffic lights"},{"location":"perception/autoware_traffic_light_visualization/#traffic_light_roi_visualizer_1","title":"traffic_light_roi_visualizer","text":""},{"location":"perception/autoware_traffic_light_visualization/#input_1","title":"Input","text":"Name Type Description <code>~/input/traffic_signals</code> tier4_perception_msgs::msg::TrafficLightArray status of traffic lights <code>~/input/image</code> sensor_msgs::msg::Image the image captured by perception cameras <code>~/input/rois</code> tier4_perception_msgs::msg::TrafficLightRoiArray the ROIs detected by <code>autoware_traffic_light_fine_detector</code> <code>~/input/rough/rois</code> (option) tier4_perception_msgs::msg::TrafficLightRoiArray the ROIs detected by <code>autoware_traffic_light_map_based_detector</code>"},{"location":"perception/autoware_traffic_light_visualization/#output_1","title":"Output","text":"Name Type Description <code>~/output/image</code> sensor_msgs::msg::Image output image with ROIs"},{"location":"perception/autoware_traffic_light_visualization/#parameters","title":"Parameters","text":""},{"location":"perception/autoware_traffic_light_visualization/#traffic_light_map_visualizer_2","title":"traffic_light_map_visualizer","text":"<p>None</p>"},{"location":"perception/autoware_traffic_light_visualization/#traffic_light_roi_visualizer_2","title":"traffic_light_roi_visualizer","text":""},{"location":"perception/autoware_traffic_light_visualization/#node-parameters","title":"Node Parameters","text":"Name Type Description Default Range use_image_transport boolean whether to apply image transport to compress the output debugging image in the traffic light fine detection true N/A"},{"location":"perception/autoware_traffic_light_visualization/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"perception/autoware_traffic_light_visualization/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"perception/autoware_traffic_light_visualization/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"perception/autoware_traffic_light_visualization/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"perception/autoware_traffic_light_visualization/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"perception/perception_utils/","title":"perception_utils","text":""},{"location":"perception/perception_utils/#perception_utils","title":"perception_utils","text":""},{"location":"perception/perception_utils/#purpose","title":"Purpose","text":"<p>This package contains a library of common functions that are useful across the perception module.</p>"},{"location":"planning/","title":"Planning Components","text":""},{"location":"planning/#planning-components","title":"Planning Components","text":""},{"location":"planning/#getting-started","title":"Getting Started","text":"<p>The Autoware Universe Planning Modules represent a cutting-edge component within the broader open-source autonomous driving software stack. These modules play a pivotal role in autonomous vehicle navigation, skillfully handling route planning, dynamic obstacle avoidance, and real-time adaptation to varied traffic conditions.</p> <ul> <li>For high level concept of Planning Components, please refer to Planning Component Design Document</li> <li>To understand how Planning Components interacts with other components, please refer to Planning Component Interface Document</li> <li>The Node Diagram illustrates the interactions, inputs, and outputs of all modules in the Autoware Universe, including planning modules.</li> </ul>"},{"location":"planning/#planning-module","title":"Planning Module","text":"<p>The Module in the Planning Component refers to the various components that collectively form the planning system of the software. These modules cover a range of functionalities necessary for autonomous vehicle planning. Autoware's planning modules are modularized, meaning users can customize which functions are enabled by changing the configuration. This modular design allows for flexibility and adaptability to different scenarios and requirements in autonomous vehicle operations.</p>"},{"location":"planning/#how-to-enable-or-disable-planning-module","title":"How to Enable or Disable Planning Module","text":"<p>Enabling and disabling modules involves managing settings in key configuration and launch files.</p>"},{"location":"planning/#key-files-for-configuration","title":"Key Files for Configuration","text":"<p>The <code>default_preset.yaml</code> file acts as the primary configuration file, where planning modules can be disabled or enabled. Furthermore, users can also set the type of motion planner across various motion planners. For example:</p> <ul> <li><code>launch_avoidance_module</code>: Set to <code>true</code> to enable the avoidance module, or <code>false</code> to disable it.</li> </ul> <p>Note</p> <p>Click here to view the <code>default_preset.yaml</code>.</p> <p>The launch files reference the settings defined in <code>default_preset.yaml</code> to apply the configurations when the behavior path planner's node is running. For instance, the parameter <code>avoidance.enable_module</code> in</p> <pre><code>&lt;param name=\"avoidance.enable_module\" value=\"$(var launch_avoidance_module)\"/&gt;\n</code></pre> <p>corresponds to launch_avoidance_module from <code>default_preset.yaml</code>.</p>"},{"location":"planning/#parameter-configuration","title":"Parameter Configuration","text":"<p>There are multiple parameters available for configuration, and users have the option to modify them in here. It's important to note that not all parameters are adjustable via <code>rqt_reconfigure</code>. To ensure the changes are effective, modify the parameters and then restart Autoware. Additionally, detailed information about each parameter is available in the corresponding documents under the planning tab.</p>"},{"location":"planning/#integrating-a-custom-module-into-autoware-a-step-by-step-guide","title":"Integrating a Custom Module into Autoware: A Step-by-Step Guide","text":"<p>This guide outlines the steps for integrating your custom module into Autoware:</p> <ul> <li>Add your modules to the <code>default_preset.yaml</code> file. For example:</li> </ul> <pre><code>- arg:\n  name: launch_intersection_module\n  default: \"true\"\n</code></pre> <ul> <li>Incorporate your modules into the launcher. For example, in behavior_planning.launch.xml:</li> </ul> <pre><code>&lt;arg name=\"launch_intersection_module\" default=\"true\"/&gt;\n\n&lt;let\n  name=\"behavior_velocity_planner_launch_modules\"\n  value=\"$(eval &amp;quot;'$(var behavior_velocity_planner_launch_modules)' + 'behavior_velocity_planner::IntersectionModulePlugin, '&amp;quot;)\"\n  if=\"$(var launch_intersection_module)\"\n/&gt;\n</code></pre> <ul> <li>If applicable, place your parameter folder within the appropriate existing parameter folder. For example, intersection_module's parameters are in behavior_velocity_planner.</li> <li>Insert the path of your parameters in the tier4_planning_component.launch.xml. For example, <code>behavior_velocity_planner_intersection_module_param_path</code> is used.</li> </ul> <pre><code>&lt;arg name=\"behavior_velocity_planner_intersection_module_param_path\" value=\"$(var behavior_velocity_config_path)/intersection.param.yaml\"/&gt;\n</code></pre> <ul> <li>Define your parameter path variable within the corresponding launcher. For example, in behavior_planning.launch.xml</li> </ul> <pre><code>&lt;param from=\"$(var behavior_velocity_planner_intersection_module_param_path)\"/&gt;\n</code></pre> <p>Note</p> <p>Depending on the specific module you wish to add, the relevant files and steps may vary. This guide provides a general overview and serves as a starting point. It's important to adapt these instructions to the specifics of your module.</p>"},{"location":"planning/#join-our-community-driven-effort","title":"Join Our Community-Driven Effort","text":"<p>Autoware thrives on community collaboration. Every contribution, big or small, is invaluable to us. Whether it's reporting bugs, suggesting improvements, offering new ideas, or anything else you can think of \u2013 we welcome all contributions with open arms.</p>"},{"location":"planning/#how-to-contribute","title":"How to Contribute?","text":"<p>Ready to contribute? Great! To get started, simply visit our Contributing Guidelines where you'll find all the information you need to jump in. This includes instructions on submitting bug reports, proposing feature enhancements, and even contributing to the codebase.</p>"},{"location":"planning/#join-our-planning-control-working-group-meetings","title":"Join Our Planning &amp; Control Working Group Meetings","text":"<p>The Planning &amp; Control working group is an integral part of our community. We meet bi-weekly to discuss our current progress, upcoming challenges, and brainstorm new ideas. These meetings are a fantastic opportunity to directly contribute to our discussions and decision-making processes.</p> <p>Meeting Details:</p> <ul> <li>Frequency: Bi-weekly</li> <li>Day: Thursday</li> <li>Time: 08:00 AM UTC (05:00 PM JST)</li> <li>Agenda: Discuss current progress, plan future developments. You can view and comment on the minutes of past meetings here.</li> </ul> <p>Interested in joining our meetings? We\u2019d love to have you! For more information on how to participate, visit the following link: How to participate in the working group.</p>"},{"location":"planning/#citations","title":"Citations","text":"<p>Occasionally, we publish papers specific to the Planning Component in Autoware. We encourage you to explore these publications and find valuable insights for your work. If you find them useful and incorporate any of our methodologies or algorithms in your projects, citing our papers would be immensely helpful. This support allows us to reach a broader audience and continue contributing to the field.</p> <p>If you use the Jerk Constrained Velocity Planning algorithm in the Motion Velocity Smoother module in the Planning Component, we kindly request you cite the relevant paper.</p> <p>Y. Shimizu, T. Horibe, F. Watanabe and S. Kato, \"Jerk Constrained Velocity Planning for an Autonomous Vehicle: Linear Programming Approach,\" 2022 International Conference on Robotics and Automation (ICRA)</p> <pre><code>@inproceedings{shimizu2022,\n  author={Shimizu, Yutaka and Horibe, Takamasa and Watanabe, Fumiya and Kato, Shinpei},\n  booktitle={2022 International Conference on Robotics and Automation (ICRA)},\n  title={Jerk Constrained Velocity Planning for an Autonomous Vehicle: Linear Programming Approach},\n  year={2022},\n  pages={5814-5820},\n  doi={10.1109/ICRA46639.2022.9812155}}\n</code></pre>"},{"location":"planning/autoware_costmap_generator/","title":"costmap_generator","text":""},{"location":"planning/autoware_costmap_generator/#costmap_generator","title":"costmap_generator","text":""},{"location":"planning/autoware_costmap_generator/#costmap_generator_node","title":"costmap_generator_node","text":"<p>This node reads <code>PointCloud</code> and/or <code>DynamicObjectArray</code> and creates an <code>OccupancyGrid</code> and <code>GridMap</code>. <code>VectorMap(Lanelet2)</code> is optional.</p>"},{"location":"planning/autoware_costmap_generator/#input-topics","title":"Input topics","text":"Name Type Description <code>~input/objects</code> autoware_perception_msgs::PredictedObjects predicted objects, for obstacles areas <code>~input/points_no_ground</code> sensor_msgs::PointCloud2 ground-removed points, for obstacle areas which can't be detected as objects <code>~input/vector_map</code> autoware_map_msgs::msg::LaneletMapBin vector map, for drivable areas <code>~input/scenario</code> tier4_planning_msgs::Scenario scenarios to be activated, for node activation"},{"location":"planning/autoware_costmap_generator/#output-topics","title":"Output topics","text":"Name Type Description <code>~output/grid_map</code> grid_map_msgs::GridMap costmap as GridMap, values are from 0.0 to 1.0 <code>~output/occupancy_grid</code> nav_msgs::OccupancyGrid costmap as OccupancyGrid, values are from 0 to 100"},{"location":"planning/autoware_costmap_generator/#output-tfs","title":"Output TFs","text":"<p>None</p>"},{"location":"planning/autoware_costmap_generator/#how-to-launch","title":"How to launch","text":"<ol> <li> <p>Execute the command <code>source install/setup.bash</code> to setup the environment</p> </li> <li> <p>Run <code>ros2 launch costmap_generator costmap_generator.launch.xml</code> to launch the node</p> </li> </ol>"},{"location":"planning/autoware_costmap_generator/#parameters","title":"Parameters","text":"Name Type Description <code>update_rate</code> double timer's update rate <code>activate_by_scenario</code> bool if true, activate by scenario = parking. Otherwise, activate if vehicle is inside parking lot. <code>use_objects</code> bool whether using <code>~input/objects</code> or not <code>use_points</code> bool whether using <code>~input/points_no_ground</code> or not <code>use_wayarea</code> bool whether using <code>wayarea</code> from <code>~input/vector_map</code> or not <code>use_parkinglot</code> bool whether using <code>parkinglot</code> from <code>~input/vector_map</code> or not <code>costmap_frame</code> string created costmap's coordinate <code>vehicle_frame</code> string vehicle's coordinate <code>map_frame</code> string map's coordinate <code>grid_min_value</code> double minimum cost for gridmap <code>grid_max_value</code> double maximum cost for gridmap <code>grid_resolution</code> double resolution for gridmap <code>grid_length_x</code> int size of gridmap for x direction <code>grid_length_y</code> int size of gridmap for y direction <code>grid_position_x</code> int offset from coordinate in x direction <code>grid_position_y</code> int offset from coordinate in y direction <code>maximum_lidar_height_thres</code> double maximum height threshold for pointcloud data (relative to the vehicle_frame) <code>minimum_lidar_height_thres</code> double minimum height threshold for pointcloud data (relative to the vehicle_frame) <code>expand_rectangle_size</code> double expand object's rectangle with this value <code>size_of_expansion_kernel</code> int kernel size for blurring effect on object's costmap"},{"location":"planning/autoware_costmap_generator/#flowchart","title":"Flowchart","text":""},{"location":"planning/autoware_diffusion_planner/","title":"Autoware Diffusion Planner","text":""},{"location":"planning/autoware_diffusion_planner/#autoware-diffusion-planner","title":"Autoware Diffusion Planner","text":""},{"location":"planning/autoware_diffusion_planner/#overview","title":"Overview","text":"<p>The Autoware Diffusion Planner is a trajectory generation module for autonomous vehicles, designed to work within the Autoware ecosystem. It leverages the Diffusion Planner model, as described in the paper \"Diffusion-Based Planning for Autonomous Driving with Flexible Guidance\" by Zheng et al. </p> <p>This planner generates smooth, feasible, and safe trajectories by considering:</p> <ul> <li>Dynamic and static obstacles</li> <li>Vehicle kinematics</li> <li>User-defined constraints</li> <li>Lanelet2 map context</li> <li>Traffic signals and speed limits</li> </ul> <p>It is implemented as a ROS 2 component node, making it easy to integrate into Autoware-based stacks. The node is aimed at working within the proposed Autoware new planning framework.</p>"},{"location":"planning/autoware_diffusion_planner/#how-to-use","title":"How to use","text":"<p>Currently, some launch files must be changed to run the planning simulator with <code>autoware_diffusion_planner</code>.</p> <p>(1) <code>/path/to/src/launcher/autoware_launch</code></p> <pre><code>diff --git a/autoware_launch/config/control/trajectory_follower/longitudinal/pid.param.yaml b/autoware_launch/config/control/trajectory_follower/longitudinal/pid.param.yaml\n--- a/autoware_launch/config/control/trajectory_follower/longitudinal/pid.param.yaml\n+++ b/autoware_launch/config/control/trajectory_follower/longitudinal/pid.param.yaml\n@@ -6,7 +6,7 @@\n     enable_overshoot_emergency: false\n     enable_large_tracking_error_emergency: true\n     enable_slope_compensation: true\n-    enable_keep_stopped_until_steer_convergence: true\n+    enable_keep_stopped_until_steer_convergence: false\n\n     # state transition\n     drive_state_stop_dist: 0.5\ndiff --git a/autoware_launch/config/system/diagnostics/planning.yaml b/autoware_launch/config/system/diagnostics/planning.yaml\n--- a/autoware_launch/config/system/diagnostics/planning.yaml\n+++ b/autoware_launch/config/system/diagnostics/planning.yaml\n@@ -11,19 +11,7 @@ units:\n           - { type: link, link: /autoware/planning/trajectory_validation }\n\n   - path: /autoware/planning/trajectory_validation\n-    type: and\n-    list:\n-      - { type: link, link: /autoware/planning/trajectory_validation/finite }\n-      - { type: link, link: /autoware/planning/trajectory_validation/interval }\n-      - { type: link, link: /autoware/planning/trajectory_validation/curvature }\n-      - { type: link, link: /autoware/planning/trajectory_validation/angle }\n-      - { type: link, link: /autoware/planning/trajectory_validation/lateral_acceleration }\n-      - { type: link, link: /autoware/planning/trajectory_validation/acceleration }\n-      - { type: link, link: /autoware/planning/trajectory_validation/deceleration }\n-      - { type: link, link: /autoware/planning/trajectory_validation/steering }\n-      - { type: link, link: /autoware/planning/trajectory_validation/steering_rate }\n-      - { type: link, link: /autoware/planning/trajectory_validation/velocity_deviation }\n-      - { type: link, link: /autoware/planning/trajectory_validation/trajectory_shift }\n+    type: ok\n\n   - path: /autoware/planning/routing/state\n     type: diag\n</code></pre> <p>(2) <code>/path/to/autoware/src/universe/autoware_universe</code></p> <pre><code>diff --git a/launch/tier4_planning_launch/launch/planning.launch.xml b/launch/tier4_planning_launch/launch/planning.launch.xml\n--- a/launch/tier4_planning_launch/launch/planning.launch.xml\n+++ b/launch/tier4_planning_launch/launch/planning.launch.xml\n@@ -40,12 +40,34 @@\n       &lt;/include&gt;\n     &lt;/group&gt;\n\n+    &lt;!-- trajectory generator --&gt;\n+    &lt;group&gt;\n+      &lt;push-ros-namespace namespace=\"trajectory_generator\"/&gt;\n+      &lt;include file=\"$(find-pkg-share autoware_diffusion_planner)/launch/diffusion_planner.launch.xml\"&gt;\n+        &lt;arg name=\"input_odometry\" value=\"/localization/kinematic_state\"/&gt;\n+        &lt;arg name=\"input_acceleration\" value=\"/localization/acceleration\"/&gt;\n+        &lt;arg name=\"input_route\" value=\"/planning/mission_planning/route\"/&gt;\n+        &lt;arg name=\"input_traffic_signals\" value=\"/perception/traffic_light_recognition/traffic_signals\"/&gt;\n+        &lt;arg name=\"input_tracked_objects\" value=\"/perception/object_recognition/tracking/objects\"/&gt;\n+        &lt;arg name=\"input_vector_map\" value=\"/map/vector_map\"/&gt;\n+        &lt;arg name=\"input_turn_indicators\" value=\"/vehicle/status/turn_indicators_status\"/&gt;\n+        &lt;arg name=\"output_trajectories\" value=\"/planning/generator/diffusion_planner/candidate_trajectories\"/&gt;\n+        &lt;arg name=\"output_turn_indicators\" value=\"/planning/turn_indicators_cmd\"/&gt;\n+      &lt;/include&gt;\n+      &lt;include file=\"$(find-pkg-share autoware_trajectory_optimizer)/launch/trajectory_optimizer.launch.xml\"&gt;\n+        &lt;arg name=\"input_trajectories\" value=\"/planning/generator/diffusion_planner/candidate_trajectories\"/&gt;\n+        &lt;arg name=\"output_traj\" value=\"/planning/trajectory\"/&gt;\n+        &lt;arg name=\"output_trajectories\" value=\"/planning/generator/trajectory_optimizer/candidate_trajectories\"/&gt;\n+      &lt;/include&gt;\n+    &lt;/group&gt;\n+\n     &lt;!-- planning validator --&gt;\n     &lt;group&gt;\n       &lt;include file=\"$(find-pkg-share autoware_planning_validator)/launch/planning_validator.launch.xml\"&gt;\n         &lt;arg name=\"container_type\" value=\"component_container_mt\"/&gt;\n         &lt;arg name=\"input_trajectory\" value=\"/planning/scenario_planning/velocity_smoother/trajectory\"/&gt;\n-        &lt;arg name=\"output_trajectory\" value=\"/planning/trajectory\"/&gt;\n+        &lt;arg name=\"output_trajectory\" value=\"/planning/trajectory/unused\"/&gt;\n         &lt;arg name=\"input_objects_topic_name\" value=\"$(var input_objects_topic_name)\"/&gt;\n         &lt;arg name=\"input_pointcloud_topic_name\" value=\"$(var input_pointcloud_topic_name)\"/&gt;\n         &lt;arg name=\"planning_validator_param_path\" value=\"$(var planning_validator_param_path)\"/&gt;\ndiff --git a/launch/tier4_planning_launch/launch/scenario_planning/lane_driving/behavior_planning/behavior_planning.launch.xml b/launch/tier4_planning_launch/launch/scenario_planning/lane_driving/behavior_planning/behavior_planning.launch.xml\n--- a/launch/tier4_planning_launch/launch/scenario_planning/lane_driving/behavior_planning/behavior_planning.launch.xml\n+++ b/launch/tier4_planning_launch/launch/scenario_planning/lane_driving/behavior_planning/behavior_planning.launch.xml\n@@ -240,7 +240,7 @@\n         &lt;remap from=\"~/input/accel\" to=\"/localization/acceleration\"/&gt;\n         &lt;remap from=\"~/input/scenario\" to=\"/planning/scenario_planning/scenario\"/&gt;\n         &lt;remap from=\"~/output/path\" to=\"path_with_lane_id\"/&gt;\n-        &lt;remap from=\"~/output/turn_indicators_cmd\" to=\"/planning/turn_indicators_cmd\"/&gt;\n+        &lt;remap from=\"~/output/turn_indicators_cmd\" to=\"/planning/turn_indicators_cmd/unused\"/&gt;\n         &lt;remap from=\"~/output/hazard_lights_cmd\" to=\"/planning/behavior_path_planner/hazard_lights_cmd\"/&gt;\n         &lt;remap from=\"~/output/modified_goal\" to=\"/planning/scenario_planning/modified_goal\"/&gt;\n         &lt;remap from=\"~/output/stop_reasons\" to=\"/planning/scenario_planning/status/stop_reasons\"/&gt;\n</code></pre> <p>(3) launch the planning simulator</p> <pre><code>ros2 launch autoware_launch planning_simulator.launch.xml \\\n  map_path:=/path/to/your/map \\\n  vehicle_model:=sample_vehicle \\\n  sensor_model:=sample_sensor_kit\n</code></pre> <p>Note: Make sure the appropriate version weight is set for the path specified in <code>planning/autoware_diffusion_planner/config/diffusion_planner.param.yaml</code>.</p> <pre><code>$ ls ~/autoware_data/diffusion_planner/v2.0/\ndiffusion_planner.onnx diffusion_planner.param.json\n</code></pre> <p>This can be downloaded from setup-dev-env.sh.</p>"},{"location":"planning/autoware_diffusion_planner/#features","title":"Features","text":"<ul> <li> <p>Diffusion-based trajectory generation for flexible and robust planning</p> <p></p> </li> </ul> <ul> <li> <p>Integration with Lanelet2 maps for lane-level context</p> <p></p> </li> </ul> <ul> <li> <p>Dynamic and static obstacle handling using perception inputs</p> <p></p> <p></p> </li> </ul> <ul> <li> <p>Traffic signal and speed limit awareness</p> <p></p> </li> </ul> <ul> <li>ONNX Runtime inference for fast neural network execution</li> <li>ROS 2 publishers for planned trajectories, predicted objects, and debug markers</li> </ul>"},{"location":"planning/autoware_diffusion_planner/#parameters","title":"Parameters","text":"Name Type Description Default Range plugins_path string Path to libautoware_tensorrt_plugins.so file for the diffusion planner $(find-pkg-share autoware_tensorrt_plugins)/plugins/libautoware_tensorrt_plugins.so N/A artifact_dir string Path to the Artifact(onnx model, etc.) directory $(env HOME)/autoware_data/diffusion_planner N/A onnx_model_path string Path to the ONNX model file for the diffusion planner $(var artifact_dir)/diffusion_planner.onnx N/A args_path string Path to model argument/configuration file $(var artifact_dir)/diffusion_planner.param.json N/A planning_frequency_hz float Planning frequency in Hz 10.0 &gt;0 ignore_neighbors boolean Ignore neighbor agents False N/A ignore_unknown_neighbors boolean Ignore neighbor agents with unknown class True N/A predict_neighbor_trajectory boolean Predict trajectories for neighbor agents True N/A traffic_light_group_msg_timeout_seconds float Timeout for traffic light group messages (seconds) 0.2 &gt;0 build_only boolean On build only, do not run the planner False N/A batch_size integer Batch size for multi-batch inference 1 \u22651 temperature array List of temperature for sampling trajectories. Its length must be the same as <code>batch_size</code>. Each value means the coefficient of the standard normal distribution which is used for each initial noise of corresponding batch index. val = N(0,1) * temperature [0.5] N/A velocity_smoothing_window integer Window size for velocity smoothing. Set to 1 to disable smoothing. 8 \u22651 stopping_threshold float Threshold for keeping the stopping state [m/s] 0.0 N/A debug_params.publish_debug_map boolean Publish debug map markers False N/A debug_params.publish_debug_route boolean Publish debug route markers True N/A <p>Parameters can be set via YAML (see <code>config/diffusion_planner.param.yaml</code>).</p>"},{"location":"planning/autoware_diffusion_planner/#inputs","title":"Inputs","text":"Topic Message Type Description <code>~/input/odometry</code> nav_msgs/msg/Odometry Ego vehicle odometry <code>~/input/acceleration</code> geometry_msgs/msg/AccelWithCovarianceStamped Ego acceleration <code>~/input/tracked_objects</code> autoware_perception_msgs/msg/TrackedObjects Detected dynamic objects <code>~/input/traffic_signals</code> autoware_perception_msgs/msg/TrafficLightGroupArray Traffic light states <code>~/input/vector_map</code> autoware_map_msgs/msg/LaneletMapBin Lanelet2 map <code>~/input/route</code> autoware_planning_msgs/msg/LaneletRoute Route information <code>~/input/turn_indicators</code> autoware_vehicle_msgs/msg/TurnIndicatorsReport Turn indicator information"},{"location":"planning/autoware_diffusion_planner/#outputs","title":"Outputs","text":"Topic Message Type Description <code>~/output/trajectory</code> autoware_planning_msgs/msg/Trajectory Planned trajectory for the ego vehicle <code>~/output/trajectories</code> autoware_internal_planning_msgs/msg/CandidateTrajectories Multiple candidate trajectories <code>~/output/predicted_objects</code> autoware_perception_msgs/msg/PredictedObjects Predicted future states of dynamic objects <code>~/output/turn_indicators</code> autoware_vehicle_msgs/msg/TurnIndicatorsCommand Planned turn indicator command <code>~/debug/lane_marker</code> visualization_msgs/msg/MarkerArray Lane debug markers <code>~/debug/route_marker</code> visualization_msgs/msg/MarkerArray Route debug markers"},{"location":"planning/autoware_diffusion_planner/#testing","title":"Testing","text":"<p>Unit tests are provided and can be run with:</p> <pre><code>colcon test --packages-select autoware_diffusion_planner\ncolcon test-result --all\n</code></pre>"},{"location":"planning/autoware_diffusion_planner/#onnx-model-and-versioning","title":"ONNX Model and Versioning","text":"<p>The Diffusion Planner relies on an ONNX model for inference. To ensure compatibility between models and the ROS 2 node implementation, the model versioning scheme follows major and minor numbers: The model version is defined either by the directory name provided to the node or within the <code>diffusion_planner.param.json</code> configuration file.</p> <ul> <li> <p>Major version   Incremented when there are changes in the model inputs/outputs or architecture.</p> <p> Models with different major versions are not compatible with the current ROS node.</p> </li> </ul> <ul> <li>Minor version   Incremented when only the weight files are updated.   As long as the major version matches, the node remains compatible, and the new model can be used directly.</li> </ul> <p>To download the latest model, simply run the provided setup script: How to set up a development environment</p>"},{"location":"planning/autoware_diffusion_planner/#model-version-history","title":"Model Version History","text":"Version Release Date Notes ROS Node Compatibility 0.1 2025/07/05 - First public release- Route planning based on TIER IV real data NG 1.0 2025/09/12 - Route Termination learning- Output turn-signal (indicator) - Lane type integration in HD map for improved accuracy- Added datasets:\u00a0\u00a0- Synthetic Data: 4.0M points\u00a0\u00a0- Real Data: 1.5M points NG 2.0 2025/11/26 - Increased the number of acceptable lane types (\"crosswalk\", \"pedestrian_lane\" and \"walkway\") for left and right boundaries. - Added <code>Polygon</code> and <code>LineString</code> as acceptable input types. - Increased the maximum length of each history record to 3 seconds. - Added support for turn_indicator as an input (this is just an interface, not used in v2.0 weights). - Increased <code>NUM_SEGMENTS_IN_LANE</code> from 70 to 140. OK"},{"location":"planning/autoware_diffusion_planner/#development-contribution","title":"Development &amp; Contribution","text":"<ul> <li>Follow the Autoware coding guidelines.</li> <li>Contributions, bug reports, and feature requests are welcome via GitHub issues and pull requests.</li> </ul>"},{"location":"planning/autoware_diffusion_planner/#references","title":"References","text":"<ul> <li>Diffusion Planner (original repo)</li> <li>Diffusion planner (our fork of the previous repo, used to train the model)</li> <li>\"Diffusion-Based Planning for Autonomous Driving with Flexible Guidance\"</li> </ul>"},{"location":"planning/autoware_diffusion_planner/#license","title":"License","text":"<p>This package is released under the Apache 2.0 License.</p>"},{"location":"planning/autoware_external_velocity_limit_selector/","title":"External Velocity Limit Selector","text":""},{"location":"planning/autoware_external_velocity_limit_selector/#external-velocity-limit-selector","title":"External Velocity Limit Selector","text":""},{"location":"planning/autoware_external_velocity_limit_selector/#purpose","title":"Purpose","text":"<p>The <code>external_velocity_limit_selector_node</code> is a node that keeps consistency of external velocity limits. This module subscribes</p> <ol> <li>velocity limit command sent by API,</li> <li>velocity limit command sent by Autoware internal modules.</li> </ol> <p>VelocityLimit.msg contains not only max velocity but also information about the acceleration/jerk constraints on deceleration. The <code>external_velocity_limit_selector_node</code> integrates the lowest velocity limit and the highest jerk constraint to calculate the hardest velocity limit that protects all the deceleration points and max velocities sent by API and Autoware internal modules.</p> <p></p>"},{"location":"planning/autoware_external_velocity_limit_selector/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>WIP</p>"},{"location":"planning/autoware_external_velocity_limit_selector/#inputs","title":"Inputs","text":"Name Type Description <code>~input/velocity_limit_from_api</code> autoware_internal_planning_msgs::msg::VelocityLimit velocity limit from api <code>~input/velocity_limit_from_internal</code> autoware_internal_planning_msgs::msg::VelocityLimit velocity limit from autoware internal modules <code>~input/velocity_limit_clear_command_from_internal</code> autoware_internal_planning_msgs::msg::VelocityLimitClearCommand velocity limit clear command"},{"location":"planning/autoware_external_velocity_limit_selector/#outputs","title":"Outputs","text":"Name Type Description <code>~output/max_velocity</code> autoware_internal_planning_msgs::msg::VelocityLimit current information of the hardest velocity limit"},{"location":"planning/autoware_external_velocity_limit_selector/#parameters","title":"Parameters","text":"Parameter Type Description <code>max_velocity</code> double default max velocity [m/s] <code>normal.min_acc</code> double minimum acceleration [m/ss] <code>normal.max_acc</code> double maximum acceleration [m/ss] <code>normal.min_jerk</code> double minimum jerk [m/sss] <code>normal.max_jerk</code> double maximum jerk [m/sss] <code>limit.min_acc</code> double minimum acceleration to be observed [m/ss] <code>limit.max_acc</code> double maximum acceleration to be observed [m/ss] <code>limit.min_jerk</code> double minimum jerk to be observed [m/sss] <code>limit.max_jerk</code> double maximum jerk to be observed [m/sss]"},{"location":"planning/autoware_external_velocity_limit_selector/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"planning/autoware_external_velocity_limit_selector/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"planning/autoware_external_velocity_limit_selector/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"planning/autoware_external_velocity_limit_selector/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"planning/autoware_external_velocity_limit_selector/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"planning/autoware_freespace_planner/","title":"The autoware_freespace_planner","text":""},{"location":"planning/autoware_freespace_planner/#the-autoware_freespace_planner","title":"The <code>autoware_freespace_planner</code>","text":""},{"location":"planning/autoware_freespace_planner/#freespace_planner_node","title":"freespace_planner_node","text":"<p><code>freespace_planner_node</code> is a global path planner node that plans trajectory in the space having static/dynamic obstacles. This node is currently based on Hybrid A* search algorithm in <code>freespace_planning_algorithms</code> package. Other algorithms such as rrt* will be also added and selectable in the future.</p> <p>Note Due to the constraint of trajectory following, the output trajectory will be split to include only the single direction path. In other words, the output trajectory doesn't include both forward and backward trajectories at once.</p>"},{"location":"planning/autoware_freespace_planner/#input-topics","title":"Input topics","text":"Name Type Description <code>~input/route</code> autoware_planning_msgs::Route route and goal pose <code>~input/occupancy_grid</code> nav_msgs::OccupancyGrid costmap, for drivable areas <code>~input/odometry</code> nav_msgs::Odometry vehicle velocity, for checking whether vehicle is stopped <code>~input/scenario</code> tier4_planning_msgs::Scenario scenarios to be activated, for node activation"},{"location":"planning/autoware_freespace_planner/#output-topics","title":"Output topics","text":"Name Type Description <code>~output/trajectory</code> autoware_planning_msgs::Trajectory trajectory to be followed <code>is_completed</code> bool (implemented as rosparam) whether all split trajectory are published"},{"location":"planning/autoware_freespace_planner/#output-tfs","title":"Output TFs","text":"<p>None</p>"},{"location":"planning/autoware_freespace_planner/#how-to-launch","title":"How to launch","text":"<ol> <li>Write your remapping info in <code>freespace_planner.launch</code> or add args when executing <code>roslaunch</code></li> <li><code>roslaunch freespace_planner freespace_planner.launch</code></li> </ol>"},{"location":"planning/autoware_freespace_planner/#parameters","title":"Parameters","text":"Name Type Description Default Range planning_algorithm string Planning algorithm to use, options: astar, rrtstar. astar ['astar', 'rrtstar'] waypoints_velocity float velocity in output trajectory (currently, only constant velocity is supported. 5.0 N/A update_rate float timer's update rate 10.0 N/A th_arrived_distance_m float Threshold for considering the vehicle has arrived at its goal. 1.0 N/A th_stopped_time_sec float Time threshold for considering the vehicle as stopped. 1.0 N/A th_stopped_velocity_mps float Velocity threshold for considering the vehicle as stopped. 0.01 N/A th_course_out_distance_m float Threshold distance for considering the vehicle has deviated from its course. 1.0 N/A th_obstacle_time_sec float Time threshold for checking obstacles along the trajectory. 1.0 N/A vehicle_shape_margin_m float Margin around the vehicle shape for planning. 1.0 N/A replan_when_obstacle_found boolean Replan path when an obstacle is found. True N/A replan_when_course_out boolean Replan path when the vehicle deviates from its course. True N/A time_limit float Time limit for the planner. 30000.0 N/A max_turning_ratio float Maximum turning ratio, relative to the actual turning limit of the vehicle. 0.5 N/A turning_steps float Number of steps for turning. 1 N/A theta_size float Number of discretized angles for search. 144 N/A angle_goal_range float Range of acceptable angles at the goal. 6.0 N/A lateral_goal_range float Lateral range of acceptable goal positions. 0.5 N/A longitudinal_goal_range float Longitudinal range of acceptable goal positions. 2.0 N/A curve_weight float Weight for curves in the cost function. 0.5 N/A reverse_weight float Weight for reverse movement in the cost function. 1.0 N/A direction_change_weight float Weight for direction changes in the cost function. 1.5 N/A obstacle_threshold float Threshold for considering an obstacle in the costmap. 100 N/A astar.search_method string Search method to use, options: forward, backward. forward ['forward', 'backward'] astar.only_behind_solutions boolean Consider only solutions behind the vehicle. False N/A astar.use_back boolean Allow reverse motion in A* search. True N/A astar.adapt_expansion_distance boolean Allow varying A* expansion distance based on space configuration. True N/A astar.expansion_distance float Distance for expanding nodes in A* search. 0.5 N/A astar.near_goal_distance float Distance threshold to consider near goal. 4.0 N/A astar.distance_heuristic_weight float Weight for the distance heuristic in A* search. 1.0 N/A astar.smoothness_weight float Weight for the smoothness (change in curvature) in A* search. 0.5 N/A astar.obstacle_distance_weight float Weight for distance to obstacle in A* search. 0.5 N/A astar.goal_lat_distance_weight float Weight for lateral distance from original goal. 0.5 N/A rrtstar.enable_update boolean Enable updates in RRT*. True N/A rrtstar.use_informed_sampling boolean Use informed sampling in RRT*. True N/A rrtstar.max_planning_time float Maximum planning time for RRT*. 150.0 N/A rrtstar.neighbor_radius float Radius for neighboring nodes in RRT*. 8.0 N/A rrtstar.margin float Margin for RRT* sampling. 0.1 N/A"},{"location":"planning/autoware_freespace_planner/#node-parameters","title":"Node parameters","text":"Parameter Type Description <code>planning_algorithms</code> string algorithms used in the node <code>vehicle_shape_margin_m</code> float collision margin in planning algorithm <code>update_rate</code> double timer's update rate <code>waypoints_velocity</code> double velocity in output trajectory (currently, only constant velocity is supported) <code>th_arrived_distance_m</code> double threshold distance to check if vehicle has arrived at the trajectory's endpoint <code>th_stopped_time_sec</code> double threshold time to check if vehicle is stopped <code>th_stopped_velocity_mps</code> double threshold velocity to check if vehicle is stopped <code>th_course_out_distance_m</code> double threshold distance to check if vehicle is out of course <code>th_obstacle_time_sec</code> double threshold time to check if obstacle is on the trajectory <code>vehicle_shape_margin_m</code> double vehicle margin <code>replan_when_obstacle_found</code> bool whether replanning when obstacle has found on the trajectory <code>replan_when_course_out</code> bool whether replanning when vehicle is out of course"},{"location":"planning/autoware_freespace_planner/#planner-common-parameters","title":"Planner common parameters","text":"Parameter Type Description <code>time_limit</code> double time limit of planning <code>maximum_turning_ratio</code> double max ratio of actual turning range to use <code>turning_steps</code> double number of turning steps within turning range <code>theta_size</code> double the number of angle's discretization <code>lateral_goal_range</code> double goal range of lateral position <code>longitudinal_goal_range</code> double goal range of longitudinal position <code>angle_goal_range</code> double goal range of angle <code>curve_weight</code> double additional cost factor for curve actions <code>reverse_weight</code> double additional cost factor for reverse actions <code>direction_change_weight</code> double additional cost factor for switching direction <code>obstacle_threshold</code> double threshold for regarding a certain grid as obstacle"},{"location":"planning/autoware_freespace_planner/#a-search-parameters","title":"A* search parameters","text":"Parameter Type Description <code>search_method</code> string method of searching, start to goal or vice versa <code>only_behind_solutions</code> bool whether restricting the solutions to be behind the goal <code>use_back</code> bool whether using backward trajectory <code>adapt_expansion_distance</code> bool if true, adapt expansion distance based on environment <code>expansion_distance</code> double length of expansion for node transitions <code>near_goal_distance</code> double near goal distance threshold <code>distance_heuristic_weight</code> double heuristic weight for estimating node's cost <code>smoothness_weight</code> double cost factor for change in curvature <code>obstacle_distance_weight</code> double cost factor for distance to obstacle <code>goal_lat_distance_weight</code> double cost factor for lateral distance from goal"},{"location":"planning/autoware_freespace_planner/#rrt-search-parameters","title":"RRT* search parameters","text":"Parameter Type Description <code>max planning time</code> double maximum planning time [msec] (used only when <code>enable_update</code> is set <code>true</code>) <code>enable_update</code> bool whether update after feasible solution found until <code>max_planning time</code> elapse <code>use_informed_sampling</code> bool Use informed RRT* (of Gammell et al.) <code>neighbor_radius</code> double neighbor radius of RRT* algorithm <code>margin</code> double safety margin ensured in path's collision checking in RRT* algorithm"},{"location":"planning/autoware_freespace_planner/#flowchart","title":"Flowchart","text":""},{"location":"planning/autoware_freespace_planning_algorithms/","title":"freespace planning algorithms","text":""},{"location":"planning/autoware_freespace_planning_algorithms/#freespace-planning-algorithms","title":"freespace planning algorithms","text":""},{"location":"planning/autoware_freespace_planning_algorithms/#role","title":"Role","text":"<p>This package is for development of path planning algorithms in free space.</p>"},{"location":"planning/autoware_freespace_planning_algorithms/#implemented-algorithms","title":"Implemented algorithms","text":"<ul> <li>Hybrid A* and RRT* (includes RRT and informed RRT*)</li> </ul> <p>Please see rrtstar.md for a note on the implementation for informed-RRT*.</p> <p>NOTE: As for RRT*, one can choose whether update after feasible solution found in RRT*. If not doing so, the algorithm is the almost (but exactly because of rewiring procedure) same as vanilla RRT. If you choose update, then you have option if the sampling after feasible solution found is \"informed\". If set true, then the algorithm is equivalent to <code>informed RRT\\* of Gammell et al. 2014</code>.</p>"},{"location":"planning/autoware_freespace_planning_algorithms/#algorithm-selection","title":"Algorithm selection","text":"<p>There is a trade-off between algorithm speed and resulting solution quality. When we sort the algorithms by the spectrum of (high quality solution/ slow) -&gt; (low quality solution / fast) it would be A* -&gt; informed RRT* -&gt; RRT. Note that in almost all case informed RRT* is better than RRT* for solution quality given the same computational time budget. So, RRT* is omitted in the comparison.</p> <p>Some selection criteria would be:</p> <ul> <li>If obstacle geometry is complex: -&gt; avoid RRT and RRT*. The resulting path could be too messy.</li> <li>If goal location is far from the start: -&gt; avoid A*. Take too long time because it based on grid discretization.</li> </ul>"},{"location":"planning/autoware_freespace_planning_algorithms/#guide-to-implement-a-new-algorithm","title":"Guide to implement a new algorithm","text":"<ul> <li>All planning algorithm class in this package must inherit <code>AbstractPlanningAlgorithm</code>   class. If necessary, please overwrite the virtual functions.</li> <li>All algorithms must use <code>nav_msgs::OccupancyGrid</code>-typed costmap.   Thus, <code>AbstractPlanningAlgorithm</code> class mainly implements the collision checking   using the costmap, grid-based indexing, and coordinate transformation related to   costmap.</li> <li>All algorithms must take both <code>PlannerCommonParam</code>-typed and algorithm-specific-   type structs as inputs of the constructor. For example, <code>AstarSearch</code> class's   constructor takes both <code>PlannerCommonParam</code> and <code>AstarParam</code>.</li> </ul>"},{"location":"planning/autoware_freespace_planning_algorithms/#running-the-standalone-tests-and-visualization","title":"Running the standalone tests and visualization","text":"<p>Building the package with ros-test and run tests:</p> <pre><code>colcon build --packages-select autoware_freespace_planning_algorithms\ncolcon test --packages-select autoware_freespace_planning_algorithms\n</code></pre> <p>Inside the test, simulation results are stored in <code>/tmp/fpalgos-{algorithm_type}-case{scenario_number}</code> as a rosbag. Loading these resulting files, by using test/debug_plot.py, one can create plots visualizing the path and obstacles as shown in the figures below. The created figures are then again saved in <code>/tmp</code>.</p>"},{"location":"planning/autoware_freespace_planning_algorithms/#a-single-curvature-case","title":"A* (single curvature case)","text":""},{"location":"planning/autoware_freespace_planning_algorithms/#informed-rrt-with-200-msec-time-budget","title":"informed RRT* with 200 msec time budget","text":""},{"location":"planning/autoware_freespace_planning_algorithms/#rrt-without-update-almost-same-as-rrt","title":"RRT* without update (almost same as RRT)","text":"<p>The black cells, green box, and red box, respectively, indicate obstacles, start configuration, and goal configuration. The sequence of the blue boxes indicate the solution path.</p>"},{"location":"planning/autoware_freespace_planning_algorithms/#extension-to-python-module-only-a-supported","title":"Extension to Python module (only A* supported)","text":"<p>There is an implementation of the extension to the python module. You can try A* search via Python by setting follows:</p> <ul> <li>parameters,</li> <li>costmap,</li> <li>start pose,</li> <li>goal pose.</li> </ul> <p>Then, you can get</p> <ul> <li>success or failure,</li> <li>searched trajectory.</li> </ul> <p>The example code is scripts/example/example.py. Note that you need to build this package and source the setup shell script in advance.</p>"},{"location":"planning/autoware_freespace_planning_algorithms/#license-notice","title":"License notice","text":"<p>Files <code>src/reeds_shepp.cpp</code> and <code>include/astar_search/reeds_shepp.h</code> are fetched from pyReedsShepp. Note that the implementation in <code>pyReedsShepp</code> is also heavily based on the code in ompl. Both <code>pyReedsShepp</code> and <code>ompl</code> are distributed under 3-clause BSD license.</p>"},{"location":"planning/autoware_freespace_planning_algorithms/rrtstar/","title":"Rrtstar","text":""},{"location":"planning/autoware_freespace_planning_algorithms/rrtstar/#note-on-implementation-of-informed-rrt","title":"Note on implementation of informed RRT*","text":""},{"location":"planning/autoware_freespace_planning_algorithms/rrtstar/#preliminary-knowledge-on-informed-rrt","title":"Preliminary knowledge on informed-RRT*","text":"<p>Let us define \\(f(x)\\) as minimum cost of the path when path is constrained to pass through \\(x\\) (so path will be \\(x_{\\mathrm{start}} \\to \\mathrm{x} \\to \\mathrm{x_{\\mathrm{goal}}}\\)). Also, let us define \\(c_{\\mathrm{best}}\\) as the current minimum cost of the feasible paths. Let us define a set $ X(f) = \\left{ x \\in X | f(x) &lt; c*{\\mathrm{best}} \\right} $. If we could sample a new point from \\(X_f\\) instead of \\(X\\) as in vanilla RRT*, chance that \\(c*{\\mathrm{best}}\\) is updated is increased, thus the convergence rate is improved.</p> <p>In most case, \\(f(x)\\) is unknown, thus it is straightforward to approximate the function \\(f\\) by a heuristic function \\(\\hat{f}\\). A heuristic function is admissible if \\(\\forall x \\in X, \\hat{f}(x) &lt; f(x)\\), which is sufficient condition of conversion to optimal path. The good heuristic function \\(\\hat{f}\\) has two properties: 1) it is an admissible tight lower bound of \\(f\\) and 2) sampling from \\(X(\\hat{f})\\) is easy.</p> <p>According to Gammell et al [1], a good heuristic function when path is always straight is \\(\\hat{f}(x) = ||x_{\\mathrm{start}} - x|| + ||x - x_{\\mathrm{goal}}||\\). If we don't assume any obstacle information the heuristic is tightest. Also, \\(X(\\hat{f})\\) is hyper-ellipsoid, and hence sampling from it can be done analytically.</p>"},{"location":"planning/autoware_freespace_planning_algorithms/rrtstar/#modification-to-fit-reeds-sheep-path-case","title":"Modification to fit reeds-sheep path case","text":"<p>In the vehicle case, state is \\(x = (x_{1}, x_{2}, \\theta)\\). Unlike normal informed-RRT* where we can connect path by a straight line, here we connect the vehicle path by a reeds-sheep path. So, we need some modification of the original algorithm a bit. To this end, one might first consider a heuristic function \\(\\hat{f}_{\\mathrm{RS}}(x) = \\mathrm{RS}(x_{\\mathrm{start}}, x) + \\mathrm{RS}(x, x_{\\mathrm{goal}}) &lt; f(x)\\) where \\(\\mathrm{RS}\\) computes reeds-sheep distance. Though it is good in the sense of tightness, however, sampling from \\(X(\\hat{f}_{RS})\\) is really difficult. Therefore, we use \\(\\hat{f}_{euc} = ||\\mathrm{pos}(x_{\\mathrm{start}}) - \\mathrm{pos}(x)|| + ||\\mathrm{pos}(x)- \\mathrm{pos}(x_{\\mathrm{goal}})||\\), which is admissible because \\(\\forall x \\in X, \\hat{f}_{euc}(x) &lt; \\hat{f}_{\\mathrm{RS}}(x) &lt; f(x)\\). Here, \\(\\mathrm{pos}\\) function returns position \\((x_{1}, x_{2})\\) of the vehicle.</p> <p>Sampling from \\(X(\\hat{f}_{\\mathrm{euc}})\\) is easy because \\(X(\\hat{f}_{\\mathrm{euc}}) = \\mathrm{Ellipse} \\times (-\\pi, \\pi]\\). Here \\(\\mathrm{Ellipse}\\)'s focal points are \\(x_{\\mathrm{start}}\\) and \\(x_{\\mathrm{goal}}\\) and conjugate diameters is $\\sqrt{c^{2}{\\mathrm{best}} - ||\\mathrm{pos}(x}) - \\mathrm{pos}(x_{\\mathrm{goal}}))|| } $ (similar to normal informed-rrtstar's ellipsoid). Please notice that \\(\\theta\\) can be arbitrary because \\(\\hat{f}_{\\mathrm{euc}}\\) is independent of \\(\\theta\\).</p> <p>[1] Gammell et al., \"Informed RRT*: Optimal sampling-based path planning focused via direct sampling of an admissible ellipsoidal heuristic.\" IROS (2014)</p>"},{"location":"planning/autoware_hazard_lights_selector/","title":"Autoware Hazard Lights Selector","text":""},{"location":"planning/autoware_hazard_lights_selector/#autoware-hazard-lights-selector","title":"Autoware Hazard Lights Selector","text":""},{"location":"planning/autoware_hazard_lights_selector/#purpose","title":"Purpose","text":"<p>The Hazard Lights Selector is a node that manages and selects hazard light commands from multiple sources (planning and system) to determine the final hazard lights state of the vehicle.</p>"},{"location":"planning/autoware_hazard_lights_selector/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The node operates on a timer-based update cycle and implements the following logic:</p> <ol> <li>Receives hazard light commands from both planning and system sources</li> <li>If either source requests hazard lights to be enabled, the final command will be to enable hazard lights</li> <li>If neither source requests hazard lights to be enabled, the final command will be to disable hazard lights</li> <li>The selected command is published at a configurable update rate</li> </ol>"},{"location":"planning/autoware_hazard_lights_selector/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"planning/autoware_hazard_lights_selector/#input","title":"Input","text":"Name Type Description <code>input/planning/hazard_lights_command</code> <code>autoware_vehicle_msgs/msg/HazardLightsCommand</code> Hazard lights command from the planning system <code>input/system/hazard_lights_command</code> <code>autoware_vehicle_msgs/msg/HazardLightsCommand</code> Hazard lights command from the system"},{"location":"planning/autoware_hazard_lights_selector/#output","title":"Output","text":"Name Type Description <code>output/hazard_lights_command</code> <code>autoware_vehicle_msgs/msg/HazardLightsCommand</code> The selected hazard lights command"},{"location":"planning/autoware_hazard_lights_selector/#parameters","title":"Parameters","text":"Name Type Default Value Description <code>update_rate</code> int 10 The update rate in Hz for publishing hazard lights commands"},{"location":"planning/autoware_hazard_lights_selector/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"planning/autoware_hazard_lights_selector/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"planning/autoware_hazard_lights_selector/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"planning/autoware_hazard_lights_selector/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"planning/autoware_hazard_lights_selector/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"planning/autoware_manual_lane_change_handler/","title":"Manual Lane Change Handler","text":""},{"location":"planning/autoware_manual_lane_change_handler/#manual-lane-change-handler","title":"Manual Lane Change Handler","text":""},{"location":"planning/autoware_manual_lane_change_handler/#purpose","title":"Purpose","text":"<p><code>Manual Lane Change Handler</code> helps shift the preferred-primitives of a route to the left or right. A manual lane-change handler rviz plugin is also available for use for the user.</p>"},{"location":"planning/autoware_manual_lane_change_handler/#example-for-using-the-manual-lane-change-handler","title":"Example for using the Manual Lane Change Handler","text":"<p>Service call to shift the preferred-lanes to the left</p> <pre><code>ros2 service call /planning/mission_planning/manual_lane_change_handler/set_preferred_lane tier4_external_api_msgs/srv/SetPreferredLane lane_change_direction:\\ 0\\\n</code></pre>"},{"location":"planning/autoware_manual_lane_change_handler/#services","title":"Services","text":"Name Type Description <code>/planning/manual_lane_change_handler/manual_lane_change_handler/set_preferred_lane</code> tier4_external_api_msgs/srv/SetPreferredLane preferred-lane request"},{"location":"planning/autoware_manual_lane_change_handler/#subscriptions","title":"Subscriptions","text":"Name Type Description <code>input/odometry</code> nav_msgs/msg/Odometry vehicle odometry <code>/planning/mission_planning/route</code> autoware_planning_msgs/msg/LaneletRoute current lanelet route"},{"location":"planning/autoware_manual_lane_change_handler/#publications","title":"Publications","text":"Name Type Description <code>~/debug/processing_time_ms</code> autoware_internal_debug_msgs/msg/Float64Stamped processing time for lane change for debug <code>~/debug/route_marker</code> visualization_msgs/msg/MarkerArray route marker for debug <code>~/debug/goal_footprint</code> visualization_msgs/msg/MarkerArray goal footprint for debug"},{"location":"planning/autoware_manual_lane_change_handler/#service-client","title":"Service client","text":"Service Name Description ~/set_preferred_lane The service to set the preferred lane"},{"location":"planning/autoware_manual_lane_change_handler/#registered-servers","title":"Registered Servers","text":"Service Name Description /planning/mission_planning/mission_planner/set_preferred_primitive The service to set the preferred primitive"},{"location":"planning/autoware_manual_lane_change_handler/#setting-preferred-lane","title":"Setting Preferred Lane","text":"<p>This service allows for shifting the current preferred lane to the left or right, or reverting to the default preferred lane received from the route planner. As the vehicle moves, the shifting is done relative to the current preferred lane.</p> <p>Based on certain criteria, shifting may be rejected, as in the following cases:</p> <ol> <li>Left or Right shift is not available due to no lane being present to shift to</li> <li>The next segment is a turn or the very last lane - this is to ensure that we can navigate the enter path and end up at the goal</li> </ol> <p></p>"},{"location":"planning/autoware_mission_planner_universe/","title":"Mission Planner","text":""},{"location":"planning/autoware_mission_planner_universe/#mission-planner","title":"Mission Planner","text":""},{"location":"planning/autoware_mission_planner_universe/#purpose","title":"Purpose","text":"<p><code>Mission Planner</code> calculates a route that navigates from the current ego pose to the goal pose following the given check points. The route is made of a sequence of lanes on a static map. Dynamic objects (e.g. pedestrians and other vehicles) and dynamic map information (e.g. road construction which blocks some lanes) are not considered during route planning. Therefore, the output topic is only published when the goal pose or check points are given and will be latched until the new goal pose or check points are given.</p> <p>The core implementation does not depend on a map format. Any planning algorithms can be added as plugin modules. In current Autoware Universe, only the plugin for Lanelet2 map format is supported.</p> <p>This package also manages routes for MRM. The <code>route_selector</code> node duplicates the <code>mission_planner</code> interface and provides it for normal and MRM respectively. It distributes route requests and planning results according to current MRM operation state.</p> <p></p>"},{"location":"planning/autoware_mission_planner_universe/#interfaces","title":"Interfaces","text":""},{"location":"planning/autoware_mission_planner_universe/#parameters","title":"Parameters","text":"Name Type Description <code>map_frame</code> string The frame name for map <code>arrival_check_angle_deg</code> double Angle threshold for goal check <code>arrival_check_lateral_distance</code> double Lateral distance threshold for goal check <code>arrival_check_longitudinal_undershoot_distance</code> double Longitudinal distance threshold for goal check when the vehicle has not yet reached the goal <code>arrival_check_longitudinal_overshoot_distance</code> double Longitudinal distance threshold for goal check when the vehicle has passed the goal <code>arrival_check_duration</code> double Duration threshold for goal check <code>goal_angle_threshold</code> double Max goal pose angle for goal approve <code>enable_correct_goal_pose</code> bool Enabling correction of goal pose according to the closest lanelet orientation <code>reroute_time_threshold</code> double If the time to the rerouting point at the current velocity is greater than this threshold, rerouting is possible <code>minimum_reroute_length</code> double Minimum Length for publishing a new route <code>consider_no_drivable_lanes</code> bool This flag is for considering no_drivable_lanes in planning or not. <code>allow_reroute_in_autonomous_mode</code> bool This is a flag to allow reroute in autonomous driving mode. If false, reroute fails. If true, only safe reroute is allowed"},{"location":"planning/autoware_mission_planner_universe/#services","title":"Services","text":"Name Type Description <code>/planning/mission_planning/mission_planner/clear_route</code> tier4_planning_msgs/srv/ClearRoute route clear request <code>/planning/mission_planning/mission_planner/set_waypoint_route</code> tier4_planning_msgs/srv/SetWaypointRoute route request with lanelet waypoints. <code>/planning/mission_planning/mission_planner/set_lanelet_route</code> tier4_planning_msgs/srv/SetLaneletRoute route request with pose waypoints. <code>/planning/mission_planning/mission_planner/set_preferred_primitive</code> tier4_planning_msgs/srv/SetPreferredPrimitive preferred-lane change request <code>/planning/mission_planning/route_selector/main/clear_route</code> tier4_planning_msgs/srv/ClearRoute main route clear request <code>/planning/mission_planning/route_selector/main/set_waypoint_route</code> tier4_planning_msgs/srv/SetWaypointRoute main route request with lanelet waypoints. <code>/planning/mission_planning/route_selector/main/set_lanelet_route</code> tier4_planning_msgs/srv/SetLaneletRoute main route request with pose waypoints. <code>/planning/mission_planning/route_selector/mrm/clear_route</code> tier4_planning_msgs/srv/ClearRoute mrm route clear request <code>/planning/mission_planning/route_selector/mrm/set_waypoint_route</code> tier4_planning_msgs/srv/SetWaypointRoute mrm route request with lanelet waypoints. <code>/planning/mission_planning/route_selector/mrm/set_lanelet_route</code> tier4_planning_msgs/srv/SetLaneletRoute mrm route request with pose waypoints."},{"location":"planning/autoware_mission_planner_universe/#subscriptions","title":"Subscriptions","text":"Name Type Description <code>input/vector_map</code> autoware_map_msgs/msg/LaneletMapBin vector map of Lanelet2 <code>input/modified_goal</code> geometry_msgs/PoseWithUuidStamped modified goal pose <code>input/operation_mode_state</code> autoware_adapi_v1_msgs/OperationModeState operation mode state <code>input/odometry</code> nav_msgs/msg/Odometry vehicle odometry"},{"location":"planning/autoware_mission_planner_universe/#publications","title":"Publications","text":"Name Type Description <code>/planning/mission_planning/state</code> tier4_planning_msgs/msg/RouteState route state <code>/planning/mission_planning/route</code> autoware_planning_msgs/LaneletRoute route <code>/planning/mission_planning/route_selector/main/state</code> tier4_planning_msgs/msg/RouteState main route state <code>/planning/mission_planning/route_selector/main/route</code> autoware_planning_msgs/LaneletRoute main route <code>/planning/mission_planning/route_selector/mrm/state</code> tier4_planning_msgs/msg/RouteState mrm route state <code>/planning/mission_planning/route_selector/mrm/route</code> autoware_planning_msgs/LaneletRoute mrm route <code>~/debug/route_marker</code> visualization_msgs/msg/MarkerArray route marker for debug <code>~/debug/goal_footprint</code> visualization_msgs/msg/MarkerArray goal footprint for debug"},{"location":"planning/autoware_mission_planner_universe/#route-section","title":"Route section","text":"<p>Route section, whose type is <code>autoware_planning_msgs/LaneletSegment</code>, is a \"slice\" of a road that bundles lane changeable lanes. Note that the most atomic unit of route is <code>autoware_planning_msgs/LaneletPrimitive</code>, which has the unique id of a lane in a vector map and its type. Therefore, route message does not contain geometric information about the lane since we did not want to have planning module\u2019s message to have dependency on map data structure.</p> <p>The ROS message of route section contains following three elements for each route section.</p> <ul> <li><code>preferred_primitive</code>: Preferred lane to follow towards the goal.</li> <li><code>primitives</code>: All neighbor lanes in the same direction including the preferred lane.</li> </ul>"},{"location":"planning/autoware_mission_planner_universe/#goal-validation","title":"Goal Validation","text":"<p>The mission planner has control mechanism to validate the given goal pose and create a route. If goal pose angle between goal pose lanelet and goal pose' yaw is greater than <code>goal_angle_threshold</code> parameter, the goal is rejected. Another control mechanism is the creation of a footprint of the goal pose according to the dimensions of the vehicle and checking whether this footprint is within the lanelets. If goal footprint exceeds lanelets, then the goal is rejected.</p> <p>At the image below, there are sample goal pose validation cases.</p> <p></p>"},{"location":"planning/autoware_mission_planner_universe/#implementation","title":"Implementation","text":""},{"location":"planning/autoware_mission_planner_universe/#mission-planner_1","title":"Mission Planner","text":"<p>Two callbacks (goal and check points) are a trigger for route planning. Routing graph, which plans route in Lanelet2, must be created before those callbacks, and this routing graph is created in vector map callback.</p> <p><code>plan route</code> is explained in detail in the following section.</p> <p></p>"},{"location":"planning/autoware_mission_planner_universe/#route-planner","title":"Route Planner","text":"<p><code>plan route</code> is executed with check points including current ego pose and goal pose.</p> <p></p> <p><code>plan path between each check points</code> firstly calculates closest lanes to start and goal pose. Then routing graph of Lanelet2 plans the shortest path from start and goal pose.</p> <p><code>initialize route lanelets</code> initializes route handler, and calculates <code>route_lanelets</code>. <code>route_lanelets</code>, all of which will be registered in route sections, are lanelets next to the lanelets in the planned path, and used when planning lane change. To calculate <code>route_lanelets</code>,</p> <ol> <li>All the neighbor (right and left) lanes for the planned path which is lane-changeable is memorized as <code>route_lanelets</code>.</li> <li>All the neighbor (right and left) lanes for the planned path which is not lane-changeable is memorized as <code>candidate_lanelets</code>.</li> <li>If the following and previous lanelets of each <code>candidate_lanelets</code> are <code>route_lanelets</code>, the <code>candidate_lanelet</code> is registered as <code>route_lanelets</code><ul> <li>This is because even though <code>candidate_lanelet</code> (an adjacent lane) is not lane-changeable, we can pass the <code>candidate_lanelet</code> without lane change if the following and previous lanelets of the <code>candidate_lanelet</code> are <code>route_lanelets</code></li> </ul> </li> </ol> <p><code>get preferred lanelets</code> extracts <code>preferred_primitive</code> from <code>route_lanelets</code> with the route handler.</p> <p><code>create route sections</code> extracts <code>primitives</code> from <code>route_lanelets</code> for each route section with the route handler, and creates route sections.</p>"},{"location":"planning/autoware_mission_planner_universe/#rerouting","title":"Rerouting","text":"<p>Reroute here means changing the route while driving. Unlike route setting, it is required to keep a certain distance from vehicle to the point where the route is changed. If the ego vehicle is not on autonomous driving state, the safety checking process will be skipped.</p> <p></p> <p>And there are three use cases that require reroute.</p> <ul> <li>Route change API</li> <li>Emergency route</li> <li>Goal modification</li> </ul>"},{"location":"planning/autoware_mission_planner_universe/#route-change-api","title":"Route change API","text":"<p>It is used when changing the destination while driving or when driving a divided loop route. When the vehicle is driving on a MRM route, normal rerouting by this interface is not allowed.</p>"},{"location":"planning/autoware_mission_planner_universe/#emergency-route","title":"Emergency route","text":"<p>The interface for the MRM that pulls over the road shoulder. It has to be stopped as soon as possible, so a reroute is required. The MRM route has priority over the normal route. And if MRM route is cleared, try to return to the normal route also with a rerouting safety check.</p>"},{"location":"planning/autoware_mission_planner_universe/#goal-modification","title":"Goal modification","text":"<p>This is a goal change to pull over, avoid parked vehicles, and so on by a planning component. If the modified goal is outside the calculated route, a reroute is required. This goal modification is executed by checking the local environment and path safety as the vehicle actually approaches the destination. And this modification is allowed for both normal_route and mrm_route. The new route generated here is sent to the AD API so that it can also be referenced by the application. Note, however, that the specifications here are subject to change in the future.</p>"},{"location":"planning/autoware_mission_planner_universe/#setting-preferred-lane","title":"Setting Preferred Lane","text":"<p>The manual lane change service allows external modification to the preferred-lanes of the current-route.</p>"},{"location":"planning/autoware_mission_planner_universe/#rerouting-limitations","title":"Rerouting Limitations","text":"<ul> <li>The safety judgment of rerouting is not guaranteed to the level of trajectory or control. Therefore, the distance to the reroute change must be large for the safety.</li> <li>The validity of the <code>modified_goal</code> needs to be guaranteed by the behavior_path_planner, e.g., that it is not placed in the wrong lane, that it can be safely rerouted, etc.</li> </ul>"},{"location":"planning/autoware_mission_planner_universe/#limitations","title":"Limitations","text":"<ul> <li>Dynamic objects (e.g. pedestrians and other vehicles) and dynamic map information (e.g. road construction which blocks some lanes) are not considered during route planning.</li> <li>Looped route is not supported.</li> </ul>"},{"location":"planning/autoware_path_optimizer/","title":"Path optimizer","text":""},{"location":"planning/autoware_path_optimizer/#path-optimizer","title":"Path optimizer","text":""},{"location":"planning/autoware_path_optimizer/#purpose","title":"Purpose","text":"<p>This package generates a trajectory that is kinematically-feasible to drive and collision-free based on the input path, drivable area. Only position and orientation of trajectory are updated in this module, and velocity is just taken over from the one in the input path.</p>"},{"location":"planning/autoware_path_optimizer/#feature","title":"Feature","text":"<p>This package is able to</p> <ul> <li>make the trajectory inside the drivable area as much as possible<ul> <li>NOTE: Static obstacles to avoid can be removed from the drivable area.</li> </ul> </li> <li>insert stop point before the planned footprint will be outside the drivable area</li> </ul> <p>Note that the velocity is just taken over from the input path.</p>"},{"location":"planning/autoware_path_optimizer/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"planning/autoware_path_optimizer/#input","title":"input","text":"Name Type Description <code>~/input/path</code> autoware_planning_msgs/msg/Path Reference path and the corresponding drivable area <code>~/input/odometry</code> nav_msgs/msg/Odometry Current Velocity of ego vehicle"},{"location":"planning/autoware_path_optimizer/#output","title":"output","text":"Name Type Description <code>~/output/trajectory</code> autoware_planning_msgs/msg/Trajectory Optimized trajectory that is feasible to drive and collision-free"},{"location":"planning/autoware_path_optimizer/#flowchart","title":"Flowchart","text":"<p>Flowchart of functions is explained here.</p> <p></p>"},{"location":"planning/autoware_path_optimizer/#createplannerdata","title":"createPlannerData","text":"<p>The following data for planning is created.</p> <pre><code>struct PlannerData\n{\n  // input\n  Header header;\n  std::vector&lt;TrajectoryPoint&gt; traj_points; // converted from the input path\n  std::vector&lt;geometry_msgs::msg::Point&gt; left_bound;\n  std::vector&lt;geometry_msgs::msg::Point&gt; right_bound;\n\n  // ego\n  geometry_msgs::msg::Pose ego_pose;\n  double ego_vel;\n};\n</code></pre>"},{"location":"planning/autoware_path_optimizer/#check-replan","title":"check replan","text":"<p>When one of the following conditions are met, trajectory optimization will be executed. Otherwise, previously optimized trajectory is used with updating the velocity from the latest input path.</p> <p>max_path_shape_around_ego_lat_dist</p> <ul> <li>Ego moves longer than <code>replan.max_ego_moving_dist</code> in one cycle. (default: 3.0 [m])<ul> <li>This is for when the ego pose is set again in the simulation.</li> </ul> </li> <li>Trajectory's end, which is considered as the goal pose, moves longer than <code>replan.max_goal_moving_dist</code> in one cycle. (default: 15.0 [ms])<ul> <li>When the goal pose is set again, the planning should be reset.</li> </ul> </li> <li>Time passes. (default: 1.0 [s])<ul> <li>The optimization is skipped for a while sine the optimization is sometimes heavy.</li> </ul> </li> <li>The input path changes laterally longer than <code>replan.max_path_shape_around_ego_lat_dist</code> in one cycle. (default: 2.0)</li> </ul>"},{"location":"planning/autoware_path_optimizer/#getmodelpredictivetrajectory","title":"getModelPredictiveTrajectory","text":"<p>This module makes the trajectory kinematically-feasible and collision-free. We define vehicle pose in the frenet coordinate, and minimize tracking errors by optimization. This optimization considers vehicle kinematics and collision checking with road boundary and obstacles. To decrease the computation cost, the optimization is applied to the shorter trajectory (default: 50 [m]) than the whole trajectory, and concatenate the remained trajectory with the optimized one at last.</p> <p>The trajectory just in front of the ego must not be changed a lot so that the steering wheel will be stable. Therefore, we use the previously generated trajectory in front of the ego.</p> <p>Optimization center on the vehicle, that tries to locate just on the trajectory, can be tuned along side the vehicle vertical axis. This parameter <code>mpt.kinematics.optimization center offset</code> is defined as the signed length from the back-wheel center to the optimization center. Some examples are shown in the following figure, and it is shown that the trajectory of vehicle shape differs according to the optimization center even if the reference trajectory (green one) is the same.</p> <p></p> <p>More details can be seen here.</p>"},{"location":"planning/autoware_path_optimizer/#applyinputvelocity","title":"applyInputVelocity","text":"<p>Velocity is assigned in the optimized trajectory from the velocity in the behavior path. The shapes of the optimized trajectory and the path are different, therefore the each nearest trajectory point to the path is searched and the velocity is interpolated with zero-order hold.</p>"},{"location":"planning/autoware_path_optimizer/#insertzerovelocityoutsidedrivablearea","title":"insertZeroVelocityOutsideDrivableArea","text":"<p>Optimized trajectory is too short for velocity planning, therefore extend the trajectory by concatenating the optimized trajectory and the behavior path considering drivability. Generated trajectory is checked if it is inside the drivable area or not, and if outside drivable area, output a trajectory inside drivable area with the behavior path or the previously generated trajectory.</p> <p>As described above, the behavior path is separated into two paths: one is for optimization and the other is the remain. The first path becomes optimized trajectory, and the second path just is transformed to a trajectory. Then a trajectory inside the drivable area is calculated as follows.</p> <ul> <li>If optimized trajectory is inside the drivable area, and the remained trajectory is inside/outside the drivable area,<ul> <li>the output trajectory will be just concatenation of those two trajectories.</li> <li>In this case, we do not care if the remained trajectory is inside or outside the drivable area since generally it is outside the drivable area (especially in a narrow road), but we want to pass a trajectory as long as possible to the latter module.</li> </ul> </li> <li>If optimized trajectory is outside the drivable area, and the remained trajectory is inside/outside the drivable area,<ul> <li>and if the previously generated trajectory is memorized,<ul> <li>the output trajectory will be the previously generated trajectory, where zero velocity is inserted to the point firstly going outside the drivable area.</li> </ul> </li> <li>and if the previously generated trajectory is not memorized,<ul> <li>the output trajectory will be a part of trajectory just transformed from the behavior path, where zero velocity is inserted to the point firstly going outside the drivable area.</li> </ul> </li> </ul> </li> </ul> <p>Optimization failure is dealt with the same as if the optimized trajectory is outside the drivable area. The output trajectory is memorized as a previously generated trajectory for the next cycle.</p> <p>Rationale In the current design, since there are some modelling errors, the constraints are considered to be soft constraints. Therefore, we have to make sure that the optimized trajectory is inside the drivable area or not after optimization.</p>"},{"location":"planning/autoware_path_optimizer/#limitation","title":"Limitation","text":"<ul> <li>Computation cost is sometimes high.</li> <li>Because of the approximation such as linearization, some narrow roads cannot be run by the planner.</li> <li>Roles of planning for <code>behavior_path_planner</code> and <code>path_optimizer</code> are not decided clearly. Both can avoid obstacles.</li> </ul>"},{"location":"planning/autoware_path_optimizer/#comparison-to-other-methods","title":"Comparison to other methods","text":"<p>Trajectory planning problem that satisfies kinematically-feasibility and collision-free has two main characteristics that makes hard to be solved: one is non-convex and the other is high dimension. Based on the characteristics, we investigate pros/cons of the typical planning methods: optimization-based, sampling-based, and learning-based method.</p>"},{"location":"planning/autoware_path_optimizer/#optimization-based-method","title":"Optimization-based method","text":"<ul> <li>pros: comparatively fast against high dimension by leveraging the gradient descent</li> <li>cons: often converge to the local minima in the non-convex problem</li> </ul>"},{"location":"planning/autoware_path_optimizer/#sampling-based-method","title":"Sampling-based method","text":"<ul> <li>pros: realize global optimization</li> <li>cons: high computation cost especially in the complex case</li> </ul>"},{"location":"planning/autoware_path_optimizer/#learning-based-method","title":"Learning-based method","text":"<ul> <li>under research yet</li> </ul> <p>Based on these pros/cons, we chose the optimization-based planner first. Although it has a cons to converge to the local minima, it can get a good solution by the preprocessing to approximate the problem to convex that almost equals to the original non-convex problem.</p>"},{"location":"planning/autoware_path_optimizer/#how-to-tune-parameters","title":"How to Tune Parameters","text":""},{"location":"planning/autoware_path_optimizer/#drivability-in-narrow-roads","title":"Drivability in narrow roads","text":"<ul> <li>modify <code>mpt.clearance.soft_clearance_from_road</code><ul> <li>This parameter describes how much margin to make between the trajectory and road boundaries.</li> <li>Due to the model error for optimization, the constraint such as collision-free is not fully met.<ul> <li>By making this parameter larger, the is for narrow-road driving may be resolved. 12180</li> </ul> </li> </ul> </li> <li>modify <code>mpt.kinematics.optimization_center_offset</code><ul> <li>The point on the vehicle, offset forward with this parameter from the base link` tries to follow the reference path.</li> </ul> </li> </ul> <ul> <li>change or tune the method to approximate footprints with a set of circles.<ul> <li>See here</li> <li>Tuning means changing the ratio of circle's radius.</li> </ul> </li> </ul>"},{"location":"planning/autoware_path_optimizer/#computation-time","title":"Computation time","text":"<ul> <li>under construction</li> </ul>"},{"location":"planning/autoware_path_optimizer/#robustness","title":"Robustness","text":"<ul> <li>Check if the trajectory before or after MPT is not robust<ul> <li>if the trajectory before MPT is not robust</li> <li>if the trajectory after MPT is not robust<ul> <li>make <code>mpt.weight.steer_input_weight</code> or <code>mpt.weight.steer_rate_weight</code> larger, which are stability of steering wheel along the trajectory.</li> </ul> </li> </ul> </li> </ul>"},{"location":"planning/autoware_path_optimizer/#other-options","title":"Other options","text":"<ul> <li><code>option.enable_skip_optimization</code> skips MPT optimization.</li> <li><code>option.enable_calculation_time_info</code> enables showing each calculation time for functions and total calculation time on the terminal.</li> <li><code>option.enable_outside_drivable_area_stop</code> enables stopping just before the generated trajectory point will be outside the drivable area.</li> </ul>"},{"location":"planning/autoware_path_optimizer/#parameters","title":"Parameters","text":"Name Type Description Default Range option.enable_skip_optimization boolean Skip elastic band and model predictive trajectory optimization. False N/A option.enable_reset_prev_optimization boolean If true, optimization has no fix constraint to the previous result. False N/A option.enable_outside_drivable_area_stop boolean Stop if the ego's trajectory footprint is outside the drivable area. False N/A option.use_footprint_polygon_for_outside_drivable_area_check boolean If false, only the footprint's corner points are considered for drivable area check. False N/A debug.enable_pub_debug_marker boolean Publish debug markers. False N/A debug.enable_pub_extra_debug_marker boolean Publish extra debug markers. False N/A debug.enable_debug_info boolean Enable debug information. False N/A debug.enable_calculation_time_info boolean Enable calculation time information. False N/A common.output_delta_arc_length float Delta arc length for output trajectory [m]. 0.5 N/A common.output_backward_traj_length float Backward length for backward trajectory from base_link [m]. 5.0 N/A common.vehicle_stop_margin_outside_drivable_area float Vehicle stop margin to let the vehicle stop before the calculated stop point if it is calculated outside the drivable area [m]. 0.0 N/A replan.max_path_shape_around_ego_lat_dist float Threshold of path shape change around ego [m]. 2.0 N/A replan.max_path_shape_forward_lon_dist float Forward point to check lateral distance difference [m]. 100.0 N/A replan.max_path_shape_forward_lat_dist float Threshold of path shape change around forward point [m]. 0.1 N/A replan.max_ego_moving_dist float Threshold of ego's moving distance for replan [m]. 5.0 N/A replan.max_goal_moving_dist float Threshold of goal's moving distance for replan [m]. 15.0 N/A replan.max_delta_time_sec float Threshold of delta time for replan [second]. 0.0 N/A option.steer_limit_constraint boolean Enable steer limit constraint. False N/A option.visualize_sampling_num integer Number of samples to visualize. 1 N/A option.enable_manual_warm_start boolean Enable manual warm start. False N/A option.enable_warm_start boolean Enable warm start. False N/A option.enable_optimization_validation boolean Enable optimization validation. False N/A common.num_points integer Number of points for optimization. 100 N/A common.delta_arc_length float Delta arc length for optimization [m]. 1.0 N/A clearance.hard_clearance_from_road float Clearance from road boundary [m] if collision_free_constraints.option.hard_constraint is true. 0.0 N/A clearance.soft_clearance_from_road float Clearance from road boundary [m] if collision_free_constraints.option.soft_constraint is true. 0.1 N/A weight.soft_collision_free_weight float Soft weight for lateral error around the middle point. 1.0 N/A weight.lat_error_weight float Weight for lateral error. 1.0 N/A weight.yaw_error_weight float Weight for yaw error. 0.0 N/A weight.yaw_error_rate_weight float Weight for yaw error rate. 0.0 N/A weight.steer_input_weight float Weight for steering input. 1.0 N/A weight.steer_rate_weight float Weight for steering rate. 1.0 N/A weight.terminal_lat_error_weight float Weight for lateral error at terminal point. 100.0 N/A weight.terminal_yaw_error_weight float Weight for yaw error at terminal point. 100.0 N/A weight.goal_lat_error_weight float Weight for lateral error at path end point. 1000.0 N/A weight.goal_yaw_error_weight float Weight for yaw error at path end point. 1000.0 N/A avoidance.max_bound_fixing_time float Maximum bound fixing time [s]. 1.0 N/A avoidance.max_longitudinal_margin_for_bound_violation float Maximum longitudinal margin for bound violation [m]. 1.0 N/A avoidance.max_avoidance_cost float Maximum avoidance cost [m]. 0.5 N/A avoidance.avoidance_cost_margin float Avoidance cost margin [m]. 0.0 N/A avoidance.avoidance_cost_band_length float Avoidance cost band length [m]. 5.0 N/A avoidance.avoidance_cost_decrease_rate float Decreased cost per point interval. 0.05 N/A avoidance.min_drivable_width float Minimum drivable width [m]. The vehicle width and this parameter are guaranteed to keep for collision-free constraint. 0.2 N/A weight.lat_error_weight float Weight for lateral error. 0.0 N/A weight.yaw_error_weight float Weight for yaw error. 10.0 N/A weight.steer_input_weight float Weight for yaw error. 100.0 N/A option.l_inf_norm boolean Use L-infinity norm for collision-free constraints. True N/A option.soft_constraint boolean Enable soft constraints. True N/A option.hard_constraint boolean Enable hard constraints. False N/A vehicle_circles.method string Method to represent footprint as circles. fitting_uniform_circle N/A bicycle_model.num_for_calculation integer Number of circles for calculation. 3 N/A bicycle_model.front_radius_ratio float Front radius ratio. 1.0 N/A bicycle_model.rear_radius_ratio float Rear radius ratio. 1.0 N/A uniform_circle.num integer Number of circles. 3 N/A uniform_circle.radius_ratio float Radius ratio. 1.0 N/A fitting_uniform_circle.num integer Number of circles. 3 N/A validation.max_lat_error float Maximum lateral error for validation [m]. 5.0 N/A validation.max_yaw_error float Maximum yaw error for validation [rad]. 1.046 N/A"},{"location":"planning/autoware_path_optimizer/#how-to-debug","title":"How To Debug","text":"<p>How to debug can be seen here.</p>"},{"location":"planning/autoware_path_optimizer/docs/debug/","title":"Debug","text":""},{"location":"planning/autoware_path_optimizer/docs/debug/#debug","title":"Debug","text":""},{"location":"planning/autoware_path_optimizer/docs/debug/#debug-visualization","title":"Debug visualization","text":"<p>The visualization markers of the planning flow (Input, Model Predictive Trajectory, and Output) are explained here.</p> <p>All the following markers can be visualized by</p> <pre><code>ros2 launch autoware_path_optimizer launch_visualiation.launch.xml vehilce_model:=sample_vehicle\n</code></pre> <p>The <code>vehicle_model</code> must be specified to make footprints with vehicle's size.</p>"},{"location":"planning/autoware_path_optimizer/docs/debug/#input","title":"Input","text":"<ul> <li>Path<ul> <li>The path generated in the <code>behavior</code> planner.</li> <li>The semitransparent and thick, green and red band, that is visualized by default.</li> </ul> </li> </ul> <ul> <li>Path Footprint<ul> <li>The path generated in the <code>behavior</code> planner is converted to footprints.</li> <li>NOTE:<ul> <li>Check if there is no high curvature.</li> <li>The path may be outside the drivable area in some cases, but it is okay to ignore it since the <code>behavior</code> planner does not support it.</li> </ul> </li> </ul> </li> </ul> <ul> <li>Drivable Area<ul> <li>The Drivable area generated in the <code>behavior</code> planner.</li> <li>The skyblue left and right line strings, that is visualized by default.</li> <li>NOTE:<ul> <li>Check if the path is almost inside the drivable area.<ul> <li>Then, the <code>path_optimizer</code> will try to make the trajectory fully inside the drivable area.</li> </ul> </li> <li>During avoidance or lane change by the <code>behavior</code> planner, please make sure that the drivable area is expanded correctly.</li> </ul> </li> </ul> </li> </ul>"},{"location":"planning/autoware_path_optimizer/docs/debug/#model-predictive-trajectory-mpt","title":"Model Predictive Trajectory (MPT)","text":"<ul> <li>MPT Reference Trajectory<ul> <li>The reference trajectory points of model predictive trajectory.</li> </ul> </li> </ul> <ul> <li>MPT Fixed Trajectory<ul> <li>The fixed trajectory points as a constraint of model predictive trajectory.</li> </ul> </li> </ul> <ul> <li>Boundaries' Width<ul> <li>The boundaries' width is calculated from the drivable area line strings.</li> </ul> </li> </ul> <ul> <li>Vehicle Circles<ul> <li>The vehicle's shape is represented by a set of circles.</li> <li>The <code>path_optimizer</code> will try to make the these circles inside the above boundaries' width.</li> </ul> </li> </ul> <ul> <li>Vehicle Circles on Trajectory<ul> <li>The vehicle's circles on the MPT trajectory.</li> <li>Check if the circles are not so big compared to the road's width.</li> </ul> </li> </ul> <ul> <li>MPT Trajectory<ul> <li>The optimized trajectory points by model predictive trajectory.</li> <li>The footprints are supposed to be fully inside the drivable area.</li> </ul> </li> </ul>"},{"location":"planning/autoware_path_optimizer/docs/debug/#output","title":"Output","text":"<ul> <li>Trajectory<ul> <li>The output trajectory.</li> <li>The dark and thin, green and red band, that is visualized by default.</li> </ul> </li> </ul> <ul> <li>Trajectory Footprint<ul> <li>The output trajectory is converted to footprints.</li> </ul> </li> </ul>"},{"location":"planning/autoware_path_optimizer/docs/debug/#calculation-time","title":"Calculation time","text":"<p>The <code>path_optimizer</code> consists of many functions such as boundaries' width calculation, collision-free planning, etc. We can see the calculation time for each function as follows.</p>"},{"location":"planning/autoware_path_optimizer/docs/debug/#raw-data","title":"Raw data","text":"<p>Enable <code>option.enable_calculation_time_info</code> or echo the topic as follows.</p> <pre><code>$ ros2 topic echo /planning/scenario_planning/lane_driving/motion_planning/path_optimizer/debug/calculation_time --field data\n---\n        insertFixedPoint:= 0.008 [ms]\n        getPaddedTrajectoryPoints:= 0.002 [ms]\n        updateConstraint:= 0.741 [ms]\n        optimizeTrajectory:= 0.101 [ms]\n        convertOptimizedPointsToTrajectory:= 0.014 [ms]\n      getEBTrajectory:= 0.991 [ms]\n          resampleReferencePoints:= 0.058 [ms]\n          updateFixedPoint:= 0.237 [ms]\n          updateBounds:= 0.22 [ms]\n          updateVehicleBounds:= 0.509 [ms]\n        calcReferencePoints:= 1.649 [ms]\n        calcMatrix:= 0.209 [ms]\n        calcValueMatrix:= 0.015 [ms]\n          calcObjectiveMatrix:= 0.305 [ms]\n          calcConstraintMatrix:= 0.641 [ms]\n          initOsqp:= 6.896 [ms]\n          solveOsqp:= 2.796 [ms]\n        calcOptimizedSteerAngles:= 9.856 [ms]\n        calcMPTPoints:= 0.04 [ms]\n      getModelPredictiveTrajectory:= 12.782 [ms]\n    optimizeTrajectory:= 12.981 [ms]\n    applyInputVelocity:= 0.577 [ms]\n    insertZeroVelocityOutsideDrivableArea:= 0.81 [ms]\n      getDebugMarker:= 0.684 [ms]\n      publishDebugMarker:= 4.354 [ms]\n    publishDebugMarkerOfOptimization:= 5.047 [ms]\n generateOptimizedTrajectory:= 20.374 [ms]\n  extendTrajectory:= 0.326 [ms]\n  publishDebugData:= 0.008 [ms]\nonPath:= 20.737 [ms]\n</code></pre>"},{"location":"planning/autoware_path_optimizer/docs/debug/#plot","title":"Plot","text":"<p>With the following script, any calculation time of the above functions can be plot.</p> <pre><code>ros2 run autoware_path_optimizer calculation_time_plotter.py\n</code></pre> <p></p> <p>You can specify functions to plot with the <code>-f</code> option.</p> <pre><code>ros2 run autoware_path_optimizer calculation_time_plotter.py -f \"onPath, generateOptimizedTrajectory, calcReferencePoints\"\n</code></pre>"},{"location":"planning/autoware_path_optimizer/docs/debug/#qa-for-debug","title":"Q&amp;A for Debug","text":""},{"location":"planning/autoware_path_optimizer/docs/debug/#the-output-frequency-is-low","title":"The output frequency is low","text":"<p>Check the function which is comparatively heavy according to this information.</p> <p>For your information, the following functions for optimization and its initialization may be heavy in some complicated cases.</p> <ul> <li>MPT<ul> <li><code>initOsqp</code></li> <li><code>solveOsqp</code></li> </ul> </li> </ul>"},{"location":"planning/autoware_path_optimizer/docs/debug/#when-a-part-of-the-trajectory-has-high-curvature","title":"When a part of the trajectory has high curvature","text":"<p>Some of the following may have an issue. Please check if there is something weird by the visualization.</p> <ul> <li>Input Path</li> <li>Drivable Area</li> <li>Boundaries' Width</li> </ul>"},{"location":"planning/autoware_path_optimizer/docs/debug/#when-the-trajectorys-shape-is-zigzag","title":"When the trajectory's shape is zigzag","text":"<p>Some of the following may have an issue. Please check if there is something weird by the visualization.</p> <ul> <li>Vehicle Circles on Trajectory</li> </ul>"},{"location":"planning/autoware_path_optimizer/docs/mpt/","title":"Model predictive trajectory","text":""},{"location":"planning/autoware_path_optimizer/docs/mpt/#model-predictive-trajectory","title":"Model predictive trajectory","text":""},{"location":"planning/autoware_path_optimizer/docs/mpt/#abstract","title":"Abstract","text":"<p>Model Predictive Trajectory (MPT) calculates the trajectory that meets the following conditions.</p> <ul> <li>Kinematically feasible for linear vehicle kinematics model</li> <li>Collision free with obstacles and road boundaries</li> </ul> <p>Conditions for collision free is considered to be not hard constraints but soft constraints. When the optimization failed or the optimized trajectory is not collision free, the output trajectory will be previously generated trajectory.</p> <p>Trajectory near the ego must be stable, therefore the condition where trajectory points near the ego are the same as previously generated trajectory is considered, and this is the only hard constraints in MPT.</p>"},{"location":"planning/autoware_path_optimizer/docs/mpt/#flowchart","title":"Flowchart","text":""},{"location":"planning/autoware_path_optimizer/docs/mpt/#vehicle-kinematics","title":"Vehicle kinematics","text":"<p>As the following figure, we consider the bicycle kinematics model in the frenet frame to track the reference path. At time step \\(k\\), we define lateral distance to the reference path, heading angle against the reference path, and steer angle as \\(y_k\\), \\(\\theta_k\\), and \\(\\delta_k\\) respectively.</p> <p></p> <p>Assuming that the commanded steer angle is \\(\\delta_{des, k}\\), the kinematics model in the frenet frame is formulated as follows. We also assume that the steer angle \\(\\delta_k\\) is first-order lag to the commanded one.</p> \\[ \\begin{align} y_{k+1} &amp; = y_{k} + v \\sin \\theta_k dt \\\\\\ \\theta_{k+1} &amp; = \\theta_k + \\frac{v \\tan \\delta_k}{L}dt - \\kappa_k v \\cos \\theta_k dt \\\\\\ \\delta_{k+1} &amp; = \\delta_k - \\frac{\\delta_k - \\delta_{des,k}}{\\tau}dt \\end{align} \\]"},{"location":"planning/autoware_path_optimizer/docs/mpt/#linearization","title":"Linearization","text":"<p>Then we linearize these equations. \\(y_k\\) and \\(\\theta_k\\) are tracking errors, so we assume that those are small enough. Therefore \\(\\sin \\theta_k \\approx \\theta_k\\).</p> <p>Since \\(\\delta_k\\) is a steer angle, it is not always small. By using a reference steer angle \\(\\delta_{\\mathrm{ref}, k}\\) calculated by the reference path curvature \\(\\kappa_k\\), we express \\(\\delta_k\\) with a small value \\(\\Delta \\delta_k\\).</p> <p>Note that the steer angle \\(\\delta_k\\) is within the steer angle limitation \\(\\delta_{\\max}\\). When the reference steer angle \\(\\delta_{\\mathrm{ref}, k}\\) is larger than the steer angle limitation \\(\\delta_{\\max}\\), and \\(\\delta_{\\mathrm{ref}, k}\\) is used to linearize the steer angle, \\(\\Delta \\delta_k\\) is \\(\\Delta \\delta_k = \\delta - \\delta_{\\mathrm{ref}, k} = \\delta_{\\max} - \\delta_{\\mathrm{ref}, k}\\), and the absolute \\(\\Delta \\delta_k\\) gets larger. Therefore, we have to apply the steer angle limitation to \\(\\delta_{\\mathrm{ref}, k}\\) as well.</p> \\[ \\begin{align} \\delta_{\\mathrm{ref}, k} &amp; = \\mathrm{clamp}(\\arctan(L \\kappa_k), -\\delta_{\\max}, \\delta_{\\max}) \\\\\\ \\delta_k &amp; = \\delta_{\\mathrm{ref}, k} + \\Delta \\delta_k, \\ \\Delta \\delta_k \\ll 1 \\\\\\ \\end{align} \\] <p>\\(\\mathrm{clamp}(v, v_{\\min}, v_{\\max})\\) is a function to convert \\(v\\) to be larger than \\(v_{\\min}\\) and smaller than \\(v_{\\max}\\).</p> <p>Using this \\(\\delta_{\\mathrm{ref}, k}\\), \\(\\tan \\delta_k\\) is linearized as follows.</p> \\[ \\begin{align} \\tan \\delta_k &amp; \\approx \\tan \\delta_{\\mathrm{ref}, k} + \\lbrace\\frac{d \\tan \\delta}{d \\delta}\\rbrace|_{\\delta = \\delta_{\\mathrm{ref}, k}} \\Delta \\delta_k \\\\\\ &amp; = \\tan \\delta_{\\mathrm{ref}, k} + \\lbrace\\frac{d \\tan \\delta}{d \\delta}\\rbrace|_{\\delta = \\delta_{\\mathrm{ref}, k}} (\\delta_{\\mathrm{ref}, k} - \\delta_k) \\\\\\ &amp; = \\tan \\delta_{\\mathrm{ref}, k} - \\frac{\\delta_{\\mathrm{ref}, k}}{\\cos^2 \\delta_{\\mathrm{ref}, k}} + \\frac{1}{\\cos^2 \\delta_{\\mathrm{ref}, k}} \\delta_k \\end{align} \\]"},{"location":"planning/autoware_path_optimizer/docs/mpt/#one-step-state-equation","title":"One-step state equation","text":"<p>Based on the linearization, the error kinematics is formulated with the following linear equations,</p> \\[ \\begin{align}     \\begin{pmatrix}         y_{k+1} \\\\\\         \\theta_{k+1}     \\end{pmatrix}     =     \\begin{pmatrix}         1 &amp; v dt \\\\\\         0 &amp; 1     \\end{pmatrix}     \\begin{pmatrix}         y_k \\\\\\         \\theta_k     \\end{pmatrix}     +     \\begin{pmatrix}         0 \\\\\\         \\frac{v dt}{L \\cos^{2} \\delta_{\\mathrm{ref}, k}}     \\end{pmatrix}     \\delta_{k}     +     \\begin{pmatrix}         0 \\\\\\         \\frac{v \\tan(\\delta_{\\mathrm{ref}, k}) dt}{L} - \\frac{v \\delta_{\\mathrm{ref}, k} dt}{L \\cos^{2} \\delta_{\\mathrm{ref}, k}} - \\kappa_k v dt     \\end{pmatrix} \\end{align} \\] <p>which can be formulated as follows with the state \\(\\mathbf{x}\\), control input \\(u\\) and some matrices, where \\(\\mathbf{x} = (y_k, \\theta_k)\\)</p> \\[ \\begin{align}   \\mathbf{x}_{k+1} = A_k \\mathbf{x}_k + \\mathbf{b}_k u_k + \\mathbf{w}_k \\end{align} \\]"},{"location":"planning/autoware_path_optimizer/docs/mpt/#time-series-state-equation","title":"Time-series state equation","text":"<p>Then, we formulate time-series state equation by concatenating states, control inputs and matrices respectively as</p> \\[ \\begin{align}   \\mathbf{x} = A \\mathbf{x}_0 + B \\mathbf{u} + \\mathbf{w} \\end{align} \\] <p>where</p> \\[ \\begin{align} \\mathbf{x} = (\\mathbf{x}^T_1, \\mathbf{x}^T_2, \\mathbf{x}^T_3, \\dots, \\mathbf{x}^T_{n-1})^T \\\\\\ \\mathbf{u} = (u_0, u_1, u_2, \\dots, u_{n-2})^T \\\\\\ \\mathbf{w} = (\\mathbf{w}^T_0, \\mathbf{w}^T_1, \\mathbf{w}^T_2, \\dots, \\mathbf{w}^T_{n-1})^T. \\\\\\ \\end{align} \\] <p>In detail, each matrices are constructed as follows.</p> \\[ \\begin{align}     \\begin{pmatrix}         \\mathbf{x}_1 \\\\\\         \\mathbf{x}_2 \\\\\\         \\mathbf{x}_3 \\\\\\         \\vdots \\\\\\         \\mathbf{x}_{n-1}     \\end{pmatrix}     =     \\begin{pmatrix}         A_0 \\\\\\         A_1 A_0 \\\\\\         A_2 A_1 A_0\\\\\\         \\vdots \\\\\\         \\prod\\limits_{k=0}^{n-1} A_{k}     \\end{pmatrix}     \\mathbf{x}_0     +     \\begin{pmatrix}       B_0 &amp; 0 &amp; &amp; \\dots &amp; 0 \\\\\\       A_0 B_0 &amp; B_1 &amp; 0 &amp; \\dots &amp; 0 \\\\\\       A_1 A_0 B_0 &amp; A_0 B_1 &amp; B_2 &amp; \\dots &amp; 0 \\\\\\       \\vdots &amp; \\vdots &amp; &amp; \\ddots &amp; 0 \\\\\\       \\prod\\limits_{k=0}^{n-3} A_k B_0 &amp; \\prod\\limits_{k=0}^{n-4} A_k B_1 &amp; \\dots &amp; A_0 B_{n-3} &amp; B_{n-2}     \\end{pmatrix}     \\begin{pmatrix}         u_0 \\\\\\         u_1 \\\\\\         u_2 \\\\\\         \\vdots \\\\\\         u_{n-2}     \\end{pmatrix}     +     \\begin{pmatrix}       I &amp; 0 &amp; &amp; \\dots &amp; 0 \\\\\\       A_0 &amp; I &amp; 0 &amp; \\dots &amp; 0 \\\\\\       A_1 A_0 &amp; A_0 &amp; I &amp; \\dots &amp; 0 \\\\\\       \\vdots &amp; \\vdots &amp; &amp; \\ddots &amp; 0 \\\\\\       \\prod\\limits_{k=0}^{n-3} A_k &amp; \\prod\\limits_{k=0}^{n-4} A_k &amp; \\dots &amp; A_0 &amp; I     \\end{pmatrix}     \\begin{pmatrix}         \\mathbf{w}_0 \\\\\\         \\mathbf{w}_1 \\\\\\         \\mathbf{w}_2 \\\\\\         \\vdots \\\\\\         \\mathbf{w}_{n-2}     \\end{pmatrix} \\end{align} \\]"},{"location":"planning/autoware_path_optimizer/docs/mpt/#free-boundary-conditioned-time-series-state-equation","title":"Free-boundary-conditioned time-series state equation","text":"<p>For path planning which does not start from the current ego pose, \\(\\mathbf{x}_0\\) should be the design variable of optimization. Therefore, we make \\(\\mathbf{u}'\\) by concatenating \\(\\mathbf{x}_0\\) and \\(\\mathbf{u}\\), and redefine \\(\\mathbf{x}\\) as follows.</p> \\[ \\begin{align}   \\mathbf{u}' &amp; = (\\mathbf{x}^T_0, \\mathbf{u}^T)^T \\\\\\   \\mathbf{x} &amp; = (\\mathbf{x}^T_0, \\mathbf{x}^T_1, \\mathbf{x}^T_2, \\dots, \\mathbf{x}^T_{n-1})^T \\end{align} \\] <p>Then we get the following state equation</p> \\[ \\begin{align}   \\mathbf{x}' = B \\mathbf{u}' + \\mathbf{w}, \\end{align} \\] <p>which is in detail</p> \\[ \\begin{align}     \\begin{pmatrix}         \\mathbf{x}_0 \\\\\\         \\mathbf{x}_1 \\\\\\         \\mathbf{x}_2 \\\\\\         \\mathbf{x}_3 \\\\\\         \\vdots \\\\\\         \\mathbf{x}_{n-1}     \\end{pmatrix}     =     \\begin{pmatrix}       I &amp; 0 &amp; \\dots &amp; &amp; &amp; 0 \\\\\\       A_0 &amp; B_0 &amp; 0 &amp; &amp; \\dots &amp; 0 \\\\\\       A_1 A_0 &amp; A_0 B_0 &amp; B_1 &amp; 0 &amp; \\dots &amp; 0 \\\\\\       A_2 A_1 A_0 &amp; A_1 A_0 B_0 &amp; A_0 B_1 &amp; B_2 &amp; \\dots &amp; 0 \\\\\\       \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp; \\ddots &amp; 0 \\\\\\       \\prod\\limits_{k=0}^{n-1} A_k &amp; \\prod\\limits_{k=0}^{n-3} A_k B_0 &amp; \\prod\\limits_{k=0}^{n-4} A_k B_1 &amp; \\dots &amp; A_0 B_{n-3} &amp; B_{n-2}     \\end{pmatrix}     \\begin{pmatrix}         \\mathbf{x}_0 \\\\\\         u_0 \\\\\\         u_1 \\\\\\         u_2 \\\\\\         \\vdots \\\\\\         u_{n-2}     \\end{pmatrix}     +     \\begin{pmatrix}       0 &amp; \\dots &amp; &amp; &amp; 0 \\\\\\       I &amp; 0 &amp; &amp; \\dots &amp; 0 \\\\\\       A_0 &amp; I &amp; 0 &amp; \\dots &amp; 0 \\\\\\       A_1 A_0 &amp; A_0 &amp; I &amp; \\dots &amp; 0 \\\\\\       \\vdots &amp; \\vdots &amp; &amp; \\ddots &amp; 0 \\\\\\       \\prod\\limits_{k=0}^{n-3} A_k &amp; \\prod\\limits_{k=0}^{n-4} A_k &amp; \\dots &amp; A_0 &amp; I     \\end{pmatrix}     \\begin{pmatrix}         \\mathbf{w}_0 \\\\\\         \\mathbf{w}_1 \\\\\\         \\mathbf{w}_2 \\\\\\         \\vdots \\\\\\         \\mathbf{w}_{n-2}     \\end{pmatrix}. \\end{align} \\]"},{"location":"planning/autoware_path_optimizer/docs/mpt/#objective-function","title":"Objective function","text":"<p>The objective function for smoothing and tracking is shown as follows, which can be formulated with value function matrices \\(Q, R\\).</p> \\[ \\begin{align} J_1 (\\mathbf{x}', \\mathbf{u}') &amp; = w_y \\sum_{k} y_k^2 + w_{\\theta} \\sum_{k} \\theta_k^2 + w_{\\delta} \\sum_k \\delta_k^2 + w_{\\dot \\delta} \\sum_k \\dot \\delta_k^2 + w_{\\ddot \\delta} \\sum_k \\ddot \\delta_k^2 \\\\\\ &amp; = \\mathbf{x}'^T Q \\mathbf{x}' + \\mathbf{u}'^T R \\mathbf{u}' \\\\\\ &amp; = \\mathbf{u}'^T H \\mathbf{u}' + \\mathbf{u}'^T \\mathbf{f} \\end{align} \\] <p>As mentioned before, the constraints to be collision free with obstacles and road boundaries are formulated to be soft constraints. Assuming that the lateral distance to the road boundaries or obstacles from the back wheel center, front wheel center, and the point between them are \\(y_{\\mathrm{base}, k}, y_{\\mathrm{top}, k}, y_{\\mathrm{mid}, k}\\) respectively, and slack variables for each point are \\(\\lambda_{\\mathrm{base}}, \\lambda_{\\mathrm{top}}, \\lambda_{\\mathrm{mid}}\\), the soft constraints can be formulated as follows.</p> \\[ y_{\\mathrm{base}, k, \\min} - \\lambda_{\\mathrm{base}, k} \\leq y_{\\mathrm{base}, k} (y_k)  \\leq y_{\\mathrm{base}, k, \\max} + \\lambda_{\\mathrm{base}, k}\\\\\\ y_{\\mathrm{top}, k, \\min} - \\lambda_{\\mathrm{top}, k} \\leq y_{\\mathrm{top}, k} (y_k) \\leq y_{\\mathrm{top}, k, \\max} + \\lambda_{\\mathrm{top}, k}\\\\\\ y_{\\mathrm{mid}, k, \\min} - \\lambda_{\\mathrm{mid}, k} \\leq y_{\\mathrm{mid}, k} (y_k) \\leq y_{\\mathrm{mid}, k, \\max} + \\lambda_{\\mathrm{mid}, k} \\\\\\ 0 \\leq \\lambda_{\\mathrm{base}, k} \\\\\\ 0 \\leq \\lambda_{\\mathrm{top}, k} \\\\\\ 0 \\leq \\lambda_{\\mathrm{mid}, k} \\] <p>Since \\(y_{\\mathrm{base}, k}, y_{\\mathrm{top}, k}, y_{\\mathrm{mid}, k}\\) is formulated as a linear function of \\(y_k\\), the objective function for soft constraints is formulated as follows.</p> \\[ \\begin{align} J_2 &amp; (\\mathbf{\\lambda}_\\mathrm{base}, \\mathbf{\\lambda}_\\mathrm{top}, \\mathbf {\\lambda}_\\mathrm{mid})\\\\\\ &amp; = w_{\\mathrm{base}} \\sum_{k} \\lambda_{\\mathrm{base}, k} + w_{\\mathrm{mid}} \\sum_k \\lambda_{\\mathrm{mid}, k} + w_{\\mathrm{top}} \\sum_k \\lambda_{\\mathrm{top}, k} \\end{align} \\] <p>Slack variables are also design variables for optimization. We define a vector \\(\\mathbf{v}\\), that concatenates all the design variables.</p> \\[ \\begin{align} \\mathbf{v} = \\begin{pmatrix}   \\mathbf{u}'^T &amp; \\mathbf{\\lambda}_\\mathrm{base}^T &amp; \\mathbf{\\lambda}_\\mathrm{top}^T &amp; \\mathbf{\\lambda}_\\mathrm{mid}^T \\end{pmatrix}^T \\end{align} \\] <p>The summation of these two objective functions is the objective function for the optimization problem.</p> \\[ \\begin{align} \\min_{\\mathbf{v}} J (\\mathbf{v}) = \\min_{\\mathbf{v}} J_1 (\\mathbf{u}') + J_2 (\\mathbf{\\lambda}_\\mathrm{base}, \\mathbf{\\lambda}_\\mathrm{top}, \\mathbf{\\lambda}_\\mathrm{mid}) \\end{align} \\] <p>As mentioned before, we use hard constraints where some trajectory points in front of the ego are the same as the previously generated trajectory points. This hard constraints is formulated as follows.</p> \\[ \\begin{align} \\delta_k = \\delta_{k}^{\\mathrm{prev}} (0 \\leq i \\leq N_{\\mathrm{fix}}) \\end{align} \\] <p>Finally we transform those objective functions to the following QP problem, and solve it.</p> \\[ \\begin{align} \\min_{\\mathbf{v}} \\ &amp; \\frac{1}{2} \\mathbf{v}^T \\mathbf{H} \\mathbf{v} + \\mathbf{f} \\mathbf{v} \\\\\\ \\mathrm{s.t.} \\ &amp; \\mathbf{b}_{lower} \\leq \\mathbf{A} \\mathbf{v} \\leq \\mathbf{b}_{upper} \\end{align} \\]"},{"location":"planning/autoware_path_optimizer/docs/mpt/#constraints","title":"Constraints","text":""},{"location":"planning/autoware_path_optimizer/docs/mpt/#steer-angle-limitation","title":"Steer angle limitation","text":"<p>Steer angle has a limitation \\(\\delta_{max}\\) and \\(\\delta_{min}\\). Therefore we add linear inequality equations.</p> \\[ \\begin{align} \\delta_{min} \\leq \\delta_i \\leq \\delta_{max} \\end{align} \\]"},{"location":"planning/autoware_path_optimizer/docs/mpt/#collision-free","title":"Collision free","text":"<p>To realize collision-free trajectory planning, we have to formulate constraints that the vehicle is inside the road and also does not collide with obstacles in linear equations. For linearity, we implemented some methods to approximate the vehicle shape with a set of circles, that is reliable and easy to implement.</p> <ol> <li>Bicycle Model</li> <li>Uniform Circles</li> <li>Fitting Uniform Circles</li> </ol> <p></p> <p>Now we formulate the linear constraints where a set of circles on each trajectory point is collision-free. By using the drivable area, we calculate upper and lower boundaries along reference points, which will be interpolated on any position on the trajectory. NOTE that upper and lower boundary is left and right, respectively.</p> <p>Assuming that upper and lower boundaries are \\(b_l\\), \\(b_u\\) respectively, and \\(r\\) is a radius of a circle, lateral deviation of the circle center \\(y'\\) has to be</p> \\[ b_l + r \\leq y' \\leq b_u - r. \\] <p></p> <p>Based on the following figure, \\(y'\\) can be formulated as follows.</p> \\[ \\begin{align} y' &amp; = L \\sin(\\theta + \\beta) + y \\cos \\beta - l \\sin(\\gamma - \\phi_a) \\\\\\ &amp; = L \\sin \\theta \\cos \\beta + L \\cos \\theta \\sin \\beta + y \\cos \\beta - l \\sin(\\gamma - \\phi_a) \\\\\\ &amp; \\approx L \\theta \\cos \\beta + L \\sin \\beta + y \\cos \\beta - l \\sin(\\gamma - \\phi_a) \\end{align} \\] \\[ b_l + r - \\lambda \\leq y' \\leq b_u - r + \\lambda. \\] \\[ \\begin{align} y' &amp; = C_1 \\mathbf{x} + C_2 \\\\\\ &amp; = C_1 (B \\mathbf{v} + \\mathbf{w}) + C_2 \\\\\\ &amp; = C_1 B \\mathbf{v} + \\mathbf{w} + C_2 \\end{align} \\] <p>Note that longitudinal position of the circle center and the trajectory point to calculate boundaries are different. But each boundaries are vertical against the trajectory, resulting in less distortion by the longitudinal position difference since road boundaries does not change so much. For example, if the boundaries are not vertical against the trajectory and there is a certain difference of longitudinal position between the circe center and the trajectory point, we can easily guess that there is much more distortion when comparing lateral deviation and boundaries.</p> \\[ \\begin{align}     A_{blk} &amp; =     \\begin{pmatrix}         C_1 B &amp; O &amp; \\dots &amp; O &amp; I_{N_{ref} \\times N_{ref}} &amp; O \\dots &amp; O\\\\\\         -C_1 B &amp; O &amp; \\dots &amp; O &amp; I &amp; O \\dots &amp; O\\\\\\         O &amp; O &amp; \\dots &amp; O &amp; I &amp; O \\dots &amp; O     \\end{pmatrix}     \\in \\mathbf{R}^{3 N_{ref} \\times D_v + N_{circle} N_{ref}} \\\\\\     \\mathbf{b}_{lower, blk} &amp; =     \\begin{pmatrix}         \\mathbf{b}_{lower} - C_1 \\mathbf{w} - C_2 \\\\\\         -\\mathbf{b}_{upper} + C_1 \\mathbf{w} + C_2 \\\\\\         O     \\end{pmatrix}     \\in \\mathbf{R}^{3 N_{ref}} \\\\\\     \\mathbf{b}_{upper, blk} &amp; = \\mathbf{\\infty}     \\in \\mathbf{R}^{3 N_{ref}} \\end{align} \\] <p>We will explain options for optimization.</p>"},{"location":"planning/autoware_path_optimizer/docs/mpt/#l-infinity-optimization","title":"L-infinity optimization","text":"<p>The above formulation is called L2 norm for slack variables. Instead, if we use L-infinity norm where slack variables are shared by enabling <code>l_inf_norm</code>.</p> \\[ \\begin{align}     A_{blk} =     \\begin{pmatrix}         C_1 B &amp; I_{N_{ref} \\times N_{ref}} \\\\\\         -C_1 B &amp; I \\\\\\         O &amp; I     \\end{pmatrix} \\in \\mathbf{R}^{3N_{ref} \\times D_v + N_{ref}} \\end{align} \\]"},{"location":"planning/autoware_path_optimizer/docs/mpt/#tips-for-stable-trajectory-planning","title":"Tips for stable trajectory planning","text":"<p>In order to make the trajectory optimization problem stabler to solve, the boundary constraint which the trajectory footprints should be inside and optimization weights are modified.</p>"},{"location":"planning/autoware_path_optimizer/docs/mpt/#keep-minimum-boundary-width","title":"Keep minimum boundary width","text":"<p>The drivable area's width is sometimes smaller than the vehicle width since the behavior module does not consider the width. To realize the stable trajectory optimization, the drivable area's width is guaranteed to be larger than the vehicle width and an additional margin in a rule-based way.</p> <p>We cannot distinguish the boundary by roads from the boundary by obstacles for avoidance in the motion planner, the drivable area is modified in the following multi steps assuming that \\(l_{width}\\) is the vehicle width and \\(l_{margin}\\) is an additional margin.</p> <p></p>"},{"location":"planning/autoware_path_optimizer/docs/mpt/#extend-violated-boundary","title":"Extend violated boundary","text":""},{"location":"planning/autoware_path_optimizer/docs/mpt/#avoid-sudden-steering","title":"Avoid sudden steering","text":"<p>When the obstacle suddenly appears which is determined to avoid by the behavior module, the drivable area's shape just in front of the ego will change, resulting in the sudden steering. To prevent this, the drivable area's shape close to the ego is fixed as previous drivable area's shape.</p> <p>Assume that \\(v_{ego}\\) is the ego velocity, and \\(t_{fix}\\) is the time to fix the forward drivable area's shape.</p> <p></p>"},{"location":"planning/autoware_path_optimizer/docs/mpt/#calculate-avoidance-cost","title":"Calculate avoidance cost","text":""},{"location":"planning/autoware_path_optimizer/docs/mpt/#change-optimization-weights","title":"Change optimization weights","text":"\\[ \\begin{align} r &amp; = \\mathrm{lerp}(w^{\\mathrm{steer}}_{\\mathrm{normal}}, w^{\\mathrm{steer}}_{\\mathrm{avoidance}}, c) \\\\\\ w^{\\mathrm{lat}} &amp; = \\mathrm{lerp}(w^{\\mathrm{lat}}_{\\mathrm{normal}}, w^{\\mathrm{lat}}_{\\mathrm{avoidance}}, r) \\\\\\ w^{\\mathrm{yaw}} &amp; = \\mathrm{lerp}(w^{\\mathrm{yaw}}_{\\mathrm{normal}}, w^{\\mathrm{yaw}}_{\\mathrm{avoidance}}, r) \\end{align} \\] <p>Assume that \\(c\\) is the normalized avoidance cost, \\(w^{\\mathrm{lat}}\\) is the weight for lateral error, \\(w^{\\mathrm{yaw}}\\) is the weight for yaw error, and other variables are as follows.</p> Parameter Type Description \\(w^{\\mathrm{steer}}_{\\mathrm{normal}}\\) double weight for steering minimization in normal cases \\(w^{\\mathrm{steer}}_{\\mathrm{avoidance}}\\) double weight for steering minimization in avoidance cases \\(w^{\\mathrm{lat}}_{\\mathrm{normal}}\\) double weight for lateral error minimization in normal cases \\(w^{\\mathrm{lat}}_{\\mathrm{avoidance}}\\) double weight for lateral error minimization in avoidance cases \\(w^{\\mathrm{yaw}}_{\\mathrm{normal}}\\) double weight for yaw error minimization in normal cases \\(w^{\\mathrm{yaw}}_{\\mathrm{avoidance}}\\) double weight for yaw error minimization in avoidance cases"},{"location":"planning/autoware_path_smoother/","title":"Path Smoothing","text":""},{"location":"planning/autoware_path_smoother/#path-smoothing","title":"Path Smoothing","text":""},{"location":"planning/autoware_path_smoother/#purpose","title":"Purpose","text":"<p>This package contains code to smooth a path or trajectory.</p>"},{"location":"planning/autoware_path_smoother/#features","title":"Features","text":""},{"location":"planning/autoware_path_smoother/#elastic-band","title":"Elastic Band","text":"<p>More details about the elastic band can be found here.</p>"},{"location":"planning/autoware_path_smoother/#parameter","title":"Parameter","text":"Name Type Description Default Range common.output_delta_arc_length float Delta arc length for output trajectory [m]. 0.5 N/A common.output_backward_traj_length float Backward length for backward trajectory from base_link [m]. 5 N/A option.enable_warm_start boolean Enable warm start for elastic band optimization. 1 N/A option.enable_optimization_validation boolean Enable optimization validation. 0 N/A common.num_points integer Number of points for optimization. 100 N/A common.delta_arc_length float Delta arc length for optimization [m]. 1 N/A clearance.num_joint_points integer Number of joint points (joint means connecting fixing and smoothing). 3 N/A clearance.clearance_for_fix float Maximum optimizing range when applying fixing. 0 N/A clearance.clearance_for_joint float Maximum optimizing range when applying jointing. 0.1 N/A clearance.clearance_for_smooth float Maximum optimizing range when applying smoothing. 0.1 N/A weight.smooth_weight float Weight for smoothing. 1 N/A weight.lat_error_weight float Weight for lateral error. 0.001 N/A qp.max_iteration integer Maximum iteration when solving QP. 10000 N/A qp.eps_abs float Absolute epsilon when solving OSQP. 1e-07 N/A qp.eps_rel float Relative epsilon when solving OSQP. 1e-07 N/A validation.max_error float Maximum error for validation [m]. 3 N/A ego_nearest_dist_threshold float Distance threshold for nearest ego search [m]. 3 N/A ego_nearest_yaw_threshold float Yaw threshold for nearest ego search [rad]. 1.046 N/A replan.enable boolean If true, only perform smoothing when the input changes significantly. 1 N/A replan.max_path_shape_around_ego_lat_dist float Threshold of path shape change around ego [m]. 2 N/A replan.max_path_shape_forward_lon_dist float Forward point to check lateral distance difference [m]. 100 N/A replan.max_path_shape_forward_lat_dist float Threshold of path shape change around forward point [m]. 0.2 N/A replan.max_ego_moving_dist float Threshold of ego's moving distance for replan [m]. 5 N/A replan.max_goal_moving_dist float Threshold of goal's moving distance for replan [m]. 15 N/A replan.max_delta_time_sec float Threshold of delta time for replan [second]. 0 N/A"},{"location":"planning/autoware_path_smoother/docs/eb/","title":"Elastic band","text":""},{"location":"planning/autoware_path_smoother/docs/eb/#elastic-band","title":"Elastic band","text":""},{"location":"planning/autoware_path_smoother/docs/eb/#abstract","title":"Abstract","text":"<p>Elastic band smooths the input path. Since the latter optimization (model predictive trajectory) is calculated on the frenet frame, path smoothing is applied here so that the latter optimization will be stable.</p> <p>Note that this smoothing process does not consider collision checking. Therefore the output path may have a collision with road boundaries or obstacles.</p>"},{"location":"planning/autoware_path_smoother/docs/eb/#flowchart","title":"Flowchart","text":""},{"location":"planning/autoware_path_smoother/docs/eb/#general-parameters","title":"General parameters","text":"Parameter Type Description <code>eb.common.num_points</code> int points for elastic band optimization <code>eb.common.delta_arc_length</code> double delta arc length for elastic band optimization"},{"location":"planning/autoware_path_smoother/docs/eb/#parameters-for-optimization","title":"Parameters for optimization","text":"Parameter Type Description <code>eb.option.enable_warm_start</code> bool flag to use warm start <code>eb.weight.smooth_weight</code> double weight for smoothing <code>eb.weight.lat_error_weight</code> double weight for minimizing the lateral error"},{"location":"planning/autoware_path_smoother/docs/eb/#parameters-for-validation","title":"Parameters for validation","text":"Parameter Type Description <code>eb.option.enable_optimization_validation</code> bool flag to validate optimization <code>eb.validation.max_error</code> double max lateral error by optimization"},{"location":"planning/autoware_path_smoother/docs/eb/#formulation","title":"Formulation","text":""},{"location":"planning/autoware_path_smoother/docs/eb/#objective-function","title":"Objective function","text":"<p>We formulate a quadratic problem minimizing the diagonal length of the rhombus on each point generated by the current point and its previous and next points, shown as the red vector's length.</p> <p></p> <p>Assuming that \\(k\\)'th point is \\(\\mathbf{p}_k = (x_k, y_k)\\), the objective function is as follows.</p> \\[ \\begin{align} \\ J &amp; = \\min \\sum_{k=1}^{n-2} ||(\\mathbf{p}_{k+1} - \\mathbf{p}_{k}) - (\\mathbf{p}_{k} - \\mathbf{p}_{k-1})||^2 \\\\\\ \\ &amp; = \\min \\sum_{k=1}^{n-2} ||\\mathbf{p}_{k+1} - 2 \\mathbf{p}_{k} + \\mathbf{p}_{k-1}||^2 \\\\\\ \\ &amp; = \\min \\sum_{k=1}^{n-2} \\{(x_{k+1} - x_k + x_{k-1})^2 + (y_{k+1} - y_k + y_{k-1})^2\\} \\\\\\ \\ &amp; = \\min     \\begin{pmatrix}         \\ x_0 \\\\\\         \\ x_1 \\\\\\         \\ x_2 \\\\\\         \\vdots \\\\\\         \\ x_{n-3}\\\\\\         \\ x_{n-2} \\\\\\         \\ x_{n-1} \\\\\\         \\ y_0 \\\\\\         \\ y_1 \\\\\\         \\ y_2 \\\\\\         \\vdots \\\\\\         \\ y_{n-3}\\\\\\         \\ y_{n-2} \\\\\\         \\ y_{n-1} \\\\\\     \\end{pmatrix}^T     \\begin{pmatrix}       1 &amp; -2 &amp; 1 &amp; 0 &amp; \\dots&amp; \\\\\\       -2 &amp; 5 &amp; -4 &amp; 1 &amp; 0 &amp;\\dots   \\\\\\       1 &amp; -4 &amp; 6 &amp; -4 &amp; 1 &amp; \\\\\\       0 &amp; 1 &amp; -4 &amp; 6 &amp; -4 &amp;   \\\\\\       \\vdots &amp; 0 &amp; \\ddots&amp;\\ddots&amp; \\ddots   \\\\\\       &amp; \\vdots &amp; &amp; &amp; \\\\\\       &amp; &amp; &amp; 1 &amp; -4 &amp; 6 &amp; -4 &amp; 1 \\\\\\       &amp; &amp; &amp; &amp; 1 &amp; -4 &amp; 5 &amp; -2 \\\\\\       &amp; &amp; &amp; &amp; &amp; 1 &amp; -2 &amp;  1&amp; \\\\\\       &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp;1 &amp; -2 &amp; 1 &amp; 0 &amp; \\dots&amp; \\\\\\       &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp;-2 &amp; 5 &amp; -4 &amp; 1 &amp; 0 &amp;\\dots   \\\\\\       &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp;1 &amp; -4 &amp; 6 &amp; -4 &amp; 1 &amp; \\\\\\       &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp;0 &amp; 1 &amp; -4 &amp; 6 &amp; -4 &amp;   \\\\\\       &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp;\\vdots &amp; 0 &amp; \\ddots&amp;\\ddots&amp; \\ddots   \\\\\\       &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; \\vdots &amp; &amp; &amp; \\\\\\       &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; 1 &amp; -4 &amp; 6 &amp; -4 &amp; 1 \\\\\\       &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; 1 &amp; -4 &amp; 5 &amp; -2 \\\\\\       &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; &amp; 1 &amp; -2 &amp;  1&amp; \\\\\\     \\end{pmatrix}     \\begin{pmatrix}         \\ x_0 \\\\\\         \\ x_1 \\\\\\         \\ x_2 \\\\\\         \\vdots \\\\\\         \\ x_{n-3}\\\\\\         \\ x_{n-2} \\\\\\         \\ x_{n-1} \\\\\\         \\ y_0 \\\\\\         \\ y_1 \\\\\\         \\ y_2 \\\\\\         \\vdots \\\\\\         \\ y_{n-3}\\\\\\         \\ y_{n-2} \\\\\\         \\ y_{n-1} \\\\\\     \\end{pmatrix} \\end{align} \\]"},{"location":"planning/autoware_path_smoother/docs/eb/#constraint","title":"Constraint","text":"<p>The distance that each point can move is limited so that the path will not changed a lot but will be smoother. In detail, the longitudinal distance that each point can move is zero, and the lateral distance is parameterized as <code>eb.clearance.clearance_for_fix</code>, <code>eb.clearance.clearance_for_joint</code> and <code>eb.clearance.clearance_for_smooth</code>.</p> <p>The following figure describes how to constrain the lateral distance to move. The red line is where the point can move. The points for the upper and lower bound are described as \\((x_k^u, y_k^u)\\) and \\((x_k^l, y_k^l)\\), respectively.</p> <p></p> <p>Based on the line equation whose slope angle is \\(\\theta_k\\) and that passes through \\((x_k, y_k)\\), \\((x_k^u, y_k^u)\\) and \\((x_k^l, y_k^l)\\), the lateral constraint is formulated as follows.</p> \\[ C_k^l \\leq C_k \\leq C_k^u \\] <p>In addition, the beginning point is fixed and the end point as well if the end point is considered as the goal. This constraint can be applied with the upper equation by changing the distance that each point can move.</p>"},{"location":"planning/autoware_path_smoother/docs/eb/#debug","title":"Debug","text":"<ul> <li>EB Fixed Trajectory<ul> <li>The fixed trajectory points as a constraint of elastic band.</li> </ul> </li> </ul> <ul> <li>EB Trajectory<ul> <li>The optimized trajectory points by elastic band.</li> </ul> </li> </ul>"},{"location":"planning/autoware_remaining_distance_time_calculator/","title":"Index","text":""},{"location":"planning/autoware_remaining_distance_time_calculator/#remaining-distance-and-time-calculator","title":"Remaining Distance and Time Calculator","text":""},{"location":"planning/autoware_remaining_distance_time_calculator/#role","title":"Role","text":"<p>This package aims to provide mission remaining distance and remaining time calculations.</p>"},{"location":"planning/autoware_remaining_distance_time_calculator/#activation-and-timing","title":"Activation and Timing","text":"<ul> <li>The calculations are activated once we have a route planned for a mission in Autoware.</li> <li>The calculations are triggered timely based on the <code>update_rate</code> parameter.</li> <li>The calculations are skipped if the scenario is PARKING, and the remaining time and distance values are set to 0.0.</li> </ul>"},{"location":"planning/autoware_remaining_distance_time_calculator/#module-parameters","title":"Module Parameters","text":"Name Type Default Value Explanation <code>update_rate</code> double 10.0 Timer callback period. [Hz]"},{"location":"planning/autoware_remaining_distance_time_calculator/#inner-workings","title":"Inner-workings","text":""},{"location":"planning/autoware_remaining_distance_time_calculator/#remaining-distance-calculation","title":"Remaining Distance Calculation","text":"<ul> <li>The remaining distance calculation is based on getting the remaining shortest path between the current vehicle pose and goal pose using <code>lanelet2</code> routing APIs.</li> <li>The remaining distance is calculated by summing the 2D length of remaining shortest path, with exception to current lanelet and goal lanelet.<ul> <li>For the current lanelet, the distance is calculated from the current vehicle position to the end of that lanelet.</li> <li>For the goal lanelet, the distance is calculated from the start of the lanelet to the goal pose in this lanelet.</li> </ul> </li> <li>When there is only one lanelet remaining, the distance is calculated by getting the 2D distance between the current vehicle pose and goal pose.</li> <li>Checks are added to handle cases when current lanelet, goal lanelet, or routing graph are not valid to prevent node process die.<ul> <li>In such cases when, last valid remaining distance and time are maintained.</li> </ul> </li> </ul>"},{"location":"planning/autoware_remaining_distance_time_calculator/#remaining-time-calculation","title":"Remaining Time Calculation","text":"<ul> <li>The remaining time currently depends on a simple equation of motion by getting the maximum velocity limit.</li> <li>The remaining distance is calculated by dividing the remaining distance by the maximum velocity limit.</li> <li>A check is added to the remaining time calculation to make sure that maximum velocity limit is greater than zero. This prevents division by zero or getting negative time value.</li> </ul>"},{"location":"planning/autoware_remaining_distance_time_calculator/#future-work","title":"Future Work","text":"<ul> <li>Find a more efficient way for remaining distance calculation instead of regularly searching the graph for finding the remaining shortest path.</li> <li>Engage more sophisticated motion models for more accurate remaining time calculations.</li> </ul>"},{"location":"planning/autoware_rtc_interface/","title":"RTC Interface","text":""},{"location":"planning/autoware_rtc_interface/#rtc-interface","title":"RTC Interface","text":""},{"location":"planning/autoware_rtc_interface/#purpose","title":"Purpose","text":"<p>RTC Interface is an interface to publish the decision status of behavior planning modules and receive execution command from external of an autonomous driving system.</p>"},{"location":"planning/autoware_rtc_interface/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The RTC Interface works by creating a communication channel between a behavior planning module and the rest of the autonomous driving system. The planning module uses the interface to publish its current status, such as whether it is safe to proceed or if it is waiting for an external command. Other modules can then subscribe to this status information and send commands to the behavior planning module, such as \"activate\" or \"deactivate\".</p>"},{"location":"planning/autoware_rtc_interface/#usage-example","title":"Usage example","text":"<pre><code>// Generate instance (in this example, \"intersection\" is selected)\nautoware::rtc_interface::RTCInterface rtc_interface(node, \"intersection\");\n\n// Generate UUID\nconst unique_identifier_msgs::msg::UUID uuid = generate_uuid(getModuleId());\n\n// Repeat while module is running\nwhile (...) {\n  // Get safety status of the module corresponding to the module id\n  const bool safe = ...\n\n  // Get the current state of the module\n  const uint8_t state = ...\n\n  // Get distance to the object corresponding to the module id\n  const double start_distance = ...\n  const double finish_distance = ...\n\n  // Get time stamp\n  const rclcpp::Time stamp = ...\n\n  // Update status\n  rtc_interface.updateCooperateStatus(uuid, safe, state, start_distance, finish_distance, stamp);\n\n  if (rtc_interface.isActivated(uuid)) {\n    // Execute planning\n  } else {\n    // Stop planning\n  }\n  // Get time stamp\n  const rclcpp::Time stamp = ...\n\n  // Publish status topic\n  rtc_interface.publishCooperateStatus(stamp);\n}\n\n// Remove the status from array\nrtc_interface.removeCooperateStatus(uuid);\n</code></pre>"},{"location":"planning/autoware_rtc_interface/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"planning/autoware_rtc_interface/#rtcinterface-constructor","title":"RTCInterface (Constructor)","text":"<pre><code>autoware::rtc_interface::RTCInterface(rclcpp::Node &amp; node, const std::string &amp; name, const bool enable_rtc = false);\n</code></pre>"},{"location":"planning/autoware_rtc_interface/#description","title":"Description","text":"<p>A constructor for <code>autoware::rtc_interface::RTCInterface</code>.</p>"},{"location":"planning/autoware_rtc_interface/#input","title":"Input","text":"<ul> <li><code>node</code> : Node calling this interface</li> <li><code>name</code> : Name of cooperate status array topic and cooperate commands service<ul> <li>Cooperate status array topic name : <code>~/{name}/cooperate_status</code></li> <li>Cooperate commands service name : <code>~/{name}/cooperate_commands</code></li> </ul> </li> <li><code>enable_rtc</code>: A boolean indicating whether RTC is enabled or not by default.</li> </ul>"},{"location":"planning/autoware_rtc_interface/#output","title":"Output","text":"<p>An instance of <code>RTCInterface</code></p>"},{"location":"planning/autoware_rtc_interface/#publishcooperatestatus","title":"publishCooperateStatus","text":"<pre><code>autoware::rtc_interface::publishCooperateStatus(const rclcpp::Time &amp; stamp)\n</code></pre>"},{"location":"planning/autoware_rtc_interface/#description_1","title":"Description","text":"<p>Publish registered cooperate status.</p>"},{"location":"planning/autoware_rtc_interface/#input_1","title":"Input","text":"<ul> <li><code>stamp</code> : Time stamp</li> </ul>"},{"location":"planning/autoware_rtc_interface/#output_1","title":"Output","text":"<p>Nothing</p>"},{"location":"planning/autoware_rtc_interface/#updatecooperatestatus","title":"updateCooperateStatus","text":"<pre><code>autoware::rtc_interface::updateCooperateStatus(const unique_identifier_msgs::msg::UUID &amp; uuid, const bool safe, const double start_distance, const double finish_distance, const rclcpp::Time &amp; stamp)\n</code></pre>"},{"location":"planning/autoware_rtc_interface/#description_2","title":"Description","text":"<p>Update cooperate status corresponding to <code>uuid</code>. If cooperate status corresponding to <code>uuid</code> is not registered yet, add new cooperate status.</p>"},{"location":"planning/autoware_rtc_interface/#input_2","title":"Input","text":"<ul> <li><code>uuid</code> : UUID for requesting module</li> <li><code>safe</code> : Safety status of requesting module</li> <li><code>start_distance</code> : Distance to the start object from ego vehicle</li> <li><code>finish_distance</code> : Distance to the finish object from ego vehicle</li> <li><code>stamp</code> : Time stamp</li> <li><code>requested</code>: A boolean indicating whether a request has been made.</li> <li><code>override_rtc_auto_mode</code>: An optional boolean to override the RTC mode (true forces AUTO, false forces MANUAL).</li> </ul>"},{"location":"planning/autoware_rtc_interface/#output_2","title":"Output","text":"<p>Nothing</p>"},{"location":"planning/autoware_rtc_interface/#removecooperatestatus","title":"removeCooperateStatus","text":"<pre><code>autoware::rtc_interface::removeCooperateStatus(const unique_identifier_msgs::msg::UUID &amp; uuid)\n</code></pre>"},{"location":"planning/autoware_rtc_interface/#description_3","title":"Description","text":"<p>Remove cooperate status corresponding to <code>uuid</code> from registered statuses.</p>"},{"location":"planning/autoware_rtc_interface/#input_3","title":"Input","text":"<ul> <li><code>uuid</code> : UUID for expired module</li> </ul>"},{"location":"planning/autoware_rtc_interface/#output_3","title":"Output","text":"<p>Nothing</p>"},{"location":"planning/autoware_rtc_interface/#clearcooperatestatus","title":"clearCooperateStatus","text":"<pre><code>autoware::rtc_interface::clearCooperateStatus()\n</code></pre>"},{"location":"planning/autoware_rtc_interface/#description_4","title":"Description","text":"<p>Remove all cooperate statuses.</p>"},{"location":"planning/autoware_rtc_interface/#input_4","title":"Input","text":"<p>Nothing</p>"},{"location":"planning/autoware_rtc_interface/#output_4","title":"Output","text":"<p>Nothing</p>"},{"location":"planning/autoware_rtc_interface/#isactivated","title":"isActivated","text":"<pre><code>autoware::rtc_interface::isActivated(const unique_identifier_msgs::msg::UUID &amp; uuid)\n</code></pre>"},{"location":"planning/autoware_rtc_interface/#description_5","title":"Description","text":"<p>Return received command status corresponding to <code>uuid</code>.</p>"},{"location":"planning/autoware_rtc_interface/#input_5","title":"Input","text":"<ul> <li><code>uuid</code> : UUID for checking module</li> </ul>"},{"location":"planning/autoware_rtc_interface/#output_5","title":"Output","text":"<p>If auto mode is enabled, return based on the safety status. If not, if received command is <code>ACTIVATED</code>, return <code>true</code>. If not, return <code>false</code>.</p>"},{"location":"planning/autoware_rtc_interface/#isregistered","title":"isRegistered","text":"<pre><code>autoware::rtc_interface::isRegistered(const unique_identifier_msgs::msg::UUID &amp; uuid)\n</code></pre>"},{"location":"planning/autoware_rtc_interface/#description_6","title":"Description","text":"<p>Return <code>true</code> if <code>uuid</code> is registered.</p>"},{"location":"planning/autoware_rtc_interface/#input_6","title":"Input","text":"<ul> <li><code>uuid</code> : UUID for checking module</li> </ul>"},{"location":"planning/autoware_rtc_interface/#output_6","title":"Output","text":"<p>If <code>uuid</code> is registered, return <code>true</code>. If not, return <code>false</code>.</p>"},{"location":"planning/autoware_rtc_interface/#debugging-tools","title":"Debugging Tools","text":"<p>There is a debugging tool called RTC Replayer for the RTC interface.</p>"},{"location":"planning/autoware_rtc_interface/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"planning/autoware_rtc_interface/#future-extensions-unimplemented-parts","title":"Future extensions / Unimplemented parts","text":""},{"location":"planning/autoware_scenario_selector/","title":"autoware_scenario_selector","text":""},{"location":"planning/autoware_scenario_selector/#autoware_scenario_selector","title":"autoware_scenario_selector","text":""},{"location":"planning/autoware_scenario_selector/#scenario_selector_node","title":"scenario_selector_node","text":"<p><code>scenario_selector_node</code> is a node that switches trajectories from each scenario.</p>"},{"location":"planning/autoware_scenario_selector/#input-topics","title":"Input topics","text":"Name Type Description <code>~input/lane_driving/trajectory</code> autoware_planning_msgs::Trajectory trajectory of LaneDriving scenario <code>~input/parking/trajectory</code> autoware_planning_msgs::Trajectory trajectory of Parking scenario <code>~input/lanelet_map</code> autoware_map_msgs::msg::LaneletMapBin <code>~input/route</code> autoware_planning_msgs::LaneletRoute route and goal pose <code>~input/odometry</code> nav_msgs::Odometry for checking whether vehicle is stopped <code>is_parking_completed</code> bool (implemented as rosparam) whether all split trajectory of Parking are published"},{"location":"planning/autoware_scenario_selector/#output-topics","title":"Output topics","text":"Name Type Description <code>~output/scenario</code> tier4_planning_msgs::Scenario current scenario and scenarios to be activated <code>~output/trajectory</code> autoware_planning_msgs::Trajectory trajectory to be followed"},{"location":"planning/autoware_scenario_selector/#output-tfs","title":"Output TFs","text":"<p>None</p>"},{"location":"planning/autoware_scenario_selector/#how-to-launch","title":"How to launch","text":"<ol> <li>Write your remapping info in <code>scenario_selector.launch</code> or add args when executing <code>roslaunch</code></li> <li><code>roslaunch autoware_scenario_selector scenario_selector.launch</code><ul> <li>If you would like to use only a single scenario, <code>roslaunch autoware_scenario_selector dummy_scenario_selector_{scenario_name}.launch</code></li> </ul> </li> </ol>"},{"location":"planning/autoware_scenario_selector/#parameters","title":"Parameters","text":"Name Type Description Default Range update_rate float timer's update rate 10 \u22650.0 th_max_message_delay_sec float threshold time of input messages' maximum delay 1 \u22650.0 th_arrived_distance_m float threshold distance to check if vehicle has arrived at the trajectory's endpoint 1 \u22650.0 th_stopped_time_sec float threshold time to check if vehicle is stopped 1 \u22650.0 th_stopped_velocity_mps float threshold velocity to check if vehicle is stopped 0.01 \u22650.0 enable_mode_switching boolean enable switching between scenario modes when ego is stuck in parking area 1 N/A"},{"location":"planning/autoware_scenario_selector/#flowchart","title":"Flowchart","text":""},{"location":"planning/autoware_surround_obstacle_checker/","title":"Surround Obstacle Checker","text":""},{"location":"planning/autoware_surround_obstacle_checker/#surround-obstacle-checker","title":"Surround Obstacle Checker","text":""},{"location":"planning/autoware_surround_obstacle_checker/#purpose","title":"Purpose","text":"<p>This module subscribes required data (ego-pose, obstacles, etc), and publishes zero velocity limit to keep stopping if any of stop conditions are satisfied.</p>"},{"location":"planning/autoware_surround_obstacle_checker/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"planning/autoware_surround_obstacle_checker/#flow-chart","title":"Flow chart","text":""},{"location":"planning/autoware_surround_obstacle_checker/#algorithms","title":"Algorithms","text":""},{"location":"planning/autoware_surround_obstacle_checker/#check-data","title":"Check data","text":"<p>Check that <code>surround_obstacle_checker</code> receives no ground pointcloud, dynamic objects and current velocity data.</p>"},{"location":"planning/autoware_surround_obstacle_checker/#get-distance-to-nearest-object","title":"Get distance to nearest object","text":"<p>Calculate distance between ego vehicle and the nearest object. In this function, it calculates the minimum distance between the polygon of ego vehicle and all points in pointclouds and the polygons of dynamic objects.</p>"},{"location":"planning/autoware_surround_obstacle_checker/#stop-requirement","title":"Stop requirement","text":"<p>If it satisfies all following conditions, it plans stopping.</p> <ul> <li>Ego vehicle is stopped</li> <li>It satisfies any following conditions<ol> <li>The distance to nearest obstacle satisfies following conditions<ul> <li>If state is <code>State::PASS</code>, the distance is less than <code>surround_check_distance</code></li> <li>If state is <code>State::STOP</code>, the distance is less than <code>surround_check_recover_distance</code></li> </ul> </li> <li>If it does not satisfies the condition in 1, elapsed time from the time it satisfies the condition in 1 is less than <code>state_clear_time</code></li> </ol> </li> </ul>"},{"location":"planning/autoware_surround_obstacle_checker/#states","title":"States","text":"<p>To prevent chattering, <code>surround_obstacle_checker</code> manages two states. As mentioned in stop condition section, it prevents chattering by changing threshold to find surround obstacle depending on the states.</p> <ul> <li><code>State::PASS</code> : Stop planning is released</li> <li><code>State::STOP</code> \uff1aWhile stop planning</li> </ul>"},{"location":"planning/autoware_surround_obstacle_checker/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"planning/autoware_surround_obstacle_checker/#input","title":"Input","text":"Name Type Description <code>/perception/obstacle_segmentation/pointcloud</code> <code>sensor_msgs::msg::PointCloud2</code> Pointcloud of obstacles which the ego-vehicle should stop or avoid <code>/perception/object_recognition/objects</code> <code>autoware_perception_msgs::msg::PredictedObjects</code> Dynamic objects <code>/localization/kinematic_state</code> <code>nav_msgs::msg::Odometry</code> Current twist <code>/tf</code> <code>tf2_msgs::msg::TFMessage</code> TF <code>/tf_static</code> <code>tf2_msgs::msg::TFMessage</code> TF static"},{"location":"planning/autoware_surround_obstacle_checker/#output","title":"Output","text":"Name Type Description <code>~/output/velocity_limit_clear_command</code> <code>autoware_internal_planning_msgs::msg::VelocityLimitClearCommand</code> Velocity limit clear command <code>~/output/max_velocity</code> <code>autoware_internal_planning_msgs::msg::VelocityLimit</code> Velocity limit command <code>~/output/no_start_reason</code> <code>diagnostic_msgs::msg::DiagnosticStatus</code> No start reason <code>~/debug/marker</code> <code>visualization_msgs::msg::MarkerArray</code> Marker for visualization <code>~/debug/footprint</code> <code>geometry_msgs::msg::PolygonStamped</code> Ego vehicle base footprint for visualization <code>~/debug/footprint_offset</code> <code>geometry_msgs::msg::PolygonStamped</code> Ego vehicle footprint with <code>surround_check_distance</code> offset for visualization <code>~/debug/footprint_recover_offset</code> <code>geometry_msgs::msg::PolygonStamped</code> Ego vehicle footprint with <code>surround_check_recover_distance</code> offset for visualization"},{"location":"planning/autoware_surround_obstacle_checker/#parameters","title":"Parameters","text":"Name Type Description Default Range pointcloud.enable_check boolean enable to check surrounding pointcloud false N/A pointcloud.surround_check_front_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 pointcloud.surround_check_side_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 pointcloud.surround_check_back_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 unknown.enable_check boolean enable to check surrounding unknown objects true N/A unknown.surround_check_front_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 unknown.surround_check_side_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 unknown.surround_check_back_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 car.enable_check boolean enable to check surrounding car true N/A car.surround_check_front_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 car.surround_check_side_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 car.surround_check_back_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 truck.enable_check boolean enable to check surrounding truck true N/A truck.surround_check_front_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 truck.surround_check_side_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 truck.surround_check_back_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 bus.enable_check boolean enable to check surrounding bus true N/A bus.surround_check_front_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 bus.surround_check_side_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 bus.surround_check_back_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 trailer.enable_check boolean enable to check surrounding trailer true N/A trailer.surround_check_front_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 trailer.surround_check_side_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 trailer.surround_check_back_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 motorcycle.enable_check boolean enable to check surrounding motorcycle true N/A motorcycle.surround_check_front_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 motorcycle.surround_check_side_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 motorcycle.surround_check_back_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 bicycle.enable_check boolean enable to check surrounding bicycle true N/A bicycle.surround_check_front_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 bicycle.surround_check_side_distance float f objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 bicycle.surround_check_back_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 pedestrian.enable_check boolean enable to check surrounding pedestrian true N/A pedestrian.surround_check_front_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 pedestrian.surround_check_side_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 pedestrian.surround_check_back_distance float If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status. [m] 0.5 \u22650.0 surround_check_hysteresis_distance float If no object exists in this hysteresis distance added to the above distance, transit to \"non-surrounding-obstacle\" status [m] 0.3 \u22650.0 state_clear_time float Threshold to clear stop state [s] 2.0 \u22650.0 stop_state_ego_speed float Threshold to check ego vehicle stopped [m/s] 0.1 \u22650.0 publish_debug_footprints boolean Publish vehicle footprint &amp; footprints with surround_check_distance and surround_check_recover_distance offsets. true N/A debug_footprint_label string select the label for debug footprint car ['pointcloud', 'unknown', 'car', 'truck', 'bus', 'trailer', 'motorcycle', 'bicycle', 'pedestrian'] Name Type Description Default value <code>enable_check</code> <code>bool</code> Indicates whether each object is considered in the obstacle check target. <code>true</code> for objects; <code>false</code> for point clouds <code>surround_check_front_distance</code> <code>bool</code> If there are objects or point clouds within this distance in front, transition to the \"exist-surrounding-obstacle\" status [m]. 0.5 <code>surround_check_side_distance</code> <code>double</code> If there are objects or point clouds within this side distance, transition to the \"exist-surrounding-obstacle\" status [m]. 0.5 <code>surround_check_back_distance</code> <code>double</code> If there are objects or point clouds within this back distance, transition to the \"exist-surrounding-obstacle\" status [m]. 0.5 <code>surround_check_hysteresis_distance</code> <code>double</code> If no object exists within <code>surround_check_xxx_distance</code> plus this additional distance, transition to the \"non-surrounding-obstacle\" status [m]. 0.3 <code>state_clear_time</code> <code>double</code> Threshold to clear stop state [s] 2.0 <code>stop_state_ego_speed</code> <code>double</code> Threshold to check ego vehicle stopped [m/s] 0.1 <code>stop_state_entry_duration_time</code> <code>double</code> Threshold to check ego vehicle stopped [s] 0.1 <code>publish_debug_footprints</code> <code>bool</code> Publish vehicle footprint with/without offsets <code>true</code>"},{"location":"planning/autoware_surround_obstacle_checker/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>To perform stop planning, it is necessary to get obstacle pointclouds data. Hence, it does not plan stopping if the obstacle is in blind spot.</p>"},{"location":"planning/autoware_surround_obstacle_checker/surround_obstacle_checker-design.ja/","title":"Surround Obstacle Checker","text":""},{"location":"planning/autoware_surround_obstacle_checker/surround_obstacle_checker-design.ja/#surround-obstacle-checker","title":"Surround Obstacle Checker","text":""},{"location":"planning/autoware_surround_obstacle_checker/surround_obstacle_checker-design.ja/#purpose","title":"Purpose","text":"<p><code>surround_obstacle_checker</code> \u306f\u3001\u81ea\u8eca\u304c\u505c\u8eca\u4e2d\u3001\u81ea\u8eca\u306e\u5468\u56f2\u306b\u969c\u5bb3\u7269\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306b\u767a\u9032\u3057\u306a\u3044\u3088\u3046\u306b\u505c\u6b62\u8a08\u753b\u3092\u884c\u3046\u30e2\u30b8\u30e5\u30fc\u30eb\u3067\u3042\u308b\u3002</p>"},{"location":"planning/autoware_surround_obstacle_checker/surround_obstacle_checker-design.ja/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"planning/autoware_surround_obstacle_checker/surround_obstacle_checker-design.ja/#flow-chart","title":"Flow chart","text":""},{"location":"planning/autoware_surround_obstacle_checker/surround_obstacle_checker-design.ja/#algorithms","title":"Algorithms","text":""},{"location":"planning/autoware_surround_obstacle_checker/surround_obstacle_checker-design.ja/#check-data","title":"Check data","text":"<p>\u70b9\u7fa4\u3001\u52d5\u7684\u7269\u4f53\u3001\u81ea\u8eca\u901f\u5ea6\u306e\u30c7\u30fc\u30bf\u304c\u53d6\u5f97\u3067\u304d\u3066\u3044\u308b\u304b\u3069\u3046\u304b\u3092\u78ba\u8a8d\u3059\u308b\u3002</p>"},{"location":"planning/autoware_surround_obstacle_checker/surround_obstacle_checker-design.ja/#get-distance-to-nearest-object","title":"Get distance to nearest object","text":"<p>\u81ea\u8eca\u3068\u6700\u8fd1\u508d\u306e\u969c\u5bb3\u7269\u3068\u306e\u8ddd\u96e2\u3092\u8a08\u7b97\u3059\u308b\u3002 \u3053\u3053\u3067\u306f\u3001\u81ea\u8eca\u306e\u30dd\u30ea\u30b4\u30f3\u3092\u8a08\u7b97\u3057\u3001\u70b9\u7fa4\u306e\u5404\u70b9\u304a\u3088\u3073\u5404\u52d5\u7684\u7269\u4f53\u306e\u30dd\u30ea\u30b4\u30f3\u3068\u306e\u8ddd\u96e2\u3092\u305d\u308c\u305e\u308c\u8a08\u7b97\u3059\u308b\u3053\u3068\u3067\u6700\u8fd1\u508d\u306e\u969c\u5bb3\u7269\u3068\u306e\u8ddd\u96e2\u3092\u6c42\u3081\u308b\u3002</p>"},{"location":"planning/autoware_surround_obstacle_checker/surround_obstacle_checker-design.ja/#stop-condition","title":"Stop condition","text":"<p>\u6b21\u306e\u6761\u4ef6\u3092\u3059\u3079\u3066\u6e80\u305f\u3059\u3068\u304d\u3001\u81ea\u8eca\u306f\u505c\u6b62\u8a08\u753b\u3092\u884c\u3046\u3002</p> <ul> <li>\u81ea\u8eca\u304c\u505c\u8eca\u3057\u3066\u3044\u308b\u3053\u3068</li> <li>\u6b21\u306e\u3046\u3061\u3044\u305a\u308c\u304b\u3092\u6e80\u305f\u3059\u3053\u3068<ol> <li>\u6700\u8fd1\u508d\u306e\u969c\u5bb3\u7269\u3068\u306e\u8ddd\u96e2\u304c\u6b21\u306e\u6761\u4ef6\u3092\u307f\u305f\u3059\u3053\u3068<ul> <li><code>State::PASS</code> \u306e\u3068\u304d\u3001<code>surround_check_distance</code> \u672a\u6e80\u3067\u3042\u308b</li> <li><code>State::STOP</code> \u306e\u3068\u304d\u3001<code>surround_check_recover_distance</code> \u4ee5\u4e0b\u3067\u3042\u308b</li> </ul> </li> <li>1 \u3092\u6e80\u305f\u3057\u3066\u3044\u306a\u3044\u3068\u304d\u30011 \u306e\u6761\u4ef6\u3092\u6e80\u305f\u3057\u305f\u6642\u523b\u304b\u3089\u306e\u7d4c\u904e\u6642\u9593\u304c <code>state_clear_time</code> \u4ee5\u4e0b\u3067\u3042\u308b\u3053\u3068</li> </ol> </li> </ul>"},{"location":"planning/autoware_surround_obstacle_checker/surround_obstacle_checker-design.ja/#states","title":"States","text":"<p>\u30c1\u30e3\u30bf\u30ea\u30f3\u30b0\u9632\u6b62\u306e\u305f\u3081\u3001<code>surround_obstacle_checker</code> \u3067\u306f\u72b6\u614b\u3092\u7ba1\u7406\u3057\u3066\u3044\u308b\u3002 Stop condition \u306e\u9805\u3067\u8ff0\u3079\u305f\u3088\u3046\u306b\u3001\u72b6\u614b\u306b\u3088\u3063\u3066\u969c\u5bb3\u7269\u5224\u5b9a\u306e\u3057\u304d\u3044\u5024\u3092\u5909\u66f4\u3059\u308b\u3053\u3068\u3067\u30c1\u30e3\u30bf\u30ea\u30f3\u30b0\u3092\u9632\u6b62\u3057\u3066\u3044\u308b\u3002</p> <ul> <li><code>State::PASS</code> \uff1a\u505c\u6b62\u8a08\u753b\u89e3\u9664\u4e2d</li> <li><code>State::STOP</code> \uff1a\u505c\u6b62\u8a08\u753b\u4e2d</li> </ul>"},{"location":"planning/autoware_surround_obstacle_checker/surround_obstacle_checker-design.ja/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"planning/autoware_surround_obstacle_checker/surround_obstacle_checker-design.ja/#input","title":"Input","text":"Name Type Description <code>/perception/obstacle_segmentation/pointcloud</code> <code>sensor_msgs::msg::PointCloud2</code> Pointcloud of obstacles which the ego-vehicle should stop or avoid <code>/perception/object_recognition/objects</code> <code>autoware_perception_msgs::msg::PredictedObjects</code> Dynamic objects <code>/localization/kinematic_state</code> <code>nav_msgs::msg::Odometry</code> Current twist <code>/tf</code> <code>tf2_msgs::msg::TFMessage</code> TF <code>/tf_static</code> <code>tf2_msgs::msg::TFMessage</code> TF static"},{"location":"planning/autoware_surround_obstacle_checker/surround_obstacle_checker-design.ja/#output","title":"Output","text":"Name Type Description <code>~/output/velocity_limit_clear_command</code> <code>autoware_internal_planning_msgs::msg::VelocityLimitClearCommand</code> Velocity limit clear command <code>~/output/max_velocity</code> <code>autoware_internal_planning_msgs::msg::VelocityLimit</code> Velocity limit command <code>~/output/no_start_reason</code> <code>diagnostic_msgs::msg::DiagnosticStatus</code> No start reason <code>~/debug/marker</code> <code>visualization_msgs::msg::MarkerArray</code> Marker for visualization"},{"location":"planning/autoware_surround_obstacle_checker/surround_obstacle_checker-design.ja/#parameters","title":"Parameters","text":"Name Type Description Default value <code>use_pointcloud</code> <code>bool</code> Use pointcloud as obstacle check <code>true</code> <code>use_dynamic_object</code> <code>bool</code> Use dynamic object as obstacle check <code>true</code> <code>surround_check_distance</code> <code>double</code> If objects exist in this distance, transit to \"exist-surrounding-obstacle\" status [m] 0.5 <code>surround_check_recover_distance</code> <code>double</code> If no object exists in this distance, transit to \"non-surrounding-obstacle\" status [m] 0.8 <code>state_clear_time</code> <code>double</code> Threshold to clear stop state [s] 2.0 <code>stop_state_ego_speed</code> <code>double</code> Threshold to check ego vehicle stopped [m/s] 0.1 <code>stop_state_entry_duration_time</code> <code>double</code> Threshold to check ego vehicle stopped [s] 0.1"},{"location":"planning/autoware_surround_obstacle_checker/surround_obstacle_checker-design.ja/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>\u3053\u306e\u6a5f\u80fd\u304c\u52d5\u4f5c\u3059\u308b\u305f\u3081\u306b\u306f\u969c\u5bb3\u7269\u70b9\u7fa4\u306e\u89b3\u6e2c\u304c\u5fc5\u8981\u306a\u305f\u3081\u3001\u969c\u5bb3\u7269\u304c\u6b7b\u89d2\u306b\u5165\u3063\u3066\u3044\u308b\u5834\u5408\u306f\u505c\u6b62\u8a08\u753b\u3092\u884c\u308f\u306a\u3044\u3002</p>"},{"location":"planning/autoware_trajectory_adapter/","title":"Trajectory Adapter","text":""},{"location":"planning/autoware_trajectory_adapter/#trajectory-adapter","title":"Trajectory Adapter","text":""},{"location":"planning/autoware_trajectory_adapter/#purposerole","title":"Purpose/Role","text":"<p>This node converts a set of ranked candidate trajectories into a single, cleaned\u2011up autoware_planning_msgs/msg/Trajectory.</p>"},{"location":"planning/autoware_trajectory_adapter/#algorithm-overview","title":"Algorithm Overview","text":"<p>Upon receiving a message, the node finds the candidate with the highest score. If no candidate is found an empty list would be published. The selected trajectory\u2019s points are then processed, ensuring that successive points match the conditions to conduct control modules.</p>"},{"location":"planning/autoware_trajectory_adapter/#interface","title":"Interface","text":""},{"location":"planning/autoware_trajectory_adapter/#topics","title":"Topics","text":"Direction Topic Name Message Type Description Subscribe <code>~/input/trajectories</code> <code>autoware_internal_planning_msgs/msg/ScoredCandidateTrajectories</code> Candidate trajectories with scores Publish <code>~/output/trajectory</code> <code>autoware_planning_msgs/msg/Trajectory</code> Re-arranged best\u2011score trajectory"},{"location":"planning/autoware_trajectory_adapter/#parameters","title":"Parameters","text":"<p>The current implementation does not expose any ROS\u00a0parameters. All behavior is hard\u2011coded.</p>"},{"location":"planning/autoware_trajectory_adapter/#future-work","title":"Future Work","text":"<p>The following features and improvements are planned for future development:</p> <ul> <li>Turn Signal Control: Implement proper turn signal activation based on trajectory curvature and lane change intentions</li> <li>Hazard Light Control: Add logic to activate hazard lights in emergency situations or when the vehicle needs to warn other traffic participants</li> </ul>"},{"location":"planning/autoware_trajectory_concatenator/","title":"Trajectory Concatenator","text":""},{"location":"planning/autoware_trajectory_concatenator/#trajectory-concatenator","title":"Trajectory Concatenator","text":""},{"location":"planning/autoware_trajectory_concatenator/#purposerole","title":"Purpose/Role","text":"<p>This node aggregates trajectory candidates from multiple trajectory generators into a single <code>autoware_internal_planning_msgs/msg/CandidateTrajectories</code> message. It is intended to be placed between trajectory generators and the selector/ranker.</p>"},{"location":"planning/autoware_trajectory_concatenator/#algorithm-overview","title":"Algorithm Overview","text":"<p>When a message arrives on <code>~/input/trajectories</code>, the node splits it by <code>generator_id</code> and updates an in-memory buffer so that only the most recent trajectory set for each generator is retained.</p> <p>A 100\u202fms timer then scans this buffer and drops any entry whose header stamp is older than the configured duration_time. Immediately after pruning, the timer concatenates all remaining trajectories and their accompanying <code>generator_info</code> arrays, publishes the aggregated message.</p>"},{"location":"planning/autoware_trajectory_concatenator/#interface","title":"Interface","text":""},{"location":"planning/autoware_trajectory_concatenator/#topics","title":"Topics","text":"Direction Topic name Message Type Description Subscriber <code>~/input/trajectories</code> <code>autoware_internal_planning_msgs/msg/CandidateTrajectories</code> Trajectory sets produced by each generator Publisher <code>~/output/trajectories</code> <code>autoware_internal_planning_msgs/msg/CandidateTrajectories</code> Concatenated list of all buffered trajectories"},{"location":"planning/autoware_trajectory_concatenator/#parameters","title":"Parameters","text":"Name Type Description Default Range duration_time float Duration time to keep the trajectory (it will be deleted once exceeding this value) 0.2 \u22650.0"},{"location":"planning/autoware_trajectory_modifier/","title":"Autoware Trajectory Modifier","text":""},{"location":"planning/autoware_trajectory_modifier/#autoware-trajectory-modifier","title":"Autoware Trajectory Modifier","text":"<p>The <code>autoware_trajectory_modifier</code> package provides a plugin-based architecture for post-processing trajectory points to improve trajectory quality and ensure vehicle safety. It takes candidate trajectories and applies various modification algorithms to enhance their feasibility and safety characteristics.</p>"},{"location":"planning/autoware_trajectory_modifier/#features","title":"Features","text":"<ul> <li>Plugin-based architecture for extensible trajectory modifications</li> <li>Stop point fixing to prevent trajectory issues near stationary conditions</li> <li>Configurable parameters to adjust modification behavior</li> </ul>"},{"location":"planning/autoware_trajectory_modifier/#architecture","title":"Architecture","text":"<p>The trajectory modifier uses a plugin-based system where different modification algorithms can be implemented as plugins. Each plugin inherits from the <code>TrajectoryModifierPluginBase</code> class and implements the required interface.</p>"},{"location":"planning/autoware_trajectory_modifier/#plugin-interface","title":"Plugin Interface","text":"<p>All modifier plugins must inherit from <code>TrajectoryModifierPluginBase</code> and implement:</p> <ul> <li><code>modify_trajectory()</code> - Main method to modify trajectory points</li> <li><code>set_up_params()</code> - Initialize plugin parameters</li> <li><code>on_parameter()</code> - Handle parameter updates</li> <li><code>is_trajectory_modification_required()</code> - Determine if modification is needed</li> </ul>"},{"location":"planning/autoware_trajectory_modifier/#current-plugins","title":"Current Plugins","text":""},{"location":"planning/autoware_trajectory_modifier/#stop-point-fixer","title":"Stop Point Fixer","text":"<p>The Stop Point Fixer plugin addresses trajectory issues when the ego vehicle is stationary or moving at very low speeds. It prevents problematic trajectory points that could cause planning issues by:</p> <ul> <li>Monitoring ego vehicle velocity and trajectory distance</li> <li>Replacing the trajectory with a single stop point when conditions are met</li> <li>Ensuring smooth operation during stationary periods</li> </ul>"},{"location":"planning/autoware_trajectory_modifier/#dependencies","title":"Dependencies","text":"<p>This package depends on the following packages:</p> <ul> <li><code>autoware_internal_planning_msgs</code>: For candidate trajectory message types</li> <li><code>autoware_planning_msgs</code>: For output trajectory message types</li> <li><code>autoware_motion_utils</code>: Motion-related utility functions</li> <li><code>autoware_trajectory</code>: Trajectory data structures and utilities</li> <li><code>autoware_utils</code>: Common utility functions</li> </ul>"},{"location":"planning/autoware_trajectory_modifier/#inputoutput","title":"Input/Output","text":"<ul> <li>Input: <code>autoware_internal_planning_msgs::msg::CandidateTrajectories</code></li> <li>Output: Modified <code>autoware_internal_planning_msgs::msg::CandidateTrajectories</code> and selected <code>autoware_planning_msgs::msg::Trajectory</code></li> </ul>"},{"location":"planning/autoware_trajectory_modifier/#parameters","title":"Parameters","text":"<ul> <li><code>use_stop_point_fixer</code>: Enable the stop point fixer modifier plugin (default: true)</li> <li><code>stop_point_fixer.velocity_threshold_mps</code>: Velocity threshold below which ego vehicle is considered stationary (default: 0.1 m/s)</li> <li><code>stop_point_fixer.min_distance_threshold_m</code>: Minimum distance threshold to trigger trajectory replacement (default: 1.0 m)</li> </ul> <p>Parameters can be set via YAML configuration files in the <code>config/</code> directory.</p>"},{"location":"planning/autoware_trajectory_modifier/#adding-new-modifier-plugins","title":"Adding New Modifier Plugins","text":"<p>To add a new modifier plugin:</p> <ol> <li>Create header and source files in <code>trajectory_modifier_plugins/</code></li> <li>Inherit from <code>TrajectoryModifierPluginBase</code></li> <li>Implement the required virtual methods</li> <li>Register the plugin in the main node's <code>initialize_modifiers()</code> method</li> <li>Add plugin-specific parameters to the schema and config files</li> </ol>"},{"location":"planning/autoware_trajectory_optimizer/","title":"Autoware Trajectory Optimizer","text":""},{"location":"planning/autoware_trajectory_optimizer/#autoware-trajectory-optimizer","title":"Autoware Trajectory Optimizer","text":"<p>The <code>autoware_trajectory_optimizer</code> package generates smooth and feasible trajectories for autonomous vehicles using a plugin-based optimization pipeline. It takes candidate trajectories as input and applies a sequence of optimization plugins to produce smooth, drivable trajectories with proper velocity and acceleration profiles.</p>"},{"location":"planning/autoware_trajectory_optimizer/#features","title":"Features","text":"<ul> <li>Plugin-based architecture - Modular optimization pipeline where each step is a separate plugin</li> <li>Multiple smoothing methods:<ul> <li>Elastic Band (EB) smoother for path optimization</li> <li>Akima spline interpolation for smooth path interpolation</li> <li>QP-based smoother with quadratic programming for path smoothing with jerk constraints</li> </ul> </li> <li>Velocity optimization - Jerk-filtered velocity smoothing from <code>autoware_velocity_smoother</code></li> <li>Trajectory validation - Removes invalid points and fixes trajectory orientation</li> <li>Backward trajectory extension - Extends trajectory using past ego states</li> <li>Dynamic parameter reconfiguration - Runtime parameter updates supported</li> </ul>"},{"location":"planning/autoware_trajectory_optimizer/#architecture","title":"Architecture","text":"<p>The package uses a pluginlib-based architecture where optimization plugins are dynamically loaded at startup. Each plugin inherits from <code>TrajectoryOptimizerPluginBase</code> and is loaded via the ROS 2 pluginlib system.</p>"},{"location":"planning/autoware_trajectory_optimizer/#plugin-loading-and-execution","title":"Plugin Loading and Execution","text":"<p>Plugins are loaded based on the <code>plugin_names</code> parameter, which defines both which plugins to load and their execution order:</p> <pre><code>plugin_names:\n  - \"autoware::trajectory_optimizer::plugin::TrajectoryPointFixer\"\n  - \"autoware::trajectory_optimizer::plugin::TrajectoryQPSmoother\"\n  - \"autoware::trajectory_optimizer::plugin::TrajectoryEBSmootherOptimizer\"\n  - \"autoware::trajectory_optimizer::plugin::TrajectorySplineSmoother\"\n  - \"autoware::trajectory_optimizer::plugin::TrajectoryVelocityOptimizer\"\n  - \"autoware::trajectory_optimizer::plugin::TrajectoryExtender\"\n  - \"autoware::trajectory_optimizer::plugin::TrajectoryPointFixer\"\n</code></pre>"},{"location":"planning/autoware_trajectory_optimizer/#available-plugins","title":"Available Plugins","text":"<ol> <li>TrajectoryPointFixer - Removes invalid/repeated points and fixes trajectory direction</li> <li>TrajectoryQPSmoother - QP-based path smoothing with jerk constraints</li> <li>TrajectoryEBSmootherOptimizer - Elastic Band path smoothing</li> <li>TrajectorySplineSmoother - Akima spline interpolation</li> <li>TrajectoryMPTOptimizer - Model predictive trajectory optimization with adaptive corridor bounds. Uses bicycle kinematics model for trajectory refinement. Disabled by default (experimental). See docs/mpt_optimizer.md for details.</li> <li>TrajectoryVelocityOptimizer - Velocity profile optimization with lateral acceleration limits</li> <li>TrajectoryExtender - Extends trajectory backward using past ego states</li> <li>TrajectoryKinematicFeasibilityEnforcer - Enforces Ackermann steering and yaw rate constraints</li> </ol> <p>Each plugin can be enabled/disabled at runtime via activation flags (e.g., <code>use_qp_smoother</code>) and manages its own configuration independently.</p>"},{"location":"planning/autoware_trajectory_optimizer/#important-plugin-ordering-constraints","title":"\u26a0\ufe0f Important: Plugin Ordering Constraints","text":"<p>The order of plugin execution is critical and must be carefully maintained:</p> <ul> <li>QP Smoother must run before EB/Akima smoothers: The QP solver relies on constant time intervals (\u0394t) between trajectory points (default: 0.1s). Both Elastic Band and Akima spline smoothers resample trajectories without preserving the time domain structure, which breaks the QP solver's assumptions. Therefore, when using multiple smoothers together, the QP smoother must execute first.</li> </ul> <ul> <li>Trajectory Extender positioning: The trajectory extender has known discontinuity issues when placed early in the pipeline. It negatively affects the QP solver results and introduces artifacts. For this reason, it has been moved to near the end of the pipeline and is disabled by default (<code>extend_trajectory_backward: false</code>). Fixing the extender's discontinuity issues is future work.</li> </ul>"},{"location":"planning/autoware_trajectory_optimizer/#qp-smoother","title":"QP Smoother","text":"<p>The QP smoother uses quadratic programming (OSQP solver) to optimize trajectory paths with advanced features:</p> <ul> <li>Objective: Minimizes path curvature while maintaining fidelity to the original trajectory</li> <li>Decision variables: Path positions (x, y) for each trajectory point</li> <li>Constraints: Fixed initial position (optionally fixed last position)</li> <li>Velocity-based fidelity: Automatically reduces fidelity weight at low speeds for aggressive smoothing of noise</li> <li>Post-processing: Recalculates velocities, accelerations, and orientations from smoothed positions</li> </ul> <p>For detailed documentation, see docs/qp_smoother.md which covers:</p> <ul> <li>Mathematical formulation</li> <li>Velocity-based fidelity weighting (sigmoid function)</li> <li>Parameter tuning guidelines</li> <li>Usage examples</li> <li>Performance characteristics</li> </ul>"},{"location":"planning/autoware_trajectory_optimizer/#dependencies","title":"Dependencies","text":"<ul> <li><code>autoware_motion_utils</code> - Trajectory manipulation utilities</li> <li><code>autoware_osqp_interface</code> - QP solver interface for QP smoother</li> <li><code>autoware_path_smoother</code> - Elastic Band smoother</li> <li><code>autoware_velocity_smoother</code> - Velocity smoothing algorithms</li> <li><code>autoware_utils</code> - Common utilities (geometry, ROS helpers)</li> <li><code>autoware_vehicle_info_utils</code> - Vehicle information</li> </ul>"},{"location":"planning/autoware_trajectory_optimizer/#parameters","title":"Parameters","text":"Name Type Description Default Range plugin_names array List of plugin class names to load in execution order. Plugins will be loaded dynamically at startup and executed in the order specified. ['autoware::trajectory_optimizer::plugin::TrajectoryPointFixer', 'autoware::trajectory_optimizer::plugin::TrajectoryKinematicFeasibilityEnforcer', 'autoware::trajectory_optimizer::plugin::TrajectoryQPSmoother', 'autoware::trajectory_optimizer::plugin::TrajectoryEBSmootherOptimizer', 'autoware::trajectory_optimizer::plugin::TrajectorySplineSmoother', 'autoware::trajectory_optimizer::plugin::TrajectoryVelocityOptimizer', 'autoware::trajectory_optimizer::plugin::TrajectoryExtender', 'autoware::trajectory_optimizer::plugin::TrajectoryPointFixer'] N/A use_akima_spline_interpolation boolean Enable Akima spline interpolation for trajectory smoothing False N/A use_eb_smoother boolean Enable Elastic Band smoother for trajectory smoothing False N/A use_qp_smoother boolean Enable QP-based trajectory smoothing with explicit jerk constraints True N/A fix_invalid_points boolean Remove repeated or invalid points, or points that go against the general trajectory direction True N/A extend_trajectory_backward boolean Extend trajectory backward using ego pose history True N/A optimize_velocity boolean Enable velocity profile optimization with jerk filtering and constraint enforcement True N/A use_kinematic_feasibility_enforcer boolean Enable kinematic feasibility enforcer plugin (Ackermann + yaw rate constraints) False N/A use_mpt_optimizer boolean Enable MPT (Model Predictive Trajectory) optimizer plugin for geometry refinement with adaptive bounds False N/A trajectory_point_fixer.resample_close_points boolean Enable resampling of close proximity points True N/A trajectory_point_fixer.min_dist_to_remove_m float Minimum distance threshold for removing close proximity points [m] 0.01 \u22650 trajectory_point_fixer.min_dist_to_merge_m float Minimum distance threshold for merging close proximity points [m] 0.05 \u22650 trajectory_kinematic_feasibility.max_yaw_rate_rad_s float Maximum yaw rate [rad/s]. Default: 0.7 rad/s (~40 deg/s) aligns with MPC controller. Typical range: 0.5-1.0 for autonomous vehicles. 0.7 \u22650.1\u22642.0 trajectory_mpt_optimizer.corridor_width_m float Base corridor width for adaptive bounds generation [m]. Defines the lateral distance from trajectory centerline to left/right bounds. 3.5 \u22652.0\u226410.0 trajectory_mpt_optimizer.enable_adaptive_width boolean Enable adaptive corridor width based on curvature and velocity. When true, corridor widens in curves and at low speeds. True N/A trajectory_mpt_optimizer.curvature_width_factor float Factor for curvature-based corridor widening [m/rad]. Higher values provide more space in curves. 0.5 \u22650.0\u22642.0 trajectory_mpt_optimizer.velocity_width_factor float Factor for velocity-based corridor widening [m]. Higher values provide more maneuvering room at low speeds. 0.3 \u22650.0\u22642.0 trajectory_mpt_optimizer.min_clearance_m float Minimum clearance from vehicle edges to corridor bounds [m]. Ensures minimum safe distance. 0.5 \u22650.0\u22642.0 trajectory_mpt_optimizer.reset_previous_data_each_iteration boolean Reset MPT's previous optimization data each cycle. Should be true for diffusion planner (new trajectories each cycle), false for incremental planners. True N/A trajectory_mpt_optimizer.enable_debug_info boolean Enable debug visualization markers for bounds and reference trajectory. False N/A trajectory_mpt_optimizer.output_delta_arc_length_m float Spacing between trajectory points in MPT optimization [m]. Smaller values provide finer resolution but increase computation. 1.0 \u22650.1\u22645.0 trajectory_mpt_optimizer.output_backward_traj_length_m float Length of trajectory extension backward from ego pose [m]. 5.0 \u22650.0\u226420.0 trajectory_mpt_optimizer.ego_nearest_dist_threshold_m float Distance threshold for finding nearest trajectory point to ego [m]. 3.0 \u22650.5\u226410.0 trajectory_mpt_optimizer.ego_nearest_yaw_threshold_deg float Yaw threshold for finding nearest trajectory point to ego [degrees]. 45.0 \u226510.0\u226490.0 trajectory_mpt_optimizer.acceleration_moving_average_window integer Moving average window size for acceleration smoothing. Larger values produce smoother acceleration but increase lag. Window of 1 = no smoothing. 5 \u22651\u226420 trajectory_extender.nearest_dist_threshold_m float Distance threshold for trajectory matching [m] 1.5 \u22650 trajectory_extender.nearest_yaw_threshold_deg float Yaw threshold for trajectory matching [deg] 60.0 \u22650 trajectory_extender.backward_trajectory_extension_m float Length to extend trajectory backward using ego history [m] 5.0 \u22650 trajectory_spline_smoother.interpolation_resolution_m float Interpolation resolution for Akima spline [m] 0.5 &gt;0 trajectory_spline_smoother.max_distance_discrepancy_m float Maximum position deviation allowed for orientation copying [m] 5.0 \u22650 trajectory_spline_smoother.preserve_input_trajectory_orientation boolean Copy orientations from input trajectory to spline output True N/A trajectory_qp_smoother.weight_smoothness float Weight for path curvature minimization (geometric smoothness) [dimensionless] 10.0 \u22650 trajectory_qp_smoother.weight_fidelity float Weight for path fidelity (staying close to original path) [dimensionless] 1.0 \u22650 trajectory_qp_smoother.time_step_s float Fixed time step for velocity/acceleration calculations [s] 0.1 &gt;0 trajectory_qp_smoother.osqp_eps_abs float OSQP absolute convergence tolerance 0.0001 &gt;0 trajectory_qp_smoother.osqp_eps_rel float OSQP relative convergence tolerance 0.0001 &gt;0 trajectory_qp_smoother.osqp_max_iter integer OSQP maximum solver iterations 100 \u22651 trajectory_qp_smoother.osqp_verbose boolean Enable OSQP verbose solver output False N/A trajectory_qp_smoother.preserve_input_trajectory_orientation boolean Copy orientations from input trajectory to smoothed output True N/A trajectory_qp_smoother.max_distance_for_orientation_m float Max distance for nearest neighbor matching when copying orientations [m] 5.0 \u22650 trajectory_qp_smoother.use_velocity_based_fidelity boolean Enable velocity-dependent fidelity weighting (lower fidelity at low speeds) True N/A trajectory_qp_smoother.velocity_threshold_mps float Velocity threshold at sigmoid midpoint for velocity-based fidelity [m/s] 0.3 \u22650 trajectory_qp_smoother.sigmoid_sharpness float Sigmoid steepness for velocity-based fidelity transition (higher = sharper) 50.0 \u22651 trajectory_qp_smoother.min_fidelity_weight float Minimum fidelity weight at very low speeds [dimensionless] 0.01 \u22650 trajectory_qp_smoother.max_fidelity_weight float Maximum fidelity weight at high speeds [dimensionless] 1.0 \u22650 trajectory_qp_smoother.num_constrained_points_start integer Number of points from the start of the trajectory to constrain as hard constraints (preserves initial state) 3 \u22650 trajectory_qp_smoother.num_constrained_points_end integer Number of points from the end of the trajectory to constrain as hard constraints 3 \u22650 trajectory_velocity_optimizer.nearest_dist_threshold_m float Distance threshold for trajectory matching [m] 1.5 \u22650 trajectory_velocity_optimizer.nearest_yaw_threshold_deg float Yaw threshold for trajectory matching [deg] 60.0 \u22650 trajectory_velocity_optimizer.target_pull_out_speed_mps float Target speed during pull-out maneuver [m/s] 1.0 \u22650 trajectory_velocity_optimizer.target_pull_out_acc_mps2 float Target acceleration during pull-out maneuver [m/s\u00b2] 1.0 \u22650 trajectory_velocity_optimizer.max_speed_mps float Maximum allowed speed [m/s] 8.33 \u22650 trajectory_velocity_optimizer.max_lateral_accel_mps2 float Maximum lateral acceleration [m/s\u00b2] 1.5 \u22650 trajectory_velocity_optimizer.set_engage_speed boolean Set minimum speed during engage False N/A trajectory_velocity_optimizer.limit_speed boolean Enable speed limiting True N/A trajectory_velocity_optimizer.limit_lateral_acceleration boolean Enable lateral acceleration limiting False N/A trajectory_velocity_optimizer.smooth_velocities boolean Enable jerk-filtered velocity smoothing False N/A jerk_filter_params.jerk_weight float Weight for smoothness cost for jerk 10.0 \u22650 jerk_filter_params.over_v_weight float Weight for over speed limit cost 100000.0 \u22650 jerk_filter_params.over_a_weight float Weight for over accel limit cost 5000.0 \u22650 jerk_filter_params.over_j_weight float Weight for over jerk limit cost 2000.0 \u22650 jerk_filter_params.jerk_filter_ds float Resampling ds for jerk filter [m] 0.1 &gt;0 common.output_delta_arc_length float Delta arc length for output trajectory [m] 0.5 &gt;0 common.output_backward_traj_length float Backward length for backward trajectory from base_link [m] 5.0 \u22650 option.enable_warm_start boolean Enable warm start for optimization True N/A option.enable_optimization_validation boolean Enable optimization validation False N/A common.num_points integer Number of points for optimization 100 \u22651 common.delta_arc_length float Delta arc length for optimization [m] 1.0 &gt;0 weight.smooth_weight float Weight for smoothness constraint 1.0 \u22650 weight.lat_error_weight float Weight for lateral error constraint 0.001 \u22650 elastic_band_params.ego_nearest_dist_threshold float Distance threshold for ego nearest search [m] 3.0 \u22650 elastic_band_params.ego_nearest_yaw_threshold float Yaw threshold for ego nearest search [rad] 1.046 \u22650 <p>Parameters can be set via YAML configuration files in the <code>config/</code> directory.</p>"},{"location":"planning/autoware_trajectory_optimizer/#parameter-types","title":"Parameter Types","text":"<ol> <li>Plugin Loading (<code>plugin_names</code>) - Array of plugin class names determining load order and execution sequence</li> <li>Activation Flags - Boolean flags for runtime enable/disable (e.g., <code>use_qp_smoother</code>, <code>use_akima_spline_interpolation</code>)</li> <li>Plugin-Specific Parameters - Namespaced parameters for each plugin (e.g., <code>trajectory_qp_smoother.weight_smoothness</code>)</li> </ol>"},{"location":"planning/autoware_trajectory_optimizer/#configuring-plugin-order","title":"Configuring Plugin Order","text":"<p>To change plugin execution order, modify the <code>plugin_names</code> array in <code>config/trajectory_optimizer.param.yaml</code>:</p> <pre><code># Example: Run spline smoother before velocity optimizer\nplugin_names:\n  - \"autoware::trajectory_optimizer::plugin::TrajectoryPointFixer\"\n  - \"autoware::trajectory_optimizer::plugin::TrajectorySplineSmoother\"\n  - \"autoware::trajectory_optimizer::plugin::TrajectoryVelocityOptimizer\"\n  - \"autoware::trajectory_optimizer::plugin::TrajectoryPointFixer\"\n</code></pre>"},{"location":"planning/autoware_trajectory_optimizer/#critical-qp-smoother-ordering-constraint","title":"CRITICAL: QP Smoother Ordering Constraint","text":"<p>The <code>TrajectoryQPSmoother</code> plugin MUST run before any plugins that resample or modify trajectory structure:</p> <ul> <li><code>TrajectorySplineSmoother</code> (Akima spline - resamples trajectory)</li> <li><code>TrajectoryEBSmootherOptimizer</code> (Elastic Band - resamples trajectory)</li> <li><code>TrajectoryVelocityOptimizer</code> (velocity smoothing with resampling)</li> <li><code>TrajectoryExtender</code> (adds/modifies points at trajectory start)</li> </ul> <p>The QP solver requires constant time intervals (\u0394t = 0.1s) between points. These plugins modify the time domain structure or add points, breaking the QP solver assumptions. If you need QP smoothing, it must appear first in the pipeline after <code>TrajectoryPointFixer</code>.</p> <p>Note: Plugin order changes require node restart. Runtime enable/disable is controlled by activation flags.</p>"},{"location":"planning/autoware_trajectory_optimizer/docs/kinematic_feasibility_enforcer/","title":"Ackermann Kinematic Feasibility Enforcer","text":""},{"location":"planning/autoware_trajectory_optimizer/docs/kinematic_feasibility_enforcer/#ackermann-kinematic-feasibility-enforcer","title":"Ackermann Kinematic Feasibility Enforcer","text":""},{"location":"planning/autoware_trajectory_optimizer/docs/kinematic_feasibility_enforcer/#overview","title":"Overview","text":"<p>The Ackermann Kinematic Feasibility Enforcer is a plugin in the trajectory optimizer that ensures planned trajectories respect vehicle kinematic constraints. It enforces steering limits and yaw rate constraints based on the Ackermann steering model, preventing the planner from generating trajectories with infeasible sharp turns or rapid heading changes.</p> <p>This plugin operates on trajectory positions and orientations while preserving velocities and time stamps to maintain compatibility with downstream optimization plugins (e.g., QP smoother).</p>"},{"location":"planning/autoware_trajectory_optimizer/docs/kinematic_feasibility_enforcer/#mathematical-formulation","title":"Mathematical Formulation","text":""},{"location":"planning/autoware_trajectory_optimizer/docs/kinematic_feasibility_enforcer/#vehicle-model","title":"Vehicle Model","text":"<p>The plugin uses an Ackermann bicycle model with the following parameters:</p> <ul> <li>L: Wheelbase (distance between front and rear axles) [m]</li> <li>\u03b4_max: Maximum steering angle [rad]</li> <li>\u03c8_dot_max: Maximum yaw rate [rad/s]</li> </ul>"},{"location":"planning/autoware_trajectory_optimizer/docs/kinematic_feasibility_enforcer/#kinematic-constraints","title":"Kinematic Constraints","text":""},{"location":"planning/autoware_trajectory_optimizer/docs/kinematic_feasibility_enforcer/#1-geometric-constraint-ackermann-model","title":"1. Geometric Constraint (Ackermann Model)","text":"<p>The maximum path curvature is limited by the maximum steering angle:</p> <pre><code>\u03ba_max = tan(\u03b4_max) / L\n</code></pre> <p>Over a distance <code>s</code>, the maximum achievable yaw change is:</p> <pre><code>\u0394\u03c8_geom = \u03ba_max * s = (tan(\u03b4_max) / L) * s\n</code></pre>"},{"location":"planning/autoware_trajectory_optimizer/docs/kinematic_feasibility_enforcer/#2-rate-constraint","title":"2. Rate Constraint","text":"<p>The yaw rate limit restricts heading changes over time:</p> <pre><code>\u0394\u03c8_rate = \u03c8_dot_max * \u0394t\n</code></pre> <p>where <code>\u0394t</code> is the time interval between trajectory points.</p>"},{"location":"planning/autoware_trajectory_optimizer/docs/kinematic_feasibility_enforcer/#3-combined-constraint","title":"3. Combined Constraint","text":"<p>At each trajectory segment, the feasible yaw change is:</p> <pre><code>\u0394\u03c8_max = min(\u0394\u03c8_geom, \u0394\u03c8_rate)\n</code></pre> <p>The actual yaw change is clamped to this limit:</p> <pre><code>\u0394\u03c8_actual = clamp(\u0394\u03c8_desired, -\u0394\u03c8_max, \u0394\u03c8_max)\n</code></pre>"},{"location":"planning/autoware_trajectory_optimizer/docs/kinematic_feasibility_enforcer/#algorithm","title":"Algorithm","text":""},{"location":"planning/autoware_trajectory_optimizer/docs/kinematic_feasibility_enforcer/#forward-propagation-approach","title":"Forward Propagation Approach","text":"<p>The plugin processes the trajectory using forward propagation from the ego vehicle pose:</p> <ol> <li>Initialize anchor: Use current ego pose as initial anchor point</li> <li>For each trajectory segment:<ul> <li>Compute desired heading from current position toward original next point</li> <li>Calculate desired yaw change: <code>\u0394\u03c8_desired = normalize(\u03c8_desired - \u03c8_current)</code></li> <li>Compute feasibility limits from both constraints</li> <li>Clamp yaw change: <code>\u0394\u03c8_clamped = clamp(\u0394\u03c8_desired, -\u0394\u03c8_max, \u0394\u03c8_max)</code></li> <li>Update heading: <code>\u03c8_new = \u03c8_current + \u0394\u03c8_clamped</code></li> <li>Recompute next point position preserving segment distance</li> <li>Update anchor to current point</li> </ul> </li> <li>Preserve arc lengths: Original segment distances are maintained to    preserve trajectory timing structure</li> </ol>"},{"location":"planning/autoware_trajectory_optimizer/docs/kinematic_feasibility_enforcer/#key-algorithm-properties","title":"Key Algorithm Properties","text":"<ul> <li>Arc length preservation: Maintains <code>dt = s / v</code> relationship for each   segment</li> <li>Forward causality: Each point depends only on previous points (no   backward propagation)</li> <li>Velocity preservation: Original velocity profile unchanged</li> <li>Time stamp preservation: Trajectory timing structure maintained for QP   smoother</li> </ul>"},{"location":"planning/autoware_trajectory_optimizer/docs/kinematic_feasibility_enforcer/#implementation-details","title":"Implementation Details","text":""},{"location":"planning/autoware_trajectory_optimizer/docs/kinematic_feasibility_enforcer/#segment-distance-pre-computation","title":"Segment Distance Pre-computation","text":"<p>Before modifying any positions, all segment distances from the original trajectory are pre-computed and stored:</p> <pre><code>for (size_t i = 0; i &lt; traj_points.size() - 1; ++i) {\n  const auto dist = calc_distance2d(traj_points[i], traj_points[i + 1]);\n  segment_distances.push_back(max(dist, min_segment_distance));\n}\n</code></pre> <p>This ensures arc lengths remain constant during forward propagation, preserving the implicit timing: <code>dt_i = s_i / v_i</code>.</p>"},{"location":"planning/autoware_trajectory_optimizer/docs/kinematic_feasibility_enforcer/#heading-update-logic","title":"Heading Update Logic","text":"<p>At each iteration:</p> <ol> <li>Extract current state: Position and heading from anchor point</li> <li>Compute desired heading: From current position toward original next    point</li> <li>Normalize angle difference: Handle \u00b1\u03c0 wrapping correctly</li> <li>Apply constraints: Both geometric and rate limits</li> <li>Update position: Place next point at fixed distance <code>s</code> along new    heading</li> <li>Update orientation: Set quaternion from clamped yaw angle</li> </ol>"},{"location":"planning/autoware_trajectory_optimizer/docs/kinematic_feasibility_enforcer/#numerical-stability","title":"Numerical Stability","text":"<ul> <li>Minimum segment distance: Enforced to be \u2265 1e-6 m for numerical stability</li> <li>Angle normalization: Uses <code>autoware_utils_math::normalize_radian()</code> to   handle wraparound</li> <li>Clamping: Uses <code>std::clamp()</code> for symmetric limit enforcement</li> </ul>"},{"location":"planning/autoware_trajectory_optimizer/docs/kinematic_feasibility_enforcer/#parameters","title":"Parameters","text":""},{"location":"planning/autoware_trajectory_optimizer/docs/kinematic_feasibility_enforcer/#vehicle-parameters-from-vehicle-info","title":"Vehicle Parameters (from Vehicle Info)","text":"<p>These are automatically loaded from vehicle configuration:</p> <ul> <li><code>wheel_base_m</code>: Wheelbase distance [m]</li> <li><code>max_steer_angle_rad</code>: Maximum steering angle [rad]</li> </ul>"},{"location":"planning/autoware_trajectory_optimizer/docs/kinematic_feasibility_enforcer/#plugin-parameters","title":"Plugin Parameters","text":"<ul> <li><code>trajectory_kinematic_feasibility.max_yaw_rate_rad_s</code> (default: 0.7 rad/s)<ul> <li>Maximum allowable yaw rate</li> <li>Default: 0.7 rad/s (~40 deg/s) aligns with MPC controller</li> <li>Typical values: 0.5-1.0 rad/s (\u2248 29-57 deg/s) for autonomous vehicles</li> <li>Higher values \u2192 less restrictive, more aggressive turns</li> <li>Lower values \u2192 smoother, more conservative trajectories</li> <li>Should be tuned based on vehicle dynamics and comfort requirements</li> </ul> </li> </ul>"},{"location":"planning/autoware_trajectory_optimizer/docs/kinematic_feasibility_enforcer/#interaction-with-other-plugins","title":"Interaction with Other Plugins","text":""},{"location":"planning/autoware_trajectory_optimizer/docs/kinematic_feasibility_enforcer/#plugin-execution-order","title":"Plugin Execution Order","text":"<p>The kinematic feasibility enforcer should run before the QP smoother in the plugin pipeline to ensure:</p> <ol> <li>Trajectory points respect kinematic constraints</li> <li>QP smoother operates on kinematically feasible input</li> <li>Arc lengths and timing structure are preserved for QP optimization</li> </ol>"},{"location":"planning/autoware_trajectory_optimizer/docs/kinematic_feasibility_enforcer/#limitations","title":"Limitations","text":"<ol> <li> <p>Path deviation: Kinematic constraints may cause significant deviation    from the original planner path, especially for aggressive maneuvers.</p> </li> <li> <p>Velocity-time assumptions: Algorithm assumes constant velocity within    each segment (<code>dt = s / v</code>). Large velocity changes may affect accuracy.</p> </li> <li> <p>Steering dynamics: Does not model steering rate limits or steering    system dynamics - only considers geometric and yaw rate constraints.</p> </li> <li> <p>Lateral acceleration: Does not directly constrain lateral acceleration    (though indirectly limited by yaw rate constraint).</p> </li> <li> <p>Reverse driving: Assumes forward motion. May need special handling for    reverse trajectories.</p> </li> <li> <p>Computation time: Forward propagation through entire trajectory adds    overhead (typically &lt; 1 ms for 100-point trajectory).</p> </li> </ol>"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/","title":"MPT (Model Predictive Trajectory) Optimizer","text":""},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#mpt-model-predictive-trajectory-optimizer","title":"MPT (Model Predictive Trajectory) Optimizer","text":""},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#overview","title":"Overview","text":"<p>The MPT optimizer is a plugin in the trajectory optimizer that refines trajectory geometry using quadratic programming-based optimization with adaptive corridor bounds. It leverages the <code>autoware_path_optimizer</code> package's MPT solver to optimize the trajectory path while ensuring the vehicle stays within dynamically-computed lateral bounds.</p> <p>After geometric optimization, the plugin recalculates trajectory timing (time_from_start) and accelerations based on the optimized positions and velocities using kinematic equations, ensuring physical consistency.</p>"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#architecture","title":"Architecture","text":""},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#two-stage-process","title":"Two-Stage Process","text":"<ol> <li>Geometric Optimization: Uses MPT solver to optimize trajectory positions within adaptive corridor bounds</li> <li>Dynamics Recalculation: Recomputes accelerations and time stamps from optimized geometry using kinematic relationships</li> </ol>"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#corridor-bounds","title":"Corridor Bounds","text":""},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#purpose","title":"Purpose","text":"<p>The MPT optimizer requires lateral bounds to constrain the optimization. Normally these bounds come from the lanelet map (road boundaries), but to reduce dependency on map data, this plugin generates simple geometric bounds by offsetting perpendicular to the input trajectory.</p> <p>This is a pragmatic workaround - the bounds are just perpendicular offsets from the trajectory centerline, nothing more.</p>"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#bound-generation","title":"Bound Generation","text":"<p>For each trajectory point, left and right bounds are created by offsetting perpendicular to the heading:</p> <pre><code>left_bound  = pose + corridor_width * [cos(yaw + \u03c0/2), sin(yaw + \u03c0/2)]\nright_bound = pose + corridor_width * [cos(yaw - \u03c0/2), sin(yaw - \u03c0/2)]\n</code></pre>"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#width-calculation","title":"Width Calculation","text":"<p>When <code>enable_adaptive_width</code> is true, the corridor width is adjusted based on simple heuristics:</p>"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#1-base-width","title":"1. Base Width","text":"<pre><code>corridor_width = base_corridor_width_m\n</code></pre>"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#2-curvature-based-adjustment","title":"2. Curvature-Based Adjustment","text":"<p>Add extra width in curves:</p> <pre><code>curvature_addition = curvature_width_factor * |\u03ba|\n</code></pre> <p>where <code>\u03ba</code> is the local path curvature (computed from three consecutive points).</p>"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#3-velocity-based-adjustment","title":"3. Velocity-Based Adjustment","text":"<p>Add extra width at low speeds:</p> <pre><code>velocity_factor = max(0, (v_max - v) / v_max)\nvelocity_addition = velocity_width_factor * velocity_factor\n</code></pre> <p>where <code>v_max = 15.0 m/s</code>.</p>"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#4-final-width","title":"4. Final Width","text":"<pre><code>corridor_width = max(\n  base_width + curvature_addition + velocity_addition,\n  vehicle_width + min_clearance\n)\n</code></pre>"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#curvature-calculation","title":"Curvature Calculation","text":"<p>Local curvature is estimated using three consecutive points:</p> <pre><code>\u03b8_1 = atan2(y_i - y_{i-1}, x_i - x_{i-1})\n\u03b8_2 = atan2(y_{i+1} - y_i, x_{i+1} - x_i)\n\u0394\u03b8 = normalize(\u03b8_2 - \u03b8_1)\narc_length = ||p_i - p_{i-1}|| + ||p_{i+1} - p_i||\n\u03ba = \u0394\u03b8 / arc_length\n</code></pre>"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#acceleration-and-timing-recalculation","title":"Acceleration and Timing Recalculation","text":"<p>After MPT optimization modifies trajectory positions, accelerations and time stamps must be recomputed to maintain physical consistency with the new geometry.</p>"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#acceleration-calculation","title":"Acceleration Calculation","text":"<p>Acceleration between consecutive points is computed using the kinematic equation relating velocity change to distance:</p> <pre><code>v\u00b2 = v\u2080\u00b2 + 2as\n</code></pre> <p>Solving for acceleration:</p> <pre><code>a = (v\u00b2 - v\u2080\u00b2) / (2s)\n</code></pre> <p>where:</p> <ul> <li><code>v\u2080</code> = velocity at current point</li> <li><code>v</code> = velocity at next point</li> <li><code>s</code> = distance between points</li> <li><code>a</code> = average acceleration over segment</li> </ul>"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#time-interval-calculation","title":"Time Interval Calculation","text":"<p>Time interval between points uses kinematic motion equations. For constant acceleration motion:</p> <pre><code>s = v\u2080t + \u00bdat\u00b2\n</code></pre> <p>Rearranging using <code>v = v\u2080 + at</code>:</p> <pre><code>v\u00b2 = v\u2080\u00b2 + 2as  \u2192  v = \u221a(v\u2080\u00b2 + 2as)\nt = (v - v\u2080) / a\n</code></pre> <p>Special cases:</p> <ul> <li>Zero acceleration (<code>|a| &lt; 1e-6</code>): Uses constant velocity model <code>t = s / v</code></li> <li>Near-zero velocity (<code>|v| &lt; 1e-3 m/s</code>): Returns fixed <code>t = 0.1 s</code> to avoid division by zero</li> <li>Invalid discriminant (<code>v\u2080\u00b2 + 2as &lt; 0</code>): Falls back to constant velocity model</li> </ul>"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#time-from-start-propagation","title":"Time from Start Propagation","text":"<p>Starting from the first point (set to <code>t = 0</code>), time is propagated forward:</p> <pre><code>t_{i+1} = t_i + \u0394t_i\n</code></pre> <p>Time is split into ROS message format:</p> <pre><code>sec = floor(t)\nnanosec = (t - sec) * 1e9\n</code></pre>"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#acceleration-smoothing","title":"Acceleration Smoothing","text":"<p>Raw accelerations computed from kinematic equations may exhibit high-frequency noise due to discrete sampling. A backward-looking moving average filter is applied:</p> <pre><code>a_smoothed[i] = (1/N) * \u03a3_{j=i-N+1}^{i} a_original[j]\n</code></pre> <p>where <code>N</code> is the window size (configurable via <code>acceleration_moving_average_window</code>).</p> <p>Properties:</p> <ul> <li>Causal filter: Only uses past values (no future information)</li> <li>Larger window: Smoother acceleration but more lag</li> <li>Window = 1: No smoothing (preserves original values)</li> <li>Last point: Always set to zero acceleration</li> </ul>"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#parameters","title":"Parameters","text":""},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#corridor-bounds-parameters","title":"Corridor Bounds Parameters","text":"Parameter Type Default Range Description <code>corridor_width_m</code> double 3.5 [2.0, 10.0] Base perpendicular offset distance from trajectory centerline [m] <code>enable_adaptive_width</code> bool true - Enable simple width adjustments based on curvature and velocity <code>curvature_width_factor</code> double 0.5 [0.0, 2.0] Additional width per unit curvature [m/rad] <code>velocity_width_factor</code> double 0.3 [0.0, 2.0] Additional width scaling at low speeds [m] <code>min_clearance_m</code> double 0.5 [0.0, 2.0] Minimum clearance from vehicle edges [m]"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#mpt-behavior-parameters","title":"MPT Behavior Parameters","text":"Parameter Type Default Description <code>reset_previous_data_each_iteration</code> bool true Reset MPT warm start data each cycle. Use true for planners that output new trajectories each cycle (e.g., diffusion planner) <code>enable_debug_info</code> bool false Publish visualization markers for bounds and reference trajectory"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#output-parameters","title":"Output Parameters","text":"Parameter Type Default Range Description <code>output_delta_arc_length_m</code> double 1.0 [0.1, 5.0] Point spacing in optimized trajectory [m] <code>output_backward_traj_length_m</code> double 5.0 [0.0, 20.0] Trajectory extension backward from ego [m]"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#ego-matching-parameters","title":"Ego Matching Parameters","text":"Parameter Type Default Range Description <code>ego_nearest_dist_threshold_m</code> double 3.0 [0.5, 10.0] Distance threshold for ego-trajectory matching [m] <code>ego_nearest_yaw_threshold_deg</code> double 45.0 [10.0, 90.0] Yaw threshold for ego-trajectory matching [deg]"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#acceleration-smoothing-parameters","title":"Acceleration Smoothing Parameters","text":"Parameter Type Default Range Description <code>acceleration_moving_average_window</code> int 5 [1, 20] Moving average window size. Larger values = smoother but more lag"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#comparison-with-other-plugins","title":"Comparison with Other Plugins","text":""},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#mpt-vs-qp-smoother","title":"MPT vs QP Smoother","text":"<p>QP Smoother - Geometric smoothing in Cartesian space:</p> <ul> <li>Minimizes path curvature via second-order finite differences</li> <li>Balances smoothness vs fidelity to original path</li> <li>No vehicle kinematic model</li> <li>Fast (~1-2ms), no bounds required</li> </ul> <p>MPT Optimizer - Model predictive with bicycle kinematics:</p> <ul> <li>Optimizes in Frenet frame (lateral error, yaw error, steering)</li> <li>Uses bicycle kinematics model with steering dynamics</li> <li>Minimizes steering angle, rate, and acceleration explicitly</li> <li>Slower (~5-20ms), requires corridor bounds</li> </ul> <p>Use QP when: Simple geometric smoothing needed, tight computational budget. Use MPT when: Kinematically-aware refinement needed, working with learning-based planners.</p>"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#mpt-vs-kinematic-feasibility-enforcer","title":"MPT vs Kinematic Feasibility Enforcer","text":"<p>Kinematic Feasibility Enforcer - Hard constraint enforcement:</p> <ul> <li>Forward propagation with yaw rate clamping</li> <li>Hard constraints (must satisfy, no violations)</li> <li>Greedy point-by-point approach</li> <li>Fast, deterministic</li> </ul> <p>MPT Optimizer - Optimization with soft constraints:</p> <ul> <li>QP optimization over prediction horizon</li> <li>Soft constraints with slack variables (can violate if beneficial)</li> <li>Globally optimizes steering smoothness</li> <li>Finds best trajectory balancing multiple objectives</li> </ul> <p>Use Enforcer when: Need minimum feasibility guarantee, computational budget critical. Use MPT when: Need smooth, optimized trajectories considering full vehicle dynamics.</p>"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#interaction-with-other-plugins","title":"Interaction with Other Plugins","text":""},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#recommended-plugin-order","title":"Recommended Plugin Order","text":"<p>The MPT optimizer should run after all geometric smoothing but before velocity optimization:</p> <pre><code>plugin_names:\n  - TrajectoryPointFixer\n  - TrajectoryKinematicFeasibilityEnforcer\n  - TrajectoryQPSmoother\n  - TrajectoryKinematicFeasibilityEnforcer # Second pass after QP smoothing\n  - TrajectoryEBSmootherOptimizer\n  - TrajectorySplineSmoother\n  - TrajectoryMPTOptimizer # Refine geometry with bounds\n  - TrajectoryVelocityOptimizer # Optimize velocity profile\n  - TrajectoryExtender\n</code></pre>"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#compatibility-notes","title":"Compatibility Notes","text":"<ul> <li>Requires non-empty bounds: MPT optimization fails if bounds are empty</li> <li>Modifies positions: Subsequent plugins must handle changed geometry</li> <li>Preserves velocities: Velocity values from input trajectory are maintained</li> <li>Recalculates accelerations: Overwrites acceleration values from previous plugins</li> <li>Recalculates time stamps: Overwrites <code>time_from_start</code> based on kinematic consistency</li> </ul>"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#collision-avoidance-capability","title":"Collision Avoidance Capability","text":"<p>Note: The underlying MPT solver from <code>autoware_path_optimizer</code> supports collision avoidance optimization when provided with drivable area bounds (where obstacles are removed from the drivable area, creating bounds that inherently avoid collisions).</p> <p>However, this plugin implementation does not utilize collision avoidance because:</p> <ul> <li>Bounds are generated as simple perpendicular offsets from the trajectory centerline</li> <li>No obstacle or predicted object data is integrated</li> <li>The drivable area is not used</li> </ul> <p>The collision-related parameters in the configuration (<code>soft_collision_free_weight</code>, <code>collision_free_constraints</code>, <code>avoidance</code> weights, vehicle circle approximations) are part of the underlying MPT solver's capabilities but remain unused in this geometric-only implementation.</p> <p>Future work: Collision avoidance could be added by integrating obstacle data and generating bounds from actual drivable area instead of geometric offsets.</p>"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#limitations","title":"Limitations","text":""},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#1-bound-requirement","title":"1. Bound Requirement","text":"<p>The MPT optimizer requires valid corridor bounds to operate. Empty bounds cause optimization to fail and the original trajectory is preserved.</p>"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#2-velocity-profile-assumptions","title":"2. Velocity Profile Assumptions","text":"<ul> <li>Assumes velocity profile from input trajectory is appropriate</li> <li>Does not modify velocities during geometric optimization</li> <li>Acceleration recalculation assumes constant acceleration between points</li> </ul>"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#3-computational-cost","title":"3. Computational Cost","text":"<p>MPT optimization using quadratic programming is more expensive than local smoothing methods:</p> <ul> <li>Typical runtime: 5-20 ms for 80-100 point trajectory</li> <li>Should be disabled (<code>use_mpt_optimizer: false</code>) if computational budget is tight</li> </ul>"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#4-acceleration-smoothing-lag","title":"4. Acceleration Smoothing Lag","text":"<p>Moving average filter introduces lag proportional to window size:</p> <ul> <li>Larger window \u2192 smoother acceleration but delayed response</li> <li>May affect real-time control performance if window is too large</li> </ul>"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#5-geometric-vs-dynamic-optimization","title":"5. Geometric vs Dynamic Optimization","text":"<p>The MPT solver optimizes geometric criteria (smoothness, bounds compliance) but does not directly optimize for:</p> <ul> <li>Jerk minimization</li> <li>Passenger comfort metrics</li> <li>Longitudinal acceleration limits</li> </ul> <p>These should be handled by dedicated plugins (e.g., <code>TrajectoryVelocityOptimizer</code>).</p>"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#6-reverse-driving","title":"6. Reverse driving","text":"<p>Assumes forward motion. May need special handling for reverse trajectories.</p>"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#when-to-use-mpt-optimizer","title":"When to Use MPT Optimizer","text":""},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#recommended-use-cases","title":"Recommended Use Cases","text":"<ul> <li>Trajectories from learning-based planners requiring geometric refinement</li> <li>When you want to avoid dependency on lanelet map bounds</li> <li>Situations where QP smoother alone produces insufficient smoothing</li> </ul>"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#when-to-disable","title":"When to Disable","text":"<ul> <li>Computational resources are limited (MPT is expensive)</li> <li>Input trajectories are already smooth and kinematically feasible</li> <li>Real-time performance is critical (&lt; 10 ms budget)</li> </ul>"},{"location":"planning/autoware_trajectory_optimizer/docs/mpt_optimizer/#debug-visualization","title":"Debug Visualization","text":"<p>When <code>enable_debug_info: true</code>, the plugin publishes visualization markers to <code>~/debug/mpt_bounds_markers</code>:</p> <ul> <li>Green lines: Left bound (perpendicular offset)</li> <li>Red lines: Right bound (perpendicular offset)</li> <li>Blue line: Reference trajectory</li> </ul> <p>These markers help visualize the generated bounds and tune width parameters.</p>"},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/","title":"QP-Based Trajectory Smoother","text":""},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/#qp-based-trajectory-smoother","title":"QP-Based Trajectory Smoother","text":""},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/#overview","title":"Overview","text":"<p>The QP (Quadratic Programming) smoother is a plugin in the trajectory optimizer that provides optimization-based trajectory smoothing using the OSQP solver. It smooths the geometric path (x,y positions) while maintaining path fidelity to the original trajectory from the planner.</p>"},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/#mathematical-formulation","title":"Mathematical Formulation","text":""},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/#decision-variables","title":"Decision Variables","text":"<p>The optimization operates on 2D path positions:</p> <pre><code>x = [x_0, y_0, x_1, y_1, ..., x_{N-1}, y_{N-1}]\n</code></pre> <p>where N is the number of trajectory points.</p>"},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/#objective-function","title":"Objective Function","text":"<p>The optimizer minimizes a weighted combination of two terms:</p> <pre><code>minimize: J = J_smoothness + J_fidelity\n</code></pre>"},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/#smoothness-term","title":"Smoothness Term","text":"<p>Minimizes path curvature using second-order finite differences:</p> <pre><code>J_smoothness = w_smoothness * \u03a3 ||(p_{i+1} - 2*p_i + p_{i-1})||\u00b2\n</code></pre> <p>This term is scaled by <code>1/dt\u00b2</code> for velocity-aware smoothing.</p>"},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/#fidelity-term","title":"Fidelity Term","text":"<p>Keeps the smoothed path close to the original trajectory:</p> <pre><code>J_fidelity = \u03a3 w_i * ||p_i - p_i^orig||\u00b2\n</code></pre> <p>where <code>w_i</code> is the per-point fidelity weight (see Velocity-Based Fidelity section).</p>"},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/#constraints","title":"Constraints","text":"<ul> <li>Start points: Configurable number of points from trajectory start are fixed (controlled by <code>num_constrained_points_start</code> parameter, default: 3)</li> <li>End points: Configurable number of points from trajectory end are fixed (controlled by <code>num_constrained_points_end</code> parameter, default: 0)</li> </ul>"},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/#velocity-based-fidelity-weighting","title":"Velocity-Based Fidelity Weighting","text":""},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/#motivation","title":"Motivation","text":"<p>At low speeds, trajectory noise from the planner can cause jittery behavior. The velocity-based fidelity feature addresses this by:</p> <ul> <li>Low speeds: Lower fidelity weight \u2192 more aggressive smoothing.</li> <li>High speeds: Higher fidelity weight \u2192 preserve planner intent.</li> </ul>"},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/#sigmoid-weight-function","title":"Sigmoid Weight Function","text":"<p>Per-point fidelity weights are computed using a sigmoid function:</p> <pre><code>w_i = w_min + (w_max - w_min) * \u03c3(k * (|v_i| - v_th))\n\nwhere \u03c3(x) = 1 / (1 + exp(-x))\n</code></pre> <p>Parameters:</p> <ul> <li><code>v_th</code>: Velocity threshold at sigmoid midpoint (default: 0.3 m/s)</li> <li><code>k</code>: Sigmoid sharpness (default: 50.0, higher = sharper transition)</li> <li><code>w_min</code>: Minimum weight at very low speeds (default: 0.01)</li> <li><code>w_max</code>: Maximum weight at high speeds (default: 1.0)</li> </ul> <p>Key Properties:</p> <ul> <li>Uses absolute velocity magnitude to handle reverse driving</li> <li>Smooth continuous transition (no discontinuities)</li> <li>Velocity threshold defines where weight is halfway between min/max</li> <li>Sharpness controls how rapidly weight changes near threshold</li> </ul>"},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/#weight-behavior","title":"Weight Behavior","text":"<pre><code>weight\n1.0 |           _______  (high speed, preserve path)\n    |          /\n    |         /\n0.01|_______/            (low speed, allow smoothing)\n      0    v_th      v_max\n           (0.3 m/s)\n             speed\n</code></pre>"},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/#post-processing","title":"Post-Processing","text":"<p>After QP optimization solves for smoothed positions, the following are recalculated:</p> <ol> <li>Orientation: Computed from path tangent between consecutive points</li> <li>Velocity: Derived from position differences and time step</li> <li>Velocity smoothing: 3-point moving average to reduce numerical noise</li> <li>Acceleration: Derived from velocity differences</li> </ol>"},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/#parameters","title":"Parameters","text":""},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/#core-optimization-weights","title":"Core Optimization Weights","text":"<ul> <li><code>weight_smoothness</code> (default: 10.0)<ul> <li>Controls smoothness penalty</li> <li>Higher values \u2192 smoother but more deviation from original path</li> </ul> </li> </ul> <ul> <li><code>weight_fidelity</code> (default: 1.0)<ul> <li>Baseline fidelity weight (used when velocity-based feature disabled)</li> <li>Higher values \u2192 closer to original path</li> </ul> </li> </ul>"},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/#velocity-based-fidelity","title":"Velocity-Based Fidelity","text":"<ul> <li><code>use_velocity_based_fidelity</code> (default: true)<ul> <li>Master switch for velocity-dependent weighting</li> <li>Set to <code>true</code> to enable feature</li> </ul> </li> </ul> <ul> <li><code>velocity_threshold_mps</code> (default: 0.3)<ul> <li>Speed at which sigmoid transitions (midpoint)</li> <li>Lower values \u2192 earlier transition to high fidelity</li> </ul> </li> </ul> <ul> <li><code>sigmoid_sharpness</code> (default: 50.0)<ul> <li>Controls transition steepness</li> <li>Range: [1, 100], higher = sharper step-like transition</li> </ul> </li> </ul> <ul> <li><code>min_fidelity_weight</code> (default: 0.01)<ul> <li>Weight applied at very low speeds</li> <li>Lower values \u2192 more aggressive smoothing when stopped/slow</li> </ul> </li> </ul> <ul> <li><code>max_fidelity_weight</code> (default: 1.0)<ul> <li>Weight applied at high speeds</li> <li>Higher values \u2192 stronger preservation of planner path</li> </ul> </li> </ul>"},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/#point-constraints","title":"Point Constraints","text":"<ul> <li><code>num_constrained_points_start</code> (default: 3)<ul> <li>Number of points from trajectory start to fix as hard constraints</li> <li>Recommended: 3 to preserve initial acceleration</li> <li>Set to 0 to allow all points to be optimized (may cause initial state discontinuities)</li> </ul> </li> </ul> <ul> <li><code>num_constrained_points_end</code> (default: 3)<ul> <li>Number of points from trajectory end to fix as hard constraints</li> <li>Set to 0 for maximum smoothness at trajectory end</li> <li>Increase if you need to preserve goal state and acceleration</li> </ul> </li> </ul>"},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/#solver-settings","title":"Solver Settings","text":"<ul> <li><code>time_step_s</code> (default: 0.1)<ul> <li>Time discretization for velocity/acceleration calculations</li> </ul> </li> </ul> <ul> <li><code>osqp_eps_abs</code> / <code>osqp_eps_rel</code> (default: 1e-4)<ul> <li>OSQP solver convergence tolerances</li> </ul> </li> </ul> <ul> <li><code>osqp_max_iter</code> (default: 100)<ul> <li>Maximum solver iterations</li> </ul> </li> </ul> <ul> <li><code>osqp_verbose</code> (default: false)<ul> <li>Enable OSQP debug output</li> </ul> </li> </ul>"},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/#orientation-correction","title":"Orientation Correction","text":"<ul> <li><code>fix_orientation</code> (default: true)<ul> <li>Apply orientation correction post-solve</li> </ul> </li> </ul> <ul> <li><code>orientation_correction_threshold_deg</code> (default: 5.0)<ul> <li>Yaw threshold for correcting QP output orientation</li> </ul> </li> </ul>"},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/#usage-examples","title":"Usage Examples","text":""},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/#example-1-enable-velocity-based-fidelity","title":"Example 1: Enable Velocity-Based Fidelity","text":"<p>To enable velocity-dependent smoothing for stop-and-go scenarios:</p> <pre><code>ros2 param set /planning/trajectory_optimizer trajectory_qp_smoother.use_velocity_based_fidelity true\n</code></pre>"},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/#example-2-tune-velocity-threshold","title":"Example 2: Tune Velocity Threshold","text":"<p>To make the transition occur at higher speeds (0.5 m/s instead of 0.2 m/s):</p> <pre><code>ros2 param set /planning/trajectory_optimizer trajectory_qp_smoother.velocity_threshold_mps 0.5\n</code></pre>"},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/#example-3-more-aggressive-low-speed-smoothing","title":"Example 3: More Aggressive Low-Speed Smoothing","text":"<p>To allow even more smoothing at low speeds:</p> <pre><code>ros2 param set /planning/trajectory_optimizer trajectory_qp_smoother.min_fidelity_weight 0.005\n</code></pre>"},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/#example-4-sharper-transition","title":"Example 4: Sharper Transition","text":"<p>To make the velocity-based transition more step-like:</p> <pre><code>ros2 param set /planning/trajectory_optimizer trajectory_qp_smoother.sigmoid_sharpness 80.0\n</code></pre>"},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/#example-5-constrain-only-first-point","title":"Example 5: Constrain Only First Point","text":"<p>For maximum smoothness (only preserve initial position):</p> <pre><code>ros2 param set /planning/trajectory_optimizer trajectory_qp_smoother.num_constrained_points_start 1\n</code></pre>"},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/#example-6-constrain-last-2-points","title":"Example 6: Constrain Last 2 Points","text":"<p>To preserve the goal state exactly:</p> <pre><code>ros2 param set /planning/trajectory_optimizer trajectory_qp_smoother.num_constrained_points_end 2\n</code></pre>"},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/#example-7-preserve-larger-initial-region","title":"Example 7: Preserve Larger Initial Region","text":"<p>To preserve the first 5 points (useful for very noisy planners):</p> <pre><code>ros2 param set /planning/trajectory_optimizer trajectory_qp_smoother.num_constrained_points_start 5\n</code></pre>"},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/#debugging","title":"Debugging","text":""},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/#view-debug-logs","title":"View Debug Logs","text":"<p>Enable debug logging to see weight statistics and solver diagnostics:</p> <pre><code>ros2 run autoware_trajectory_optimizer autoware_trajectory_optimizer_node --ros-args --log-level trajectory_optimizer:=debug\n</code></pre>"},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/#check-parameter-values","title":"Check Parameter Values","text":"<pre><code># Check if velocity-based fidelity is enabled\nros2 param get /planning/trajectory_optimizer trajectory_qp_smoother.use_velocity_based_fidelity\n\n# Check weight configuration\nros2 param get /planning/trajectory_optimizer trajectory_qp_smoother.min_fidelity_weight\nros2 param get /planning/trajectory_optimizer trajectory_qp_smoother.max_fidelity_weight\n\n# Check constraint configuration\nros2 param get /planning/trajectory_optimizer trajectory_qp_smoother.num_constrained_points_start\nros2 param get /planning/trajectory_optimizer trajectory_qp_smoother.num_constrained_points_end\n</code></pre>"},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/#monitor-performance","title":"Monitor Performance","text":"<pre><code># View processing time metrics\nros2 topic echo /planning/trajectory_optimizer/debug/processing_time_detail_ms\n</code></pre>"},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/#limitations","title":"Limitations","text":"<ol> <li>Path deviation: Heavy smoothing can cause deviation from original path</li> <li>Velocity discontinuities: Velocity is recalculated, may differ from planner intent</li> <li>Computational cost: QP solve adds overhead compared to simpler smoothers</li> <li>Parameter sensitivity: Requires tuning for specific vehicle dynamics</li> </ol>"},{"location":"planning/autoware_trajectory_optimizer/docs/qp_smoother/#references","title":"References","text":"<ul> <li>OSQP Solver: https://osqp.org/</li> <li>Autoware OSQP Interface: <code>autoware_osqp_interface</code> package</li> <li>Parent Package: <code>autoware_trajectory_optimizer</code></li> </ul>"},{"location":"planning/autoware_trajectory_ranker/","title":"autoware_trajectory_ranker","text":""},{"location":"planning/autoware_trajectory_ranker/#autoware_trajectory_ranker","title":"autoware_trajectory_ranker","text":""},{"location":"planning/autoware_trajectory_ranker/#purpose","title":"Purpose","text":"<p>The <code>autoware_trajectory_ranker</code> package provides a flexible and extensible framework for evaluating and ranking multiple trajectory candidates in autonomous driving scenarios. It transforms a set of viable candidate paths into an ordered list by attaching a scalar score to every trajectory based on safety, comfort, and efficiency criteria.</p> <p>This decoupling of upstream path-generation logic from downstream motion-control logic enables:</p> <ul> <li>Diverse trajectory generators (rule-based, ML-based, optimization-based) to coexist</li> <li>Controllers to simply follow the highest-ranked path</li> <li>Improved overall driving performance through multi-criteria optimization</li> <li>Architectural flexibility through a plugin-based metric system</li> </ul>"},{"location":"planning/autoware_trajectory_ranker/#features","title":"Features","text":"<ul> <li>Plugin-based Architecture: Dynamically load and configure evaluation metrics via pluginlib</li> <li>Multi-criteria Evaluation: Score trajectories based on multiple metrics simultaneously</li> <li>Time-based Weighting: Apply temporal decay to metrics for better near-term decision making</li> <li>Extensible Framework: Easy to add new metrics through the plugin system</li> <li>Real-time Processing: Efficient evaluation of multiple trajectories per planning cycle</li> </ul>"},{"location":"planning/autoware_trajectory_ranker/#algorithm-overview","title":"Algorithm Overview","text":"<p>At every planning period the node latches the most recent odometry and perception context, then processes trajectories through five distinct stages:</p> <ol> <li>Ingestion: Receives candidate trajectories from upstream generators</li> <li>Resampling: Converts each path into a common frame and resamples to fixed resolution</li> <li>Evaluation: Applies configured metric plugins to compute cost vectors</li> <li>Aggregation: Combines metrics using weights and temporal decay</li> <li>Publication: Outputs scored trajectories and debug information</li> </ol>"},{"location":"planning/autoware_trajectory_ranker/#trajectory-resampling","title":"Trajectory Resampling","text":"<p>Every incoming trajectory is re-interpolated relative to the ego pose so that:</p> <ul> <li>First point aligns with current vehicle position</li> <li>Path comprises exactly <code>sample_num</code> points</li> <li>Points are spaced at fixed temporal intervals</li> <li>All metrics operate on consistent trajectory representations</li> </ul> <p>This ensures that metric plugins can assume identical temporal spacing even when generators produce paths of different lengths or densities.</p>"},{"location":"planning/autoware_trajectory_ranker/#metric-evaluation","title":"Metric Evaluation","text":"<p>For each resampled path, the evaluator executes a plugin chain specified in the configuration. Each metric plugin:</p> <ul> <li>Computes a vector of values (one per trajectory point)</li> <li>Returns costs reflecting safety, comfort, or efficiency considerations</li> <li>Can be classified as either maximization or deviation metrics</li> </ul>"},{"location":"planning/autoware_trajectory_ranker/#score-aggregation","title":"Score Aggregation","text":"<p>The aggregation stage:</p> <ol> <li>Applies per-metric weights from configuration</li> <li>Multiplies by temporal decay weights (exponential decay for near-term bias)</li> <li>Normalizes across trajectory candidates</li> <li>Sums to produce a single scalar score per trajectory</li> </ol>"},{"location":"planning/autoware_trajectory_ranker/#available-metrics","title":"Available Metrics","text":"<p>The package includes the following built-in metric plugins:</p> Metric Type Description Class Name TravelDistance Maximization Measures progress along trajectory <code>autoware::trajectory_ranker::metrics::TravelDistance</code> LateralAcceleration Deviation Evaluates lateral acceleration for comfort <code>autoware::trajectory_ranker::metrics::LateralAcceleration</code> LongitudinalJerk Deviation Measures longitudinal jerk for smoothness <code>autoware::trajectory_ranker::metrics::LongitudinalJerk</code> TimeToCollision Maximization Calculates time to collision with objects <code>autoware::trajectory_ranker::metrics::TimeToCollision</code> LateralDeviation Deviation Measures deviation from preferred lanes <code>autoware::trajectory_ranker::metrics::LateralDeviation</code> SteeringConsistency Deviation Evaluates steering command consistency <code>autoware::trajectory_ranker::metrics::SteeringConsistency</code>"},{"location":"planning/autoware_trajectory_ranker/#metric-types","title":"Metric Types","text":"<ul> <li>Maximization: Higher values are better (e.g., distance traveled, time to collision)</li> <li>Deviation: Lower values are better (e.g., lateral acceleration, jerk)</li> </ul>"},{"location":"planning/autoware_trajectory_ranker/#input-output","title":"Input / Output","text":""},{"location":"planning/autoware_trajectory_ranker/#input-topics","title":"Input Topics","text":"Topic Type Description <code>~/input/trajectories</code> <code>autoware_internal_planning_msgs/msg/CandidateTrajectories</code> Candidate trajectories to evaluate <code>~/input/objects</code> <code>autoware_perception_msgs/msg/PredictedObjects</code> Predicted objects for collision checking <code>~/input/odometry</code> <code>nav_msgs/msg/Odometry</code> Current vehicle state <code>~/input/map</code> <code>autoware_map_msgs/msg/LaneletMapBin</code> Lanelet2 HD map <code>~/input/route</code> <code>autoware_planning_msgs/msg/LaneletRoute</code> Current route"},{"location":"planning/autoware_trajectory_ranker/#output-topics","title":"Output Topics","text":"Topic Type Description <code>~/output/trajectories</code> <code>autoware_internal_planning_msgs/msg/ScoredCandidateTrajectories</code> Scored candidate trajectories <code>~/debug/processing_time_detail</code> <code>autoware_utils_debug/msg/ProcessingTimeDetail</code> Processing time statistics"},{"location":"planning/autoware_trajectory_ranker/#parameters","title":"Parameters","text":""},{"location":"planning/autoware_trajectory_ranker/#core-parameters","title":"Core Parameters","text":"<p>The package uses the Generate Parameter Library for configuration. Parameters are defined in <code>param/trajectory_ranker_parameters.yaml</code>:</p>"},{"location":"planning/autoware_trajectory_ranker/#parameter-structure","title":"Parameter Structure","text":"<ul> <li>evaluation.sample_num: Number of points in resampled trajectories (default: 16)</li> <li>evaluation.resolution: Time interval between trajectory points in seconds (default: 0.5)</li> <li>evaluation.metrics.name: Array of metric plugin class names (fully qualified)</li> <li>evaluation.metrics.maximum: Maximum expected values for each metric (used for normalization)</li> <li>evaluation.score_weight: Weight for each metric in final score calculation</li> <li>evaluation.time_decay_weight.s0-s5: Temporal weights for metrics (s0=first metric, s1=second, etc.)</li> </ul>"},{"location":"planning/autoware_trajectory_ranker/#usage","title":"Usage","text":""},{"location":"planning/autoware_trajectory_ranker/#basic-launch","title":"Basic Launch","text":"<pre><code>ros2 launch autoware_trajectory_ranker trajectory_ranker.launch.xml\n</code></pre>"},{"location":"planning/autoware_trajectory_ranker/#launch-with-custom-parameters","title":"Launch with Custom Parameters","text":"<pre><code>ros2 launch autoware_trajectory_ranker trajectory_ranker.launch.xml \\\n    trajectory_ranker_param_file:=/path/to/your/config.yaml\n</code></pre>"},{"location":"planning/autoware_trajectory_safety_filter/","title":"Trajectory safety filter","text":""},{"location":"planning/autoware_trajectory_safety_filter/#trajectory-safety-filter","title":"Trajectory safety filter","text":""},{"location":"planning/autoware_trajectory_safety_filter/#purposerole","title":"Purpose/Role","text":"<p>This node performs a safety check on each candidate trajectory before it enters the final ranking stage.\u00a0It drops paths that are physically impossible for the ego vehicle or that create an obvious driving risk.</p>"},{"location":"planning/autoware_trajectory_safety_filter/#algorithm-overview","title":"Algorithm Overview","text":"<p>The node operates in three broad steps: collect the latest environment inputs, screen trajectories through a set of feasibility checks, then republish whichever paths survive.</p> <p>Checks applied to each trajectory:</p> <ul> <li>Data validity: removes trajectories that contain NaNs, non\u2011finite numbers, inconsistent timestamps, or are too short.</li> <li>Lane adherence: removes trajectories that will exit all lanelets within the configured look\u2011ahead time.</li> <li>Collision risk: removes trajectories whose estimated time\u2011to\u2011collision with any predicted object falls below threshold in the look\u2011ahead time.</li> </ul> <p>After these checks, the remaining trajectories, along with their original <code>generator_info</code>, are published.</p>"},{"location":"planning/autoware_trajectory_safety_filter/#interface","title":"Interface","text":""},{"location":"planning/autoware_trajectory_safety_filter/#topics","title":"Topics","text":"Direction Topic name Message type Description Subscribe <code>~/input/trajectories</code> <code>autoware_internal_planning_msgs/CandidateTrajectories</code> Candidate trajectories Subscribe <code>~/input/lanelet2_map</code> <code>autoware_map_msgs/msg/LaneletMapBin</code> HD map Subscribe <code>~/input/odometry</code> <code>nav_msgs/msg/Odometry</code> Current ego pose Subscribe <code>~/input/objects</code> <code>autoware_perception_msgs/msg/PredictedObjects</code> Obstacles for collision checking Publish <code>~/output/trajectories</code> <code>autoware_internal_planning_msgs/CandidateTrajectories</code> Trajectories that pass all feasibility checks"},{"location":"planning/autoware_trajectory_safety_filter/#parameters","title":"Parameters","text":"Parameter name Type Default Description <code>filter_names</code> string array [] List of safety filter plugins to use (e.g., OutOfLaneFilter, CollisionFilter) <code>out_of_lane.time</code> double 3.0 Look-ahead time [s] during which the trajectory must stay inside a lane <code>out_of_lane.min_value</code> double 0.0 Minimum distance [m] from lane boundary <code>collision.time</code> double 3.0 Look-ahead time [s] for collision search <code>collision.min_value</code> double 2.0 Minimum acceptable time to collision [s]"},{"location":"planning/autoware_trajectory_safety_filter/#future-work","title":"Future Work","text":""},{"location":"planning/autoware_trajectory_safety_filter/#performance-optimization","title":"Performance Optimization","text":"<p>The current implementation can be further optimized for computational efficiency:</p> <ul> <li>Caching Strategy: Implement smarter caching mechanisms for lanelet queries and boundary checks to avoid redundant computations across similar trajectories</li> <li>Adaptive Resolution: Dynamically adjust the checking resolution based on vehicle speed and trajectory curvature to balance accuracy and performance</li> </ul>"},{"location":"planning/autoware_trajectory_traffic_rule_filter/","title":"Autoware Trajectory Traffic Rule Filter","text":""},{"location":"planning/autoware_trajectory_traffic_rule_filter/#autoware-trajectory-traffic-rule-filter","title":"Autoware Trajectory Traffic Rule Filter","text":""},{"location":"planning/autoware_trajectory_traffic_rule_filter/#purpose","title":"Purpose","text":"<p>The <code>autoware_trajectory_traffic_rule_filter</code> package provides a plugin-based filtering system for candidate trajectories based on traffic rules. It evaluates trajectories against various traffic regulations and safety constraints to ensure compliance with traffic laws.</p>"},{"location":"planning/autoware_trajectory_traffic_rule_filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"planning/autoware_trajectory_traffic_rule_filter/#architecture","title":"Architecture","text":"<p>The package uses a plugin architecture that allows for flexible and extensible traffic rule checking:</p> <ol> <li>Main Node: <code>TrajectoryTrafficRuleFilter</code> - Manages plugins and coordinates filtering</li> <li>Plugin Interface: <code>TrafficRuleFilterInterface</code> - Base class for all filter plugins</li> <li>Filter Plugins: Individual filters that implement specific traffic rule checks</li> </ol>"},{"location":"planning/autoware_trajectory_traffic_rule_filter/#filter-plugins","title":"Filter Plugins","text":""},{"location":"planning/autoware_trajectory_traffic_rule_filter/#trafficlightfilter","title":"TrafficLightFilter","text":"<ul> <li>Validates trajectory compliance with traffic signals</li> <li>Monitors traffic light states from perception system</li> <li>Checks if trajectory would pass through red lights</li> <li>Uses <code>isTrafficSignalStop()</code> from autoware traffic light utils</li> <li>Allows trajectories only when traffic lights permit passage</li> </ul>"},{"location":"planning/autoware_trajectory_traffic_rule_filter/#interface","title":"Interface","text":""},{"location":"planning/autoware_trajectory_traffic_rule_filter/#topics","title":"Topics","text":"Direction Topic Name Type Description Input <code>~/input/candidate_trajectories</code> <code>autoware_internal_planning_msgs::msg::CandidateTrajectories</code> Candidate trajectories to be filtered Input <code>~/input/lanelet2_map</code> <code>autoware_map_msgs::msg::LaneletMapBin</code> Lanelet2 map containing traffic rule info Input <code>~/input/traffic_signals</code> <code>autoware_perception_msgs::msg::TrafficLightGroupArray</code> Current traffic light states Output <code>~/output/candidate_trajectories</code> <code>autoware_internal_planning_msgs::msg::CandidateTrajectories</code> Filtered trajectories that comply with traffic rules"},{"location":"planning/autoware_trajectory_traffic_rule_filter/#parameters","title":"Parameters","text":"Name Type Description Default Value <code>filter_names</code> string array List of filter plugins to load See config file"},{"location":"planning/autoware_trajectory_traffic_rule_filter/#plugin-configuration","title":"Plugin Configuration","text":"<p>The active filters are specified in <code>config/trajectory_traffic_rule_filter.param.yaml</code>:</p> <pre><code>/**:\n  ros__parameters:\n    filter_names:\n      - \"autoware::trajectory_traffic_rule_filter::plugin::TrafficLightFilter\"\n</code></pre>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_avoidance_by_lane_change_module/","title":"Avoidance by lane change design","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_avoidance_by_lane_change_module/#avoidance-by-lane-change-design","title":"Avoidance by lane change design","text":"<p>This is a sub-module to avoid obstacles by lane change maneuver.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_avoidance_by_lane_change_module/#purpose-role","title":"Purpose / Role","text":"<p>This module is designed as one of the obstacle avoidance features and generates a lane change path if the following conditions are satisfied.</p> <ul> <li>Exist lane changeable lanelet.</li> <li>Exist avoidance target objects on ego driving lane.</li> </ul> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_avoidance_by_lane_change_module/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>Basically, this module is implemented by reusing the avoidance target filtering logic of the existing Static Object Avoidance Module and the path generation logic of the Normal Lane Change Module. On the other hand, the conditions under which the module is activated differ from those of a normal avoidance module.</p> <p>Check that the following conditions are satisfied after the filtering process for the avoidance target.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_avoidance_by_lane_change_module/#number-of-the-avoidance-target-objects","title":"Number of the avoidance target objects","text":"<p>This module is launched when the number of avoidance target objects on EGO DRIVING LANE is greater than <code>execute_object_num</code>. If there are no avoidance targets in the ego driving lane or their number is less than the parameter, the obstacle is avoided by normal avoidance behavior (if the normal avoidance module is registered).</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_avoidance_by_lane_change_module/#lane-change-end-point-condition","title":"Lane change end point condition","text":"<p>Unlike the normal avoidance module, which specifies the shift line end point, this module does not specify its end point when generating a lane change path. On the other hand, setting <code>execute_only_when_lane_change_finish_before_object</code> to <code>true</code> will activate this module only if the lane change can be completed before the avoidance target object.</p> <p>Although setting the parameter to <code>false</code> would increase the scene of avoidance by lane change, it is assumed that sufficient lateral margin may not be ensured in some cases because the vehicle passes by the side of obstacles during the lane change.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_avoidance_by_lane_change_module/#parameters","title":"Parameters","text":"Name Unit Type Description Default value execute_object_num [-] int Number of avoidance target objects on ego driving lane is greater than this value, this module will be launched. 1 execute_object_longitudinal_margin [m] double [maybe unused] Only when distance between the ego and avoidance target object is longer than this value, this module will be launched. 0.0 execute_only_when_lane_change_finish_before_object [-] bool If this flag set <code>true</code>, this module will be launched only when the lane change end point is NOT behind the avoidance target object. true"},{"location":"planning/behavior_path_planner/autoware_behavior_path_bidirectional_traffic_module/","title":"Bidirectional Traffic","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_bidirectional_traffic_module/#bidirectional-traffic","title":"Bidirectional Traffic","text":"<p>This module enables trajectory planning for vehicles on \"single-lane bidirectional traffic\" roads in autonomous driving systems. Here, \"single-lane bidirectional traffic\" refers to roads without a centerline, such as those commonly found in residential areas, as shown in the image below\uff0e</p> <p></p> <p>In the following descriptions, the term bidirectional traffic refers specifically to such single-lane bidirectional roads as shown in the image above.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_bidirectional_traffic_module/#purposerole","title":"Purpose/Role","text":"<p>The main roles of this module are as follows:</p> <ul> <li> <p>Generating a driving trajectory along the left (or right) side of the road center</p> <p>This module provides a function to generate a trajectory along the left (or right) side of the road center on bidirectional traffic lanes. In some countries, traffic laws require vehicles to drive on the left or right side of the road center on bidirectional roads. For example, in Japan, Article 18, Paragraph 1 of the Road Traffic Act requires that vehicles must keep to the left side of the road center. Because of these legal requirements, this module provides the function to generate a trajectory that follows the left (or right) side of the road center on bidirectional roads.</p> </li> </ul> <ul> <li> <p>Giving way to oncoming vehicles on bidirectional traffic roads</p> <p>This module provides the function for the ego vehicle to stop on the left side of the road and give way to oncoming vehicles in all situations where vehicles must pass each other on bidirectional roads. Bidirectional traffic roads are often narrow, and it is dangerous for both the ego vehicle and the oncoming vehicle to pass while moving simultaneously. To avoid such dangerous situations, this module enables the ego vehicle to stop on the left side of the road until the oncoming vehicle has completely passed.</p> </li> </ul>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_bidirectional_traffic_module/#the-representation-of-bidirectional-traffic-on-a-lanelet-map","title":"The representation of bidirectional traffic on a Lanelet map","text":"<p>Bidirectional lanelets are represented on a Lanelet map as two lanes with opposite directions that share the same LineString as their boundary. There is no need to assign any special tags to the lanelets.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_bidirectional_traffic_module/#limitations","title":"Limitations","text":"<p>This module does not support the following situations or functionalities.</p> <ul> <li> <p>Handling situations where passing is not possible</p> <p>Depending on factors such as lane width, the size of the ego vehicle and the oncoming vehicle, and the surrounding environment, simple behaviors like those shown in the video above may not be sufficient for passing. In such cases, special behaviours such as reversing or clearing an intersection may be required. However, this module does not provide such functionality.</p> </li> </ul> <ul> <li> <p>When the oncoming vehicle is giving way or moving slowly</p> <p>In cases where the oncoming vehicle is trying to give way or is traveling at a low speed, it may be more appropriate for the ego vehicle to continue driving rather than stopping to give way. However, this module does not make any decision about whether to give way to the oncoming vehicle -- it always pulls over to the left and stops.</p> </li> </ul> <ul> <li> <p>Does not support the functionality of pulling into the shoulder to stop and give way</p> <p>Depending on the road width, the ego vehicle may need to enter the shoulder in order to give way to an oncoming vehicle. However, this module does not support such functionality.</p> </li> </ul>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_bidirectional_traffic_module/#inner-workingsalgorithms","title":"Inner workings/Algorithms","text":"<p>This module does not support the following situations or functionalities.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_bidirectional_traffic_module/#keep-left","title":"Keep Left","text":"<p>This module detects bidirectional lanelets from the lanelet map and performs a keep left maneuver. When keeping left, the shift starts from the beginning of the lanelet immediately before the bidirectional lanelet, and ends at the end of the bidirectional lanelet.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_bidirectional_traffic_module/#give-way","title":"Give Way","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_bidirectional_traffic_module/#state-transition-of-giveway","title":"State transition of GiveWay","text":"<p>To perform the give way maneuver, the GiveWay module has five internal states.</p> <ul> <li> <p>NoNeedToGiveWay</p> <p>No oncoming vehicles present and driving normally or oncoming vehicles are far away.</p> <p></p> </li> </ul> <ul> <li> <p>ApproachingToShift</p> <p>If an oncoming vehicle is detected, it generates a path to shift left and pull over, approaching the shift start point.</p> <p></p> </li> </ul> <ul> <li> <p>ShiftingRoadside</p> <p></p> </li> </ul> <ul> <li> <p>WaitingForOncomingCarsToPass</p> <p>The ego vehicle stops and waits until all oncoming vehicles have passed.</p> <p></p> </li> </ul> <ul> <li> <p>BackToNormalLane</p> <p>After the oncoming vehicle has passed, the ego vehicle performs a shift to return to its original driving lane.</p> <p></p> </li> </ul> <p>The state transition diagram of the GiveWay module is shown below.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_bidirectional_traffic_module/#how-to-decide-shift-length-to-pull-over","title":"How to decide shift length to pull over?","text":"<p>When an oncoming vehicle is detected, it is necessary to determine how much longitudinal distance to consume for the shift.</p> <p></p> <p>The figure above illustrates the distances related to lane shifting. The shift prepare distance is the distance required to prepare for shifting before reaching the shift start point, and it is calculated by multiplying the parameter time_to_prepare_pull_over by the ego_velocity.</p> <p>These distances are calculated when the state is NoNeedToGiveWay. Once the ego vehicle enters the shift prepare distance, the state transitions to ApproachingToShift. When it enters the shift distance, the state transitions to ShiftingRoadside.</p> <p>While in the NoNeedToGiveWay state, the calculation of the shift_distance and the corresponding state transitions are handled as follows.</p> <p></p> <p>We now provide a detailed explanation of the following four calculations:</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_bidirectional_traffic_module/#1-calculate-lateral_shift_distance","title":"1. Calculate <code>lateral_shift_distance</code>","text":"<p><code>lateral_shift_distance</code> is the lateral distance that the ego vehicle needs to shift. This value is calculated from <code>min_distance_from_roadside</code> and <code>shift_distance_to_pull_over_from_center_line</code>, based on <code>road_width</code> and <code>vehicle_width</code>, as follows:</p> \\[   \\text{lateral_shift_distance} = \\text{max}(\\text{road_width} / 2 - \\text{vehicle_width} / 2 - \\text{min_distance_from_roadside}, \\text{shift_distance_to_pull_over_from_center_line}) \\]"},{"location":"planning/behavior_path_planner/autoware_behavior_path_bidirectional_traffic_module/#2-calculate-allowed_longitudinal_distance","title":"2. Calculate <code>allowed_longitudinal_distance</code>","text":"<p><code>allowed_longitudinal_distance</code> is the minimum longitudinal distance required for the ego vehicle to perform the shift. Shifting in a shorter distance would require a sharp steering maneuver, which is unsafe. This distance is calculated such that the lateral jerk derived from <code>lateral_shift_distance</code> and <code>ego_velocity</code> does not exceed <code>max_lateral_jerk</code>:</p> \\[   \\text{allowed_longitudinal_distance} = \\text{calc_longitudinal_dist_from_jerk}(\\text{lateral_shift_distance}, \\text{max_lateral_jerk}, \\text{ego_velocity}) \\]"},{"location":"planning/behavior_path_planner/autoware_behavior_path_bidirectional_traffic_module/#3-calculate-desired_pull_over_point","title":"3. Calculate <code>desired_pull_over_point</code>","text":"<p>If the shift starts too early, the ego vehicle may have to wait a long time for the oncoming vehicle to pass. If it starts too late, there is a risk of collision. To avoid both, the ideal point to begin the pull-over is calculated based on the ego speed and the oncoming vehicle's speed, assuming the ego vehicle will stop and wait for <code>wait_time_for_oncoming_car</code> seconds:</p> \\[   \\text{desired_pull_over_point} = \\frac{(\\text{distance_to_oncoming_car} - \\text{oncoming_car_speed} \\cdot \\text{wait_time_for_oncoming_car}) \\cdot \\text{ego_speed}}{\\text{ego_speed} + \\text{oncoming_car_speed}} \\]"},{"location":"planning/behavior_path_planner/autoware_behavior_path_bidirectional_traffic_module/#4-calculate-max_longitudinal_distance","title":"4. Calculate <code>max_longitudinal_distance</code>","text":"<p>An overly long shift distance may appear unnatural. To prevent this, the maximum longitudinal distance is calculated based on the minimum lateral jerk:</p> \\[   \\text{max_longitudinal_distance} = \\text{calc_longitudinal_dist_from_jerk}(\\text{lateral_shift_distance}, \\text{min_lateral_jerk}, \\text{ego_velocity}) \\]"},{"location":"planning/behavior_path_planner/autoware_behavior_path_bidirectional_traffic_module/#parameters","title":"Parameters","text":"Name Type Description Default Range time_to_prepare_pull_over float Time to prepare for pull over. When an oncoming vehicle is detected, ego_speed x time_to_prepare_pull_over [m] starts shifting from ahead. Unit is [s]. 1 \u22650.0 min_distance_from_roadside float Minimum distance from left roadside to pull over. Unit is [m]. 0.2 \u22650.0 keep_left_distance_from_center_line float Distance to keep left from center line. Unit is [m]. 0.5 \u22650.0 shift_distance_to_pull_over_from_center_line float Distance to shift to the left from center line when pulling over. Unit is [m]. 1.2 \u22650.0 wait_time_for_oncoming_car float Wait time for oncoming car from ego vehicle stops to ego vehicle starts. Unit is [s]. 15 \u22650.0 max_lateral_jerk float Maximum lateral jerk when pulling over. Unit is [m/s^3]. 3 \u22650.0 min_lateral_jerk float Minimum lateral jerk when pulling over. Unit is [m/s^3]. 2 \u22650.0"},{"location":"planning/behavior_path_planner/autoware_behavior_path_dynamic_obstacle_avoidance_module/","title":"Avoidance module for dynamic objects","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_dynamic_obstacle_avoidance_module/#avoidance-module-for-dynamic-objects","title":"Avoidance module for dynamic objects","text":"<p>This module is under development.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_dynamic_obstacle_avoidance_module/#purpose-role","title":"Purpose / Role","text":"<p>This module provides avoidance functions for vehicles, pedestrians, and obstacles in the vicinity of the ego's path in combination with the autoware_path_optimizer. Each module performs the following roles. Dynamic Avoidance module cuts off the drivable area according to the position and velocity of the target to be avoided. Obstacle Avoidance module modifies the path to be followed so that it fits within the received drivable area.</p> <p>Static obstacle's avoidance functions are also provided by the Static Avoidance module, but these modules have different roles. The Static Obstacle Avoidance module performs avoidance through the outside of own lanes but cannot avoid the moving objects. On the other hand, this module can avoid moving objects. For this reason, the word \"dynamic\" is used in the module's name. The table below lists the avoidance modules that can handle each situation.</p> avoid within the own lane avoid through the outside of own lanes avoid not-moving objects Avoidance Module  Dynamic Avoidance Module + Obstacle Avoidance Module Avoidance Module avoid moving objects Dynamic Avoidance Module + Obstacle Avoidance Module No Module (Under Development)"},{"location":"planning/behavior_path_planner/autoware_behavior_path_dynamic_obstacle_avoidance_module/#policy-of-algorithms","title":"Policy of algorithms","text":"<p>Here, we describe the policy of inner algorithms. The inner algorithms can be separated into two parts: The first decides whether to avoid the obstacles and the second cuts off the drivable area against the corresponding obstacle.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_dynamic_obstacle_avoidance_module/#select-obstacles-to-avoid","title":"Select obstacles to avoid","text":"<p>To decide whether to avoid an object, both the predicted path and the state (pose and twist) of each object are used. The type of objects the user wants this module to avoid is also required. Using this information, the module decides to avoid objects that obstruct the ego's passage and can be avoided.</p> <p>The definition of obstruct the ego's passage is implemented as the object that collides within seconds. The other, can be avoided denotes whether it can be avoided without risk to the passengers or the other vehicles. For this purpose, the module judges whether the obstacle can be avoided with satisfying the constraints of lateral acceleration and lateral jerk. For example, the module decides not to avoid an object that is too close or fast in the lateral direction.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_dynamic_obstacle_avoidance_module/#cuts-off-the-drivable-area-against-the-selected-vehicles","title":"Cuts off the drivable area against the selected vehicles","text":"<p>For the selected obstacles to be avoided, the module cuts off the drivable area. As inputs to decide the shapes of cut-off polygons, poses of the obstacles are mainly used, assuming they move in parallel to the ego's path, instead of its predicted path. This design arises from that the predicted path of objects is not accurate enough to use the path modifications (at least currently). Furthermore, the output drivable area shape is designed as a rectangular cutout along the ego's path to make the computation scalar rather than planar.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_dynamic_obstacle_avoidance_module/#determination-of-lateral-dimension","title":"Determination of lateral dimension","text":"<p>The lateral dimensions of the polygon are calculated as follows. The polygon's width to extract from the drivable area is the obstacle width and <code>drivable_area_generation.lat_offset_from_obstacle</code>. We can limit the lateral shift length by <code>drivable_area_generation.max_lat_offset_to_avoid</code>.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_dynamic_obstacle_avoidance_module/#determination-of-longitudinal-dimension","title":"Determination of longitudinal dimension","text":"<p>Then, extracting the same directional and opposite directional obstacles from the drivable area will work as follows considering TTC (time to collision).</p> <p>Regarding the same directional obstacles, obstacles whose TTC is negative will be ignored (e.g., The obstacle is in front of the ego, and the obstacle's velocity is larger than the ego's velocity.).</p> <p>Same directional obstacles (Parameter names may differ from implementation) </p> <p>Opposite directional obstacles (Parameter names may differ from implementation) </p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_dynamic_obstacle_avoidance_module/#cuts-off-the-drivable-area-against-the-selected-pedestrians","title":"Cuts off the drivable area against the selected pedestrians","text":"<p>Then, we describe the logic to generate the drivable areas against pedestrians to be avoided. Objects of this type are considered to have priority right of way over the ego's vehicle while ensuring a minimum safety of the ego's vehicle. In other words, the module assigns a drivable area to an obstacle with a specific margin based on the predicted paths with specific confidences for a specific time interval, as shown in the following figure.</p>  Restriction areas are generated from each pedestrian's predicted paths <p>Apart from polygons for objects, the module also generates another polygon to ensure the ego's safety, i.e., to avoid abrupt steering or significant changes from the path. This is similar to avoidance against the vehicles and takes precedence over keeping a safe distance from the object to be avoided. As a result, as shown in the figure below, the polygons around the objects reduced by the secured polygon of the ego are subtracted from the ego's drivable area.</p>  Ego's minimum requirements are prioritized against object margin"},{"location":"planning/behavior_path_planner/autoware_behavior_path_dynamic_obstacle_avoidance_module/#example","title":"Example","text":"Avoidance for the bus departure Avoidance on curve  Avoidance against the opposite direction vehicle Avoidance for multiple vehicle"},{"location":"planning/behavior_path_planner/autoware_behavior_path_dynamic_obstacle_avoidance_module/#future-works","title":"Future works","text":"<p>Currently, the path shifting length is limited to 0.5 meters or less by <code>drivable_area_generation.max_lat_offset_to_avoid</code>. This is caused by the lack of functionality to work with other modules and the structure of the planning component. Due to this issue, this module can only handle situations where a small avoidance width is sufficient. This issue is the most significant for this module. In addition, the ability of this module to extend the drivable area as needed is also required.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_dynamic_obstacle_avoidance_module/#parameters","title":"Parameters","text":"<p>Under development</p> Name Unit Type Description Default value target_object.car [-] bool The flag whether to avoid cars or not true target_object.truck [-] bool The flag whether to avoid trucks or not true ... [-] bool ... ... target_object.min_obstacle_vel [m/s] double Minimum obstacle velocity to avoid 1.0 drivable_area_generation.lat_offset_from_obstacle [m] double Lateral offset to avoid from obstacles 0.8 drivable_area_generation.max_lat_offset_to_avoid [m] double Maximum lateral offset to avoid 0.5 drivable_area_generation.overtaking_object.max_time_to_collision [s] double Maximum value when calculating time to collision 3.0 drivable_area_generation.overtaking_object.start_duration_to_avoid [s] double Duration to consider avoidance before passing by obstacles 4.0 drivable_area_generation.overtaking_object.end_duration_to_avoid [s] double Duration to consider avoidance after passing by obstacles 5.0 drivable_area_generation.overtaking_object.duration_to_hold_avoidance [s] double Duration to hold avoidance after passing by obstacles 3.0 drivable_area_generation.oncoming_object.max_time_to_collision [s] double Maximum value when calculating time to collision 3.0 drivable_area_generation.oncoming_object.start_duration_to_avoid [s] double Duration to consider avoidance before passing by obstacles 9.0 drivable_area_generation.oncoming_object.end_duration_to_avoid [s] double Duration to consider avoidance after passing by obstacles 0.0"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/","title":"Goal Planner design","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#goal-planner-design","title":"Goal Planner design","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#purpose-role","title":"Purpose / Role","text":"<p>goal_planner generates a smooth path toward the goal and additionally searches for safe path and goal to execute dynamic pull_over on the road shoulders lanes following the traffic rules.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#design","title":"Design","text":"<p>If goal modification is not allowed, just park at the designated fixed goal using <code>fixed_goal_planner</code>.</p> <p>If allowed, <code>rough_goal_planner</code> works to park around the vacant spots in the shoulder lanes around the goal by executing pull_over toward left or right side of the road lanes.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#trigger-condition","title":"trigger condition","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#fixed_goal_planner","title":"fixed_goal_planner","text":"<p><code>fixed_goal_planner</code> just plans a smooth path to the designated goal. NOTE: this planner does not perform the several features described below, such as \"goal search\", \"collision check\", \"safety check\", etc.</p> <p><code>fixed_goal_planner</code> is used when both conditions are met.</p> <ul> <li>Route is set with <code>allow_goal_modification=false</code>. This is the default.</li> <li>The goal is set on <code>road</code> lanes.</li> </ul> <p>If the path given to goal_planner covers the goal, <code>fixed_goal_planner</code> smoothly connects the goal and the path points around the goal within the radius of <code>refine_goal_search_radius_range</code> using spline interpolation.</p> <p></p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#rough_goal_planner","title":"rough_goal_planner","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#pull-over-on-road-lane","title":"pull over on road lane","text":"<p><code>rough_goal_planner</code> is triggered following the behavior_path_planner scene module interface namely through <code>isExecutionRequested</code> function and it returns true when following two conditions are met.</p> <ul> <li>The distance between the goal and ego get shorter than \\(\\max\\)(<code>pull_over_minimum_request_length</code>, stop distance with decel and jerk constraints).</li> <li>Route is set with <code>allow_goal_modification=true</code> or is on a <code>road_shoulder</code> type lane.<ul> <li>We can set this option with SetRoute api service.</li> <li>We support <code>2D Rough Goal Pose</code> with the key bind <code>r</code> in RViz, but in the future there will be a panel of tools to manipulate various Route API from RViz.</li> </ul> </li> </ul> <p></p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#finish-condition","title":"finish condition","text":"<ul> <li>The distance to the goal from ego is lower than threshold (default: &lt; <code>1m</code>).</li> <li>Ego is stopped.<ul> <li>The speed is lower than threshold (default: &lt; <code>0.01m/s</code>).</li> </ul> </li> </ul>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#general-parameters-for-goal_planner","title":"General parameters for goal_planner","text":"Name Unit Type Description Default value th_arrived_distance [m] double distance threshold for arrival of path termination 1.0 th_stopped_velocity [m/s] double velocity threshold for arrival of path termination 0.01 th_stopped_time [s] double time threshold for arrival of path termination 2.0 center_line_path_interval [m] double reference center line path point interval 1.0"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#goal-search","title":"Goal Search","text":"<p>To execute safe pull over in the presence of parked vehicles and other obstacles, collision free areas are searched within a certain range around the original goal. The selected best goal pose will be published as <code>/planning/scenario_planning/modified_goal</code>.</p> <p>goal search video</p> <p>First, the original(designated) goal is provided, and a refined goal pose is obtained so that it is at least <code>margin_from_boundary</code> offset from the edge of the lane.</p> <p></p> <p>Second, goal candidates are searched in the interval of [<code>-forward_goal_search_length</code>, <code>backward_goal_search_length</code>] in the longitudinal direction and in the interval of [<code>longitudinal_margin</code>,<code>longitudinal_margin+max_lateral_offset</code>] in the lateral direction centered around the refined goal.</p> <p></p> <p>Each goal candidate is prioritized and pull over paths are generated by each planner for each goal candidate. The priority of a goal candidate is determined by a sort policy using several distance metrics from the refined goal.</p> <p>The <code>minimum_longitudinal_distance</code> policy sorts the goal candidates to assign higher priority to goal with smaller longitudinal distance and then auxiliary to goal with smaller lateral distance, to prioritize goal candidates that are close to the original goal.</p> <p>The <code>minimum_weighted_distance</code> policy sorts the goal candidates by the weighted sum of lateral distance and longitudinal distance <code>longitudinal_distance + lateral_cost*lateral_distance</code>.</p> <p></p> <p>The following figure is an example of minimum_weighted_distance.\u200b The white number indicates the goal candidate priority, and the smaller the number, the higher the priority. the 0 goal indicates the original refined goal.</p> <p></p> <p>To achieve a goal pose which is easy to start the maneuvering after arrival, the goal candidate pose is aligned so that ego center becomes parallel to the shoulder lane boundary at that pose.</p> <p></p> <p>If the footprint in each goal candidate is within <code>object_recognition_collision_check_margin</code> from one of the parked object, or the longitudinal distance to one of the parked objects from that goal candidate is less than <code>longitudinal_margin</code>, it is determined to be unsafe. These goals are not selected. If <code>use_occupancy_grid_for_goal_search</code> is enabled, collision detection on the grid is also performed with <code>occupancy_grid_collision_check_margin</code>.</p> <p>Red goal candidates in the below figure indicate unsafe ones.</p> <p></p> <p></p> <p>Also, if <code>prioritize_goals_before_objects</code> is enabled, the number of objects that need to be avoided before reaching the goal is counted, and the goal candidate with the number are prioritized.</p> <p>The images represent a count of objects to be avoided at each range, with priority given to those with the lowest number, regardless of the aforementioned distances.</p> <p></p> <p>The gray numbers represent objects to avoid, and you can see that the goal in front has a higher priority in this case.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#busstoparea","title":"BusStopArea","text":"<p>If the flag <code>use_bus_stop_area</code> is true, the goal search is limited inside the <code>BusStopArea</code> regulatory element polygon. The goal candidates are searched more densely compared to road shoulder parking method, and the goal candidate that keeps the ego footprint inside the <code>BusStopArea</code> is accepted. Refer to BusStopArea spec for more detail.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#parameters-for-goal-search","title":"Parameters for goal search","text":"Name Unit Type Description Default value goal_priority [-] string In case <code>minimum_longitudinal_distance</code>, sort with smaller longitudinal distances taking precedence over smaller lateral distances. In case <code>minimum_weighted_distance</code>, sort with the sum of weighted lateral distance and longitudinal distance <code>minimum_weighted_distance</code> lateral_weight [-] double Weight for lateral distance used when <code>minimum_weighted_distance</code> 40.0 prioritize_goals_before_objects [-] bool If there are objects that may need to be avoided, prioritize the goal in front of them true parking_policy [-] string Specifies which side of the road to park on. Determines whether to search for parking spots on the left or right side of the road. Options: <code>left_side</code> or <code>right_side</code> <code>left_side</code> forward_goal_search_length [m] double length of forward range to be explored from the original goal 20.0 backward_goal_search_length [m] double length of backward range to be explored from the original goal 20.0 goal_search_interval [m] double distance interval for goal search 2.0 longitudinal_margin [m] double margin between ego-vehicle at the goal position and obstacles 3.0 max_lateral_offset [m] double maximum offset of goal search in the lateral direction 0.5 lateral_offset_interval [m] double distance interval of goal search in the lateral direction 0.25 ignore_distance_from_lane_start [m] double This parameter ensures that the distance between the start of the shoulder lane and the goal is not less than the specified value. It's used to prevent setting goals too close to the beginning of the shoulder lane, which might lead to unsafe or impractical pull-over maneuvers. Increasing this value will force the system to ignore potential goal positions near the start of the shoulder lane, potentially leading to safer and more comfortable pull-over locations. 0.0 margin_from_boundary [m] double distance margin from edge of the shoulder lane 0.5"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#pull-over","title":"Pull Over","text":"<p>Since the path candidates generation is time consuming, goal_planner employs two separate threads to generate path candidates in the background and get latest candidates asynchronously. One is <code>LaneParkingThread</code> which plans path candidates on road shoulder lanes and the other is <code>FreespaceParkingThread</code> which plans on freespace area. The normal process of goal_planner is executed on the main thread.</p> <p>Although the two threads are running periodically, the primary background process is performed only when following conditions are met in order not to consume computational resource.</p> <ul> <li>ego has approached the goal within the threshold of <code>pull_over_prepare_length</code></li> <li>upstream module path shape has changed from the one which was sent by the main thread in previous process</li> <li>upstream module path shape has changed from the one which was used for path candidates generation in the previous process</li> </ul> <p><code>LaneParkingThread</code> executes either</p> <ul> <li>shift based path planning</li> <li>arc forward, arc backward path planning</li> <li>bezier based path planning</li> </ul> <p>depending on the situation and configuration. If <code>use_bus_stop_area</code> is true and the goal is on a BusStopArea regulatory element and the estimated pull over angle(the difference of pose between the shift start and shift end) is larger than <code>bezier_parking.pull_over_angle_threshold</code>, bezier based path planner works to generate path candidates. Otherwise shift based path planner works. bezier based path planner tends to generate more natural paths on a curved lane than shift based path planner, so it is used if the shift requires a certain amount of pose rotation.</p> <p>The overall flow is as follows.</p> <p></p> <p>The main thread and the each thread communicate by sending request and response respectively. The main thread sends latest main thread data as <code>LaneParkingRequest/FreespaceParkingRequest</code> and each thread sets <code>LaneParkingResponse/FreespaceParkingResponse</code> as the output when it's finished. The bluish blocks on the flow diagram are the critical section.</p> <p>While</p> <ul> <li>there are no path candidates, or</li> <li>the threads fail to generate candidates, or</li> <li>the main thread cannot nail down that the selected candidate is SAFE against dynamic objects(which means the DecisionState is not still <code>DECIDED</code>)</li> </ul> <p>the main thread inserts a stop pose either at <code>closest_start_pose</code> which is the closest shift start pose among the path candidates, or at the position which is certain distance before the closest goal candidate.</p> <p>Once the main thread finally selected the best pull over path, goal_planner transits to <code>DECIDED</code> state and it sets <code>SAFE</code> as the RTC status(NOTE: this <code>SAFE</code> means that \"a safe pull over path has been finally selected\".)</p> <p>If there are no path candidates or the selected path is not SAFE and thus <code>the LaneParkingThread</code> causes ego to get stuck, the <code>FreespaceParkingThread</code> is triggered by the stuck detection and it starts generating path candidates using freespace parking algorithms. If a valid freespace path is found and ego is still stuck, the freespace path is used instead. If the selected lane parking pull over path becomes collision-free again in case the blocking parked objects moved, and the path is continuous from current freespace path, lane parking pull over path is selected again.</p> Name Unit Type Description Default value pull_over_minimum_request_length [m] double when the ego-vehicle approaches the goal by this distance or a safe distance to stop, pull over is activated. 100.0 pull_over_velocity [m/s] double decelerate to this speed by the goal search area 3.0 pull_over_minimum_velocity [m/s] double speed of pull_over after stopping once. this prevents excessive acceleration. 1.38 decide_path_distance [m] double decide path if it approaches this distance relative to the parking position. after that, no path planning and goal search are performed 10.0 maximum_deceleration [m/s2] double maximum deceleration. it prevents sudden deceleration when a parking path cannot be found suddenly 1.0 maximum_jerk [m/s3] double maximum jerk for path planning 1.0 low_velocity_threshold [m/s] double velocity threshold to determine if the vehicle is in low velocity range and should apply buffer for stopping distance calculation 1.38 stopping_distance_buffer [m] double buffer distance added to the stopping distance when the vehicle is stopped or in low velocity range 2.0 path_priority [-] string In case <code>efficient_path</code> use a goal that can generate an efficient path which is set in <code>efficient_path_order</code>. In case <code>close_goal</code> use the closest goal to the original one. efficient_path efficient_path_order [-] string efficient order of pull over planner along lanes excluding freespace pull over [\"SHIFT\", \"ARC_FORWARD\", \"ARC_BACKWARD\"] lane_departure_check_expansion_margin [m] double margin to expand the ego vehicle footprint when doing lane departure checks 0.0"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#shift-parking","title":"shift parking","text":"<p>Pull over distance is calculated by the speed, lateral deviation, and the lateral jerk. The lateral jerk is searched for among the predetermined minimum and maximum values.</p> <ol> <li>Apply uniform offset to centerline of shoulder lane for ensuring margin</li> <li>The interval of shift start and end is shifted by the shift based path planner</li> <li>Combine this path with center line of road lane and the remaining shoulder lane centerline</li> </ol> <p></p> <p>shift_parking video</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#parameters-for-shift-parking","title":"Parameters for shift parking","text":"Name Unit Type Description Default value enable_shift_parking [-] bool flag whether to enable shift parking true shift_sampling_num [-] int Number of samplings in the minimum to maximum range of lateral_jerk 4 maximum_lateral_jerk [m/s3] double maximum lateral jerk 2.0 minimum_lateral_jerk [m/s3] double minimum lateral jerk 0.5 deceleration_interval [m] double distance of deceleration section 15.0 after_shift_straight_distance [m] double straight line distance after pull over end point 1.0"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#geometric-parallel-parking","title":"geometric parallel parking","text":"<p>This method generate two arc paths with discontinuous curvature. It stops twice in the middle of the path to do dry steering. There are two path generation methods: forward and backward.</p> <p>See also [1] for details of the algorithm. There is also a simple python implementation.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#parameters-geometric-parallel-parking","title":"Parameters geometric parallel parking","text":"Name Unit Type Description Default value arc_path_interval [m] double interval between arc path points 1.0 max_steer_angle_margin_scale [-] double scaling factor applied to the maximum steering angle (max_steer_angle) defined in vehicle_info parameter 0.72"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#arc-forward-parking","title":"arc forward parking","text":"<p>Generate two forward arc paths.</p> <p></p> <p>arc_forward_parking video</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#parameters-arc-forward-parking","title":"Parameters arc forward parking","text":"Name Unit Type Description Default value enable_arc_forward_parking [-] bool flag whether to enable arc forward parking true after_forward_parking_straight_distance [m] double straight line distance after pull over end point 2.0 forward_parking_velocity [m/s] double velocity when forward parking 1.38 forward_parking_lane_departure_margin [m/s] double lane departure margin for front left corner of ego-vehicle when forward parking 0.0"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#arc-backward-parking","title":"arc backward parking","text":"<p>Generate two backward arc paths.</p> <p>.</p> <p>arc_backward_parking video</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#parameters-arc-backward-parking","title":"Parameters arc backward parking","text":"Name Unit Type Description Default value enable_arc_backward_parking [-] bool flag whether to enable arc backward parking true after_backward_parking_straight_distance [m] double straight line distance after pull over end point 2.0 backward_parking_velocity [m/s] double velocity when backward parking -1.38 backward_parking_lane_departure_margin [m/s] double lane departure margin for front right corner of ego-vehicle when backward 0.0"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#freespace-parking","title":"freespace parking","text":"<p>If the vehicle gets stuck with <code>LaneParkingPlanning</code>, <code>FreespaceParkingPlanner</code> is triggered.</p> <p>To run this feature, you need to set <code>parking_lot</code> to the map, <code>activate_by_scenario</code> of costmap_generator to <code>false</code> and <code>enable_freespace_parking</code> to <code>true</code></p> <p></p> <p>Simultaneous execution with <code>avoidance_module</code> in the flowchart is under development.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#parameters-freespace-parking","title":"Parameters freespace parking","text":"Name Unit Type Description Default value enable_freespace_parking [-] bool This flag enables freespace parking, which runs when the vehicle is stuck due to e.g. obstacles in the parking area. true <p>See freespace_planner for other parameters.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#bezier-parking","title":"bezier parking","text":"<p>shift based path planner tends to generate unnatural path when the shift lane is curved as illustrated below.</p> <p></p> <p>bezier based path planner interpolates the shift path start and end pose using tbe bezier curve for a several combination of parameters, to obtain a better result through the later selection process. In the below screenshot the goal is on a BusStopArea and <code>use_bus_stop_area</code> is set to true, so bezier planner is triggered instead. Internally, goalplanner first tries to use _shift planner, and if it turns out that the shift start and end is not parallel, it switches to bezier planner from the next process.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#parameters-for-bezier-parking","title":"Parameters for bezier parking","text":"Name Unit Type Description Default value pull_over_angle_threshold [rad] double threshold angle difference between shift start and end pose to trigger bezier planner instead of shift planner 0.5 after_shift_straight_distance [m] double straight line distance after pull over end pose to goal pose 1.5 lateral_acceleration_threshold [m/s^2] double maximum allowable lateral acceleration near the path start. Used to filter out path candidates with excessive lateral acceleration. Paths where the yaw change or lateral deviation at a point <code>velocity \u00d7 lateral_acceleration_filtering_duration</code> ahead exceeds this limit are rejected. 1.0 lateral_acceleration_filtering_duration [s] double time duration from the start pose used to evaluate lateral acceleration. The check evaluates the path geometry at a point calculated as <code>velocity \u00d7 duration</code> ahead of the start pose. Note: This check does NOT evaluate lateral acceleration at every point along the path during this duration. Instead, it only evaluates the lateral acceleration at the single endpoint by approximating the path as a circular arc from the start pose to that point. 0.5"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#collision-check-for-path-generation","title":"collision check for path generation","text":"<p>To select a safe one from the path candidates, collision is checked against parked objects for each path.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#occupancy-grid-based-collision-check","title":"occupancy grid based collision check","text":"<p>Generate footprints from ego-vehicle path points and determine obstacle collision from the value of occupancy_grid of the corresponding cell.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#parameters-for-occupancy-grid-based-collision-check","title":"Parameters for occupancy grid based collision check","text":"Name Unit Type Description Default value use_occupancy_grid_for_goal_search [-] bool flag whether to use occupancy grid for goal search collision check true use_occupancy_grid_for_goal_longitudinal_margin [-] bool flag whether to use occupancy grid for keeping longitudinal margin false use_occupancy_grid_for_path_collision_check [-] bool flag whether to use occupancy grid for collision check false occupancy_grid_collision_check_margin [m] double margin to calculate ego-vehicle cells from footprint. 0.0 theta_size [-] int size of theta angle to be considered. angular resolution for collision check will be 2\\(\\pi\\) / theta_size [rad]. 360 obstacle_threshold [-] int threshold of cell values to be considered as obstacles 60"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#object-recognition-based-collision-check","title":"object recognition based collision check","text":"<p>collision is checked for each of the path candidates. There are three margins for this purpose.</p> <ul> <li><code>object_recognition_collision_check_margin</code> is margin in all directions of ego.</li> <li>In the forward direction, a margin is added by the braking distance calculated from the current speed and maximum deceleration. The maximum distance is The maximum value of the distance is suppressed by the <code>object_recognition_collision_check_max_extra_stopping_margin</code></li> <li>In curves, the lateral margin is larger than in straight lines.This is because curves are more prone to control errors or to fear when close to objects (The maximum value is limited by <code>object_recognition_collision_check_max_extra_stopping_margin</code>, although it has no basis.)</li> </ul> <p>Before these margins are applied, objects are filtered based on their location relative to road boundaries (linestring withroad_border type ). Objects located on the opposite side of relevant road borders from the ego vehicle are excluded from the collision check process. This ensures that the planner focuses only on objects that pose a potential risk within the intended driving corridor.</p> <p></p> <p></p> <p>Then there is the concept of soft and hard margins. Although not currently parameterized, if a collision-free path can be generated by a margin several times larger than <code>object_recognition_collision_check_margin</code>, then the priority is higher.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#parameters-for-object-recognition-based-collision-check","title":"Parameters for object recognition based collision check","text":"Name Unit Type Description Default value use_object_recognition [-] bool flag whether to use object recognition for collision check true object_recognition_collision_check_soft_margins [m] vector[double] soft margins for collision check when path generation. It is not strictly the distance between footprints, but the maximum distance when ego and objects are oriented. [5.0, 4.5, 4.0, 3.5, 3.0, 2.5, 2.0, 1.5, 1.0] object_recognition_collision_check_hard_margins [m] vector[double] hard margins for collision check when path generation [0.6] object_recognition_collision_check_max_extra_stopping_margin [m] double maximum value when adding longitudinal distance margin for collision check considering stopping distance 1.0 collision_check_outer_margin_factor [-] double factor to extend the collision check margin from the inside margin to the outside in the curved path 2.0 detection_bound_offset [m] double expand pull over lane with this offset to make detection area for collision check of path generation 15.0"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#safety-check","title":"safety check","text":"<p>Perform safety checks on moving objects. If the object is determined to be dangerous, no path decision is made and no approval is given,</p> <ul> <li>path decision is not made and approval is not granted.</li> <li>After approval, the ego vehicle stops under deceleration and jerk constraints.</li> </ul> <p>This module has two methods of safety check, <code>RSS</code> and <code>integral_predicted_polygon</code>.</p> <p><code>RSS</code> method is a method commonly used by other behavior path planner modules, see RSS based safety check utils explanation.</p> <p><code>integral_predicted_polygon</code> is a more safety-oriented method. This method is implemented because speeds during pull over are lower than during driving, and fewer objects travel along the edge of the lane. (It is sometimes too reactive and may be less available.) This method integrates the footprints of egos and objects at a given time and checks for collisions between them.</p> <p></p> <p>In addition, the safety check has a time hysteresis, and if the path is judged \"safe\" for a certain period of time(<code>keep_unsafe_time</code>), it is finally treated as \"safe\".</p> <pre><code>                         ==== is_safe\n                         ---- current_is_safe\n       is_safe\n        ^\n        |\n        |                   time\n      1 +--+    +---+       +---=========   +--+\n        |  |    |   |       |           |   |  |\n        |  |    |   |       |           |   |  |\n        |  |    |   |       |           |   |  |\n        |  |    |   |       |           |   |  |\n      0 =========================-------==========--&gt; t\n</code></pre>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#parameters-for-safety-check","title":"Parameters for safety check","text":"Name Unit Type Description Default value method [-] string method for safety check. <code>RSS</code> or <code>integral_predicted_polygon</code> <code>integral_predicted_polygon</code> keep_unsafe_time [s] double safety check Hysteresis time. if the path is judged \"safe\" for the time it is finally treated as \"safe\". 3.0 check_all_predicted_path - bool Flag to check all predicted paths true publish_debug_marker - bool Flag to publish debug markers false <code>collision_check_yaw_diff_threshold</code> [rad] double Maximum yaw difference between ego and object when executing rss-based collision checking 3.1416"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#parameters-for-rss-safety-check","title":"Parameters for RSS safety check","text":"Name Unit Type Description Default value rear_vehicle_reaction_time [s] double Reaction time for rear vehicles 2.0 rear_vehicle_safety_time_margin [s] double Safety time margin for rear vehicles 1.0 lateral_distance_max_threshold [m] double Maximum lateral distance threshold 2.0 longitudinal_distance_min_threshold [m] double Minimum longitudinal distance threshold 3.0 longitudinal_velocity_delta_time [s] double Delta time for longitudinal velocity 0.8"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#parameters-for-integral_predicted_polygon-safety-check","title":"Parameters for integral_predicted_polygon safety check","text":"Name Unit Type Description Default value forward_margin [m] double forward margin for ego footprint 1.0 backward_margin [m] double backward margin for ego footprint 1.0 lat_margin [m] double lateral margin for ego footprint 1.0 time_horizon [s] double Time width to integrate each footprint 10.0"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#path-deciding","title":"path deciding","text":"<p>When ego approached the start of the temporarily selected pull over path within the distance of <code>decide_path_distance</code>, if it is collision-free at that time and safe against dynamic objects, it transitions to <code>DECIDING</code>. And if those conditions hold for a certain period of time, it transitions to <code>DECIDED</code> and the selected path is fixed.</p> <p> Open</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#parameters-for-path-decision-state-controller","title":"Parameters for path decision state controller","text":"Name Unit Type Description Default value check_collision_duration [s] double Duration to continuously check collision before transition from DECIDING to DECIDED state. This ensures the path remains safe for a period of time before final commitment. 1.0"},{"location":"planning/behavior_path_planner/autoware_behavior_path_goal_planner_module/#unimplemented-parts-limitations","title":"Unimplemented parts / limitations","text":"<ul> <li>Only shift pull over can be executed concurrently with other modules</li> <li>Parking in tight spots and securing margins are traded off. A mode is needed to reduce the margin by using a slower speed depending on the situation, but there is no mechanism for dynamic switching of speeds.</li> <li>Parking space available depends on visibility of objects, and sometimes parking decisions cannot be made properly.</li> <li>Margin to unrecognized objects(Not even unknown objects) depends on the occupancy grid. May get too close to unrecognized ground objects because the objects that are allowed to approach (e.g., grass, leaves) are indistinguishable.</li> </ul> <p>Unimplemented parts / limitations for freespace parking</p> <ul> <li>When a short path is generated, the ego does can not drive with it.</li> <li>Complex cases take longer to generate or fail.</li> <li>The drivable area is not guaranteed to fit in the parking_lot.</li> </ul>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/","title":"Lane Change design","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#lane-change-design","title":"Lane Change design","text":"<p>The Lane Change module is activated when lane change is needed (Ego is not on preferred lane), and activation requirements are satisfied.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#lane-change-requirements","title":"Lane Change Requirements","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#prerequisites","title":"Prerequisites","text":"<p>The type of lane boundary in the HD map has to be one of the following:</p> <ul> <li>Dashed lane marking: Lane changes are permitted in both directions.</li> <li>Dashed marking on the left and solid on the right: Lane changes are allowed from left to right.</li> <li>Dashed marking on the right and solid on the left: Lane changes are allowed from right to left.</li> <li><code>allow_lane_change</code> tags is set as <code>true</code></li> </ul>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#activation-conditions","title":"Activation Conditions","text":"<p>The lane change module will activate under the following conditions :</p> <ul> <li>The ego-vehicle is NOT on a <code>preferred_lane</code>.</li> <li>Distance to start of <code>target_lane</code> is less than <code>maximum_prepare_length</code></li> <li>The ego-vehicle is NOT close to a regulatory element:<ul> <li>Distance to next regulatory element is greater than <code>maximum_prepare_length</code>.</li> <li>Considers distance to traffic light. (If param <code>regulation.traffic_light</code> is enabled)</li> <li>Considers distance to crosswalk. (If param <code>regulation.crosswalk</code> is enabled)</li> <li>Considers distance to intersection. (If param <code>regulation.intersection</code> is enabled)</li> </ul> </li> </ul> <p>Warning</p> <p>If ego vehicle is stuck, lane change will be enabled near crosswalk/intersection. Ego is considered stuck if it stops more than <code>stuck_detection.stop_time</code>. Ego is considered to be stopping if its velocity is smaller than <code>stuck_detection.velocity</code>.</p> <p>The following figure illustrates the logic for checking if lane change is required:</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#ready-conditions","title":"Ready Conditions","text":"<ul> <li>Valid lane change path is found.</li> <li>Lane change path is safe; does not collide with other dynamic objects.</li> <li>Lane change candidate path is approved by an operator.</li> </ul>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#implementation","title":"Implementation","text":"<p>Lane change module uses a sampling based approach for generating a valid and safe lane changing trajectory. The process for generating the trajectory includes object filtering, metrics sampling, candidate paths generation, and lastly candidate paths evaluation. Additionally the lane change module is responsible for turn signal activation when appropriate, and inserting a stop point when necessary.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#global-flowchart","title":"Global Flowchart","text":"<p>The following diagram, illustrates the overall flow of the lane change module implementation.</p> <p></p> <p>The lane change module first updates the necessary data for lane change such as lanes information and transient data. Then filters the detected objects to be used for safety evaluation (see Object Filtering).</p> <p>If the lane change requirements are met, the turn signal is activated and the lane change module will proceed to generating a candidate path. Lane change candidate paths are generated by sampling different metrics and evaluating the validity of the corresponding generated trajectory. More details can be found in Generating Lane Change Candidate Path;</p> <p>When a valid candidate path is generated, a safety evaluation is conducted to check for any risk of collision or hindrance. The details of the safety evaluation can be found in Safety Checks.</p> <p>Once a valid and safe candidate path is found, the drivable area is generated and the path is executed. While executing the lane change maneuver, the safety of the approved path is continuously monitored to ensure there is no chance of collision or other hindrance. If the approved path remains safe and completion checks are met (see Lane Change Completion Checks) the module will transit to SUCCESS state. In case the approved path is deemed to be no longer safe, the lane change module will attempt to abort the lane change maneuver (see Aborting Lane Change). When the lane change maneuver is aborted successfully the module will transit to FAILURE state, and the process is restarted.</p> <p>If the lane change module fails to find a valid and safe candidate path, the module will continue executing the previously approved path and insert a stop point along the path where appropriate, for more details refer to Stopping Behavior.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#generating-lane-change-candidate-path","title":"Generating Lane Change Candidate Path","text":"<p>The lane change candidate path is divided into two phases: preparation and lane-changing. The following figure illustrates each phase of the lane change candidate path.</p> <p></p> <p>The following chart illustrates the process of sampling candidate paths for lane change.</p> <p></p> <p>The following chart demonstrates the process of generating a valid candidate path with path shifter method.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#prepare-phase","title":"Prepare Phase","text":"<p>The prepare phase is the first section of the lane change candidate path and the corresponding prepare segment consists of a subsection of the current reference path along the current lane. The length of the prepare phase trajectory is computed as follows.</p> <pre><code>prepare_distance = current_speed * prepare_duration + 0.5 * lon_acceleration * prepare_duration^2\n</code></pre> <p>The prepare phase trajectory is valid if:</p> <ul> <li>The length of the prepare phase trajectory is greater than the distance to start of target lane</li> <li>The length of the prepare phase trajectory is less than the distance to terminal start point</li> </ul>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#lane-changing-phase","title":"Lane-changing Phase","text":"<p>The lane-changing phase consists of the shifted path that moves ego from current lane to the target lane. Total duration of lane-changing phase is computed from the <code>shift_length</code>, <code>lateral_jerk</code> and <code>lateral_acceleration</code>.</p> <p>In principle, positive longitudinal acceleration is considered during lane-changing phase, and is computed as follows.</p> <pre><code>lane_changing_acceleration = std::clamp((max_path_velocity - initial_lane_changing_velocity) / lane_changing_time,\n  0.0, prepare_longitudinal_acc);\n</code></pre> <p>Where <code>max_path_velocity</code> is the current path speed limit.</p> <p>Warning</p> <p>If the longitudinal acceleration of prepare phase is negative (slowing down), AND ego is near terminal, then the lane-changing longitudinal acceleration will also be negative and its value is decided by the parameter <code>lane_changing_decel_factor</code>.</p> <p>The lane changing distance is then computed as follows.</p> <pre><code>lane_changing_distance = initial_lane_changing_velocity * lane_changing_duration + 0.5 * lon_acceleration * lane_changing_duration^2\n</code></pre> <p>The <code>backward_length_buffer_for_end_of_lane</code> is added to allow some window for any possible delay, such as control or mechanical delay during brake lag.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#sampling-multiple-candidate-paths","title":"Sampling Multiple Candidate Paths","text":"<p>In order to find a valid and safe lane change path it might be necessary to generate multiple candidate path samples. The lane change module does this by sampling one or more of: <code>prepare_duration</code>, <code>longitudinal_acceleration</code>, and <code>lateral_acceleration</code>.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#prepare-duration-sampling","title":"Prepare Duration Sampling","text":"<p>In principle, a fixed prepare duration is assumed when generating lane change candidate path. The default prepare duration value is determined from the min and max values set in the lane change parameters, as well as the duration of turn signal activation. For example, when the lane change module first activates and turn signal is activated then prepare duration will be <code>max_prepare_duration</code>, as time passes and a path is still not approved, the prepare duration will decrease gradually down to <code>min_prepare_duration</code>. The formula is as follows.</p> <pre><code>prepare_duration = std::max(max_prepare_duration - turn_signal_duration, min_prepare_duration);\n</code></pre> <p>Note</p> <p>When the current ego velocity is lower than the <code>min_lane_change_velocity</code>, the <code>min_prepare_duration</code> is adjusted to ensure sufficient time for reaching <code>min_lane_change_velocity</code> assuming <code>max_longitudinal_acceleration</code>.</p> <p>Warning</p> <p>The value of the prepare duration impacts lane change cancelling behavior. A shorter prepare duration results in a smaller window in which lane change maneuver can be cancelled. See Evaluating Ego Vehicle's Position to Prevent Abrupt Maneuvers for more details.</p> <p>When ego vehicles is close to the terminal start, we need to sample multiple prepare duration values to find a valid and safe path. In this case prepare duration values are sampled starting from <code>max_prepare_duration</code> down to <code>0.0</code> at a fixed time interval of <code>0.5 s</code>.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#longitudinal-acceleration-sampling","title":"Longitudinal Acceleration Sampling","text":"<p>In principle, maximum longitudinal acceleration is assumed for generating lane change candidate path. However in certain situations, we need to sample multiple longitudinal acceleration values to find a valid and safe candidate path. The lower and upper bounds of the longitudinal acceleration sampled are determined from the values specified in the lane change parameters and common planner parameters, as follows</p> <pre><code>maximum_longitudinal_acceleration = min(common_param.max_acc, lane_change_param.max_acc)\nminimum_longitudinal_acceleration = max(common_param.min_acc, lane_change_param.min_acc)\n</code></pre> <p>where <code>common_param</code> is vehicle common parameter, which defines vehicle common maximum longitudinal acceleration and deceleration. Whereas, <code>lane_change_param</code> has maximum longitudinal acceleration and deceleration for the lane change module. For example, if a user set and <code>common_param.max_acc=1.0</code> and <code>lane_change_param.max_acc=0.0</code>, <code>maximum_longitudinal_acceleration</code> becomes <code>0.0</code>, and the lane change does not accelerate in the lane change phase.</p> <p>The <code>longitudinal_acceleration_resolution</code> is determine by the following</p> <pre><code>longitudinal_acceleration_resolution = (maximum_longitudinal_acceleration - minimum_longitudinal_acceleration) / longitudinal_acceleration_sampling_num\n</code></pre> <p>The chart below illustrates the conditions under which longitudinal acceleration values are sampled.</p> <p></p> <p>while the following describes the process by which longitudinal accelerations are sampled.</p> <p></p> <p>The following figure illustrates when <code>longitudinal_acceleration_sampling_num = 4</code>. Assuming that <code>maximum_acceleration = 1.0</code> and <code>minimum_acceleration = 1.0</code> then <code>a0 == 1.0</code>, <code>a1 == 0.5</code>, <code>a2 == 0.0</code>, <code>a3 == -0.5</code> and <code>a4 == -1.0</code>. <code>a0</code> is the expected lane change trajectory when sampling is not required.</p> <p></p> <p>Which path will be chosen depends on validity and safety checks.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#lateral-acceleration-sampling","title":"Lateral Acceleration Sampling","text":"<p>In addition to sampling longitudinal acceleration, we also sample lane change paths by varying the lateral acceleration. Lateral acceleration affects the lane changing duration, a lower value results in a longer trajectory, while a higher value results in a shorter trajectory. This allows the lane change module to explore shorter trajectories through higher lateral acceleration when there is limited space for the lane change.</p> <p>The maximum and minimum lateral accelerations are defined in the lane change parameter file as a map. The range of lateral acceleration is determined for each velocity by linearly interpolating the values in the map. Let's assume we have the following map</p> Ego Velocity Minimum lateral acceleration Maximum lateral acceleration 0.0 0.2 0.3 2.0 0.2 0.4 4.0 0.3 0.4 6.0 0.3 0.5 <p>In this case, when the current velocity of the ego vehicle is 3.0, the minimum and maximum lateral accelerations are 0.25 and 0.4 respectively. These values are obtained by linearly interpolating the second and third rows of the map, which provide the minimum and maximum lateral acceleration values.</p> <p>Within this range, we sample the lateral acceleration for the ego vehicle. Similar to the method used for sampling longitudinal acceleration, the resolution of lateral acceleration (lateral_acceleration_resolution) is determined by the following:</p> <pre><code>lateral_acceleration_resolution = (maximum_lateral_acceleration - minimum_lateral_acceleration) / lateral_acceleration_sampling_num\n</code></pre>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#terminal-lane-change-path","title":"Terminal Lane Change Path","text":"<p>Depending on the space configuration around the Ego vehicle, it is possible that a valid LC path cannot be generated. If that happens, then Ego will get stuck at <code>terminal_start</code> and not be able to proceed. Therefore we introduced the terminal LC path feature; when ego gets near to the terminal point (dist to <code>terminal_start</code> is less than the maximum lane change length) a terminal lane changing path will be computed starting from the terminal start point on the current lane and connects to the target lane. The terminal path only needs to be computed once in each execution of LC module. If no valid candidate paths are found in the path generation process, then the terminal path will be used as a fallback candidate path, the safety of the terminal path is not ensured and therefore it can only be force approved. The following images illustrate the expected behavior without and with the terminal path feature respectively:</p> <p></p> <p></p> <p>Additionally if terminal path feature is enabled and path is computed, stop point placement can be configured to be at the edge of the current lane instead of at the <code>terminal_start</code> position, as indicated by the dashed red line in the image above.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#generating-path-using-frenet-planner","title":"Generating Path Using Frenet Planner","text":"<p>Warning</p> <p>Generating path using Frenet planner applies only when ego is near terminal start</p> <p>If the ego vehicle is far from the terminal, the lane change module defaults to using the path shifter. This ensures that the lane change is completed while the target lane remains a neighbor of the current lane. However, this approach may result in high curvature paths near the terminal, potentially causing long vehicles to deviate from the lane.</p> <p>To address this, the lane change module provides an option to choose between the path shifter and the Frenet planner. The Frenet planner allows for some flexibility in the lane change endpoint, extending the lane changing end point slightly beyond the current lane's neighbors.</p> <p>The following table provides comparisons between the planners</p> With Path Shifter With Frenet Planner <p>Note</p> <p>The planner can be enabled or disabled using the <code>frenet.enable</code> flag.</p> <p>Note</p> <p>Since only a segment of the target lane is used as input to generate the lane change path, the end pose of the lane change segment may not smoothly connect to the target lane centerline. To address this, increase the value of <code>frenet.th_curvature_smoothing</code> to improve the smoothness.</p> <p>Note</p> <p>The yaw difference threshold (<code>frenet.th_yaw_diff</code>) limits the maximum curvature difference between the end of the prepare segment and the lane change segment. This threshold might prevent the generation of a lane change path when the lane curvature is high. In such cases, you can increase the frenet.th_yaw_diff value. However, note that if the prepare path was initially shifted by other modules, the resultant steering may not be continuous.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#candidate-path-validity","title":"Candidate Path Validity","text":"<p>It is a prerequisite, that both prepare length and lane-changing length are valid, such that:</p> <ol> <li>The prepare segment length is greater than the distance from ego to target lane start.</li> <li>The prepare segment length is smaller than the distance from ego to terminal start.</li> <li>The lane-changing distance is smaller than the remaining distance after prepare segment to terminal end.</li> <li>The lane-changing distance is smaller than the remaining distance after prepare segment to the next regulatory element.</li> </ol> <p>If so, a candidate path is considered valid if:</p> <ol> <li>The lane changing start point (end of prepare segment) is valid; it is within the target lane neighbor's polygon.</li> <li>The distance from ego to the end of the current lanes is sufficient to perform a single lane change.</li> <li>The distance from ego to the goal along the current lanes is adequate to complete multiple lane changes.</li> <li>The distance from ego to the end of the target lanes is adequate for completing multiple lane changes.</li> </ol> <p>The following flow chart illustrates the validity check.</p> <p></p> <p>Warning</p> <p>A valid path does NOT mean that the path is safe, however it will be available as a candidate path and can be force approved by operator. A path needs to be both valid AND safe to be automatically approved.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#lane-change-completion-checks","title":"Lane Change Completion Checks","text":"<p>To determine if the ego vehicle has successfully changed lanes, one of two criteria must be met: either the longitudinal or the lateral criteria.</p> <p>For the longitudinal criteria, the ego vehicle must pass the lane-changing end pose and be within the <code>finish_judge_buffer</code> distance from it. The module then checks if the ego vehicle is in the target lane. If true, the module returns success. This check ensures that the planner manager updates the root lanelet correctly based on the ego vehicle's current pose. Without this check, if the ego vehicle is changing lanes while avoiding an obstacle and its current pose is in the original lane, the planner manager might set the root lanelet as the original lane. This would force the ego vehicle to perform the lane change again. With the target lane check, the ego vehicle is confirmed to be in the target lane, and the planner manager can correctly update the root lanelets.</p> <p>If the longitudinal criteria are not met, the module evaluates the lateral criteria. For the lateral criteria, the ego vehicle must be within <code>finish_judge_lateral_threshold</code> distance from the target lane's centerline, and the angle deviation must be within <code>finish_judge_lateral_angle_deviation</code> degrees. The angle deviation check ensures there is no sudden steering. If the angle deviation is set too high, the ego vehicle's orientation could deviate significantly from the centerline, causing the trajectory follower to aggressively correct the steering to return to the centerline. Keeping the angle deviation value as small as possible avoids this issue.</p> <p>The process of determining lane change completion is shown in the following diagram.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#safety-checks","title":"Safety Checks","text":"<p>A candidate path needs to be both valid and safe for it to be executed. After generating a candidate path and validating it, the path will be checked against surrounding objects to ensure its safety. However the impacts of an object depends on its categorization, therefore it is necessary to filter the predicted objects before performing the safety checks.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#object-filtering","title":"Object filtering","text":"<p>In order to perform safety checks on the sampled candidate paths, it is needed to categorize the predicted objects based on their current pose and behavior at the time. These categories help determine how each object impacts the lane change process and guide the safety evaluation.</p> <p>The predicted objects are divided into four main categories:</p> <ul> <li>Target Lane Leading: Objects that overlap with the target lane and are in front of the ego vehicle. This category is further divided into three subcategories:<ul> <li>Moving: Objects with a velocity above a certain threshold.</li> <li>Stopped: Stationary objects within the target lane.</li> <li>Stopped at Bound: Objects outside the target lane but close to its boundaries.</li> </ul> </li> <li>Target Lane Trailing: Objects that overlap with the target lane or any lanes preceding the target lane. Only moving vehicles are included in this category.</li> <li>Current Lane: Objects in front of the ego vehicle in the ego vehicle's current lane.</li> <li>Others: Any objects not classified into the above categories.</li> </ul> <p></p> <p>Furthermore, for Target Lane Leading and Current Lane objects, only those positioned within the lane boundary or before the goal position are considered. Objects exceeding the end of the lane or the goal position are classified as Others.</p> <p>Once objects are filtered into their respective categories, they are sorted by distance closest to the ego vehicle to farthest.</p> <p>The following diagram illustrates the filtering process,</p> <p></p> <p>Note</p> <p>As shown in the flowchart, oncoming objects are also filtered out. The filtering process considers the difference between the current orientation of the ego vehicle and that of the object. However, depending on the map's geometry, a certain threshold may need to be allowed. This threshold can be configured using the parameter collision_check.th_incoming_object_yaw.</p> <p>Note</p> <p>The Target Lane Leading's Stopped at Boundary objects are detected using the expanded area of the target lane beyond its original boundaries. The parameters <code>lane_expansion.left_offset</code> and <code>lane_expansion.right_offset</code> can be configured to adjust the expanded width.</p> <p></p> Without Lane Expansion With Lane Expansion"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#candidate-path-safety","title":"Candidate Path Safety","text":"<p>A candidate path is considered safe if:</p> <ol> <li>There are no overtaking objects when the ego vehicle exits the turn-direction lane. (see Overtaking Object Check)</li> <li>There is no parked vehicle along the target lane ahead of ego (see Delay Lane Change Check)</li> <li>The path does NOT cause ego footprint to exceed the target lane opposite boundary</li> <li>The path passes the collision check (See Collision Check)</li> </ol>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#overtaking-object-check","title":"Overtaking Object Check","text":"<p>When ego is exiting an intersection on a turning lane, there is a possibility that a rear vehicle will attempt to overtake the ego vehicle. Which can be dangerous if ego is also trying to perform a lane change. Therefore lane change module will adopt a more conservative behavior in such situation.</p> <p>If the ego vehicle is currently within an intersection on a turning lane, as shown in the figure below, the generated candidate paths will be marked as unsafe.</p> <p></p> <p>Additionally, if the ego vehicle has just exited the turn lane of an intersection and its distance from the intersection is within the <code>backward_length_from_intersection</code>, as shown in the figure below, the generated candidate paths will also be marked as unsafe.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#delay-lane-change-check","title":"Delay Lane Change Check","text":"<p>In certain situations, when there are stopped vehicles along the target lane ahead of Ego vehicle, to avoid getting stuck, it is desired to perform the lane change maneuver after the stopped vehicle. To do so, all static objects ahead of ego along the target lane are checked in order from closest to furthest, if any object satisfies the following conditions, lane change will be delayed and candidate path will be rejected.</p> <ol> <li>The distance from object to terminal end is sufficient to perform lane change</li> <li>The distance to object is less than the lane changing length</li> <li>The distance from object to next object is sufficient to perform lane change</li> </ol> <p>If the parameter <code>check_only_parked_vehicle</code> is set to <code>true</code>, the check will only consider objects which are determined as parked. More details on parked vehicle detection can be found in documentation for avoidance module.</p> <p>The following flow chart illustrates the delay lane change check.</p> <p></p> <p>The following figures demonstrate different situations under which delay action will or won't be triggered. In each figure the target lane vehicles are assumed to be stopped. The target lane vehicle responsible for triggering the delay action is marked with blue color.</p> <ul> <li>Delay lane change will be triggered as there is sufficient distance ahead.</li> </ul> <p></p> <ul> <li>Delay lane change will NOT be triggered as there is no sufficient distance ahead.</li> </ul> <p></p> <ul> <li>Delay lane change will be triggered by fist NPC as there is sufficient distance ahead.</li> </ul> <p></p> <ul> <li>Delay lane change will be triggered by second NPC as there is sufficient distance ahead.</li> </ul> <p></p> <ul> <li>Delay lane change will NOT be triggered as there is no sufficient distance ahead.</li> </ul> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#collision-check","title":"Collision Check","text":"<p>To ensure the safety of the lane change candidate path an RSS check is performed against the surrounding predicted objects. More details on the collision check implementation can be found in safety check utils explanation</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#collision-check-in-prepare-phase","title":"Collision Check In Prepare Phase","text":"<p>The collision check can be applied to the lane changing section only or to the entire candidate path by enabling the flag <code>enable_collision_check_at_prepare_phase</code>. Enabling this flag ensures that the ego vehicle secures enough inter-vehicle distance ahead of target lane rear vehicle before attempting a lane change. The following image illustrates the differences between the <code>false</code> and <code>true</code> cases.</p> <p></p> <p>Note</p> <p>When ego vehicles is stuck, i.e it is stopped, and there is an obstacle in front or is at end of current lane. Then the safety check for lane change is relaxed compared to normal times.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#stopping-behavior","title":"Stopping Behavior","text":"<p>The stopping behavior of the ego vehicle is determined based on various factors, such as the number of lane changes required, the presence of obstacles, and the position of blocking objects in relation to the lane change plan. The objective is to choose a suitable stopping point that allows for a safe and effective lane change while adapting to different traffic scenarios.</p> <p>The following flowchart and subsections explain the conditions for deciding where to insert a stop point when an obstacle is ahead.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#ego-vehicles-stopping-position-when-an-object-exists-ahead","title":"Ego vehicle's stopping position when an object exists ahead","text":"<p>When the ego vehicle encounters an obstacle ahead, it stops while maintaining a safe distance to prepare for a possible lane change. The exact stopping position depends on factors like whether the target lane is clear or if the lane change needs to be delayed. The following explains how different stopping scenarios are handled:</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#when-the-near-the-end-of-the-lane-change","title":"When the near the end of the lane change","text":"<p>Whether the target lane has obstacles or is clear, the ego vehicle stops while keeping a safe distance from the obstacle ahead, ensuring there is enough room for the lane change.</p> <p></p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#when-the-ego-vehicle-is-not-near-the-end-of-the-lane-change","title":"When the ego vehicle is not near the end of the lane change","text":"<p>The ego vehicle stops while maintaining a safe distance from the obstacle ahead, ensuring there is enough space for a lane change.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#ego-vehicles-stopping-position-when-an-object-exists-in-the-lane-changing-section","title":"Ego vehicle's stopping position when an object exists in the lane changing section","text":"<p>If there are objects within the lane change section of the target lane, the ego vehicle stops closer to the obstacle ahead, without maintaining the usual distance for a lane change.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#when-near-the-end-of-the-lane-change","title":"When near the end of the lane change","text":"<p>Regardless of whether there are obstacles in the target lane, the ego vehicle stops while keeping a safe distance from the obstacle ahead, allowing for the lane change.</p> <p></p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#when-not-near-the-end-of-the-lane-change","title":"When not near the end of the lane change","text":"<p>If there are no obstacles in the lane change section of the target lane, the ego vehicle stops while keeping a safe distance from the obstacle ahead to accommodate the lane change.</p> <p></p> <p>If there are obstacles within the lane change section of the target lane, the ego vehicle stops closer to the obstacle ahead, without keeping the usual distance needed for a lane change.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#when-the-target-lane-is-far-away","title":"When the target lane is far away","text":"<p>If the target lane for the lane change is far away and not next to the current lane, the ego vehicle stops closer to the obstacle ahead, as maintaining the usual distance for a lane change is not necessary.</p> <p></p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#when-target-lane-is-blocked-and-multiple-lane-changes","title":"When target lane is blocked and multiple lane changes","text":"<p>When ego vehicle needs to perform multiple lane changes to reach the <code>preferred_lane</code>, and the <code>target_lane</code> is blocked, for example, due to incoming vehicles, the ego vehicle must stop at a sufficient distance from the lane end and wait for the <code>target_lane</code> to clear. The minimum stopping distance can be computed from shift length and minimum lane changing velocity.</p> <pre><code>lane_changing_time = f(shift_length, lat_acceleration, lat_jerk)\nminimum_lane_change_distance = minimum_prepare_length + minimum_lane_changing_velocity * lane_changing_time + lane_change_finish_judge_buffer\n</code></pre> <p>The following figure illustrates when the lane is blocked in multiple lane changes cases.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#aborting-lane-change","title":"Aborting Lane Change","text":"<p>Once the lane change path is approved, there are several situations where we may need to abort the maneuver. The abort process is triggered when any of the following conditions is met</p> <ol> <li>The ego vehicle is near a traffic light, crosswalk, or intersection, and it is possible to complete the lane change after the ego vehicle passes these areas.</li> <li>The target object list is updated, requiring us to delay lane change</li> <li>The lane change is forcefully canceled via RTC.</li> <li>The path has become unsafe. (see Checking Approved Path Safety)</li> </ol> <p>Furthermore, if the path has become unsafe, there are three possible outcomes for the maneuver:</p> <ol> <li>CANCEL: The approved path has become unsafe while ego is still in prepare phase. Lane change path is canceled, and the ego vehicle resumes its previous maneuver.</li> <li>ABORT: The approved path has become unsafe while ego is in lane changing phase. Lane change module generates a return path to bring the ego vehicle back to its current lane.</li> <li>CRUISE or STOP: If aborting is not feasible, the ego vehicle continues with the lane change. Another module should decide whether the ego vehicle should cruise or stop in this scenario.</li> </ol> <p>CANCEL can be enabled by setting the <code>cancel.enable_on_prepare_phase</code> flag to <code>true</code>, and ABORT can be enabled by setting the <code>cancel.enable_on_lane_changing_phase</code> flag to true.</p> <p>Warning</p> <p>Enabling CANCEL is a prerequisite for enabling ABORT.</p> <p>Warning</p> <p>When CANCEL is disabled, all maneuvers will default to either CRUISE or STOP.</p> <p>The chart shows the high level flow of the lane change abort process.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#preventing-oscillating-paths-when-unsafe","title":"Preventing Oscillating Paths When Unsafe","text":"<p>Lane change paths can oscillate when conditions switch between safe and unsafe. To address this, a hysteresis count check is added before executing an abort maneuver. When the path is unsafe, the <code>unsafe_hysteresis_count_</code> increases. If it exceeds the <code>unsafe_hysteresis_threshold</code>, an abort condition check is triggered. This logic stabilizes the path approval process and prevents abrupt changes caused by temporary unsafe conditions.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#evaluating-ego-vehicles-position-to-prevent-abrupt-maneuvers","title":"Evaluating Ego Vehicle's Position to Prevent Abrupt Maneuvers","text":"<p>To avoid abrupt maneuvers during CANCEL or ABORT, the lane change module ensures the ego vehicle can safely return to the original lane. This is done through geometric checks that verify whether the ego vehicle remains within the lane boundaries.</p> <p>The edges of the ego vehicle\u2019s footprint are compared against the boundary of the current lane to determine if they exceed the overhang tolerance, <code>cancel.overhang_tolerance</code>. If the distance from any edge of the footprint to the boundary exceeds this threshold, the vehicle is considered to be diverging.</p> <p>The footprints checked against the lane boundary include:</p> <ol> <li>Current Footprint: Based on the ego vehicle's current position.</li> <li> <p>Future Footprint: Based on the ego vehicle's estimated position after traveling a distance, calculated as \\(\ud835\udc51_{est}=\ud835\udc63_{ego} \\cdot \\Delta_{\ud835\udc61}\\), where</p> <ul> <li>\\(v_{ego}\\) is ego vehicle's current velocity</li> <li>\\(\\Delta_{t}\\) is parameterized time constant value, <code>cancel.delta_time</code>.</li> </ul> <p>as depicted in the following diagram</p> <p></p> </li> </ol> <p>Note</p> <p>The ego vehicle is considered capable of safely returning to the current lane only if BOTH the current and future footprint checks are <code>true</code>.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#checking-approved-path-safety","title":"Checking Approved Path Safety","text":"<p>The lane change module samples accelerations along the path and recalculates velocity to perform safety checks. The motivation for this feature is explained in the Limitation section.</p> <p>The computation of sampled accelerations is as follows:</p> <p>Let</p> \\[ \\text{resolution} = \\frac{a_{\\text{min}} - a_{\\text{LC}}}{N} \\] <p>The sequence of sampled accelerations is then given as</p> \\[ \\text{acc} = a_{\\text{LC}} + k \\cdot \\text{resolution}, \\quad k = [0, N] \\] <p>where</p> <ul> <li>\\(a_{\\text{min}}\\), is the minimum of the parameterized global acceleration constant <code>normal.min_acc</code> or the parameterized constant <code>trajectory.min_longitudinal_acceleration</code>.</li> <li>\\(a_{\\text{LC}}\\) is the acceleration used to generate the approved path.</li> <li>\\(N\\) is the parameterized constant <code>cancel.deceleration_sampling</code></li> </ul> <p>If none of the sampled accelerations pass the safety check, the lane change path will be canceled, subject to the hysteresis check.</p> <p>Note</p> <p>Applying this fix allows the vehicle to change lanes more easily behind a leading object.</p> <p>Warning</p> <p>Although the safety check assumes deceleration, it actually executes the original path velocity.</p> <p>The behavior module assumes that downstream modules (e.g., obstacle stop, cruise planner) will handle actual velocity adjustments. Because of this, deceleration sampling is applied only to leading objects, not trailing or adjacent ones.</p> <ul> <li>For leading objects, secondary safety layers exist \u2014 for example, obstacle stop or cruise planner modules that can modify the velocity profile in response to sudden deceleration.</li> <li>For trailing or nearby objects, such mechanisms do not exist (obstacle stop and cruise do not apply to trailing objects).</li> </ul> <p>Therefore, applying deceleration sampling in these cases could lead to false negatives, i.e.: the safety check would assume ego is decelerating, when in reality, ego cannot decelerate.</p> <p>In practice, other modules (e.g., run out) may occasionally cause ego to decelerate, indirectly affecting safety check behavior for trailing objects. However, these activations are situation-dependent and not guaranteed. Hence, no deceleration sampling is applied to trailing objects.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#cancel","title":"Cancel","text":"<p>Cancelling lane change is possible as long as the ego vehicle is in the prepare phase and has not started deviating from the current lane center line. When lane change is canceled, the approved path is reset. After the reset, the ego vehicle will return to following the original reference path (the last approved path before the lane change started), as illustrated in the following image:</p> <p></p> <p>The following parameters can be configured to tune the behavior of the cancel process:</p> <ol> <li>Safety constraints for cancel.</li> <li>Safety constraints for parked vehicle.</li> </ol> <p>Note</p> <p>To ensure feasible behavior, all safety constraint values must be equal to or less than their corresponding parameters in the execution settings.</p> <ul> <li>The closer the values, the more conservative the lane change behavior will be. This means it will be easier to cancel the lane change but harder for the ego vehicle to complete a lane change.</li> <li>The larger the difference, the more aggressive the lane change behavior will be. This makes it harder to cancel the lane change but easier for the ego vehicle to change lanes.</li> </ul>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#abort","title":"Abort","text":"<p>During the prepare phase, the ego vehicle follows the previously approved path. However, once the ego vehicle begins the lane change, its heading starts to diverge from this path. Resetting to the previously approved path in this situation would cause abrupt steering, as the controller would attempt to rapidly realign the vehicle with the reference trajectory.</p> <p>Instead, the lane change module generates an abort path. This return path is specifically designed to guide the ego vehicle back to the current lane, avoiding any sudden maneuvers. The following image provides an illustration of the abort process.</p> <p></p> <p>The abort path is generated by shifting the approved lane change path using the path shifter. This ensures the continuity in lateral velocity, and prevents abrupt changes in the vehicle\u2019s movement. The abort start shift and abort end shift are computed as follows:</p> <ol> <li>Start Shift: \\(d_{start}^{abort} = v_{ego} \\cdot \\Delta_{t}\\)</li> <li>End Shift: \\(d_{end}^{abort} = v_{ego} \\cdot ( \\Delta_{t} + t_{end} )\\)</li> </ol> <ul> <li>\\(v_{ego}\\) is ego vehicle's current velocity</li> <li>\\(\\Delta_{t}\\) is parameterized time constant value, <code>cancel.delta_time</code>.</li> <li>\\(t_{end}\\) is the parameterized time constant value, <code>cancel.duration</code>.</li> </ul> <p>as depicted in the following diagram</p> <p></p> <p>Note</p> <p>When executing the abort process, comfort is not a primary concern. However, due to safety considerations, limited real-world testing has been conducted to tune or validate this parameter. Currently, the maximum lateral jerk is set to an arbitrary value. To avoid generating a path with excessive lateral jerk, this value can be configured using <code>cancel.max_lateral_jerk</code>.</p> <p>Note</p> <p>Lane change module returns <code>ModuleStatus::FAILURE</code> once abort is completed.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#stopcruise","title":"Stop/Cruise","text":"<p>Once canceling or aborting the lane change is no longer an option, the ego vehicle will proceed with the lane change. This can happen in the following situations:</p> <ul> <li>The ego vehicle is performing a lane change near a terminal or dead-end, making it impossible to return to the original lane. In such cases, completing the lane change is necessary.</li> <li>If safety parameters are tuned too aggressively, it becomes harder to cancel or abort the lane change. This reduces tolerance for unexpected behaviors from surrounding vehicles, such as a trailing vehicle in the target lane suddenly accelerating or a leading vehicle suddenly decelerating. Aggressive settings leave less room for error during the maneuver.</li> </ul> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#parameters","title":"Parameters","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#essential-lane-change-parameters","title":"Essential lane change parameters","text":"<p>The following parameters are configurable in lane_change.param.yaml</p> Name Unit Type Description Default value <code>time_limit</code> [ms] double Time limit for lane change candidate path generation 50.0 <code>backward_lane_length</code> [m] double The backward length to check incoming objects in lane change target lane. 200.0 <code>backward_length_buffer_for_end_of_lane</code> [m] double The end of lane buffer to ensure ego vehicle has enough distance to start lane change 3.0 <code>backward_length_buffer_for_blocking_object</code> [m] double The end of lane buffer to ensure ego vehicle has enough distance to start lane change when there is an object in front 3.0 <code>backward_length_from_intersection</code> [m] double Distance threshold from the last intersection to invalidate or cancel the lane change path 5.0 <code>enable_stopped_vehicle_buffer</code> [-] bool If true, will keep enough distance from current lane front stopped object to perform lane change when possible true <code>trajectory.max_prepare_duration</code> [s] double The maximum preparation time for the ego vehicle to be ready to perform lane change. 4.0 <code>trajectory.min_prepare_duration</code> [s] double The minimum preparation time for the ego vehicle to be ready to perform lane change. 2.0 <code>trajectory.lateral_jerk</code> [m/s3] double Lateral jerk value for lane change path generation 0.5 <code>trajectory.minimum_lane_changing_velocity</code> [m/s] double Minimum speed during lane changing process. 2.78 <code>trajectory.lon_acc_sampling_num</code> [-] int Number of possible lane-changing trajectories that are being influenced by longitudinal acceleration 3 <code>trajectory.lat_acc_sampling_num</code> [-] int Number of possible lane-changing trajectories that are being influenced by lateral acceleration 3 <code>trajectory.max_longitudinal_acc</code> [m/s2] double maximum longitudinal acceleration for lane change 1.0 <code>trajectory.min_longitudinal_acc</code> [m/s2] double maximum longitudinal deceleration for lane change -1.0 <code>trajectory.lane_changing_decel_factor</code> [-] double longitudinal deceleration factor during lane changing phase 0.5 <code>trajectory.th_prepare_curvature</code> [-] double If the maximum curvature of the prepare segment exceeds the threshold, the prepare segment is invalid. 0.03 <code>min_length_for_turn_signal_activation</code> [m] double Turn signal will be activated if the ego vehicle approaches to this length from minimum lane change length 10.0 <code>lateral_acceleration.velocity</code> [m/s] double Reference velocity for lateral acceleration calculation (look up table) [0.0, 4.0, 10.0] <code>lateral_acceleration.min_values</code> [m/s2] double Min lateral acceleration values corresponding to velocity (look up table) [0.4, 0.4, 0.4] <code>lateral_acceleration.max_values</code> [m/s2] double Max lateral acceleration values corresponding to velocity (look up table) [0.65, 0.65, 0.65]"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#parameter-to-judge-if-lane-change-is-completed","title":"Parameter to judge if lane change is completed","text":"<p>The following parameters are used to judge lane change completion.</p> Name Unit Type Description Default value <code>lane_change_finish_judge_buffer</code> [m] double The longitudinal distance starting from the lane change end pose. 2.0 <code>finish_judge_lateral_threshold</code> [m] double The lateral distance from targets lanes' centerline. Used in addition with <code>finish_judge_lateral_angle_deviation</code> 0.1 <code>finish_judge_lateral_angle_deviation</code> [deg] double Ego angle deviation with reference to target lanes' centerline. Used in addition with <code>finish_judge_lateral_threshold</code> 2.0"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#lane-change-regulations","title":"Lane change regulations","text":"Name Unit Type Description Default value <code>regulation.crosswalk</code> [-] boolean Considers lane change regulation at crosswalks. If set to <code>true</code>, lane changing are disabled at crosswalks. true <code>regulation.intersection</code> [-] boolean Considers lane change regulation at intersections. If set to <code>true</code>, lane changing are disabled at intersections. true <code>regulation.traffic_light</code> [-] boolean Considers lane change regulation at traffic lights. If set to <code>true</code>, lane changing are disabled near traffic lights. true"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#ego-vehicle-stuck-detection","title":"Ego vehicle stuck detection","text":"Name Unit Type Description Default value <code>stuck_detection.velocity</code> [m/s] double Velocity threshold for ego vehicle stuck detection 0.1 <code>stuck_detection.stop_time</code> [s] double Stop time threshold for ego vehicle stuck detection 3.0"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#delay-lane-change","title":"Delay Lane Change","text":"Name Unit Type Description Default value <code>delay_lane_change.enable</code> [-] bool Flag to enable/disable lane change delay feature true <code>delay_lane_change.check_only_parked_vehicle</code> [-] bool Flag to limit delay feature for only parked vehicles false <code>delay_lane_change.min_road_shoulder_width</code> [m] double Width considered as road shoulder if lane doesn't have road shoulder when checking for parked vehicle 0.5 <code>delay_lane_change.th_parked_vehicle_shift_ratio</code> [-] double Stopped vehicles beyond this distance ratio from center line will be considered as parked 0.6"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#terminal-lane-change-path_1","title":"Terminal Lane Change Path","text":"<p>The following parameters are used to configure terminal lane change path feature.</p> Name Unit Type Description Default value <code>terminal_path.enable</code> [-] bool Flag to enable/disable terminal path feature true <code>terminal_path.disable_near_goal</code> [-] bool Flag to disable terminal path feature if ego is near goal true <code>terminal_path.stop_at_boundary</code> [-] bool If true, ego will stop at current lane boundary instead of middle of lane false"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#generating-lane-changing-path-using-frenet-planner","title":"Generating Lane Changing Path using Frenet Planner","text":"<p>Warning</p> <p>Only applicable when ego is near terminal start</p> Name Unit Type Description Default value <code>frenet.enable</code> [-] bool Flag to enable/disable frenet planner when ego is near terminal start. true <code>frenet.use_entire_remaining_distance</code> [-] bool Flag to configure lc length, if true entire remaining distance to lane end is used for lc path generation, else will generate path assuming minimum lc length false <code>frenet.th_yaw_diff</code> [deg] double If the yaw diff between of the prepare segment's end and lane changing segment's start exceed the threshold , the lane changing segment is invalid. 10.0 <code>frenet.th_curvature_smoothing</code> [-] double Filters and appends target path points with curvature below the threshold to candidate path. 0.1 <code>frenet.th_average_curvature</code> [-] double Remove path with average curvature above the threshold. Path only removed if there is more than 1 candidate, and the first path is always kept. 0.015"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#collision-checks","title":"Collision checks","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#target-objects","title":"Target Objects","text":"Name Unit Type Description Default value <code>target_object.car</code> [-] boolean Include car objects for safety check true <code>target_object.truck</code> [-] boolean Include truck objects for safety check true <code>target_object.bus</code> [-] boolean Include bus objects for safety check true <code>target_object.trailer</code> [-] boolean Include trailer objects for safety check true <code>target_object.unknown</code> [-] boolean Include unknown objects for safety check true <code>target_object.bicycle</code> [-] boolean Include bicycle objects for safety check true <code>target_object.motorcycle</code> [-] boolean Include motorcycle objects for safety check true <code>target_object.pedestrian</code> [-] boolean Include pedestrian objects for safety check true"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#common","title":"common","text":"Name Unit Type Description Default value <code>safety_check.lane_expansion.left_offset</code> [m] double Expand the left boundary of the detection area, allowing objects previously outside on the left to be detected and registered as targets. 0.0 <code>safety_check.lane_expansion.right_offset</code> [m] double Expand the right boundary of the detection area, allowing objects previously outside on the right to be detected and registered as targets. 0.0"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#additional-parameters","title":"Additional parameters","text":"Name Unit Type Description Default value <code>collision_check.enable_for_prepare_phase.general_lanes</code> [-] boolean Perform collision check starting from the prepare phase for situations not explicitly covered by other settings (e.g., intersections). If <code>false</code>, collision check only evaluated for lane changing phase. false <code>collision_check.enable_for_prepare_phase.intersection</code> [-] boolean Perform collision check starting from prepare phase when ego is in intersection. If <code>false</code>, collision check only evaluated for lane changing phase. true <code>collision_check.enable_for_prepare_phase.turns</code> [-] boolean Perform collision check starting from prepare phase when ego is in lanelet with turn direction tags. If <code>false</code>, collision check only evaluated for lane changing phase. true <code>collision_check.check_current_lanes</code> [-] boolean If true, the lane change module always checks objects in the current lanes for collision assessment. If false, it only checks objects in the current lanes when the ego vehicle is stuck. false <code>collision_check.check_other_lanes</code> [-] boolean If true, the lane change module includes objects in other lanes when performing collision assessment. false <code>collision_check.use_all_predicted_paths</code> [-] boolean If false, use only the predicted path that has the maximum confidence. true <code>collision_check.prediction_time_resolution</code> [s] double Time resolution for object's path interpolation and collision check. 0.5 <code>collision_check.yaw_diff_threshold</code> [rad] double Maximum yaw difference between predicted ego pose and predicted object pose when executing rss-based collision checking 3.1416 <code>collision_check.th_incoming_object_yaw</code> [rad] double Maximum yaw difference between current ego pose and current object pose. Objects with a yaw difference exceeding this value are excluded from the safety check. 2.3562"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#safety-constraints-when-ego-is-in-prepare-phase","title":"safety constraints when ego is in prepare phase","text":"Name Unit Type Description Default value <code>safety_check.prepare.expected_front_deceleration</code> [m/s^2] double The front object's maximum deceleration when the front vehicle perform sudden braking. (*1) -1.0 <code>safety_check.prepare.expected_rear_deceleration</code> [m/s^2] double The rear object's maximum deceleration when the rear vehicle perform sudden braking. (*1) -1.0 <code>safety_check.prepare.rear_vehicle_reaction_time</code> [s] double The reaction time of the rear vehicle driver which starts from the driver noticing the sudden braking of the front vehicle until the driver step on the brake. 1.0 <code>safety_check.prepare.rear_vehicle_safety_time_margin</code> [s] double The time buffer for the rear vehicle to come into complete stop when its driver perform sudden braking. 0.8 <code>safety_check.prepare.lateral_distance_max_threshold</code> [m] double The lateral distance threshold that is used to determine whether lateral distance between two object is enough and whether lane change is safe. 0.5 <code>safety_check.prepare.longitudinal_distance_min_threshold</code> [m] double The longitudinal distance threshold that is used to determine whether longitudinal distance between two object is enough and whether lane change is safe. 1.0 <code>safety_check.prepare.longitudinal_velocity_delta_time</code> [m] double The time multiplier that is used to compute the actual gap between vehicle at each predicted points (not RSS distance) 0.0 <code>safety_check.prepare.extended_polygon_policy</code> [-] string Policy used to determine the polygon shape for the safety check. Available options are: <code>rectangle</code> or <code>along-path</code>. <code>rectangle</code>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#safety-constraints-during-lane-change-path-is-computed","title":"safety constraints during lane change path is computed","text":"Name Unit Type Description Default value <code>safety_check.execution.expected_front_deceleration</code> [m/s^2] double The front object's maximum deceleration when the front vehicle perform sudden braking. (*1) -1.0 <code>safety_check.execution.expected_rear_deceleration</code> [m/s^2] double The rear object's maximum deceleration when the rear vehicle perform sudden braking. (*1) -1.0 <code>safety_check.execution.rear_vehicle_reaction_time</code> [s] double The reaction time of the rear vehicle driver which starts from the driver noticing the sudden braking of the front vehicle until the driver step on the brake. 2.0 <code>safety_check.execution.rear_vehicle_safety_time_margin</code> [s] double The time buffer for the rear vehicle to come into complete stop when its driver perform sudden braking. 1.0 <code>safety_check.execution.lateral_distance_max_threshold</code> [m] double The lateral distance threshold that is used to determine whether lateral distance between two object is enough and whether lane change is safe. 2.0 <code>safety_check.execution.longitudinal_distance_min_threshold</code> [m] double The longitudinal distance threshold that is used to determine whether longitudinal distance between two object is enough and whether lane change is safe. 3.0 <code>safety_check.execution.longitudinal_velocity_delta_time</code> [m] double The time multiplier that is used to compute the actual gap between vehicle at each predicted points (not RSS distance) 0.8 <code>safety_check.execution.extended_polygon_policy</code> [-] string Policy used to determine the polygon shape for the safety check. Available options are: <code>rectangle</code> or <code>along-path</code>. <code>rectangle</code>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#safety-constraints-specifically-for-stopped-or-parked-vehicles","title":"safety constraints specifically for stopped or parked vehicles","text":"Name Unit Type Description Default value <code>safety_check.parked.expected_front_deceleration</code> [m/s^2] double The front object's maximum deceleration when the front vehicle perform sudden braking. (*1) -1.0 <code>safety_check.parked.expected_rear_deceleration</code> [m/s^2] double The rear object's maximum deceleration when the rear vehicle perform sudden braking. (*1) -2.0 <code>safety_check.parked.rear_vehicle_reaction_time</code> [s] double The reaction time of the rear vehicle driver which starts from the driver noticing the sudden braking of the front vehicle until the driver step on the brake. 1.0 <code>safety_check.parked.rear_vehicle_safety_time_margin</code> [s] double The time buffer for the rear vehicle to come into complete stop when its driver perform sudden braking. 0.8 <code>safety_check.parked.lateral_distance_max_threshold</code> [m] double The lateral distance threshold that is used to determine whether lateral distance between two object is enough and whether lane change is safe. 1.0 <code>safety_check.parked.longitudinal_distance_min_threshold</code> [m] double The longitudinal distance threshold that is used to determine whether longitudinal distance between two object is enough and whether lane change is safe. 3.0 <code>safety_check.parked.longitudinal_velocity_delta_time</code> [m] double The time multiplier that is used to compute the actual gap between vehicle at each predicted points (not RSS distance) 0.8 <code>safety_check.parked.extended_polygon_policy</code> [-] string Policy used to determine the polygon shape for the safety check. Available options are: <code>rectangle</code> or <code>along-path</code>. <code>rectangle</code>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#safety-constraints-to-cancel-lane-change-path","title":"safety constraints to cancel lane change path","text":"Name Unit Type Description Default value <code>safety_check.cancel.expected_front_deceleration</code> [m/s^2] double The front object's maximum deceleration when the front vehicle perform sudden braking. (*1) -1.0 <code>safety_check.cancel.expected_rear_deceleration</code> [m/s^2] double The rear object's maximum deceleration when the rear vehicle perform sudden braking. (*1) -2.0 <code>safety_check.cancel.rear_vehicle_reaction_time</code> [s] double The reaction time of the rear vehicle driver which starts from the driver noticing the sudden braking of the front vehicle until the driver step on the brake. 1.5 <code>safety_check.cancel.rear_vehicle_safety_time_margin</code> [s] double The time buffer for the rear vehicle to come into complete stop when its driver perform sudden braking. 0.8 <code>safety_check.cancel.lateral_distance_max_threshold</code> [m] double The lateral distance threshold that is used to determine whether lateral distance between two object is enough and whether lane change is safe. 1.0 <code>safety_check.cancel.longitudinal_distance_min_threshold</code> [m] double The longitudinal distance threshold that is used to determine whether longitudinal distance between two object is enough and whether lane change is safe. 2.5 <code>safety_check.cancel.longitudinal_velocity_delta_time</code> [m] double The time multiplier that is used to compute the actual gap between vehicle at each predicted points (not RSS distance) 0.6 <code>safety_check.cancel.extended_polygon_policy</code> [-] string Policy used to determine the polygon shape for the safety check. Available options are: <code>rectangle</code> or <code>along-path</code>. <code>rectangle</code>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#safety-constraints-used-during-lane-change-path-is-computed-when-ego-is-stuck","title":"safety constraints used during lane change path is computed when ego is stuck","text":"Name Unit Type Description Default value <code>safety_check.stuck.expected_front_deceleration</code> [m/s^2] double The front object's maximum deceleration when the front vehicle perform sudden braking. (*1) -1.0 <code>safety_check.stuck.expected_rear_deceleration</code> [m/s^2] double The rear object's maximum deceleration when the rear vehicle perform sudden braking. (*1) -1.0 <code>safety_check.stuck.rear_vehicle_reaction_time</code> [s] double The reaction time of the rear vehicle driver which starts from the driver noticing the sudden braking of the front vehicle until the driver step on the brake. 2.0 <code>safety_check.stuck.rear_vehicle_safety_time_margin</code> [s] double The time buffer for the rear vehicle to come into complete stop when its driver perform sudden braking. 1.0 <code>safety_check.stuck.lateral_distance_max_threshold</code> [m] double The lateral distance threshold that is used to determine whether lateral distance between two object is enough and whether lane change is safe. 2.0 <code>safety_check.stuck.longitudinal_distance_min_threshold</code> [m] double The longitudinal distance threshold that is used to determine whether longitudinal distance between two object is enough and whether lane change is safe. 3.0 <code>safety_check.stuck.longitudinal_velocity_delta_time</code> [m] double The time multiplier that is used to compute the actual gap between vehicle at each predicted points (not RSS distance) 0.8 <code>safety_check.stuck.extended_polygon_policy</code> [-] string Policy used to determine the polygon shape for the safety check. Available options are: <code>rectangle</code> or <code>along-path</code>. <code>rectangle</code> <p>(*1) the value must be negative.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#abort-lane-change","title":"Abort lane change","text":"<p>The following parameters are configurable in <code>lane_change.param.yaml</code>.</p> Name Unit Type Description Default value <code>cancel.enable_on_prepare_phase</code> [-] boolean Enable cancel lane change true <code>cancel.enable_on_lane_changing_phase</code> [-] boolean Enable abort lane change. false <code>cancel.delta_time</code> [s] double The time taken to start steering to return to the center line. 3.0 <code>cancel.duration</code> [s] double The time taken to complete returning to the center line. 3.0 <code>cancel.max_lateral_jerk</code> [m/sss] double The maximum lateral jerk for abort path 1000.0 <code>cancel.overhang_tolerance</code> [m] double Lane change cancel is prohibited if the vehicle head exceeds the lane boundary more than this tolerance distance 0.0 <code>cancel.unsafe_hysteresis_threshold</code> [-] int threshold that helps prevent frequent switching between safe and unsafe decisions 10 <code>cancel.deceleration_sampling_num</code> [-] int Number of deceleration patterns to check safety to cancel lane change 5"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#debug","title":"Debug","text":"<p>The following parameters are configurable in <code>lane_change.param.yaml</code>.</p> Name Unit Type Description Default value <code>publish_debug_marker</code> [-] boolean Flag to publish debug marker false"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#debug-marker-visualization","title":"Debug Marker &amp; Visualization","text":"<p>To enable the debug marker, execute (no restart is needed)</p> <pre><code>ros2 param set /planning/scenario_planning/lane_driving/behavior_planning/behavior_path_planner lane_change.publish_debug_marker true\n</code></pre> <p>or simply set the <code>publish_debug_marker</code> to <code>true</code> in the <code>lane_change.param.yaml</code> for permanent effect (restart is needed).</p> <p>Then add the marker</p> <pre><code>/planning/scenario_planning/lane_driving/behavior_planning/behavior_path_planner/debug/lane_change_left\n</code></pre> <p>in <code>rviz2</code>.</p> <p></p> <p></p> <p></p> <p>Available information</p> <ol> <li>Ego to object relation, plus safety check information</li> <li>Ego vehicle interpolated pose up to the latest safety check position.</li> <li>Object is safe or not, shown by the color of the polygon (Green = Safe, Red = unsafe)</li> <li>Valid candidate paths.</li> <li>Position when lane changing start and end.</li> </ol>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_lane_change_module/#limitation","title":"Limitation","text":"<ol> <li>When a lane change is canceled, the lane change module returns <code>ModuleStatus::FAILURE</code>. As the module is removed from the approved module stack (see Failure modules), a new instance of the lane change module is initiated. Due to this, any information stored prior to the reset is lost. For example, the <code>lane_change_prepare_duration</code> in the <code>TransientData</code> is reset to its maximum value.</li> <li>The lane change module has no knowledge of any velocity modifications introduced to the path after it is approved. This is because other modules may add deceleration points after subscribing to the behavior path planner output, and the final velocity is managed by the velocity smoother. Since this limitation affects CANCEL, the lane change module mitigates it by sampling accelerations along the approved lane change path. These sampled accelerations are used during safety checks to estimate the velocity that might occur if the ego vehicle decelerates.</li> <li>Ideally, the abort path should account for whether its execution would affect trailing vehicles in the current lane. However, the lane change module does not evaluate such interactions or assess whether the abort path is safe. As a result, the abort path is not guaranteed to be safe. To minimize the risk of unsafe situations, the abort maneuver is only permitted if the ego vehicle has not yet diverged from the current lane.</li> <li>Due to limited resources, the abort path logic is not fully optimized. The generated path may overshoot, causing the return trajectory to slightly shift toward the opposite lane. This can be dangerous, especially if the opposite lane has traffic moving in the opposite direction. Furthermore, the logic does not account for different vehicle types, which can lead to varying effects. For instance, the behavior might differ significantly between a bus and a small passenger car.</li> </ol>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/","title":"Behavior Path Planner","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/#behavior-path-planner","title":"Behavior Path Planner","text":"<p>The Behavior Path Planner's main objective is to significantly enhance the safety of autonomous vehicles by minimizing the risk of accidents. It improves driving efficiency through time conservation and underpins reliability with its rule-based approach. Additionally, it allows users to integrate their own custom behavior modules or use it with different types of vehicles, such as cars, buses, and delivery robots, as well as in various environments, from busy urban streets to open highways.</p> <p>The module begins by thoroughly analyzing the ego vehicle's current situation, including its position, speed, and surrounding environment. This analysis leads to essential driving decisions about lane changes or stopping and subsequently generates a path that is both safe and efficient. It considers road geometry, traffic rules, and dynamic conditions while also incorporating obstacle avoidance to respond to static and dynamic obstacles such as other vehicles, pedestrians, or unexpected roadblocks, ensuring safe navigation.</p> <p>Moreover, the planner responds to the behavior of other traffic participants, predicting their actions and accordingly adjusting the vehicle's path. This ensures not only the safety of the autonomous vehicle but also contributes to smooth traffic flow. Its adherence to traffic laws, including speed limits and compliance with traffic signals, further guarantees lawful and predictable driving behavior. The planner is also designed to minimize sudden or abrupt maneuvers, aiming for a comfortable and natural driving experience.</p> <p>Note</p> <p>The Planning Component Design documentation outlines the foundational philosophy guiding the design and future development of the Behavior Path Planner module. We strongly encourage readers to consult this document to understand the rationale behind its current configuration and the direction of its ongoing development.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/#purpose-use-cases","title":"Purpose / Use Cases","text":"<p>Essentially, the module has three primary responsibilities:</p> <ol> <li>Creating a path based on the traffic situation.</li> <li>Generating drivable area, i.e. the area within which the vehicle can maneuver.</li> <li>Generating turn signal commands to be relayed to the vehicle interface.</li> </ol>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/#features","title":"Features","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/#supported-scene-modules","title":"Supported Scene Modules","text":"<p>Behavior Path Planner has the following scene modules</p> Name Description Details Lane Following This module generates a reference path from lanelet centerline. LINK Static Obstacle Avoidance This module generates an avoidance path when there are objects that should be avoided. LINK Dynamic Obstacle Avoidance WIP LINK Avoidance By Lane Change This module generates a lane change path when there are objects that should be avoided. LINK Lane Change This module is performed when it is necessary and a collision check with other vehicles is cleared. LINK External Lane Change WIP LINK Goal Planner This module is performed when the ego vehicle is in a driving lane and the goal is in the shoulder lane. The ego vehicle will stop at the goal. LINK Start Planner This module is performed when the ego vehicle is stationary and the footprint of the ego vehicle is included in the shoulder lane. This module ends when the ego vehicle merges into the road. LINK Side Shift This module shifts the path to the left or right based on external instructions, intended for remote control applications. LINK <p>Note</p> <p>Click on the following images to view videos of their execution</p> <p></p> <p>Note</p> <p>Users can refer to Planning component design for some additional behavior.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/#how-to-add-or-implement-new-module","title":"How to add or implement new module","text":"<p>All scene modules are implemented by inheriting the base class <code>scene_module_interface.hpp</code>.</p> <p>Warning</p> <p>The remainder of this subsection is a work in progress (WIP).</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/#planner-manager","title":"Planner Manager","text":"<p>The Planner Manager's responsibilities include:</p> <ol> <li>Activating the relevant scene module in response to the specific situation faced by the autonomous vehicle. For example, when a parked vehicle blocks the ego vehicle's driving lane, the manager would engage the avoidance module.</li> <li>Managing the execution order when multiple modules are running simultaneously. For instance, if both the lane-changing and avoidance modules are operational, the manager decides which should take precedence.</li> <li>Merging paths from multiple modules when they are activated simultaneously and each generates its own path, thereby creating a single functional path.</li> </ol> <p>Note</p> <p>To check the scene module's transition \u2013 i.e., registered, approved and candidate modules \u2013 set <code>verbose: true</code> in the Behavior Path Planner configuration file.</p> <p></p> <p>Note</p> <p>For more in-depth information, refer to the Manager design document.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/#inputs-outputs-api","title":"Inputs / Outputs / API","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/#input","title":"Input","text":"Name Required? Type Description ~/input/odometry \u25cb <code>nav_msgs::msg::Odometry</code> For ego velocity ~/input/accel \u25cb <code>geometry_msgs::msg::AccelWithCovarianceStamped</code> For ego acceleration ~/input/objects \u25cb <code>autoware_perception_msgs::msg::PredictedObjects</code> Dynamic objects from the perception module ~/input/occupancy_grid_map \u25cb <code>nav_msgs::msg::OccupancyGrid</code> Occupancy grid map from the perception module. This is used for only the Goal Planner module ~/input/traffic_signals \u25cb <code>autoware_perception_msgs::msg::TrafficLightGroupArray</code> Traffic signal information from the perception module ~/input/vector_map \u25cb <code>autoware_map_msgs::msg::LaneletMapBin</code> Vector map information ~/input/route \u25cb <code>autoware_planning_msgs::msg::LaneletRoute</code> Current route from start to goal ~/input/scenario \u25cb <code>autoware_internal_planning_msgs::msg::Scenario</code> Launches Behavior Path Planner if current scenario == <code>Scenario:LaneDriving</code> ~/input/lateral_offset \u25b3 <code>tier4_planning_msgs::msg::LateralOffset</code> Lateral offset to trigger side shift ~/system/operation_mode/state \u25cb <code>autoware_adapi_v1_msgs::msg::OperationModeState</code> Allows the planning module to know if vehicle is in autonomous mode or if it can be controlled<sup>ref</sup> <ul> <li>\u25cb Mandatory: The planning module would not work if anyone of these were not present.</li> <li>\u25b3 Optional: Some modules would not work, but the planning module can still be operated.</li> </ul>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/#output","title":"Output","text":"Name Type Description QoS Durability ~/output/path <code>autoware_internal_planning_msgs::msg::PathWithLaneId</code> The path generated by modules <code>volatile</code> ~/output/turn_indicators_cmd <code>autoware_vehicle_msgs::msg::TurnIndicatorsCommand</code> Turn indicators command <code>volatile</code> ~/output/hazard_lights_cmd <code>autoware_vehicle_msgs::msg::HazardLightsCommand</code> Hazard lights command <code>volatile</code> ~/output/modified_goal <code>autoware_planning_msgs::msg::PoseWithUuidStamped</code> Output modified goal commands <code>transient_local</code> ~/output/reroute_availability <code>tier4_planning_msgs::msg::RerouteAvailability</code> The path the module is about to take. To be executed as soon as external approval is obtained <code>volatile</code>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/#debug","title":"Debug","text":"Name Type Description QoS Durability ~/debug/avoidance_debug_message_array <code>tier4_planning_msgs::msg::AvoidanceDebugMsgArray</code> Debug message for avoidance. Notifies users of reasons avoidance path cannot be generated <code>volatile</code> ~/debug/lane_change_debug_message_array <code>tier4_planning_msgs::msg::LaneChangeDebugMsgArray</code> Debug message for lane change. Notifies users of unsafe conditions during lane-changing process <code>volatile</code> ~/debug/maximum_drivable_area <code>visualization_msgs::msg::MarkerArray</code> Shows maximum static drivable area <code>volatile</code> ~/debug/turn_signal_info <code>visualization_msgs::msg::MarkerArray</code> TBA <code>volatile</code> ~/debug/bound <code>visualization_msgs::msg::MarkerArray</code> Debug for static drivable area <code>volatile</code> ~/planning/path_candidate/* <code>autoware_planning_msgs::msg::Path</code> The path before approval <code>volatile</code> ~/planning/path_reference/* <code>autoware_planning_msgs::msg::Path</code> Reference path generated by each module <code>volatile</code> <p>Note</p> <p>For specific information about which topics are being subscribed to and published, refer to behavior_path_planner.xml.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/#how-to-enable-or-disable-modules","title":"How to Enable or Disable Modules","text":"<p>Enabling and disabling the modules in the Behavior Path Planner is primarily managed through two key files: <code>default_preset.yaml</code> and <code>behavior_path_planner.launch.xml</code>.</p> <p>The <code>default_preset.yaml</code> file acts as a configuration file for enabling or disabling specific modules within the planner. It contains a series of arguments which represent the Behavior Path Planner's modules or features. For example:</p> <ul> <li><code>launch_static_obstacle_avoidance_module</code>: Set to <code>true</code> to enable the avoidance module, or <code>false</code> to disable it.</li> </ul> <p>Note</p> <p>Click here to view the <code>default_preset.yaml</code>.</p> <p>The <code>behavior_path_planner.launch.xml</code> file references the settings defined in <code>default_preset.yaml</code> to apply the configurations when the Behavior Path Planner's node is running. For instance, the parameter <code>static_obstacle_avoidance.enable_module</code> in the following segment corresponds to launch_static_obstacle_avoidance_module from <code>default_preset.yaml</code>:</p> <pre><code>&lt;param name=\"static_obstacle_avoidance.enable_module\" value=\"$(var launch_static_obstacle_avoidance_module)\"/&gt;\n</code></pre> <p>Therefore, to enable or disable a module, simply set the corresponding module in <code>default_preset.yaml</code> to <code>true</code> or <code>false</code>. These changes will be applied upon the next launch of Autoware.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/#generating-path","title":"Generating Path","text":"<p>A sophisticated methodology is used for path generation, particularly focusing on maneuvers like lane changes and avoidance. At the core of this design is the smooth lateral shifting of the reference path, achieved through a constant-jerk profile. This approach ensures a consistent rate of change in acceleration, facilitating smooth transitions and minimizing abrupt changes in lateral dynamics, crucial for passenger comfort and safety.</p> <p>The design involves complex mathematical formulations for calculating the lateral shift of the vehicle's path over time. These calculations include determining lateral displacement, velocity, and acceleration, while considering the vehicle's lateral acceleration and velocity limits. This is essential for ensuring that the vehicle's movements remain safe and manageable.</p> <p>The <code>ShiftLine</code> struct (as seen here) is utilized to represent points along the path where the lateral shift starts and ends. It includes details like the start and end points in absolute coordinates, the relative shift lengths at these points compared to the reference path, and the associated indexes on the reference path. This struct is integral to managing the path shifts, as it allows the path planner to dynamically adjust the trajectory based on the vehicle's current position and planned maneuver.</p> <p>Furthermore, the design and its implementation incorporate various equations and mathematical models to calculate essential parameters for the path shift. These include the total distance of the lateral shift, the maximum allowable lateral acceleration and jerk, and the total time required for the shift. Practical considerations are also noted, such as simplifying assumptions in the absence of a specific time interval for most lane change and avoidance cases.</p> <p>The shifted path generation logic enables the Behavior Path Planner to dynamically generate safe and efficient paths, precisely controlling the vehicle\u2019s lateral movements to ensure the smooth execution of lane changes and avoidance maneuvers. This careful planning and execution adhere to the vehicle's dynamic capabilities and safety constraints, maximizing efficiency and safety in autonomous vehicle navigation.</p> <p>Note</p> <p>If you're a math lover, refer to Path Generation Design for the nitty-gritty.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/#collision-assessment-safety-check","title":"Collision Assessment / Safety Check","text":"<p>The purpose of the collision assessment function in the Behavior Path Planner is to evaluate the potential for collisions with target objects across all modules. It is utilized in two scenarios:</p> <ol> <li>During candidate path generation, to ensure that the generated candidate path is collision-free.</li> <li>When the path is approved by the manager, and the ego vehicle is executing the current module. If the current situation is deemed unsafe, depending on each module's requirements, the planner will either cancel the execution or opt to execute another module.</li> </ol> <p>The safety check process involves several steps. Initially, it obtains the pose of the target object at a specific time, typically through interpolation of the predicted path. It then checks for any overlap between the ego vehicle and the target object at this time. If an overlap is detected, the path is deemed unsafe. The function also identifies which vehicle is in front by using the arc length along the given path. The function operates under the assumption that accurate data on the position, velocity, and shape of both the ego vehicle (the autonomous vehicle) and any target objects are available. It also relies on the yaw angle of each point in the predicted paths of these objects, which is expected to point towards the next path point.</p> <p>A critical part of the safety check is the calculation of the RSS (Responsibility-Sensitive Safety) distance-inspired algorithm. This algorithm considers factors such as reaction time, safety time margin, and the velocities and decelerations of both vehicles. Extended object polygons are created for both the ego and target vehicles. Notably, the rear object\u2019s polygon is extended by the RSS distance longitudinally and by a lateral margin. The function finally checks for overlap between this extended rear object polygon and the front object polygon. Any overlap indicates a potential unsafe situation.</p> <p>However, the module does have a limitation concerning the yaw angle of each point in the predicted paths of target objects, which may not always accurately point to the next point, leading to potential inaccuracies in some edge cases.</p> <p>Note</p> <p>For further reading on the collision assessment  method, please refer to Safety check utils</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/#generating-drivable-area","title":"Generating Drivable Area","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/#static-drivable-area-logic","title":"Static Drivable Area logic","text":"<p>The drivable area is used to determine the area in which the ego vehicle can travel. The primary goal of static drivable area expansion is to ensure safe travel by generating an area that encompasses only the necessary spaces for the vehicle's current behavior, while excluding non-essential areas. For example, while <code>avoidance</code> module is running, the drivable area includes additional space needed for maneuvers around obstacles, and it limits the behavior by not extending the avoidance path outside of lanelet areas.</p> <p>Static drivable area expansion operates under assumptions about the correct arrangement of lanes and the coverage of both the front and rear of the vehicle within the left and right boundaries. Key parameters for drivable area generation include extra footprint offsets for the ego vehicle, the handling of dynamic objects, maximum expansion distance, and specific methods for expansion. Additionally, since each module generates its own drivable area, before passing it as the input to generate the next running module's drivable area, or before generating a unified drivable area, the system sorts drivable lanes based on the vehicle's passage order. This ensures the correct definition of the lanes used in drivable area generation.</p> <p>Note</p> <p>Further details can be found in Drivable Area Design.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/#dynamic-drivable-area-logic","title":"Dynamic Drivable Area Logic","text":"<p>Large vehicles require much more space, which sometimes causes them to veer out of their current lane. A typical example being a bus making a turn at a corner. In such cases, relying on a static drivable area is insufficient, since the static method depends on lane information provided by high-definition maps. To overcome the limitations of the static approach, the dynamic drivable area expansion algorithm adjusts the navigable space for an autonomous vehicle in real-time. It conserves computational power by reusing previously calculated path data, updating only when there is a significant change in the vehicle's position. The system evaluates the minimum lane width necessary to accommodate the vehicle's turning radius and other dynamic factors. It then calculates the optimal expansion of the drivable area's boundaries to ensure there is adequate space for safe maneuvering, taking into account the vehicle's path curvature. The rate at which these boundaries can expand or contract is moderated to maintain stability in the vehicle's navigation. The algorithm aims to maximize the drivable space while avoiding fixed obstacles and adhering to legal driving limits. Finally, it applies these boundary adjustments and smooths out the path curvature calculations to ensure a safe and legally compliant navigable path is maintained throughout the vehicle's operation.</p> <p>Note</p> <p>The feature can be enabled in the drivable_area_expansion.param.yaml.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/#generating-turn-signal","title":"Generating Turn Signal","text":"<p>The Behavior Path Planner module uses the <code>autoware_vehicle_msgs::msg::TurnIndicatorsCommand</code> to output turn signal commands (see TurnIndicatorsCommand.idl). The system evaluates the driving context and determines when to activate turn signals based on its maneuver planning\u2014like turning, lane changing, or obstacle avoidance.</p> <p>Within this framework, the system differentiates between desired and required blinker activations. Desired activations are those recommended by traffic laws for typical driving scenarios, such as signaling before a lane change or turn. Required activations are those that are deemed mandatory for safety reasons, like signaling an abrupt lane change to avoid an obstacle.</p> <p>The <code>TurnIndicatorsCommand</code> message structure has a command field that can take one of several constants: <code>NO_COMMAND</code> indicates no signal is necessary, <code>DISABLE</code> to deactivate signals, <code>ENABLE_LEFT</code> to signal a left turn, and <code>ENABLE_RIGHT</code> to signal a right turn. The Behavior Path Planner sends these commands at the appropriate times, based on its rules-based system that considers both the desired and required scenarios for blinker activation.</p> <p>Note</p> <p>For more in-depth information, refer to Turn Signal Design document.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/#rerouting","title":"Rerouting","text":"<p>Warning</p> <p>The rerouting feature is under development. Further information will be included at a later date.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/#parameters-and-configuration","title":"Parameters and Configuration","text":"<p>The configuration files are organized in a hierarchical directory structure for ease of navigation and management. Each subdirectory contains specific configuration files relevant to its module. The root directory holds general configuration files that apply to the overall behavior of the planner. The following is an overview of the directory structure with the respective configuration files.</p> <pre><code>behavior_path_planner\n\u251c\u2500\u2500 behavior_path_planner.param.yaml\n\u251c\u2500\u2500 drivable_area_expansion.param.yaml\n\u251c\u2500\u2500 scene_module_manager.param.yaml\n\u251c\u2500\u2500 static_obstacle_avoidance\n\u2502   \u2514\u2500\u2500 static_obstacle_avoidance.param.yaml\n\u251c\u2500\u2500 avoidance_by_lc\n\u2502   \u2514\u2500\u2500 avoidance_by_lc.param.yaml\n\u251c\u2500\u2500 dynamic_obstacle_avoidance\n\u2502   \u2514\u2500\u2500 dynamic_obstacle_avoidance.param.yaml\n\u251c\u2500\u2500 goal_planner\n\u2502   \u2514\u2500\u2500 goal_planner.param.yaml\n\u251c\u2500\u2500 lane_change\n\u2502   \u2514\u2500\u2500 lane_change.param.yaml\n\u251c\u2500\u2500 side_shift\n\u2502   \u2514\u2500\u2500 side_shift.param.yaml\n\u2514\u2500\u2500 start_planner\n    \u2514\u2500\u2500 start_planner.param.yaml\n</code></pre> <p>Similarly, the common directory contains configuration files that are used across various modules, providing shared parameters and settings essential for the functioning of the Behavior Path Planner:</p> <pre><code>common\n\u251c\u2500\u2500 common.param.yaml\n\u251c\u2500\u2500 costmap_generator.param.yaml\n\u2514\u2500\u2500 nearest_search.param.yaml\n</code></pre> <p>The preset directory contains the configurations for managing the operational state of various modules. It includes the default_preset.yaml file, which specifically caters to enabling and disabling modules within the system.</p> <pre><code>preset\n\u2514\u2500\u2500 default_preset.yaml\n</code></pre>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/#limitations-future-work","title":"Limitations &amp; Future Work","text":"<ol> <li>The Goal Planner module cannot be simultaneously executed together with other modules.</li> <li>The module is not designed as a plugin. Integrating a custom module is not straightforward. Users have to modify part of the Behavior Path Planner's main code.</li> </ol>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_interface_design/","title":"Interface design","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_interface_design/#interface-design","title":"Interface design","text":"<p>Warning</p> <p>Under Construction</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_limitations/","title":"Limitations","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_limitations/#limitations","title":"Limitations","text":"<p>The document describes the limitations that are currently present in the <code>behavior_path_planner</code> module.</p> <p>The following items (but not limited to) fall in the scope of limitation:</p> <ul> <li>limitations due to the third-party API design and requirement</li> <li>limitations due to any shortcoming out of the developer's control.</li> </ul>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_limitations/#limitation-multiple-connected-opposite-lanes-require-linestring-with-shared-id","title":"Limitation: Multiple connected opposite lanes require Linestring with shared ID","text":"<p>To fully utilize the <code>Lanelet2</code>'s API, the design of the vector map (<code>.osm</code>) needs to follow all the criteria described in <code>Lanelet2</code> documentation. Specifically, in the case of 2 or more lanes, the Linestrings that divide the current lane with the opposite/adjacent lane need to have a matching <code>Linestring ID</code>. Assume the following ideal case.</p> <p></p> <p>In the image, <code>Linestring ID51</code> is shared by <code>Lanelet A</code> and <code>Lanelet B</code>. Hence we can directly use the available <code>left</code>, <code>adjacentLeft</code>, <code>right</code>, <code>adjacentRight</code> and <code>findUsages</code> method within <code>Lanelet2</code>'s API to directly query the direction and opposite lane availability.</p> <pre><code>const auto right_lane = routing_graph_ptr_-&gt;right(lanelet);\nconst auto adjacent_right_lane = routing_graph_ptr_-&gt;adjacentRight(lanelet);\nconst auto opposite_right_lane = lanelet_map_ptr_-&gt;laneletLayer.findUsages(lanelet.rightBound().invert());\n</code></pre> <p>The following images show the situation where these API does not work directly. This means that we cannot use them straight away, and several assumptions and logical instruction are needed to make these APIs work.</p> <p></p> <p>In this example (multiple linestring issues), <code>Lanelet C</code> contains <code>Linestring ID61</code> and <code>ID62</code>, while <code>Lanelet D</code> contains <code>Linestring ID63</code> and <code>ID 64</code>. Although the <code>Linestring ID62</code> and <code>ID64</code> have identical point IDs and seem visually connected, the API will treat these Linestring as though they are separated. When it searches for any <code>Lanelet</code> that is connected via <code>Linestring ID62</code>, it will return <code>NULL</code>, since <code>ID62</code> only connects to <code>Lanelet C</code> and not other <code>Lanelet</code>.</p> <p>Although, in this case, it is possible to forcefully search the lanelet availability by checking the lanelet that contains the points, using<code>getLaneletFromPoint</code> method. But, the implementation requires complex rules for it to work. Take the following images as an example.</p> <p></p> <p>Assume <code>Object X</code> is in <code>Lanelet F</code>. We can forcefully search <code>Lanelet E</code> via <code>Point 7</code>, and it will work if <code>Point 7</code> is utilized by only 2 lanelet. However, the complexity increases when we want to start searching for the direction of the opposite lane. We can infer the direction of the lanelet by using mathematical operations (dot product of vector <code>V_ID72</code> (<code>Point 6</code> minus <code>Point 9</code>), and <code>V_ID74</code> (<code>Point 7</code> minus <code>Point 8</code>). But, notice that we did not use Point 7 in V_ID72. This is because searching it requires an iteration, adding additional non-beneficial computation.</p> <p>Suppose the points are used by more than 2 lanelets. In that case, we have to find the differences for all lanelet, and the result might be undefined. The reason is that the differences between the coordinates do not reflect the actual shape of the lanelet. The following image demonstrates this point.</p> <p></p> <p></p> <p>There are many other available solutions to try. However, further attempt to solve this might cause issues in the future, especially for maintaining or scaling up the software.</p> <p>In conclusion, the multiple Linestring issues will not be supported. Covering these scenarios might give the user an \"everything is possible\" impression. This is dangerous since any attempt to create a non-standardized vector map is not compliant with safety regulations.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_limitations/#limitation-avoidance-at-corners-and-intersections","title":"Limitation: Avoidance at Corners and Intersections","text":"<p>Currently, the implementation doesn't cover avoidance at corners and intersections. The reason is similar to here. However, this case can still be supported in the future (assuming the vector map is defined correctly).</p> <p></p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_limitations/#limitation-chattering-shifts","title":"Limitation: Chattering shifts","text":"<p>There are possibilities that the shifted path chatters as a result of various factors. For example, bounded box shape or position from the perception input. Sometimes, it is difficult for the perception to get complete information about the object's size. As the object size is updated, the object length will also be updated. This might cause shifts point to be re-calculated, therefore resulting in chattering shift points.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_manager_design/","title":"Manager design","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_manager_design/#manager-design","title":"Manager design","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_manager_design/#purpose-role","title":"Purpose / Role","text":"<p>The manager launches and executes scene modules in <code>behavior_path_planner</code> depending on the use case, and has been developed to achieve following features:</p> <ul> <li>Multiple modules can run simultaneously in series in order to achieve more complex use cases. For example, as shown in the following video, this manager make it possible to avoid a parked vehicle during lane change maneuver.</li> <li>Flexible development by not relying on framework from external libraries.</li> </ul> <p></p> <p>Movie</p> <p>Support status:</p> Name Simple exclusive execution Advanced simultaneous execution Avoidance Avoidance By Lane Change Lane Change External Lane Change Goal Planner (without goal modification) Goal Planner (with goal modification) Pull Out Side Shift <p>Click here for supported scene modules.</p> <p>Warning</p> <p>It is still under development and some functions may be unstable.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_manager_design/#overview","title":"Overview","text":"<p>The manager is the core part of the <code>behavior_path_planner</code> implementation. It outputs path based on the latest data.</p> <p>The manager has sub-managers for each scene module, and its main task is</p> <ul> <li>set latest planner data to scene modules via sub-managers.</li> <li>check scene module's request status via sub-managers.</li> <li>launch scene modules that make execution request.</li> <li>execute launched modules.</li> <li>delete scene expired modules.</li> </ul> <p>Additionally, the manager generates root reference path, and if any other modules don't request execution, the path is used as the planning result of <code>behavior_path_planner</code>.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_manager_design/#slot","title":"Slot","text":"<p>The manager owns several containers of sub-managers, namely slots, that holds/runs several sub-managers and send the output to the next slot. Given the initial reference path, each slot processes the input path and the output path is processed by the next slot. The final slot output is utilized as the output of the manager. The slot passes following information</p> <pre><code>struct SlotOutput\n{\n  BehaviorModuleOutput valid_output;\n\n  // if candidate module is running, valid_output contains the planning by candidate module. In\n  // that case, downstream slots will just run aproved modules and do not try to launch new\n  // modules\n  bool is_upstream_candidate_exclusive{false};\n\n  // if this slot failed, downstream slots need to refresh approved/candidate modules and just\n  // forward valid_output of this slot output\n  bool is_upstream_failed_approved{false};\n\n  // if the approved module in this slot returned to isWaitingApproval, downstream slots need to\n  // refresh candidate once\n  bool is_upstream_waiting_approved{false};\n};\n</code></pre>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_manager_design/#sub-managers","title":"Sub-managers","text":"<p>The sub-manager's main task is</p> <ul> <li>store the launched modules in internal vectors <code>registered_modules_</code>.</li> <li>create scene module instance.</li> <li>pass scene module's instance to the manager.</li> <li>delete expired scene module instance from <code>registered_modules_</code>.</li> <li>publish debug markers.</li> </ul> <p> </p> sub-managers <p>Sub-manager is registered on the manager with the following function.</p> <pre><code>/**\n * @brief register managers.\n * @param manager pointer.\n */\nvoid registerSceneModuleManager(const SceneModuleManagerPtr &amp; manager_ptr)\n{\n  RCLCPP_INFO(logger_, \"register %s module\", manager_ptr-&gt;getModuleName().c_str());\n  manager_ptrs_.push_back(manager_ptr);\n  processing_time_.emplace(manager_ptr-&gt;getModuleName(), 0.0);\n}\n</code></pre> <p>Code is here</p> <p>Sub-manager has the following parameters that are needed by the manager to manage the launched modules, and these parameters can be set for each module.</p> <pre><code>struct ModuleConfigParameters\n{\n  bool enable_module{false};\n  bool enable_rtc{false};\n  bool enable_simultaneous_execution_as_approved_module{false};\n  bool enable_simultaneous_execution_as_candidate_module{false};\n  uint8_t priority{0};\n};\n</code></pre> <p>Code is here</p> Name Type Description <code>enable_module</code> bool if true, the sub-manager is registered on the manager. <code>enable_rtc</code> bool if true, the scene modules should be approved by (request to cooperate)rtc function. if false, the module can be run without approval from rtc. <code>enable_simultaneous_execution_as_candidate_module</code> bool if true, the manager allows its scene modules to run with other scene modules as candidate module. <code>enable_simultaneous_execution_as_approved_module</code> bool if true, the manager allows its scene modules to run with other scene modules as approved module. <code>priority</code> uint8_t the manager decides execution priority based on this parameter. The smaller the number is, the higher the priority is."},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_manager_design/#scene-modules","title":"Scene modules","text":"<p>Scene modules receives necessary data and RTC command, and outputs candidate path(s), reference path and RTC cooperate status. When multiple modules run in series, the output of the previous module is received as input and the information is used to generate a new modified path, as shown in the following figure. And, when one module is running alone, it receives a reference path generated from the centerline of the lane in which Ego is currently driving as previous module output.</p> <p> </p> scene module I/O Type Description IN <code>behavior_path_planner::BehaviorModuleOutput</code> previous module output. contains data necessary for path planning. IN <code>behavior_path_planner::PlannerData</code> contains data necessary for path planning. IN <code>tier4_planning_msgs::srv::CooperateCommands</code> contains approval data for scene module's path modification. (details) OUT <code>behavior_path_planner::BehaviorModuleOutput</code> contains modified path, turn signal information, etc... OUT <code>tier4_planning_msgs::msg::CooperateStatus</code> contains RTC cooperate status. (details) OUT <code>autoware_planning_msgs::msg::Path</code> candidate path output by a module that has not received approval for path change. when it approved, the ego's following path is switched to this path. (just for visualization) OUT <code>autoware_planning_msgs::msg::Path</code> reference path generated from the centerline of the lane the ego is going to follow. (just for visualization) OUT <code>visualization_msgs::msg::MarkerArray</code> virtual wall, debug info, etc... <p>Scene modules running on the manager are stored on the candidate modules stack or approved modules stack depending on the condition whether the path modification has been approved or not.</p> Stack Approval condition Description candidate modules Not approved The candidate modules whose modified path has not been approved by RTC is stored in vector <code>candidate_module_ptrs_</code> in the manager. The candidate modules stack is updated in the following order. 1. The manager selects only those modules that can be executed based on the configuration of the sub-manager whose scene module requests execution. 2. Determines the execution priority. 3. Executes them as candidate module. All of these modules receive the decided (approved) path from approved modules stack and RUN in PARALLEL.  approved modules Already approved When the path modification is approved via RTC commands, the manager moves the candidate module to approved modules stack. These modules are stored in <code>approved_module_ptrs_</code>. In this stack, all scene modules RUN in SERIES."},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_manager_design/#process-flow","title":"Process flow","text":"<p>There are 6 steps in one process:</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_manager_design/#step1","title":"Step1","text":"<p>At first, the manager set latest planner data, and run all approved modules and get output path. At this time, the manager checks module status and removes expired modules from approved modules stack.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_manager_design/#step2","title":"Step2","text":"<p>Input approved modules output and necessary data to all registered modules, and the modules judge the necessity of path modification based on it. The manager checks which module makes execution request.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_manager_design/#step3","title":"Step3","text":"<p>Check request module existence.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_manager_design/#step4","title":"Step4","text":"<p>The manager decides which module to execute as candidate modules from the modules that requested to execute path modification.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_manager_design/#step5","title":"Step5","text":"<p>Decides the priority order of execution among candidate modules. And, run all candidate modules. Each modules outputs reference path and RTC cooperate status.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_manager_design/#step6","title":"Step6","text":"<p>Move approved module to approved modules stack from candidate modules stack.</p> <p></p> <p>and, within a single planning cycle, these steps are repeated until the following conditions are satisfied.</p> <ul> <li>Any modules don't make a request of path modification. (Check in Step3)</li> <li>Any candidate modules' request are not approved. (Check in Step5)</li> </ul> <p></p> <pre><code>  while (rclcpp::ok()) {\n    /**\n     * STEP1: get approved modules' output\n     */\n    const auto approved_modules_output = runApprovedModules(data);\n\n    /**\n     * STEP2: check modules that need to be launched\n     */\n    const auto request_modules = getRequestModules(approved_modules_output);\n\n    /**\n     * STEP3: if there is no module that need to be launched, return approved modules' output\n     */\n    if (request_modules.empty()) {\n      processing_time_.at(\"total_time\") = stop_watch_.toc(\"total_time\", true);\n      return approved_modules_output;\n    }\n\n    /**\n     * STEP4: if there is module that should be launched, execute the module\n     */\n    const auto [highest_priority_module, candidate_modules_output] =\n      runRequestModules(request_modules, data, approved_modules_output);\n    if (!highest_priority_module) {\n      processing_time_.at(\"total_time\") = stop_watch_.toc(\"total_time\", true);\n      return approved_modules_output;\n    }\n\n    /**\n     * STEP5: if the candidate module's modification is NOT approved yet, return the result.\n     * NOTE: the result is output of the candidate module, but the output path don't contains path\n     * shape modification that needs approval. On the other hand, it could include velocity profile\n     * modification.\n     */\n    if (highest_priority_module-&gt;isWaitingApproval()) {\n      processing_time_.at(\"total_time\") = stop_watch_.toc(\"total_time\", true);\n      return candidate_modules_output;\n    }\n\n    /**\n     * STEP6: if the candidate module is approved, push the module into approved_module_ptrs_\n     */\n    addApprovedModule(highest_priority_module);\n    clearCandidateModules();\n  }\n</code></pre> <p>Code is here</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_manager_design/#priority-of-execution-request","title":"Priority of execution request","text":"<p>Compare priorities parameter among sub-managers to determine the order of execution based on config. Therefore, the priority between sub-modules does NOT change at runtime.</p> <pre><code>  /**\n   * @brief swap the modules order based on it's priority.\n   * @param modules.\n   * @details for now, the priority is decided in config file and doesn't change runtime.\n   */\n  void sortByPriority(std::vector&lt;SceneModulePtr&gt; &amp; modules) const\n  {\n    // TODO(someone) enhance this priority decision method.\n    std::sort(modules.begin(), modules.end(), [this](auto a, auto b) {\n      return getManager(a)-&gt;getPriority() &lt; getManager(b)-&gt;getPriority();\n    });\n  }\n</code></pre> <p>Code is here</p> <p>In the future, however, we are considering having the priorities change dynamically depending on the situation in order to achieve more complex use cases.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_manager_design/#how-to-decide-which-request-modules-to-run","title":"How to decide which request modules to run?","text":"<p>On this manager, it is possible that multiple scene modules may request path modification at same time. In that case, the modules to be executed as candidate module is determined in the following order.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_manager_design/#step1_1","title":"Step1","text":"<p>Push back the modules that make a request to <code>request_modules</code>.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_manager_design/#step2_1","title":"Step2","text":"<p>Check approved modules stack, and remove non-executable modules from<code>request_modules</code> based on the following condition.</p> <ul> <li>Condition A. approved module stack is empty.</li> <li>Condition B. all modules in approved modules stack support simultaneous execution as approved module (<code>enable_simultaneous_execution_as_approved_module</code> is <code>true</code>).</li> <li>Condition C. the request module supports simultaneous execution as approved module.</li> </ul> <p></p> <p>Executable or not:</p> Condition A Condition B Condition C Executable as candidate modules? YES - YES YES YES - NO YES NO YES YES YES NO YES NO NO NO NO YES NO NO NO NO NO <p>If a module that doesn't support simultaneous execution exists in approved modules stack (NOT satisfy Condition B), no more modules can be added to the stack, and therefore none of the modules can be executed as candidate.</p> <p>For example, if approved module's setting of <code>enable_simultaneous_execution_as_approved_module</code> is ENABLE, then only modules whose the setting is ENABLE proceed to the next step.</p> <p></p> <p>Other examples:</p> Process Description If approved modules stack is empty, then all request modules proceed to the next step, regardless of the setting of <code>enable_simultaneous_execution_as_approved_module</code>. If approved module's setting of <code>enable_simultaneous_execution_as_approved_module</code> is DISABLE, then all request modules are discarded."},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_manager_design/#step3_1","title":"Step3","text":"<p>Sort <code>request_modules</code> by priority.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_manager_design/#step4_1","title":"Step4","text":"<p>Check and pick up executable modules as candidate in order of priority based on the following conditions.</p> <ul> <li>Condition A. candidate module stack is empty.</li> <li>Condition B. all modules in candidate modules stack support simultaneous execution as candidate module (<code>enable_simultaneous_execution_as_candidate_module</code> is <code>true</code>).</li> <li>Condition C. the request module supports simultaneous execution as candidate module.</li> </ul> <p></p> <p>Executable or not:</p> Condition A Condition B Condition C Executable as candidate modules? YES - YES YES YES - NO YES NO YES YES YES NO YES NO NO NO NO YES NO NO NO NO NO <p>For example, if the highest priority module's setting of <code>enable_simultaneous_execution_as_candidate_module</code> is DISABLE, then all modules after the second priority are discarded.</p> <p></p> <p>Other examples:</p> Process Description If a module with a higher priority exists, lower priority modules whose setting of <code>enable_simultaneous_execution_as_candidate_module</code> is DISABLE are discarded. If all modules' setting of <code>enable_simultaneous_execution_as_candidate_module</code> is ENABLE, then all modules proceed to the next step."},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_manager_design/#step5_1","title":"Step5","text":"<p>Run all candidate modules.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_manager_design/#how-to-decide-which-modules-output-to-use","title":"How to decide which module's output to use?","text":"<p>Sometimes, multiple candidate modules are running simultaneously.</p> <p></p> <p>In this case, the manager selects a candidate modules which output path is used as <code>behavior_path_planner</code> output by approval condition in the following rules.</p> <ul> <li>Rule A. Regardless of the priority in the sub-manager (<code>priority</code>), approved modules always have a higher priority than unapproved modules.</li> <li>Rule B. If the approval status is the same, sort according to the sub-manager's priority.</li> </ul> Module A's approval condition Module A's priority Module B's approval condition Module B's priority Final priority Approved 1 Approved 99 Module A &gt; Module B Approved 1 Not approved 99 Module A &gt; Module B Not approved 1 Approved 99 Module B &gt; Module A Not approved 1 Not approved 99 Module A &gt; Module B <p>Note</p> <p>The smaller the number is, the higher the priority is.</p> <p> </p> module priority <p></p> <p>Additionally, the manager moves the highest priority module to approved modules stack if it is already approved.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_manager_design/#scene-module-unregister-process","title":"Scene module unregister process","text":"<p>The manager removes expired module in approved modules stack based on the module's status.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_manager_design/#waiting-approval-modules","title":"Waiting approval modules","text":"<p>If one module requests multiple path changes, the module may be back to waiting approval condition again. In this case, the manager moves the module to candidate modules stack. If there are some modules that was pushed back to approved modules stack later than the waiting approved module, it is also removed from approved modules stack.</p> <p>This is because module C is planning output path with the output of module B as input, and if module B is removed from approved modules stack and the input of module C changes, the output path of module C may also change greatly, and the output path will be unstable.</p> <p>As a result, the module A's output is used as approved modules stack.</p> <p></p> <p>If this case happened in the slot, <code>is_upstream_waiting_approved</code> is set to true.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_manager_design/#failure-modules","title":"Failure modules","text":"<p>If a module returns <code>ModuleStatus::FAILURE</code>, the manager removes the failed module. Additionally, all modules after the failed module are removed, even if they did not return <code>ModuleStatus::FAILURE</code>. These modules are not added back to the candidate modules stack and will instead run again from the beginning. Once these modules are removed, the output of the module prior to the failed module will be used as the planner's output.</p> <p>As shown in the example below, modules B, A, and C are running. When module A returns <code>ModuleStatus::FAILURE</code>, both module A and C are removed from the approved modules stack. Module B's output is then used as the final output of the planner.</p> <p></p> <p>If this case happened in the slot, <code>is_upstream_failed_approved</code> is set to true.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_manager_design/#succeeded-modules","title":"Succeeded modules","text":"<p>The succeeded modules return the status <code>ModuleStatus::SUCCESS</code>. The manager removes those modules based on Last In First Out policy. In other words, if a module added later to approved modules stack is still running (is in <code>ModuleStatus::RUNNING</code>), the manager doesn't remove the succeeded module. The reason for this is the same as in removal for waiting approval modules, and is to prevent sudden changes of the running module's output.</p> <p></p> <p></p> <p>As an exception, if Lane Change module returns status <code>ModuleStatus::SUCCESS</code>, the manager doesn't remove any modules until all modules is in status <code>ModuleStatus::SUCCESS</code>. This is because when the manager removes the Lane Change (normal LC, external LC, avoidance by LC) module as succeeded module, the manager updates the information of the lane Ego is currently driving in, so root reference path (= module A's input path) changes significantly at that moment.</p> <p></p> <p></p> <p>When the manager removes succeeded modules, the last added module's output is used as approved modules stack.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_manager_design/#slot-output-propagation","title":"Slot output propagation","text":"<p>As the initial solution, following SlotOutput is passed to the first slot.</p> <pre><code>  SlotOutput result_output = SlotOutput{\n    getReferencePath(data),\n    false,\n    false,\n    false,\n  };\n</code></pre> <p>If a slot turned out to be <code>is_upstream_failed_approved</code>, then all the subsequent slots are refreshed and have all of their approved_modules and candidate_modules cleared. The valid_output is just transferred to the end without any modification.</p> <p>If a slot turned out to be <code>is_upstream_waiting_approved</code>, then all the subsequent slots clear their candidate_modules once and apply their approved_modules to obtain the slot output.</p> <p>If a slot turned out to be <code>is_upstream_candidate_exclusive</code>, it means that a not <code>simultaneously_executable_as_candidate</code> module is running in that slot. Then all the subsequent modules just apply their approved_modules without trying to launch new candidate_modules.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_manager_design/#reference-path-generation","title":"Reference path generation","text":"<p>The reference path is generated from the centerline of the lanelet sequence obtained from the current route lanelet, and it is not only used as an input to the first added module of approved modules stack, but also used as the output of <code>behavior_path_planner</code> if none of the modules are running.</p> <p> </p> root reference path generation <p>The current route lanelet keeps track of the route lanelet currently followed by the planner. It is initialized as the closest lanelet within the route. It is then updated as ego travels along the route such that (1) it follows the previous current route lanelet and (2) it is the closest lanelet within the route.</p> <p>The current route lanelet can be reset to the closest lanelet within the route, ignoring whether it follows the previous current route lanelet .</p> <p></p> <p>The manager needs to know the ego behavior and then generate a root reference path from the lanes that Ego should follow.</p> <p>For example, during autonomous driving, even if Ego moves into the next lane in order to avoid a parked vehicle, the target lanes that Ego should follow will NOT change because Ego will return to the original lane after the avoidance maneuver. Therefore, the manager does NOT reset the current route lanelet, even if the avoidance maneuver is finished.</p> <p></p> <p>On the other hand, if the lane change is successful, the manager resets the current route lanelet because the lane that Ego should follow changes.</p> <p></p> <p>In addition, while manually driving (i.e., either the <code>OperationModeState</code> is different from <code>AUTONOMOUS</code> or the Autoware control is not engaged), the manager resets the current route lanelet at each iteration because the ego vehicle may move to an adjacent lane regardless of the decision of the autonomous driving system. The only exception is when a module is already approved, allowing testing the module's behavior while manually driving.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_manager_design/#drivable-area-generation","title":"Drivable area generation","text":"<p>Warning</p> <p>Under Construction</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner/docs/behavior_path_planner_manager_design/#turn-signal-management","title":"Turn signal management","text":"<p>Warning</p> <p>Under Construction</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_drivable_area_design/","title":"Drivable Area design","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_drivable_area_design/#drivable-area-design","title":"Drivable Area design","text":"<p>Drivable Area represents the area where ego vehicle can pass.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_drivable_area_design/#purpose-role","title":"Purpose / Role","text":"<p>In order to defined the area that ego vehicle can travel safely, we generate drivable area in behavior path planner module. Our drivable area is represented by two line strings, which are <code>left_bound</code> line and <code>right_bound</code> line respectively. Both <code>left_bound</code> and <code>right_bound</code> are created from left and right boundaries of lanelets. Note that <code>left_bound</code> and <code>right bound</code> are generated by <code>generateDrivableArea</code> function.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_drivable_area_design/#assumption","title":"Assumption","text":"<p>Our drivable area has several assumptions.</p> <ul> <li>Drivable Area should have all of the necessary area but should not represent unnecessary area for current behaviors. For example, when ego vehicle is in <code>follow lane</code> mode, drivable area should not contain adjacent lanes.</li> </ul> <ul> <li>When generating a drivable area, lanes need to be arranged in the order in which cars pass by (More details can be found in following sections).</li> </ul> <ul> <li>Both left and right bounds should cover the front of the path and the end of the path.</li> </ul>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_drivable_area_design/#limitations","title":"Limitations","text":"<p>Currently, when clipping left bound or right bound, it can clip the bound more than necessary and the generated path might be conservative.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_drivable_area_design/#parameters-for-drivable-area-generation","title":"Parameters for drivable area generation","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_drivable_area_design/#static-expansion","title":"Static expansion","text":"Name Unit Type Description Default value drivable_area_right_bound_offset [m] double right offset length to expand drivable area 5.0 drivable_area_left_bound_offset [m] double left offset length to expand drivable area 5.0 drivable_area_types_to_skip [-] string linestring types (as defined in the lanelet map) that will not be expanded road_border"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_drivable_area_design/#dynamic-expansion","title":"Dynamic expansion","text":"Name Unit Type Description Default value enabled [-] boolean if true, dynamically expand the drivable area based on the path curvature true print_runtime [-] boolean if true, runtime is logged by the node true max_expansion_distance [m] double maximum distance by which the original drivable area can be expanded (no limit if set to 0) 0.0 smoothing.curvature_average_window [-] int window size used for smoothing the curvatures using a moving window average 3 smoothing.max_bound_rate [m/m] double maximum rate of change of the bound lateral distance over its arc length 1.0 smoothing.arc_length_range [m] double arc length range where an expansion distance is initially applied 2.0 ego.extra_wheel_base [m] double extra ego wheelbase 0.0 ego.extra_front_overhang [m] double extra ego overhang 0.5 ego.extra_width [m] double extra ego width 1.0 object_exclusion.exclude_static [-] boolean if true, the drivable area is not expanded over static objects true object_exclusion.exclude_dynamic [-] boolean if true, the drivable area is not expanded in the predicted path of dynamic objects true object_exclusion.th_stopped_object_velocity [m/s] double velocity threshold for static objects 1.0 object_exclusion.safety_margin.front [m] double extra length to add to the front of the object footprint 0.75 object_exclusion.safety_margin.rear [m] double extra length to add to the rear of the object footprint 0.75 object_exclusion.safety_margin.left [m] double extra length to add to the left of the object footprint 0.75 object_exclusion.safety_margin.right [m] double extra length to add to the right of the object footprint 0.75 path_preprocessing.max_arc_length [m] double maximum arc length along the path where the ego footprint is projected (0.0 means no limit) 100.0 path_preprocessing.resample_interval [m] double fixed interval between resampled path points (0.0 means path points are directly used) 2.0 path_preprocessing.reuse_max_deviation [m] double if the path changes by more than this value, the curvatures are recalculated. Otherwise they are reused 0.5 avoid_linestring.types [-] string array linestring types in the lanelet maps that will not be crossed when expanding the drivable area [\"road_border\", \"curbstone\"] avoid_linestring.distance [m] double distance to keep between the drivable area and the linestrings to avoid 0.0"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_drivable_area_design/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>This section gives details of the generation of the drivable area (<code>left_bound</code> and <code>right_bound</code>).</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_drivable_area_design/#drivable-lanes-generation","title":"Drivable Lanes Generation","text":"<p>Before generating drivable areas, drivable lanes need to be sorted. Drivable Lanes are selected in each module (<code>Lane Follow</code>, <code>Avoidance</code>, <code>Lane Change</code>, <code>Goal Planner</code>, <code>Pull Out</code> and etc.), so more details about selection of drivable lanes can be found in each module's document. We use the following structure to define the drivable lanes.</p> <pre><code>struct DrivalbleLanes\n{\n    lanelet::ConstLanelet right_lanelet; // right most lane\n    lanelet::ConstLanelet left_lanelet; // left most lane\n    lanelet::ConstLanelets middle_lanelets; // middle lanes\n};\n</code></pre> <p>The image of the sorted drivable lanes is depicted in the following picture.</p> <p></p> <p>Note that, the order of drivable lanes become</p> <pre><code>drivable_lanes = {DrivableLane1, DrivableLanes2, DrivableLanes3, DrivableLanes4, DrivableLanes5}\n</code></pre>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_drivable_area_design/#drivable-area-generation","title":"Drivable Area Generation","text":"<p>In this section, a drivable area is created using drivable lanes arranged in the order in which vehicles pass by. We created <code>left_bound</code> from left boundary of the leftmost lanelet and <code>right_bound</code> from right boundary of the rightmost lanelet. The image of the created drivable area will be the following blue lines. Note that the drivable area is defined in the <code>Path</code> and <code>PathWithLaneId</code> messages as</p> <pre><code>std::vector&lt;geometry_msgs::msg::Point&gt; left_bound;\nstd::vector&lt;geometry_msgs::msg::Point&gt; right_bound;\n</code></pre> <p>and each point of right bound and left bound has a position in the absolute coordinate system.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_drivable_area_design/#drivable-area-expansion","title":"Drivable Area Expansion","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_drivable_area_design/#static-expansion_1","title":"Static Expansion","text":"<p>Each module can statically expand the left and right bounds of the target lanes by the parameter defined values. This enables large vehicles to pass narrow curve. The image of this process can be described as</p> <p></p> <p>Note that we only expand right bound of the rightmost lane and left bound of the leftmost lane.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_drivable_area_design/#dynamic-expansion_1","title":"Dynamic Expansion","text":"<p>The drivable area can also be expanded dynamically based on a minimum width calculated from the path curvature and the ego vehicle's properties. If static expansion is also enabled, the dynamic expansion will be done after the static expansion such that both expansions are applied.</p> Without dynamic expansion With dynamic expansion <p>Next we detail the algorithm used to expand the drivable area bounds.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_drivable_area_design/#1-calculate-and-smooth-the-path-curvature","title":"1 Calculate and smooth the path curvature","text":"<p>To avoid sudden changes of the dynamically expanded drivable area, we first try to reuse as much of the previous path and its calculated curvatures as possible. Previous path points and curvatures are reused up to the first previous path point that deviates from the new path by more than the <code>reuse_max_deviation</code> parameter. At this stage, the path is also resampled according to the <code>resampled_interval</code> and cropped according to the <code>max_arc_length</code>. With the resulting preprocessed path points and previous curvatures, curvatures of the new path points are calculated using the 3 points method and smoothed using a moving window average with window size <code>curvature_average_window</code>.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_drivable_area_design/#2-for-each-path-point-calculate-the-closest-bound-segment-and-the-minimum-drivable-area-width","title":"2 For each path point, calculate the closest bound segment and the minimum drivable area width","text":"<p>Each path point is projected on the original left and right drivable area bounds to calculate its corresponding bound index, original distance from the bounds, and the projected point. Additionally, for each path point, the minimum drivable area width is calculated using the following equation:  Where \\(W\\) is the minimum drivable area width, \\(a\\), is the front overhang of ego, \\(l\\) is the wheelbase of ego, \\(w\\) is the width of ego, and \\(k\\) is the path curvature. This equation was derived from the work of Lim, H., Kim, C., and Jo, A., \"Model Predictive Control-Based Lateral Control of Autonomous Large-Size Bus on Road with Large Curvature,\" SAE Technical Paper 2021-01-0099, 2021.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_drivable_area_design/#3-calculate-maximum-expansion-distances-of-each-bound-point-based-on-detected-objects-and-linestring-of-the-vector-map-optional","title":"3 Calculate maximum expansion distances of each bound point based on detected objects and linestring of the vector map (optional)","text":"<p>For each drivable area bound point, we calculate its maximum expansion distance as its distance to the closest \"obstacle\" which could be:</p> <ol> <li>Map linestring with type <code>avoid_linestrings.type</code>.</li> <li>Static object footprint (if <code>object_exclusion.exclude_static</code> is set to <code>true</code>).</li> <li>Dynamic object path footprint (if <code>object_exclusion.exclude_dynamic</code> is set to <code>true</code>).</li> </ol> <p>If <code>max_expansion_distance</code> is not <code>0.0</code>, it is use here if smaller than the distance to the closest obstacle.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_drivable_area_design/#4-calculate-by-how-much-each-bound-point-should-be-pushed-away-from-the-path","title":"4 Calculate by how much each bound point should be pushed away from the path","text":"<p>For each bound point, a shift distance is calculated. such that the resulting width between corresponding left and right bound points is as close as possible to the minimum width calculated in step 2 but the individual shift distance stays bellow the previously calculated maximum expansion distance.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_drivable_area_design/#5-shift-bound-points-by-the-values-calculated-in-step-4-and-remove-all-loops-in-the-resulting-bound","title":"5 Shift bound points by the values calculated in step 4 and remove all loops in the resulting bound","text":"<p>Finally, each bound point is shifted away from the path by the distance calculated in step 4. Once all points have been shifted, loops are removed from the bound and we obtain our final expanded drivable area.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_drivable_area_design/#visualizing-maximum-drivable-area-debug","title":"Visualizing maximum drivable area (Debug)","text":"<p>Sometimes, the developers might get a different result between two maps that may look identical during visual inspection.</p> <p>For example, in the same area, one can perform avoidance and another cannot. This might be related to the maximum drivable area issues due to the non-compliance vector map design from the user.</p> <p>To debug the issue, the maximum drivable area boundary can be visualized.</p> <p></p> <p></p> <p>The maximum drivable area can be visualize by adding the marker from <code>/planning/scenario_planning/lane_driving/behavior_planning/behavior_path_planner/maximum_drivable_area</code></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_drivable_area_design/#expansion-with-hatched-road-markings-area","title":"Expansion with hatched road markings area","text":"<p>If the hatched road markings area is defined in the lanelet map, the area can be used as a drivable area. Since the area is expressed as a polygon format of Lanelet2, several steps are required for correct expansion.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_path_generation_design/","title":"Path Generation Design","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_path_generation_design/#path-generation-design","title":"Path Generation Design","text":"<p>This document explains how the path is generated for lane change and avoidance, etc. The implementation can be found in path_shifter.hpp.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_path_generation_design/#overview","title":"Overview","text":"<p>The base idea of the path generation in lane change and avoidance is to smoothly shift the reference path, such as the center line, in the lateral direction. This is achieved by using a constant-jerk profile as in the figure below. More details on how it is used can be found in README. It is assumed that the reference path is smooth enough for this algorithm.</p> <p>The figure below explains how the application of a constant lateral jerk \\(l^{'''}(s)\\) can be used to induce lateral shifting. In order to comply with the limits on lateral acceleration and velocity, zero-jerk time is employed in the figure ( \\(T_a\\) and \\(T_v\\) ). In each interval where constant jerk is applied, the shift position \\(l(s)\\) can be characterized by a third-degree polynomial. Therefore the shift length from the reference path can then be calculated by combining spline curves.</p> <p></p> <p>Note that, due to the rarity of the \\(T_v\\) in almost all cases of lane change and avoidance, \\(T_v\\) is not considered in the current implementation.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_path_generation_design/#mathematical-derivation","title":"Mathematical derivation","text":"<p>With initial longitudinal velocity \\(v_0^{\\rm lon}\\) and longitudinal acceleration \\(a^{\\rm lon}\\), longitudinal position \\(s(t)\\) and longitudinal velocity at each time \\(v^{\\rm lon}(t)\\) can be derived as:</p> \\[ \\begin{align} s_1&amp;= v^{\\rm lon}_0 T_j + \\frac{1}{2} a^{\\rm lon} T_j^2 \\\\ v_1&amp;= v^{\\rm lon}_0 + a^{\\rm lon} T_j \\\\ s_2&amp;= v^{\\rm lon}_1 T_a + \\frac{1}{2} a^{\\rm lon} T_a^2 \\\\ v_2&amp;= v^{\\rm lon}_1 + a^{\\rm lon} T_a \\\\ s_3&amp;= v^{\\rm lon}_2 T_j + \\frac{1}{2} a^{\\rm lon} T_j^2 \\\\ v_3&amp;= v^{\\rm lon}_2 + a^{\\rm lon} T_j \\\\ s_4&amp;= v^{\\rm lon}_3 T_v + \\frac{1}{2} a^{\\rm lon} T_v^2 \\\\ v_4&amp;= v^{\\rm lon}_3 + a^{\\rm lon} T_v \\\\ s_5&amp;= v^{\\rm lon}_4 T_j + \\frac{1}{2} a^{\\rm lon} T_j^2 \\\\ v_5&amp;= v^{\\rm lon}_4 + a^{\\rm lon} T_j \\\\ s_6&amp;= v^{\\rm lon}_5 T_a + \\frac{1}{2} a^{\\rm lon} T_a^2 \\\\ v_6&amp;= v^{\\rm lon}_5 + a^{\\rm lon} T_a \\\\ s_7&amp;= v^{\\rm lon}_6 T_j + \\frac{1}{2} a^{\\rm lon} T_j^2 \\\\ v_7&amp;= v^{\\rm lon}_6 + a^{\\rm lon} T_j \\end{align} \\] <p>By applying simple integral operations, the following analytical equations can be derived to describe the shift distance \\(l(t)\\) at each time under lateral jerk, lateral acceleration, and velocity constraints.</p> \\[ \\begin{align} l_1&amp;= \\frac{1}{6}jT_j^3\\\\[10pt] l_2&amp;= \\frac{1}{6}j T_j^3 + \\frac{1}{2} j T_a T_j^2 + \\frac{1}{2} j T_a^2 T_j\\\\[10pt] l_3&amp;= j  T_j^3 + \\frac{3}{2} j T_a T_j^2 + \\frac{1}{2} j T_a^2 T_j\\\\[10pt] l_4&amp;= j T_j^3 + \\frac{3}{2} j T_a T_j^2 + \\frac{1}{2} j T_a^2 T_j + j(T_a + T_j)T_j T_v\\\\[10pt] l_5&amp;= \\frac{11}{6} j T_j^3 + \\frac{5}{2} j T_a T_j^2 + \\frac{1}{2} j T_a^2 T_j + j(T_a + T_j)T_j T_v \\\\[10pt] l_6&amp;= \\frac{11}{6} j T_j^3 + 3 j T_a T_j^2 + j T_a^2 T_j + j(T_a + T_j)T_j T_v\\\\[10pt] l_7&amp;= 2 j T_j^3 + 3 j T_a T_j^2 + j T_a^2 T_j + j(T_a + T_j)T_j T_v \\end{align} \\] <p>These equations are used to determine the shape of a path. Additionally, by applying further mathematical operations to these basic equations, the expressions of the following subsections can be derived.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_path_generation_design/#calculation-of-maximum-acceleration-from-transition-time-and-final-shift-length","title":"Calculation of maximum acceleration from transition time and final shift length","text":"<p>In cases where there are no limitations on lateral velocity and lateral acceleration, the maximum lateral acceleration during the shifting can be calculated as follows. The constant-jerk time is given by \\(T_j = T_{\\rm total}/4\\) because of its symmetric property. Since \\(T_a=T_v=0\\), the final shift length \\(L=l_7=2jT_j^3\\) can be determined using the above equation. The maximum lateral acceleration is then given by \\(a_{\\rm max} =jT_j\\). This results in the following expression for the maximum lateral acceleration:</p> \\[ \\begin{align} a_{\\rm max}^{\\rm lat}  = \\frac{8L}{T_{\\rm total}^2} \\end{align} \\]"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_path_generation_design/#calculation-of-ta-tj-and-jerk-from-acceleration-limit","title":"Calculation of Ta, Tj and jerk from acceleration limit","text":"<p>In cases where there are no limitations on lateral velocity, the constant-jerk and acceleration times, as well as the required jerk can be calculated from the acceleration limit, total time, and final shift length as follows. Since \\(T_v=0\\), the final shift length is given by:</p> \\[ \\begin{align} L = l_7 = 2 j T_j^3 + 3 j T_a T_j^2 + j T_a^2 T_j \\end{align} \\] <p>Additionally, the velocity profile reveals the following relations:</p> \\[ \\begin{align} a_{\\rm lim}^{\\rm lat} &amp;= j T_j\\\\ T_{\\rm total} &amp;= 4T_j + 2T_a \\end{align} \\] <p>By solving these three equations, the following can be obtained:</p> \\[ \\begin{align} T_j&amp;=\\frac{T_{\\rm total}}{2} - \\frac{2L}{a_{\\rm lim}^{\\rm lat} T_{\\rm total}}\\\\[10pt] T_a&amp;=\\frac{4L}{a_{\\rm lim}^{\\rm lat} T_{\\rm total}} - \\frac{T_{\\rm total}}{2}\\\\[10pt] jerk&amp;=\\frac{2a_{\\rm lim} ^2T_{\\rm total}}{a_{\\rm lim}^{\\rm lat} T_{\\rm total}^2-4L} \\end{align} \\] <p>Where \\(T_j\\) is the constant-jerk time, \\(T_a\\) is the constant acceleration time, \\(j\\) is the required jerk, \\(a_{\\rm lim}^{\\rm lat}\\) is the lateral acceleration limit, and \\(L\\) is the final shift length.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_path_generation_design/#calculation-of-required-time-from-jerk-and-acceleration-constraint","title":"Calculation of required time from jerk and acceleration constraint","text":"<p>In cases where there are no limitations on lateral velocity, the total time required for shifting can be calculated from the lateral jerk and lateral acceleration limits and the final shift length as follows. By solving the two equations given above:</p> \\[ L = l_7 = 2 j T_j^3 + 3 j T_a T_j^2 + j T_a^2 T_j,\\quad a_{\\rm lim}^{\\rm lat} = j T_j \\] <p>We obtain the following expressions:</p> \\[ \\begin{align} T_j &amp;= \\frac{a_{\\rm lim}^{\\rm lat}}{j}\\\\[10pt] T_a &amp;= \\frac{1}{2}\\sqrt{\\frac{a_{\\rm lim}^{\\rm lat}}{j}^2 + \\frac{4L}{a_{\\rm lim}^{\\rm lat}}} - \\frac{3a_{\\rm lim}^{\\rm lat}}{2j} \\end{align} \\] <p>The total time required for shifting can then be calculated as \\(T_{\\rm total}=4T_j+2T_a\\).</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_path_generation_design/#limitation","title":"Limitation","text":"<ul> <li>Since \\(T_v\\) is zero in almost all cases of lane change and avoidance, \\(T_v\\) is not considered in the current implementation.</li> </ul>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_safety_check/","title":"Safety Check Utils","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_safety_check/#safety-check-utils","title":"Safety Check Utils","text":"<p>Safety check function checks if the given path will collide with a given target object.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_safety_check/#purpose-role","title":"Purpose / Role","text":"<p>In the behavior path planner, certain modules (e.g., lane change) need to perform collision checks to ensure the safe navigation of the ego vehicle. These utility functions assist the user in conducting safety checks with other road participants.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_safety_check/#assumptions","title":"Assumptions","text":"<p>The safety check module is based on the following assumptions:</p> <ol> <li>Users must provide the position, velocity, and shape of both the ego and target objects to the utility functions.</li> <li>The yaw angle of each point in the predicted path of both the ego and target objects should point to the next point in the path.</li> <li>The safety check module uses RSS distance to determine the safety of a potential collision with other objects.</li> </ol>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_safety_check/#limitations","title":"Limitations","text":"<p>Currently the yaw angle of each point of predicted paths of a target object does not point to the next point. Therefore, the safety check function might returns incorrect result for some edge case.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_safety_check/#inner-working-algorithm","title":"Inner working / Algorithm","text":"<p>The flow of the safety check algorithm is described in the following explanations.</p> <p></p> <p>Here we explain each step of the algorithm flow.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_safety_check/#1-get-pose-of-the-target-object-at-a-given-time","title":"1. Get pose of the target object at a given time","text":"<p>For the first step, we obtain the pose of the target object at a given time. This can be done by interpolating the predicted path of the object.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_safety_check/#2-check-overlap","title":"2. Check overlap","text":"<p>With the interpolated pose obtained in the step.1, we check if the object and ego vehicle overlaps at a given time. If they are overlapped each other, the given path is unsafe.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_safety_check/#3-get-front-object","title":"3. Get front object","text":"<p>After the overlap check, it starts to perform the safety check for the broader range. In this step, it judges if ego or target object is in front of the other vehicle. We use arc length of the front point of each object along the given path to judge which one is in front of the other. In the following example, target object (red rectangle) is running in front of the ego vehicle (black rectangle).</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_safety_check/#4-calculate-rss-distance","title":"4. Calculate RSS distance","text":"<p>After we find which vehicle is running ahead of the other vehicle, we start to compute the RSS distance. With the reaction time \\(t_{reaction}\\) and safety time margin \\(t_{margin}\\), RSS distance can be described as:</p> \\[ rss_{dist} = v_{rear} (t_{reaction} + t_{margin}) + \\frac{v_{rear}^2}{2|a_{rear, decel}|} - \\frac{v_{front}^2}{2|a_{front, decel|}} \\] <p>where \\(V_{front}\\), \\(v_{rear}\\) are front and rear vehicle velocity respectively and \\(a_{rear, front}\\), \\(a_{rear, decel}\\) are front and rear vehicle deceleration. Note that RSS distance is normally used for objects traveling in the same direction, if the yaw difference between a given ego pose and object pose is more than a user-defined yaw difference threshold, the rss collision check will be skipped for that specific pair of poses.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_safety_check/#5-create-extended-ego-and-target-object-polygons","title":"5. Create extended ego and target object polygons","text":"<p>In this step, we compute extended ego and target object polygons. The extended polygons can be described as:</p> <p></p> <p>As the picture shows, we expand the rear object polygon. For the longitudinal side, we extend it with the RSS distance, and for the lateral side, we extend it by the lateral margin.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_safety_check/#6-check-overlap","title":"6. Check overlap","text":"<p>Similar to the previous step, we check the overlap of the extended rear object polygon and front object polygon. If they are overlapped each other, we regard it as the unsafe situation.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_turn_signal_design/","title":"Turn Signal design","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_turn_signal_design/#turn-signal-design","title":"Turn Signal design","text":"<p>Turn Signal decider determines necessary blinkers.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_turn_signal_design/#purpose-role","title":"Purpose / Role","text":"<p>This module is responsible for activating a necessary blinker during driving. It uses rule-based algorithm to determine blinkers, and the details of this algorithm are described in the following sections. Note that this algorithm is strictly based on the Japanese Road Traffic Law.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_turn_signal_design/#assumptions","title":"Assumptions","text":"<p>Autoware has following order of priorities for turn signals.</p> <ol> <li>Activate turn signal to safely navigate ego vehicle and protect other road participants</li> <li>Follow traffic laws</li> <li>Follow human driving practices</li> </ol>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_turn_signal_design/#limitations","title":"Limitations","text":"<p>Currently, this algorithm can sometimes give unnatural (not wrong) blinkers in complicated situations. This is because it tries to follow the road traffic law and cannot solve <code>blinker conflicts</code> clearly in that environment.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_turn_signal_design/#parameters-for-turn-signal-decider","title":"Parameters for turn signal decider","text":"Name Unit Type Description Default value turn_signal_intersection_search_distance [m] double constant search distance to decide activation of blinkers at intersections 30 turn_signal_intersection_angle_threshold_degree deg double angle threshold to determined the end point of intersection required section 15 turn_signal_minimum_search_distance [m] double minimum search distance for avoidance and lane change 10 turn_signal_search_time [s] double search time for to decide activation of blinkers 3.0 turn_signal_shift_length_threshold [m] double shift length threshold to decide activation of blinkers 0.3 turn_signal_remaining_shift_length_threshold [m] double When the ego's current shift length minus its end shift length is less than this threshold, the turn signal will be turned off. 0.1 turn_signal_remaining_distance_to_bound_threshold [m] double Distance from ego vehicle's front edge to the far-side boundary of the target merging lane. The turn signal will be turned off when the value is less than the threshold. 1.3 turn_signal_on_swerving [-] bool flag to activate blinkers when swerving true turn_signal_roundabout_on_entry [-] string turn signal on entry to roundabout. \"None\", \"Left\", \"Right\" \"None\" turn_signal_roundabout_entry_indicator_persistence [-] bool whether to keep the indicator on after entering the roundabout. If true, the indicator will be kept on until the exit point. If false, the indicator will be turned off after entering the roundabout. false turn_signal_roundabout_on_exit [-] string turn signal on exit from roundabout. \"None\", \"Left\", \"Right\" \"Left\" turn_signal_roundabout_search_distance [m] double constant search distance to decide activation of blinkers at roundabouts 30.0 turn_signal_roundabout_angle_threshold_degree deg double angle threshold to determined the end point of roundabout required section 15.0 turn_signal_roundabout_backward_depth [-] int maximum number of lanelets to look back when determining the backward length for roundabout turn signal logic. Set to -1 for unlimited depth. 50 <p>Note that the default values for <code>turn_signal_intersection_search_distance</code> and <code>turn_signal_search_time</code> is strictly followed by Japanese Road Traffic Laws. So if your country does not allow to use these default values, you should change these values in configuration files.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_turn_signal_design/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>In this algorithm, it assumes that each blinker has two sections, which are <code>desired section</code> and <code>required section</code>. The image of these two sections are depicted in the following diagram.</p> <p></p> <p>These two sections have the following meanings.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_turn_signal_design/#-desired-section","title":"- Desired Section","text":"<pre><code>- This section is defined by road traffic laws. It cannot be longer or shorter than the designated length defined by the law.\n- In this section, you do not have to activate the designated blinkers if it is dangerous to do so.\n</code></pre>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_turn_signal_design/#-required-section","title":"- Required Section","text":"<pre><code>- In this section, ego vehicle must activate designated blinkers. However, if there are blinker conflicts, it must solve them based on the algorithm we mention later in this document.\n- Required section cannot be longer than desired section.\n</code></pre> <p>When turning on the blinker, it decides whether or not to turn on the specified blinker based on the distance from the front of the ego vehicle to the start point of each section. Conversely, when turning off the blinker, it calculates the distance from the base link of the ego vehicle to the end point of each section and decide whether or not to turn it off based on that.</p> <p></p> <p>For left turn, right turn, avoidance, lane change, goal planner and pull out, we define these two sections, which are elaborated in the following part.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_turn_signal_design/#1-left-and-right-turn","title":"1. Left and Right turn","text":"<p>Turn signal decider checks each lanelet on the map if it has <code>turn_direction</code> information. If a lanelet has this information, it activates necessary blinker based on this information.</p> <ul> <li>desired start point   The <code>search_distance</code> for blinkers at intersections is <code>v * turn_signal_search_time + turn_signal_intersection_search_distance</code>. Then the start point becomes <code>search_distance</code> meters before the start point of the intersection lanelet(depicted in green in the following picture), where <code>v</code> is the velocity of the ego vehicle. However, if we set <code>turn_signal_distance</code> in the lanelet, we use that length as search distance.</li> </ul> <ul> <li>desired end point   Terminal point of the intersection lanelet.</li> </ul> <ul> <li>required start point   Initial point of the intersection lanelet.</li> </ul> <ul> <li>required end point   The earliest point that satisfies the following condition. \\(\\theta - \\theta_{\\textrm{end}} &lt; \\delta\\), where \\(\\theta_{\\textrm{end}}\\) is yaw angle of the terminal point of the lanelet, \\(\\theta\\) is the angle of a required end point and \\(\\delta\\) is the threshold defined by the user.</li> </ul> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_turn_signal_design/#2-avoidance","title":"2. Avoidance","text":"<p>Avoidance can be separated into two sections, first section and second section. The first section is from the start point of the path shift to the end of the path shift. The second section is from the end of shift point to the end of avoidance. Note that avoidance module will not activate turn signal when its shift length is below <code>turn_signal_shift_length_threshold</code>.</p> <p>First section</p> <ul> <li>desired start point   <code>v * turn_signal_search_time</code> meters before the start point of the avoidance shift path.</li> </ul> <ul> <li>desired end point   Shift complete point where the path shift is completed.</li> </ul> <ul> <li>required start point   Avoidance start point.</li> </ul> <ul> <li>required end point   Shift complete point same as the desired end point.</li> </ul> <p></p> <p>Second section</p> <ul> <li>desired start point   Shift complete point.</li> </ul> <ul> <li>desired end point   Avoidance terminal point</li> </ul> <ul> <li>required start point   Shift complete point.</li> </ul> <ul> <li>required end point   Avoidance terminal point.</li> </ul> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_turn_signal_design/#3-lane-change","title":"3. Lane Change","text":"<ul> <li>desired start point   <code>v * turn_signal_search_time</code> meters before the start point of the lane change path.</li> </ul> <ul> <li>desired end point   Terminal point of the lane change path.</li> </ul> <ul> <li>required start point   Initial point of the lane change path.</li> </ul> <ul> <li>required end point   Cross point with lane change path and boundary line of the adjacent lane.</li> </ul>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_turn_signal_design/#4-pull-out-start-planner","title":"4. Pull out (Start Planner)","text":"<ul> <li>desired start point   Start point of the path of pull out.</li> </ul> <ul> <li>desired end point   Terminal point of the path of pull out.</li> </ul> <ul> <li>required start point   Start point of the path pull out.</li> </ul> <ul> <li>required end point   Terminal point of the path of pull out.</li> </ul> <p>Note</p> <p>The blinker also deactivate based on the remaining lateral distance to the target lane boundary.</p> <p>If this distance is smaller than the configurable parameter <code>turn_signal_remaining_distance_to_bound_threshold</code>, the blinker will turn off immediately.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_turn_signal_design/#5-goal-planner","title":"5. Goal Planner","text":"<ul> <li>desired start point   <code>v * turn_signal_search_time</code> meters before the start point of the pull over path.</li> </ul> <ul> <li>desired end point   Terminal point of the path of pull over.</li> </ul> <ul> <li>required start point   Start point of the path of pull over.</li> </ul> <ul> <li>required end point   Terminal point of the path of pull over.</li> </ul>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_turn_signal_design/#6-roundabout","title":"6. Roundabout","text":"<p>For roundabout scenarios, the turn signal decider handles both entry and exit turn signals based on the <code>roundabout</code> attribute in the lanelet map. The roundabout functionality supports the following configurations:</p> <p>Entry Turn Signal:</p> <ul> <li>desired start point: <code>v * turn_signal_search_time</code> meters before the start point of the roundabout entry lanelet.</li> <li>desired end point: Terminal point of the roundabout entry lanelet (or exit lanelet if <code>turn_signal_roundabout_entry_indicator_persistence</code> is enabled).</li> <li>required start point: Initial point of the roundabout entry lanelet.</li> <li>required end point: The earliest point that satisfies the following condition. \\(\\theta - \\theta_{\\textrm{end}} &lt; \\delta\\), where \\(\\theta_{\\textrm{end}}\\) is yaw angle of the terminal point of the lanelet, \\(\\theta\\) is the angle of a required end point and \\(\\delta\\) is the threshold defined by the user. (This is the same as the right/left turn signal logic.)</li> </ul> <p>Exit Turn Signal:</p> <ul> <li>desired start point: Start point of the roundabout exit lanelet (or from a lanelet with <code>enable_exit_turn_signal</code> attribute if found).</li> <li>desired end point: Terminal point of the roundabout exit lanelet.</li> <li>required start point: Initial point of the roundabout exit lanelet.</li> <li>required end point: The earliest point that satisfies the following condition. \\(\\theta - \\theta_{\\textrm{end}} &lt; \\delta\\), where \\(\\theta_{\\textrm{end}}\\) is yaw angle of the terminal point of the lanelet, \\(\\theta\\) is the angle of a required end point and \\(\\delta\\) is the threshold defined by the user.(This is the same as the right/left turn signal logic.)</li> </ul> <p>Special Features:</p> <ul> <li>Indicator persistence: When <code>turn_signal_roundabout_entry_indicator_persistence</code> is enabled, the entry turn signal remains active until the vehicle exits the roundabout.</li> </ul>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_turn_signal_design/#example-parameter-settings-based-on-country-specific-rules","title":"Example parameter settings based on country-specific rules","text":"Country Entry (<code>turn_signal_roundabout_on_entry</code>) Exit (<code>turn_signal_roundabout_on_exit</code>) Entry indicator persistence (<code>turn_signal_roundabout_entry_indicator_persistence</code>) Notes Japan \"None\" \"Left\" false Germany \"None\" \"Right\" false South Korea \"Left\" \"Right\" false"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_turn_signal_design/#blinker-conflicts","title":"Blinker conflicts","text":"<p>When ego vehicle has to activate several blinkers, it uses the following rules to decide which blinker to activate. Note that this algorithm is based on the Japanese Road Traffic Law, so it may not be suitable for other countries. When it comes to handle several blinkers, it gives priority to the first blinker that comes first. However, this rule sometimes activate unnatural blinkers, so turn signal decider uses the following five rules to decide the necessary turn signal.</p> <ul> <li>pattern1   </li> </ul> <ul> <li>pattern2   </li> </ul> <ul> <li>pattern3   </li> </ul> <ul> <li>pattern4   </li> </ul> <ul> <li>pattern5   </li> </ul> <p>Based on these five rules, turn signal decider can solve <code>blinker conflicts</code>. The following pictures show some examples of this kind of conflicts.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_turn_signal_design/#-several-right-and-left-turns-on-short-sections","title":"- Several right and left turns on short sections","text":"<p>In this scenario, ego vehicle has to pass several turns that are close each other. Since this pattern can be solved by the pattern1 rule, the overall result is depicted in the following picture.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_turn_signal_design/#-avoidance-with-left-turn-1","title":"- Avoidance with left turn (1)","text":"<p>In this scene, ego vehicle has to deal with the obstacle that is on its original path as well as make a left turn. The overall result can be varied by the position of the obstacle, but the image of the result is described in the following picture.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_turn_signal_design/#-avoidance-with-left-turn-2","title":"- Avoidance with left turn (2)","text":"<p>Same as the previous scenario, ego vehicle has to avoid the obstacle as well as make a turn left. However, in this scene, the obstacle is parked after the intersection. Similar to the previous one, the overall result can be varied by the position of the obstacle, but the image of the result is described in the following picture.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_planner_common/docs/behavior_path_planner_turn_signal_design/#-lane-change-and-left-turn","title":"- Lane change and left turn","text":"<p>In this scenario, ego vehicle has to do lane change before making a left turn. In the following example, ego vehicle does not activate left turn signal until it reaches the end point of the lane change path.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_sampling_planner_module/","title":"Behavior Path Sampling Based Planner","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_sampling_planner_module/#behavior-path-sampling-based-planner","title":"Behavior Path Sampling Based Planner","text":"<p>WARNING: This module is experimental and has not been properly tested on a real vehicle, use only on simulations.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_sampling_planner_module/#purpose","title":"Purpose","text":"<p>This package implements a node that uses sampling based planning to generate a drivable trajectory for the behavior path planner. It is heavily based off the sampling_based_planner module.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_sampling_planner_module/#features","title":"Features","text":"<p>This package is able to:</p> <ul> <li>create a smooth trajectory to avoid static obstacles.</li> <li>guarantees the generated trajectory (if any) complies with customizable hard constraints.</li> <li>transitions to a success state after the ego vehicle merges to its goal lane.</li> <li>re-use previously generated outputs to re-sample new alternative paths</li> </ul> <p>Note that the velocity is just taken over from the input path.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_sampling_planner_module/#algorithm","title":"Algorithm","text":"<p>Sampling based planning is decomposed into 3 successive steps:</p> <ol> <li>Sampling: candidate trajectories are generated.</li> <li>Pruning: invalid candidates are discarded.</li> <li>Selection: the best remaining valid candidate is selected.</li> </ol>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_sampling_planner_module/#sampling","title":"Sampling","text":"<p>Candidate trajectories are generated based on the current ego state and some target state. 1 sampling algorithms is currently implemented: polynomials in the frenet frame.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_sampling_planner_module/#pruning","title":"Pruning","text":"<p>The validity of each candidate trajectory is checked using a set of hard constraints.</p> <ul> <li>collision: ensure no collision with static obstacles;</li> <li>curvature: ensure smooth curvature;</li> <li>drivable area: ensure the trajectory stays within the drivable area.</li> </ul>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_sampling_planner_module/#selection","title":"Selection","text":"<p>Among the valid candidate trajectories, the best one is determined using a set of soft constraints (i.e., objective functions).</p> <ul> <li>curvature: prefer smoother trajectories;</li> <li>length: prefer trajectories with longer remaining path length;</li> <li>lateral deviation: prefer trajectories close to the goal.</li> </ul> <p>Each soft constraint is associated with a weight to allow tuning of the preferences.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_sampling_planner_module/#limitations","title":"Limitations","text":"<p>The quality of the candidates generated with polynomials in frenet frame greatly depend on the reference path. If the reference path is not smooth, the resulting candidates will probably be un-drivable.</p> <p>Failure to find a valid trajectory current results in a suddenly stopping trajectory.</p> <p>The module has troubles generating paths that converge rapidly to the goal lanelet. Basically, after overcoming all obstacles, the module should prioritize paths that rapidly make the ego vehicle converge back to its goal lane (ie. paths with low average and final lateral deviation). However, this does not function properly at the moment.</p> <p>Detection of proper merging can be rough: Sometimes, the module when detects that the ego has converged on the goal lanelet and that there are no more obstacles, the planner transitions to the goal planner, but the transition is not very smooth and could cause discomfort for the user.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_sampling_planner_module/#future-works","title":"Future works","text":"<p>Some possible improvements for this module include:</p> <p>-Implementing a dynamic weight tuning algorithm: Dynamically change weights depending on the scenario (ie. to prioritize more the paths with low curvature and low avg. lat. deviation after all obstacles have been avoided).</p> <p>-Implementing multi-objective optimization to improve computing time and possibly make a more dynamic soft constraints weight tuning. Related publication.</p> <p>-Implement bezier curves as another method to obtain samples, see the sampling_based_planner module.</p> <p>-Explore the possibility to replace several or other behavior path modules with the sampling based behavior path module.</p> <p>-Perform real-life tests of this module once it has matured and some of its limitations have been solved.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_sampling_planner_module/#other-possibilities","title":"Other possibilities","text":"<p>The module is currently aimed at creating paths for static obstacle avoidance. However, the nature of sampling planner allows this module to be expanded or repurposed to other tasks such as lane changes, dynamic avoidance and general reaching of a goal. It is possible, with a good dynamic/scenario dependant weight tuning to use the sampling planning approach as a replacement for the other behavior path modules, assuming good candidate pruning and good soft constraints weight tuning.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_side_shift_module/","title":"Side Shift design","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_side_shift_module/#side-shift-design","title":"Side Shift design","text":"<p>(For remote control) Shift the path to left or right according to an external instruction.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_side_shift_module/#overview-of-the-side-shift-module-process","title":"Overview of the Side Shift Module Process","text":"<ol> <li>Receive the required lateral offset input.</li> <li>Update the <code>requested_lateral_offset_</code> under the following conditions:    a. Verify if the last update time has elapsed.    b. Ensure the required lateral offset value is different from the previous one.</li> <li>Insert the shift points into the path if the side shift module's status is not in the SHIFTING status.</li> </ol> <p>Please be aware that <code>requested_lateral_offset_</code> is continuously updated with the latest values and is not queued.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_side_shift_module/#statuses-of-the-side-shift","title":"Statuses of the Side Shift","text":"<p>The side shift has three distinct statuses. Note that during the SHIFTING status, the path cannot be updated:</p> <ol> <li>BEFORE_SHIFT: Preparing for shift.</li> <li>SHIFTING: Currently in the process of shifting.</li> <li>AFTER_SHIFT: Shift completed.</li> </ol> <p> </p> side shift status"},{"location":"planning/behavior_path_planner/autoware_behavior_path_side_shift_module/#flowchart","title":"Flowchart","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/","title":"Start Planner design","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#start-planner-design","title":"Start Planner design","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#purpose-role","title":"Purpose / Role","text":"<p>This module generates and plans a path for safely merging from the shoulder lane or side of road lane into the center of the road lane.</p> <p>Specifically, it includes the following features:</p> <ul> <li>Plan the path to automatically start from the shoulder lane or side of road lane to center of road lane.</li> <li>When parked vehicles are present on the shoulder lane, the module generates a path that allows for starting with a gap of a specified margin.</li> <li>If a collision with other traffic participants is detected while traveling on the generated path, it will stop as much as possible.</li> </ul> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#use-cases","title":"Use Cases","text":"<p>Give an typical example of how path generation is executed. Showing example of path generation starts from shoulder lane, but also from side of road lane can be generated.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#use-case-1-shift-pull-out","title":"Use Case 1: Shift pull out","text":"<p>In the shoulder lane, when there are no parked vehicles ahead and the shoulder lane is sufficiently long, a forward shift pull out path (a clothoid curve with consistent jerk) is generated.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#use-case-2-geometric-pull-out","title":"Use Case 2: Geometric pull out","text":"<p>In the shoulder lane, when there are objects in front and the lane is not sufficiently long behind, a geometric pull out path is generated.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#use-case-3-backward-and-shift-pull-out","title":"Use Case 3: Backward and shift pull out","text":"<p>In the shoulder lane, when there are parked vehicles ahead and the lane is sufficiently long behind, a path that involves reversing before generating a forward shift pull out path is created.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#use-case-4-backward-and-geometric-pull-out","title":"Use Case 4: Backward and geometric pull out","text":"<p>In the shoulder lane, when there is an object ahead and not enough space to reverse sufficiently, a path that involves reversing before making an geometric pull out is generated.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#use-case-5-clothoid-pull-out","title":"Use Case 5: Clothoid pull out","text":"<p>This method creates smooth paths using clothoid curves that provide continuous curvature transitions.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#use-case-6-freespace-pull-out","title":"Use Case 6: Freespace pull out","text":"<p>If the map is annotated with the information that a free space path can be generated in situations where both shift and geometric pull out paths are impossible to create, a path based on the free space algorithm will be generated.</p> <p></p> <p>As a note, the patterns for generating these paths are based on default parameters, but as will be explained in the following sections, it is possible to control aspects such as making paths that involve reversing more likely to be generated, or making geometric paths more likely to be generated, by changing the path generation policy or adjusting the margin around static objects.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#limitations","title":"Limitations","text":"<p>Here are some notable limitations:</p> <ul> <li>If parked vehicles exist in front of, behind, or on both sides of the shoulder, and it's not possible to maintain a specified distance from them, a stopping path will be generated, leading to a potential deadlock.</li> <li>In the default settings of the behavior_path_planner, an avoidance module operates in a later stage and attempts to avoid objects. However, it is not guaranteed that the start_planner module will output a path that can successfully navigate around obstacles. This means that if an unavoidable path is generated, it could result in a deadlock.</li> <li>The performance of safety check relies on the accuracy of the predicted paths generated by the map_based_prediction node. It's important to note that, currently, predicted paths that consider acceleration are not generated, and paths for low-speed objects may not be accurately produced, which requires caution.</li> <li>Given the current specifications of the rule-based algorithms, there is a trade-off between the safety of a path and its naturalness to humans. While it's possible to adjust parameters to manage this trade-off, improvements are necessary to better reconcile these aspects.</li> </ul>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#startend-conditions","title":"Start/End Conditions","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#start-conditions","title":"Start Conditions","text":"<p>The <code>StartPlannerModule</code> is designed to initiate its execution based on specific criteria evaluated by the <code>isExecutionRequested</code> function. The module will not start under the following conditions:</p> <ol> <li> <p>Start pose on the middle of the road: The module will not initiate if the start pose of the vehicle is determined to be in the middle of the road. This ensures the planner starts from a roadside position.</p> </li> <li> <p>Vehicle is far from start position: If the vehicle is far from the start position, the module will not execute. This prevents redundant execution when the new goal is given.</p> </li> <li> <p>Vehicle reached goal: The module will not start if the vehicle has already reached its goal position, avoiding unnecessary execution when the destination is attained.</p> </li> <li> <p>Vehicle in motion: If the vehicle is still moving, the module will defer starting. This ensures that planning occurs from a stable, stationary state for safety.</p> </li> <li> <p>Goal behind in same route segment: The module will not initiate if the goal position is behind the ego vehicle within the same route segment. This condition is checked to avoid complications with planning routes that require the vehicle to move backward on its current path, which is currently not supported.</p> </li> </ol>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#end-conditions","title":"End Conditions","text":"<p>The <code>StartPlannerModule</code> terminates when specific conditions are met, depending on the type of planner being used. The <code>canTransitSuccessState</code> function determines whether the module should transition to the success state based on the following criteria:</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#when-the-freespace-planner-is-active","title":"When the Freespace Planner is active","text":"<ul> <li>If the end point of the freespace path is reached, the module transitions to the success state.</li> </ul>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#when-any-other-type-of-planner-is-active","title":"When any other type of planner is active","text":"<p>The transition to the success state is determined by the following conditions:</p> <ul> <li>If a reverse path is being generated or the search for a pull out path fails:<ul> <li>The module does not transition to the success state.</li> </ul> </li> <li>If the end point of the pull out path's shift section is reached:<ul> <li>The module transitions to the success state.</li> </ul> </li> </ul> <p>The flowchart below illustrates the decision-making process in the <code>canTransitSuccessState</code> function:</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#concept-of-safety-assurance","title":"Concept of safety assurance","text":"<p>The approach to collision safety is divided into two main components: generating paths that consider static information, and detecting collisions with dynamic obstacles to ensure the safety of the generated paths.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#1-generating-path-with-static-information","title":"1. Generating path with static information","text":"<ul> <li>Path deviation checks: This ensures that the path remains within the designated lanelets. By default, this feature is active, but it can be deactivated if necessary.</li> </ul> <ul> <li>Static obstacle clearance from the path: This involves verifying that a sufficient margin around static obstacles is maintained. The process includes creating a vehicle-sized footprint from the current position to the pull-out endpoint, which can be adjusted via parameters. The distance to static obstacle polygons is then calculated. If this distance is below a specified threshold, the path is deemed unsafe. Threshold levels (e.g., [2.0, 1.0, 0.5, 0.1]) can be configured, and the system searches for paths that meet the highest possible threshold based on a set search priority explained in following section, ensuring the selection of the safe path based on the policy. If no path meets the minimum threshold, it's determined that no safe path is available.</li> </ul> <ul> <li> <p>Clearance from stationary objects: Maintaining an adequate distance from stationary objects positioned in front of and behind the vehicle is imperative for safety. Despite the path and stationary objects having a confirmed margin, the path is deemed unsafe if the distance from the shift start position to a front stationary object falls below <code>collision_check_margin_from_front_object</code> meters, or if the distance to a rear stationary object is shorter than <code>back_objects_collision_check_margin</code> meters.</p> <ul> <li>Why is a margin from the front object necessary?   Consider a scenario in a \"geometric pull out path\" where the clearance from the path to a static obstacle is minimal, and there is a stopped vehicle ahead. In this case, although the path may meet safety standards and thus be generated, a concurrently operating avoidance module might deem it impossible to avoid the obstacle, potentially leading to vehicle deadlock. To ensure there is enough distance for avoidance maneuvers, the distance to the front obstacle is assessed. Increasing this parameter can prevent immobilization within the avoidance module but may also lead to the frequent generation of backward paths or geometric pull out path, resulting in paths that may seem unnatural to humans.</li> </ul> <ul> <li>Why is a margin from the rear object necessary?   For objects ahead, another behavior module can intervene, allowing the path to overwrite itself through an avoidance plan, even if the clearance from the path to a static obstacle is minimal, thus maintaining a safe distance from static obstacles. However, for objects behind the vehicle, it is impossible for other behavior modules other than the start_planner to alter the path to secure a margin, potentially leading to a deadlock by an action module like \"obstacle_stop_module\" and subsequent immobilization. Therefore, a margin is set for stationary objects at the rear.</li> </ul> </li> </ul> <p>Here's the expression of the steps start pose searching steps, considering the <code>collision_check_margins</code> is set at [2.0, 1.0, 0.5, 0.1] as example. The process is as follows:</p> <ol> <li> <p>Generating start pose candidates</p> <ul> <li>Set the current position of the vehicle as the base point.</li> <li>Determine the area of consideration behind the vehicle up to the <code>max_back_distance</code>.</li> <li>Generate candidate points for the start pose in the backward direction at intervals defined by <code>backward_search_resolution</code>.</li> <li>Include the current position as one of the start pose candidates.</li> </ul> <p></p> </li> <li> <p>Starting search at maximum margin</p> <ul> <li>Begin the search with the largest threshold (e.g., 2.0 meters).</li> <li>Evaluate each start pose candidate to see if it maintains a margin of more than 2.0 meters.</li> <li>Simultaneously, verify that the path generated from that start pose meets other necessary criteria (e.g., path deviation check).</li> <li>Following the search priority described later, evaluate each in turn and adopt the start pose if it meets the conditions.</li> </ul> </li> <li> <p>Repeating search according to threshold levels</p> <ul> <li>If no start pose meeting the conditions is found, lower the threshold to the next level (e.g., 1.0 meter) and repeat the search.</li> </ul> </li> <li> <p>Continuing the search</p> <ul> <li>Continue the search until a start pose that meets the conditions is found, or the threshold level reaches the minimum value (e.g., 0.1 meter).</li> <li>The aim of this process is to find a start pose that not only secures as large a margin as possible but also satisfies the conditions required for the path.</li> </ul> </li> <li> <p>Generating a stop path</p> <ul> <li>If no start pose satisfies the conditions at any threshold level, generate a stop path to ensure safety.</li> </ul> </li> </ol>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#search-priority","title":"search priority","text":"<p>If a safe path with sufficient clearance for static obstacles cannot be generated forward, a backward search from the vehicle's current position is conducted to locate a suitable start point for a pull out path generation.</p> <p>During this backward search, different policies can be applied based on <code>search_policy</code> parameter:</p> <p>Selecting <code>planner_priority</code> focuses on creating efficient paths by trying all candidates for each planner type first. Opting for <code>distance_priority</code> aims to find a location with the least possible backward movement by trying all planners for each candidate first.</p> <p></p> <p><code>PriorityOrder</code> is defined as a vector of pairs, where each element consists of a <code>size_t</code> index representing a start pose candidate index, and the planner type. The PriorityOrder vector is processed sequentially from the beginning, meaning that the pairs listed at the top of the vector are given priority in the selection process for pull out path generation.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#planner_priority","title":"<code>planner_priority</code>","text":"<p>When <code>search_policy</code> is set to <code>planner_priority</code>, the <code>PriorityOrder</code> array is populated in such a way that all start pose candidates are tried for each planner type before moving on to the next planner type. This prioritization is reflected in the order of the array, with all candidates for the first planner being listed before any candidates for the second planner.</p> Index Planner Type 0 shift_pull_out 1 shift_pull_out ... ... N shift_pull_out 0 geometric_pull_out 1 geometric_pull_out ... ... N geometric_pull_out <p>This approach prioritizes trying all candidates with each planner type before proceeding to the next planner type, which may be efficient in situations where certain planner types are more likely to be appropriate.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#distance_priority","title":"<code>distance_priority</code>","text":"<p>For <code>search_policy</code> set to <code>distance_priority</code>, the array alternates between planner types for each start pose candidate, which can minimize the distance the vehicle needs to move backward if the earlier candidates are successful.</p> Index Planner Type 0 shift_pull_out 0 geometric_pull_out 1 shift_pull_out 1 geometric_pull_out ... ... N shift_pull_out N geometric_pull_out <p>This ordering is beneficial when the priority is to minimize the backward distance traveled, giving an equal chance for each planner to succeed at the closest possible starting position.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#2-collision-detection-with-dynamic-obstacles","title":"2. Collision detection with dynamic obstacles","text":"<ul> <li>Applying RSS in Dynamic Collision Detection: Collision detection is based on the RSS (Responsibility-Sensitive Safety) model to evaluate if a safe distance is maintained. See safety check feature explanation</li> </ul> <ul> <li>Collision check performed range: Safety checks for collisions with dynamic objects are conducted within the defined boundaries between the start and end points of each maneuver, ensuring the ego vehicle does not impede or hinder the progress of dynamic objects that come from behind it.</li> </ul> <ul> <li>Collision response policy: Should a collision with dynamic objects be detected along the generated path, deactivate module decision is registered if collision detection occurs before departure. If the vehicle has already commenced movement, an attempt to stop will be made, provided it's feasible within the braking constraints and that the rear vehicle can pass through the gap between the ego vehicle and the lane border.</li> </ul>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#example-of-safety-check-performed-range-for-shift-pull-out","title":"example of safety check performed range for shift pull out","text":"<p>Give an example of safety verification range for shift pull out. The safety check is performed from the start of the shift to the end of the shift. And if the vehicle footprint does not leave enough space for a rear vehicle to drive through, the safety check against dynamic objects is disabled.</p> <p></p> <p>As a note, no safety check is performed during backward movements. Safety verification commences at the point where the backward motion ceases.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#rtc-interface","title":"RTC interface","text":"<p>The system operates distinctly under manual and auto mode, especially concerning the before the departure and interaction with dynamic obstacles. Below are the specific behaviors for each mode:</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#when-approval-is-required","title":"When approval is required?","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#forward-driving","title":"Forward driving","text":"<ul> <li>Start approval required: Even if a path is generated, approval is required to initiate movement. If a dynamic object poses a risk, such as an approaching vehicle from behind, candidate paths may be displayed, but approval is necessary for departure.</li> </ul>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#backward-driving-forward-driving","title":"Backward driving + forward driving","text":"<ul> <li>Multiple approvals required: When the planned path includes a backward driving, multiple approvals are needed before starting the reverse and again before restarting forward movement. Before initiating forward movement, the system conducts safety checks against dynamic obstacles. If a detection is detected, approval for departure is necessary.</li> </ul> <p>This differentiation ensures that the vehicle operates safely by requiring human intervention in manual mode(<code>enable_rtc</code> is true) at critical junctures and incorporating automatic safety checks in both modes to prevent unsafe operations in the presence of dynamic obstacles.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#design","title":"Design","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#general-parameters-for-start_planner","title":"General parameters for start_planner","text":"Name Unit Type Description Default value th_arrived_distance [m] double distance threshold for arrival of path termination 1.0 th_stopped_velocity [m/s] double velocity threshold for arrival of path termination 0.01 th_stopped_time [s] double time threshold for arrival of path termination 1.0 th_distance_to_middle_of_the_road [m] double distance threshold to determine if the vehicle is on the middle of the road 0.1 collision_check_margins [m] double Obstacle collision check margins list [2.0, 1.0, 0.5, 0.1] shift_collision_check_distance_from_end [m] double collision check distance from end shift end pose -10.0 geometric_collision_check_distance_from_end [m] double collision check distance from end geometric end pose 0.0 collision_check_margin_from_front_object [m] double collision check margin from front object 5.0 skip_rear_vehicle_check - bool flag to skip rear vehicle check (rear vehicle check is performed to skip safety check and proceed with departure when the ego vehicle is obstructing the progress of a rear vehicle) false extra_width_margin_for_rear_obstacle [m] double extra width that is added to the perceived rear obstacle's width when deciding if the rear obstacle can overtake the ego vehicle while it is merging to a lane from the shoulder lane 0.5 object_types_to_check_for_path_generation.check_car - bool flag to check car for path generation true object_types_to_check_for_path_generation.check_truck - bool flag to check truck for path generation true object_types_to_check_for_path_generation.check_bus - bool flag to check bus for path generation true object_types_to_check_for_path_generation.check_bicycle - bool flag to check bicycle for path generation true object_types_to_check_for_path_generation.check_motorcycle - bool flag to check motorcycle for path generation true object_types_to_check_for_path_generation.check_pedestrian - bool flag to check pedestrian for path generation true object_types_to_check_for_path_generation.check_unknown - bool flag to check unknown for path generation true center_line_path_interval [m] double reference center line path point interval 1.0 <p>| clothoid_initial_velocity | [m/s] | double | initial velocity for clothoid path | 1.0 | | clothoid_acceleration | [m/s\u00b2] | double | acceleration for clothoid path | 1.0 | | clothoid_max_steer_angles_deg | [deg] | double | maximum steer angles to try | [5.0, 10.0, 20.0] | | clothoid_max_steer_angle_rate_deg_per_sec | [deg/s] | double | maximum steer angle rate | 10.0 | | check_clothoid_path_lane_departure | - | bool | flag to check if clothoid path footprints are out of lane | true |</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#ego-vehicles-velocity-planning","title":"Ego vehicle's velocity planning","text":"<p>WIP</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#safety-check-in-free-space-area","title":"Safety check in free space area","text":"<p>WIP</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#parameters-for-safety-check","title":"Parameters for safety check","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#stop-condition-parameters","title":"Stop Condition Parameters","text":"<p>Parameters under <code>stop_condition</code> define the criteria for stopping conditions.</p> Name Unit Type Description Default value maximum_deceleration_for_stop [m/s^2] double Maximum deceleration allowed for a stop 1.0 maximum_jerk_for_stop [m/s^3] double Maximum jerk allowed for a stop 1.0"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#ego-predicted-path-parameters","title":"Ego Predicted Path Parameters","text":"<p>Parameters under <code>path_safety_check.ego_predicted_path</code> specify the ego vehicle's predicted path characteristics.</p> Name Unit Type Description Default value min_velocity [m/s] double Minimum velocity of the ego vehicle's predicted path 0.0 acceleration [m/s^2] double Acceleration for the ego vehicle's predicted path 1.0 max_velocity [m/s] double Maximum velocity of the ego vehicle's predicted path 1.0 time_horizon_for_front_object [s] double Time horizon for predicting front objects 10.0 time_horizon_for_rear_object [s] double Time horizon for predicting rear objects 10.0 time_resolution [s] double Time resolution for the ego vehicle's predicted path 0.5 delay_until_departure [s] double Delay until the ego vehicle departs 1.0"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#target-object-filtering-parameters","title":"Target Object Filtering Parameters","text":"<p>Parameters under <code>target_filtering</code> are related to filtering target objects for safety check.</p> Name Unit Type Description Default value safety_check_time_horizon [s] double Time horizon for predicted paths of the ego and dynamic objects 5.0 safety_check_time_resolution [s] double Time resolution for predicted paths of the ego and dynamic objects 1.0 object_check_forward_distance [m] double Forward distance for object detection 10.0 object_check_backward_distance [m] double Backward distance for object detection 100.0 ignore_object_velocity_threshold [m/s] double Velocity threshold below which objects are ignored 1.0 object_types_to_check.check_car - bool Flag to check cars true object_types_to_check.check_truck - bool Flag to check trucks true object_types_to_check.check_bus - bool Flag to check buses true object_types_to_check.check_trailer - bool Flag to check trailers true object_types_to_check.check_bicycle - bool Flag to check bicycles true object_types_to_check.check_motorcycle - bool Flag to check motorcycles true object_types_to_check.check_pedestrian - bool Flag to check pedestrians true object_types_to_check.check_unknown - bool Flag to check unknown object types false object_lane_configuration.check_current_lane - bool Flag to check the current lane true object_lane_configuration.check_right_side_lane - bool Flag to check the right side lane true object_lane_configuration.check_left_side_lane - bool Flag to check the left side lane true object_lane_configuration.check_shoulder_lane - bool Flag to check the shoulder lane true object_lane_configuration.check_other_lane - bool Flag to check other lanes false include_opposite_lane - bool Flag to include the opposite lane in check false invert_opposite_lane - bool Flag to invert the opposite lane check false check_all_predicted_path - bool Flag to check all predicted paths true use_all_predicted_path - bool Flag to use all predicted paths true use_predicted_path_outside_lanelet - bool Flag to use predicted paths outside of lanelets false"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#safety-check-parameters","title":"Safety Check Parameters","text":"<p>Parameters under <code>safety_check_params</code> define the configuration for safety check.</p> Name Unit Type Description Default value enable_safety_check - bool Flag to enable safety check true check_all_predicted_path - bool Flag to check all predicted paths true publish_debug_marker - bool Flag to publish debug markers false rss_params.rear_vehicle_reaction_time [s] double Reaction time for rear vehicles 2.0 rss_params.rear_vehicle_safety_time_margin [s] double Safety time margin for rear vehicles 1.0 rss_params.lateral_distance_max_threshold [m] double Maximum lateral distance threshold 2.0 rss_params.longitudinal_distance_min_threshold [m] double Minimum longitudinal distance threshold 3.0 rss_params.longitudinal_velocity_delta_time [s] double Delta time for longitudinal velocity 0.8 hysteresis_factor_expand_rate - double Rate to expand/shrink the hysteresis factor 1.0 collision_check_yaw_diff_threshold - double Maximum yaw difference between ego and object when executing rss-based collision checking 1.578"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#path-generation","title":"Path Generation","text":"<p>There are three path generation methods.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#shift-pull-out","title":"shift pull out","text":"<p>This is the most basic method of starting path planning and is used on road lanes and shoulder lanes when there is no particular obstruction.</p> <p>Pull out distance is calculated by the speed, lateral deviation, and the lateral jerk. The lateral jerk is searched for among the predetermined minimum and maximum values, and the one that generates a safe path is selected.</p> <ul> <li>Generate the road lane centerline and shift it to the current position.</li> <li>In the section between merge start and end, path is shifted by a method that is used to generate avoidance path (four segmental constant jerk polynomials)</li> <li>Combine this path with center line of road lane</li> </ul> <p></p> <p>shift pull out video</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#parameters-for-shift-pull-out","title":"parameters for shift pull out","text":"Name Unit Type Description Default value check_shift_path_lane_departure [-] bool flag whether to check if shift path footprints are out of lane true allow_check_shift_path_lane_departure_override [-] bool flag to override/cancel lane departure check if the ego vehicle's starting pose is already out of lane false shift_pull_out_velocity [m/s] double velocity of shift pull out 2.0 pull_out_sampling_num [-] int Number of samplings in the minimum to maximum range of lateral_jerk 4 maximum_lateral_jerk [m/s3] double maximum lateral jerk 2.0 minimum_lateral_jerk [m/s3] double minimum lateral jerk 0.1 minimum_shift_pull_out_distance [m] double minimum shift pull out distance. if calculated pull out distance is shorter than this, use this for path generation. 0.0 maximum_curvature [1/m] double maximum curvature. Calculate the required pull out distance from this maximum curvature, assuming the reference path is considered a straight line and shifted by two approximate arcs. This does not compensate for curvature in a shifted path or curve. 0.07 end_pose_curvature_threshold [1/m] double The curvature threshold which is used for calculating the shift pull out distance. The shift end pose is shifted forward so that curvature on shift end pose is less than this value. This is to prevent the generated path from having a large curvature when the end pose is on a curve. If a shift end pose with a curvature below the threshold is not found, the shift pull out distance is used as the distance to the point with the lowest curvature among the points beyond a certain distance. 0.01"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#geometric-pull-out","title":"geometric pull out","text":"<p>Generate two arc paths with discontinuous curvature. Ego-vehicle stops once in the middle of the path to control the steer on the spot. See also [1] for details of the algorithm.</p> <p></p> <p>geometric pull out video</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#parameters-for-geometric-pull-out","title":"parameters for geometric pull out","text":"Name Unit Type Description Default value divide_pull_out_path [-] bool flag whether to divide arc paths. The path is assumed to be divided because the curvature is not continuous. But it requires a stop during the departure. false geometric_pull_out_velocity [m/s] double velocity of geometric pull out 1.0 lane_departure_margin [m] double margin of deviation to lane right 0.2 lane_departure_check_expansion_margin [m] double margin to expand the ego vehicle footprint when doing lane departure checks 0.0 geometric_pull_out_max_steer_angle_margin_scale [-] double scaling factor applied to the maximum steering angle (max_steer_angle) defined in vehicle_info parameter 0.72"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#clothoid-pull-out","title":"clothoid pull out","text":"<p>Generate smooth paths using clothoid curves that provide continuous curvature transitions. The clothoid path consists of three segments: entry clothoid, circular arc, and exit clothoid, ensuring smooth steering transitions.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#path-generation-flow","title":"Path Generation Flow","text":"<p>The path is generated with following flow.</p> <ol> <li> <p>Parameter Setting and Initialization</p> </li> <li> <p>Get lane information</p> <ul> <li>Get road lane (target lane) and shoulder lane (start lane) information.</li> </ul> </li> <li> <p>Start and Target Path Generation</p> <ul> <li>Generate the centerline path of the target lane.</li> <li>Generated the straight path of the shoulder lane.</li> </ul> </li> <li> <p>Circular Arc Path Generation (red dotted line in the following figure)</p> <ul> <li>Generate circular arc path using the same method as geometric pull out.</li> <li>Calculate composite arc path with two arc segments (entry and exit arcs).</li> <li>This process is repeated with gradually increasing maximum steering angles from the parameter list <code>clothoid_max_steer_angles_deg</code> (e.g., [5.0, 10.0, 20.0] degrees) until a valid path is found.</li> </ul> </li> <li> <p>Clothoid Approximation</p> <ul> <li>Convert the circular arc segments to clothoid curves.</li> <li>Generate three-segment clothoid path: entry clothoid \u2192 circular arc \u2192 exit clothoid for each arc path (black dotted line in the following figure).</li> <li>Apply rigid transform to align the start and goal points of the approximated clothoid path with the original arc path (black line in the following figure).</li> </ul> </li> <li> <p>Lane Departure Check and Path Validation</p> </li> <li> <p>Collision Check</p> </li> </ol> <p></p> <p>The following diagram illustrates the process flow for generating clothoid paths:</p> <pre><code>flowchart TD\n    A[Start: Parameter Setting and Initialization] --&gt; B[Get Lane Information]\n    B --&gt; C[Start and Target Path Generation]\n    C --&gt; D[Select a candidate max steer angle]\n    D --&gt; E[Generate Circular Arc Path]\n    E --&gt; F[Clothoid Approximation]\n    F --&gt; G[Lane Departure Check and Path Validation]\n    G --&gt; H[Collision Check]\n    H --&gt; I[Is the path valid?]\n    I --&gt;|No| D\n    I --&gt;|Yes| J[End: Return Path]\n    D --&gt;|No More Candidate| K[Path Generation Failure]</code></pre>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#parameters-for-clothoid-pull-out","title":"parameters for clothoid pull out","text":"Name Unit Type Description Default value clothoid_initial_velocity [m/s] double initial velocity for clothoid path 1.0 clothoid_acceleration [m/s\u00b2] double acceleration for clothoid path 1.0 clothoid_max_steer_angles_deg [deg] double maximum steer angles to try [5.0, 10.0, 20.0] clothoid_max_steer_angle_rate_deg_per_sec [deg/s] double maximum steer angle rate 10.0 clothoid_collision_check_distance_from_end [m] double collision check distance from end for clothoid planner 0.0 check_clothoid_path_lane_departure [-] bool flag whether to check if clothoid path footprints are out of lane true"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#limitation","title":"Limitation","text":"<ul> <li>Violation of the max curvature: During the rigid transformation of the approximated clothoid path to align with the original circular arc path, the scaling factor may cause the maximum steering angle constraint to be violated. When the scale factor is smaller than 1.0, the curvature of the transformed clothoid path may exceed the maximum curvature corresponding to the specified maximum steering angle.</li> </ul> <ul> <li>Yaw Angle Deviation at Path Endpoints: The rigid transformation process introduces yaw angle deviations at the start and end points of the clothoid path. As shown in the figure above, note that the yaw angles at the start and end points differ between the original circular arc path (red dotted line) and the transformed clothoid path (black solid line).</li> </ul>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#backward-pull-out-start-point-search","title":"backward pull out start point search","text":"<p>If a safe path cannot be generated from the current position, search backwards for a pull out start point at regular intervals(default: <code>2.0</code>).</p> <p></p> <p>pull out after backward driving video</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#parameters-for-backward-pull-out-start-point-search","title":"parameters for backward pull out start point search","text":"Name Unit Type Description Default value enable_back [-] bool flag whether to search backward for start_point true search_priority [-] string[] list of planner types in priority order. Available: \"SHIFT\", \"GEOMETRIC\", \"CLOTHOID\" [\"SHIFT\",\"GEOMETRIC\"] search_policy [-] string search policy: \"planner_priority\" (planner-first: SHIFT all candidates, then GEOMETRIC ...) or \"distance_priority\" (candidate-first: 0m SHIFT, 0m GEOMETRIC, 2m SHIFT, ...) \"planner_priority\" max_back_distance [m] double maximum back distance 30.0 backward_search_resolution [m] double distance interval for searching backward pull out start point 2.0 backward_path_update_duration [s] double time interval for searching backward pull out start point. this prevents chattering between back driving and pull_out 3.0 ignore_distance_from_lane_end [m] double If distance from shift start pose to end of shoulder lane is less than this value, this start pose candidate is ignored 15.0"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#freespace-pull-out","title":"freespace pull out","text":"<p>If the vehicle gets stuck with pull out along lanes, execute freespace pull out. To run this feature, you need to set <code>parking_lot</code> to the map, <code>activate_by_scenario</code> of costmap_generator to <code>false</code> and <code>enable_freespace_planner</code> to <code>true</code></p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#unimplemented-parts-limitations-for-freespace-pull-out","title":"Unimplemented parts / limitations for freespace pull out","text":"<ul> <li>When a short path is generated, the ego does can not drive with it.</li> <li>Complex cases take longer to generate or fail.</li> <li>The drivable area is not guaranteed to fit in the parking_lot.</li> </ul>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_start_planner_module/#parameters-freespace-parking","title":"Parameters freespace parking","text":"Name Unit Type Description Default value enable_freespace_planner [-] bool this flag activates a free space pullout that is executed when a vehicle is stuck due to obstacles in the lanes where the ego is located true end_pose_search_start_distance [m] double distance from ego to the start point of the search for the end point in the freespace_pull_out driving lane 20.0 end_pose_search_end_distance [m] double distance from ego to the end point of the search for the end point in the freespace_pull_out driving lane 30.0 end_pose_search_interval [m] bool interval to search for the end point in the freespace_pull_out driving lane 2.0 <p>See freespace_planner for other parameters.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/","title":"Avoidance module for static objects","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#avoidance-module-for-static-objects","title":"Avoidance module for static objects","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#purposerole","title":"Purpose/Role","text":"<p>This is a rule-based avoidance module, which runs based on perception output data, HDMap, current path and route. This module is designed to create avoidance paths for static (=stopped) objects in simple situations. Currently, this module doesn't support dynamic (=moving) objects.</p> <p></p> <p>This module has an RTC interface, and the user can select operation mode from MANUAL/AUTO depending on the performance of the vehicle's sensors. If the user selects MANUAL mode, this module outputs a candidate avoidance path and awaits operator approval. In the case where the sensor/perception performance is insufficient and false positives occur, we recommend MANUAL mode to prevent unnecessary avoidance maneuvers.</p> <p>If the user selects AUTO mode, this module modifies the current following path without operator approval. If the sensor/perception performance is sufficient, use AUTO mode.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#limitations","title":"Limitations","text":"<p>This module allows developers to design vehicle behavior in avoidance planning using specific rules. Due to the property of rule-based planning, the algorithm cannot compensate for not colliding with obstacles in complex cases. This is a trade-off between \"be intuitive and easy to design\" and \"be hard to tune but can handle many cases\". This module adopts the former policy and therefore this output should be checked more strictly in the later stage. In the .iv reference implementation, there is another avoidance module in the motion planning module that uses optimization to handle the avoidance in complex cases. (Note that, the motion planner needs to be adjusted so that the behavior result will not be changed much in the simple case and this is a typical challenge for the behavior-motion hierarchical architecture.)</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#why-is-avoidance-in-behavior-module","title":"Why is avoidance in behavior module?","text":"<p>This module executes avoidance over lanes, and the decision requires the lane structure information to take care of traffic rules (e.g. it needs to send an indicator signal when the vehicle crosses a lane). The difference between the motion and behavior modules in the planning stack is whether the planner takes traffic rules into account, which is why this avoidance module exists in the behavior module.</p> <p></p> <p>If you would like to know the overview rather than the detail, please skip the next section and refer to FAQ.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#inner-workingsalgorithms","title":"Inner workings/Algorithms","text":"<p>This module mainly has two parts, target filtering and path generation. At first, all objects are filtered by several conditions. In this step, the module checks avoidance feasibility and necessity. After that, this module generates avoidance path outline, which we call shift line, based on filtered objects. The shift lines are set into path shifter, which is a library for path generation, to create a smooth shift path. Additionally, this module has a feature to check non-target objects so that the ego can avoid target objects safely. This feature receives a generated avoidance path and surrounding objects, and judges the current situation. Lastly, this module updates current ego behavior.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#target-object-filtering","title":"Target object filtering","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#overview","title":"Overview","text":"<p>The module uses the following conditions to filter avoidance target objects.</p> Check condition Target class Details If conditions are not met Is an avoidance target class object? All Use can select avoidance target class from config file. Never avoid it. Is a stopped object? All Objects keep higher speed than <code>th_moving_speed</code> for longer period of time than <code>th_moving_time</code> is judged as moving. Never avoid it. Is within detection area? All The module creates detection area to filter target objects roughly based on lateral margin in config file. (see here) Never avoid it. Isn't there enough lateral distance between the object and path? All - Never avoid it. Is near the centerline of ego lane? All - It depends on other conditions. Is there a crosswalk near the object? Pedestrian, Bicycle The module doesn't avoid the Pedestrian and Bicycle nearer the crosswalk because the ego should stop in front of it if they're crossing the road. (see here) Never avoid it. Is the distance between the object and traffic light along the path longer than the threshold? Car, Truck, Bus, Trailer The module uses this condition when there is ambiguity about whether the vehicle is parked. It depends on other conditions. Is the distance between the object and crosswalk light along the path longer than threshold? Car, Truck, Bus, Trailer Same as above. It depends on other conditions. Is the stopping time longer than threshold? Car, Truck, Bus, Trailer Same as above. It depends on other conditions. Is within intersection? Car, Truck, Bus, Trailer The module assumes that there isn't any parked vehicle within intersection. It depends on other conditions. Is on ego lane? Car, Truck, Bus, Trailer - It depends on other conditions. Is a parked vehicle? Car, Truck, Bus, Trailer The module judges whether the vehicle is a parked vehicle based on its lateral offset. (see here) It depends on other conditions. Is merging into ego lane from other lane? Car, Truck, Bus, Trailer The module judges the vehicle behavior based on its yaw angle and offset direction. (see here) It depends on other conditions. Is merging into other lane from ego lane? Car, Truck, Bus, Trailer Same as above. It depends on other conditions."},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#common-conditions","title":"Common conditions","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#detection-area","title":"Detection area","text":"<p>The module generates detection area for target filtering based on the following parameters:</p> <pre><code>      # avoidance is performed for the object type with true\n      target_object:\n      ...\n          lateral_margin:\n            soft_margin: 0.3                            # [m]\n            hard_margin: 0.2                            # [m]\n            hard_margin_for_parked_vehicle: 0.7         # [m]\n      ...\n      # For target object filtering\n      target_filtering:\n      ...\n        # detection area generation parameters\n        detection_area:\n          static: false                                 # [-]\n          min_forward_distance: 50.0                    # [m]\n          max_forward_distance: 150.0                   # [m]\n          backward_distance: 10.0                       # [m]\n</code></pre>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#width-of-detection-area","title":"Width of detection area","text":"<ol> <li>Get the largest lateral margin of all classes (Car, Truck, ...). The margin is the sum of <code>soft_margin</code> and <code>hard_margin_for_parked_vehicle</code>.</li> <li>The detection area width is the sum of ego vehicle width and the largest lateral margin.</li> </ol>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#longitudinal-distance-of-detection-area","title":"Longitudinal distance of detection area","text":"<p>If the parameter <code>detection_area.static</code> is set to <code>true</code>, the module creates detection area whose longitudinal distance is <code>max_forward_distance</code>.</p> <p>If the parameter <code>detection_area.static</code> is set to <code>false</code>, the module creates a detection area so that the ego can avoid objects with minimum lateral jerk value. Thus, the longitudinal distance depends on maximum lateral shift length, lateral jerk constraints and current ego speed. Additionally, it has to consider the distance used for the preparation phase.</p> <pre><code>...\n    const auto max_shift_length = std::max(\n      std::abs(parameters_-&gt;max_right_shift_length), std::abs(parameters_-&gt;max_left_shift_length));\n    const auto dynamic_distance =\n      PathShifter::calcLongitudinalDistFromJerk(max_shift_length, getLateralMinJerkLimit(), speed);\n\n    return std::clamp(\n      1.5 * dynamic_distance + getNominalPrepareDistance(),\n      parameters_-&gt;object_check_min_forward_distance,\n      parameters_-&gt;object_check_max_forward_distance);\n</code></pre> <p></p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#conditions-for-non-vehicle-type-objects","title":"Conditions for non-vehicle type objects","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#for-crosswalk-users","title":"For crosswalk users","text":"<p>If Pedestrian and Bicycle are closer to crosswalk than threshold 2.0m (hard coded for now), the module judges they're crossing the road and never avoids them.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#conditions-for-vehicle-type-objects","title":"Conditions for vehicle type objects","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#judge-vehicle-behavior","title":"Judge vehicle behavior","text":"<p>The module classifies vehicles into the following three behaviors based on yaw angle and offset direction.</p> <pre><code># params for filtering objects that are in intersection\nintersection:\n  yaw_deviation: 0.349 # [rad] (default 20.0deg)\n</code></pre> Behavior Details Figure NONE If the object's relative yaw angle to lane is less than threshold <code>yaw_deviation</code>, it is classified into <code>NONE</code>. MERGING See following flowchart. DEVIATING See following flowchart. <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#judge-if-its-a-parked-vehicle","title":"Judge if it's a parked vehicle","text":"<p>Not only the length from the centerline, but also the length from the road shoulder is calculated and used for the filtering process. In this logic, it calculates ratio of actual shift length to shiftable shift length as follows. If the result is larger than threshold <code>th_shiftable_ratio</code>, the module judges the vehicle is a parked vehicle.</p> \\[ L_{d} = \\frac{W_{lane} - W_{obj}}{2}, \\\\ ratio =  \\frac{L_{a}}{L_{d}} \\] <ul> <li>\\(L_{d}\\) : shiftable length.</li> <li>\\(L_{a}\\) : actual shift length.</li> <li>\\(W_{lane}\\) : lane width.</li> <li>\\(W_{obj}\\) : object width.</li> </ul> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#target-object-filtering_1","title":"Target object filtering","text":"Situation Details Ego behavior Vehicle is within intersection area defined in HDMap. The module ignores vehicles following a lane or merging into ego lane. Never avoid it. Vehicle is on ego lane. There are adjacent lanes for both sides. Never avoid it. Vehicle is merging into other lane from ego lane. Most of its footprint is on ego lane. Never avoid it. Vehicle is merging into ego lane from other lane. Most of its footprint is on ego lane. Never avoid it. Vehicle does not appear to be parked and is stopped in front of a crosswalk or traffic light. Never avoid it. Vehicle stops on ego lane while pulling over to the side of the road. Avoid it immediately. Vehicle stops on adjacent lane. Avoid it immediately. Vehicle stops on ego lane without pulling over to the side of the road. Set the parameter <code>avoidance_for_ambiguous_vehicle.enable</code> to <code>true</code>, the module avoids ambiguous vehicle. Vehicle is merging into ego lane from other lane. Set the parameter <code>avoidance_for_ambiguous_vehicle.enable</code> to <code>true</code>, the module avoids ambiguous vehicle. Vehicle is merging into other lane from ego lane. Set the parameter <code>avoidance_for_ambiguous_vehicle.enable</code> to <code>true</code>, the module avoids ambiguous vehicle."},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#flowchart","title":"Flowchart","text":"<p>There are three main filtering functions <code>isSatisfiedWithCommonCondition()</code>, <code>isSatisfiedWithVehicleCondition()</code> and <code>isSatisfiedWithNonVehicleCondition()</code>. The filtering process is executed according to the following flowchart. Additionally, the module checks avoidance necessity in <code>isNoNeedAvoidanceBehavior()</code> based on the object pose, ego path and lateral margin in the config file.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#common-conditions_1","title":"Common conditions","text":"<p>At first, the function <code>isSatisfiedWithCommonCondition()</code> includes conditions used for all object classes.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#conditions-for-vehicle","title":"Conditions for vehicle","text":"<p>Target class:</p> <ul> <li>Car</li> <li>Truck</li> <li>Bus</li> <li>Trailer</li> </ul> <p>As a next step, the object is filtered by a condition specialized for its class.</p> <p></p> <p></p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#conditions-for-non-vehicle-objects","title":"Conditions for non-vehicle objects","text":"<ul> <li>Pedestrian</li> <li>Bicycle</li> </ul>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#when-target-object-has-gone","title":"When target object has gone","text":"<p>User can select the ego behavior when the target object has gone.</p> <pre><code>cancel:\n  enable: true # [-]\n</code></pre> <p>If the above parameter is <code>true</code>, this module reverts avoidance path when the following conditions are met.</p> <ul> <li>All target objects have gone.</li> <li>The ego vehicle hasn't initiated avoidance maneuver yet.</li> </ul> <p></p> <p>If the parameter is <code>false</code>, this module keeps running even after the target object has gone.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#path-generation","title":"Path generation","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#how-to-prevent-shift-line-chattering-that-is-caused-by-perception-noise","title":"How to prevent shift line chattering that is caused by perception noise","text":"<p>Since the object recognition result contains noise related to position, orientation and polygon shape, if the module uses the raw object recognition results in path generation, the output path will be directly affected by the noise. Therefore, in order to reduce the influence of the noise, this module generates a polygon for each target object, and the output path is generated based on that.</p> <p></p> <p>The envelope polygon is a rectangle box, whose size depends on the object's polygon and buffer parameter <code>envelope_buffer_margin</code>. Additionally, it is always parallel to the reference path. When the module finds a target object for the first time, it initializes the polygon.</p> <pre><code>        car:\n          ...\n          envelope_buffer_margin: 0.5                   # [m] FOR DEVELOPER\n</code></pre> <p></p> <p>The module creates a one-shot envelope polygon by using the latest object pose and raw polygon in every planning cycle. On the other hand, the module uses the envelope polygon information created in the last planning cycle in order to update the envelope polygon with consideration of the pose covariance.</p> <p>If the pose covariance is smaller than the threshold, the envelope polygon would be updated according to the following logic. If the one-shot envelope polygon is not within the previous envelope polygon, the module creates a new envelope polygon. Otherwise, it keeps the previous envelope polygon.</p> <p></p> <p>When the pose covariance is larger than the threshold, it is compared with the maximum pose covariance of each object. If the value is smaller, the one-shot envelope polygon is used directly as the envelope polygon. Otherwise, it keeps the previous envelope polygon.</p> <p>By doing this process, the envelope polygon size and pose will converge even if perception output includes noise in object pose or shape.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#relationship-between-envelope-polygon-and-avoidance-path","title":"Relationship between envelope polygon and avoidance path","text":"<p>The avoidance path has two shift sections, whose start or end point position depends on the envelope polygon. The end point of the avoidance shift section and start point of the return shift section are fixed based on the envelope polygon and the other side edges are dynamically changed based on ego speed, shift length, lateral jerk constraints, etc.</p> <p>The lateral positions of the two points are decided so that there can be enough space (=lateral margin) between ego body and the most overhang point of the envelope polygon edge points. User can adjust lateral margin with the following parameters.</p> <pre><code>        car:\n          ...\n          lateral_margin:\n            soft_margin: 0.3                            # [m]\n            hard_margin: 0.2                            # [m]\n            hard_margin_for_parked_vehicle: 0.7         # [m]\n</code></pre> <p>The longitudinal positions depends on the envelope polygon, ego vehicle specification and the following parameters. The longitudinal distance between avoidance shift section end point and envelope polygon (=front longitudinal buffer) is the sum of <code>front_overhang</code> defined in <code>vehicle_info.param.yaml</code> and <code>longitudinal_margin</code> if the parameter <code>consider_front_overhang</code> is <code>true</code>. If <code>consider_front_overhang</code> is <code>false</code>, only <code>longitudinal_margin</code> is considered. Similarly, the distance between the return shift section start point and envelope polygon (=rear longitudinal buffer) is the sum of <code>rear_overhang</code> and <code>longitudinal_margin</code>.</p> <pre><code>      target_object:\n        car:\n          ...\n          longitudinal_margin: 0.0                      # [m]\n\n      ...\n      avoidance:\n        ...\n        longitudinal:\n          ...\n          consider_front_overhang: true                 # [-]\n          consider_rear_overhang: true                  # [-]\n</code></pre> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#lateral-margin","title":"Lateral margin","text":"<p>As mentioned above, user can adjust lateral margin by changing the following two types of parameters. The <code>soft_margin</code> is a soft constraint parameter for lateral margin. The <code>hard_margin</code> and <code>hard_margin_for_parked_vehicle</code> are hard constraint parameters.</p> <pre><code>        car:\n          ...\n          lateral_margin:\n            soft_margin: 0.3                            # [m]\n            hard_margin: 0.2                            # [m]\n            hard_margin_for_parked_vehicle: 0.7         # [m]\n</code></pre> <p>Basically, this module tries to generate an avoidance path in order to keep lateral distance, which is the sum of <code>soft_margin</code> and <code>hard_margin</code>/<code>hard_margin_for_parked_vehicle</code>, from the avoidance target object.</p> <p></p> <p>But if there isn't enough space to keep <code>soft_margin</code> distance, this module shortens soft constraint lateral margin. The parameter <code>soft_margin</code> is a maximum value of soft constraint, and actual soft margin can be a value between 0.0 and <code>soft_margin</code>. On the other hand, this module definitely keeps <code>hard_margin</code> or <code>hard_margin_for_parked_vehicle</code> depending on the situation. Thus, the minimum value of total lateral margin is <code>hard_margin</code>/<code>hard_margin_for_parked_vehicle</code>, and the maximum value is the sum of <code>hard_margin</code>/<code>hard_margin_for_parked_vehicle</code> and <code>soft_margin</code>.</p> <p>The following figure shows the situation where this module shortens lateral soft constraint in order not to drive in the opposite lane when user sets parameter <code>use_lane_type</code> to <code>same_direction_lane</code>.</p> <p></p> <p>This module avoids not only parked vehicles but also non-parked vehicles that stop temporarily for some reason (e.g. waiting for traffic light to change from red to green). Additionally, this module has two types of hard margin parameters, <code>hard_margin</code> and <code>hard_margin_for_parked_vehicle</code> and judges if it is a parked vehicle or not for each vehicle because it takes the risk of vehicle doors opening suddenly and people getting out from parked vehicles into consideration.</p> <p>Users should set <code>hard_margin_for_parked_vehicle</code> larger than <code>hard_margin</code> to prevent collisions with doors or people who suddenly exit a vehicle.</p> <p>This module has only one parameter <code>soft_margin</code> for soft lateral margin constraint.</p> <p></p> <p>As the hard margin parameters define the distance the user definitely wants to maintain, they are used in the logic to check whether the ego can pass the side of the target object without executing an avoidance maneuver as well.</p> <p>If the lateral distance is less than <code>hard_margin</code>/<code>hard_margin_for_parked_vehicle</code> when assuming that the ego follows the current lane without an avoidance maneuver, this module thinks the ego can not pass the side of the object safely and the ego must avoid it. In this case, this module inserts a stop point until the avoidance maneuver is allowed to execute so that the ego can avoid the object after approval. (For example, the ego keeps stopping in front of such an object until the operator approves the avoidance maneuver if the module is in MANUAL mode.)</p> <p></p> <p>On the other hand, if the lateral distance is larger than <code>hard_margin</code>/<code>hard_margin_for_parked_vehicle</code>, this module doesn't insert a stop point even when it is waiting for approval because it thinks it is possible to pass the side of the object safely.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#when-there-is-not-enough-space","title":"When there is not enough space","text":"<p>This module inserts a stop point only when the ego can potentially avoid the object. So, if it is not able to keep a distance more than <code>hard_margin</code>/<code>hard_margin_for_parked_vehicle</code>, this module does nothing. The following figure shows the situation where this module is not able to keep enough lateral distance when the user sets parameter <code>use_lane_type</code> to <code>same_direction_lane</code>.</p> <p></p> <p>Info</p> <p>In this situation, the obstacle stop feature in obstacle_cruise_module is responsible for ego vehicle safety.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#shift-length-calculation","title":"Shift length calculation","text":"<p>The lateral shift length is the sum of <code>overhang_distance</code>, lateral margin, whose value is set in the config file, and half of ego vehicle width defined in <code>vehicle_info.param.yaml</code>. On the other hand, the module limits the shift length depending on the space the module can use for an avoidance maneuver and the parameters <code>soft_drivable_bound_margin</code> <code>hard_drivable_bound_margin</code>. Basically, the shift length is limited so that the ego doesn't get closer than <code>soft_drivable_bound_margin</code> to the drivable boundary. But the module allows the threshold to be relaxed from <code>soft_drivable_bound_margin</code> to <code>hard_drivable_bound_margin</code> when the road is narrow.</p> <p></p> <p>Usable lanes for the avoidance module can be selected using the config file.</p> <pre><code>      ...\n      # drivable lane setting. This module is able to use not only current lane but also right/left lane\n      # if the current lane(=lanelet::Lanelet) and the right/left lane share the boundary(=lanelet::Linestring) in HDMap.\n      # \"current_lane\"           : use only current lane. This module doesn't use adjacent lane to avoid object.\n      # \"same_direction_lane\"    : this module uses same direction lane to avoid object if needed.\n      # \"opposite_direction_lane\": this module uses both same direction and opposite direction lanes.\n      use_lane_type: \"opposite_direction_lane\"\n</code></pre> <p>When user set parameter <code>use_lane_type</code> to <code>opposite_direction_lane</code>, it is possible to use opposite lane.</p> <p></p> <p>When user set parameter <code>use_lane_type</code> to <code>same_direction_lane</code>, the module doesn't create path that overlaps opposite lane.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#shift-line-generation","title":"Shift line generation","text":"<p>As mentioned above, the end point of the avoidance shift path and the start point of the return shift path, which are FIXED points, are calculated from envelope polygon. As a next step, the module adjusts the other side points depending on shift length, current ego speed and lateral jerk constrain params defined in the config file.</p> <p>Since the two points are always on the centerline of the ego lane, the module only calculates longitudinal distance between the shift start and end point based on the following function. This function is defined in the path shifter library. See this page as well.</p> <pre><code>double PathShifter::calcLongitudinalDistFromJerk(\n  const double lateral, const double jerk, const double velocity)\n{\n  const double j = std::abs(jerk);\n  const double l = std::abs(lateral);\n  const double v = std::abs(velocity);\n  if (j &lt; 1.0e-8) {\n    return 1.0e10;\n  }\n  return 4.0 * std::pow(0.5 * l / j, 1.0 / 3.0) * v;\n}\n</code></pre> <p>We call the line that connects shift start and end point <code>shift_line</code>, which the avoidance path is generated from with spline completion.</p> <p>The start point of avoidance has another longitudinal constraint. In order to keep turning on the blinker for a few seconds before starting the avoidance maneuver, the avoidance start point must be further than the value (we call the distance <code>prepare_length</code>.) depending on ego speed from ego position.</p> <pre><code>longitudinal:\n  min_prepare_time: 1.0 # [s]\n  max_prepare_time: 2.0 # [s]\n  min_prepare_distance: 1.0 # [m]\n</code></pre> <p>The <code>prepare_length</code> is calculated as the product of ego speed and <code>max_prepare_time</code>. (When the ego speed is zero, <code>min_prepare_distance</code> is used.)</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#planning-at-red-traffic-light","title":"Planning at RED traffic light","text":"<p>This module takes traffic light information into account so that the ego can behave properly. Sometimes, the ego straddles the lane boundary but we want to prevent the ego from stopping in front of a red traffic signal in such a situation. This is because the ego will block adjacent lanes and it is inconvenient for other vehicles.</p> <p></p> <p>So, this module controls shift length and shift start/end point in order to prevent the above situation.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#control-shift-length","title":"Control shift length","text":"<p>At first, if the ego hasn't initiated an avoidance maneuver yet, this module limits maximum shift length and uses ONLY current lane during a red traffic signal. This prevents the ego from blocking other vehicles even if this module executes the avoidance maneuver and the ego is caught by a red traffic signal.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#control-avoidance-shift-start-point","title":"Control avoidance shift start point","text":"<p>Additionally, if the target object is farther than the stop line of the traffic light, this module sets the avoidance shift start point on the stop line in order to prevent the ego from stopping at a red traffic signal in the middle of an avoidance maneuver.</p> <p> </p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#control-return-shift-end-point","title":"Control return shift end point","text":"<p>If the ego has already initiated an avoidance maneuver, this module tries to set the return shift end point on the stop line.</p> <p> </p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#safety-check","title":"Safety check","text":"<p>This feature can be enabled by setting the following parameter to <code>true</code>.</p> <pre><code>      safety_check:\n        ...\n        enable: true                                    # [-]\n</code></pre> <p>This module pays attention not only to avoidance target objects but also non-target objects that are near the avoidance path, and if the avoidance path is unsafe due to surrounding objects, it reverts the avoidance maneuver and yields the lane to them.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#yield-maneuver","title":"Yield maneuver","text":"<p>Additionally, this module basically inserts a stop point in front of an avoidance target during yielding maneuvers in order to keep enough distance to avoid the target when it is safe to do so. If the shift side lane is congested, the ego stops at a point and waits.</p> <p>This feature can be enabled by setting the following parameter to <code>true</code>.</p> <pre><code>yield:\n  enable: true # [-]\n</code></pre> <p></p> <p>But if the lateral margin is larger than <code>hard_margin</code> (or <code>hard_margin_for_parked_vehicle</code>), this module doesn't insert a stop point because the ego can pass the side of the object safely without an avoidance maneuver.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#safety-check-target-lane","title":"Safety check target lane","text":"<p>User can select the safety check area with the following parameters. Basically, we recommend the following configuration to check only the shift side lane. If users want to confirm safety strictly, please set <code>check_current_lane</code> and/or <code>check_other_side_lane</code> to <code>true</code>.</p> <pre><code>      safety_check:\n      ...\n        check_current_lane: false                       # [-]\n        check_shift_side_lane: true                     # [-]\n        check_other_side_lane: false                    # [-]\n</code></pre> <p>In the avoidance module, the function <code>path_safety_checker::isCentroidWithinLanelet</code> is used for filtering objects by lane.</p> <pre><code>bool isCentroidWithinLanelet(const PredictedObject &amp; object, const lanelet::ConstLanelet &amp; lanelet)\n{\n  const auto &amp; object_pos = object.kinematics.initial_pose_with_covariance.pose.position;\n  lanelet::BasicPoint2d object_centroid(object_pos.x, object_pos.y);\n  return boost::geometry::within(object_centroid, lanelet.polygon2d().basicPolygon());\n}\n</code></pre> <p>Info</p> <p>If <code>check_current_lane</code> and/or <code>check_other_side_lane</code> are set to <code>true</code>, the possibility of false positives and unnecessary yield maneuvers increase.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#safety-check-algorithm","title":"Safety check algorithm","text":"<p>This module uses common safety check logic implemented in <code>path_safety_checker</code> library. See this page.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#limitation","title":"Limitation","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#limitation-1","title":"Limitation-1","text":"<p>The current behavior when the module judges it is unsafe is so conservative because it is difficult to achieve aggressive maneuvers (e.g. increase speed in order to increase the distance from rear vehicle) for current planning architecture.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#limitation-2","title":"Limitation-2","text":"<p>The yield maneuver is executed ONLY when the vehicle has NOT initiated avoidance maneuver yet. (This module checks objects in the opposite lane but it is necessary to find very far objects to judge whether it is unsafe before avoidance maneuver.) If it detects a vehicle which is approaching the ego during an avoidance maneuver, this module doesn't revert the path or insert a stop point. For now, there is no feature to deal with this situation in this module. Thus, a new module is needed to adjust the path to avoid moving objects, or an operator must override.</p> <p>Info</p> <p>This module has a threshold parameter <code>th_avoid_execution</code> for the shift length, and judges that the vehicle is initiating avoidance if the vehicle current shift exceeds the value.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#other-features","title":"Other features","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#compensation-for-detection-lost","title":"Compensation for detection lost","text":"<p>In order to prevent chattering of recognition results, once an obstacle is targeted, it is held for a while even if it disappears. This is effective when recognition is unstable. However, since it will result in over-detection (increases number of false-positives), it is necessary to adjust parameters according to the recognition accuracy (if <code>object_last_seen_threshold = 0.0</code>, the recognition result is 100% trusted).</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#drivable-area-expansion","title":"Drivable area expansion","text":"<p>This module supports drivable area expansion for following polygons defined in HDMap.</p> <ul> <li>Intersection area</li> <li>Hatched road marking</li> <li>Freespace area</li> </ul> <p>Please set the flags to <code>true</code> when user wants to make it possible to use those areas in avoidance maneuvers.</p> <pre><code># drivable lane setting. This module is able to use not only current lane but also right/left lane\n# if the current lane(=lanelet::Lanelet) and the right/left lane share the boundary(=lanelet::Linestring) in HDMap.\n# \"current_lane\"           : use only current lane. This module doesn't use adjacent lane to avoid object.\n# \"same_direction_lane\"    : this module uses the same direction lane to avoid object if needed.\n# \"opposite_direction_lane\": this module uses both same direction and opposite direction lanes.\nuse_lane_type: \"opposite_direction_lane\"\n# drivable area setting\nuse_intersection_areas: true\nuse_hatched_road_markings: true\nuse_freespace_areas: true\n</code></pre> use_lane_type: same_direction_lane use_lane_type: opposite_direction_lane intersection area The intersection area is defined on Lanelet map. See here hatched road markings The hatched road marking is defined on Lanelet map. See here freespace area The freespace area is defined on Lanelet map. (unstable)"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#future-extensionsunimplemented-parts","title":"Future extensions/Unimplemented parts","text":"<ul> <li>Consideration of the speed of the avoidance target<ul> <li>In the current implementation, only stopped vehicle is targeted as an avoidance target. It is needed to support the overtaking function for low-speed vehicles, such as a bicycle. (It is actually possible to overtake the low-speed objects by changing the parameter, but the logic is not supported and thus the safety cannot be guaranteed.)</li> <li>Overtaking (e.g., to overtake a vehicle running in front at 20 km/h at 40 km/h) may need to be handled outside the avoidance module. It should be discussed which module should handle it.</li> </ul> </li> </ul> <ul> <li>Cancel avoidance when target disappears<ul> <li>In the current implementation, even if the avoidance target disappears, the avoidance path will remain. If there is no longer a need to avoid, it must be canceled.</li> </ul> </li> </ul> <ul> <li>Improved performance of avoidance target selection<ul> <li>Essentially, avoidance targets are judged based on whether they are static objects or not. For example, a vehicle waiting at a traffic light should not be avoided because we know that it will start moving in the future. However this decision cannot be made in the current Autoware due to the lack of perception functions. Therefore, the current avoidance module limits the avoidance target to vehicles parked on the shoulder of the road, and executes avoidance only for vehicles that are stopped away from the center of the lane. However, this logic cannot avoid a vehicle that has broken down and is stopped in the center of the lane, which should be recognized as a static object by the perception module. There is room for improvement in the performance of this decision.</li> </ul> </li> </ul> <ul> <li>Resampling path<ul> <li>Now the rough resolution resampling is processed to the output path in order to reduce the computational cost for the later modules. This resolution is set to a uniformly large value (e.g. <code>5m</code>), but a small resolution should be applied for complex paths.</li> </ul> </li> </ul>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#debug","title":"Debug","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#show-rclcpp_debug-on-console","title":"Show <code>RCLCPP_DEBUG</code> on console","text":"<p>All debug messages are logged in the following namespaces.</p> <ul> <li><code>planning.scenario_planning.lane_driving.behavior_planning.behavior_path_planner.static_obstacle_avoidance</code> or,</li> <li><code>planning.scenario_planning.lane_driving.behavior_planning.behavior_path_planner.static_obstacle_avoidance.utils</code></li> </ul> <p>User can see debug information with the following command.</p> <pre><code>ros2 service call /planning/scenario_planning/lane_driving/behavior_planning/behavior_path_planner/config_logger logging_demo/srv/ConfigLogger \"{logger_name: 'planning.scenario_planning.lane_driving.behavior_planning.behavior_path_planner.static_obstacle_avoidance', level: DEBUG}\"\n</code></pre>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#visualize-debug-markers","title":"Visualize debug markers","text":"<p>User can enable publishing of debug markers with the following parameters.</p> <pre><code>debug:\n  enable_other_objects_marker: false\n  enable_other_objects_info: false\n  enable_detection_area_marker: false\n  enable_drivable_bound_marker: false\n  enable_safety_check_marker: false\n  enable_shift_line_marker: false\n  enable_lane_marker: false\n  enable_misc_marker: false\n</code></pre> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#echoing-debug-message-to-find-out-why-the-objects-were-ignored","title":"Echoing debug message to find out why the objects were ignored","text":"<p>If for some reason, no shift point is generated for your object, you can check for the failure reason via <code>ros2 topic echo</code>.</p> <p></p> <p>To print the debug message, just run the following</p> <pre><code>ros2 topic echo /planning/scenario_planning/lane_driving/behavior_planning/behavior_path_planner/debug/avoidance_debug_message_array\n</code></pre>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#frequently-asked-questions","title":"Frequently asked questions","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#target-objects","title":"Target objects","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#does-it-avoid-static-objects-and-dynamic-objects","title":"Does it avoid static objects and dynamic objects?","text":"<p>This module avoids static (stopped) objects and does not support dynamic (moving) objects avoidance. Dynamic objects are handled within the dynamic obstacle avoidance module.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#what-type-class-of-object-would-it-avoid","title":"What type (class) of object would it avoid?","text":"<p>It avoids cars, trucks, buses, trailers, bicycles, motorcycles, pedestrians, and unknown objects by default. Details are in the Target object filtering section. The above objects are divided into vehicle type objects and non-vehicle type objects; the target object filtering would differ for vehicle types and non-vehicle types.</p> <ul> <li>Vehicle type objects: Car, Truck, Bus, Trailer, Motorcycle</li> <li>Non-vehicle type objects: Pedestrian, Bicycle</li> </ul>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#how-does-it-judge-if-it-is-a-target-object-or-not","title":"How does it judge if it is a target object or not?","text":"<p>The conditions for vehicle type objects and non-vehicle type objects are different. However, the main idea is that static objects on road shoulders within the planned path would be avoided. Below are some examples when an avoidance path is generated for vehicle type objects.</p> <ul> <li>Vehicle stopping on ego lane while pulling over to the side of the road</li> <li>Vehicle stopping on adjacent lane</li> </ul> <p>For more details refer to vehicle type object and non-vehicle object.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#what-is-an-ambiguous-target","title":"What is an ambiguous target?","text":"<p>An ambiguous target refers to objects that may not be clearly identifiable as avoidance target due to limitations of current Autoware (ex. parked vehicle in the center of a lane). This module will avoid clearly defined static objects automatically, whereas ambiguous targets would need some operator intervention.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#how-can-i-visualize-the-target-object","title":"How can I visualize the target object?","text":"<p>Target objects can be visualized using RViz, where the module's outputs, such as detected obstacles and planned avoidance paths, are displayed. For further information refer to the debug marker section.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#how-can-i-check-the-lateral-distance-to-an-obstacle","title":"How can I check the lateral distance to an obstacle?","text":"<p>Currently, there isn't any topic that outputs the relative position with the ego vehicle and target object. Visual confirmation on RViz would be the only solution for now.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#does-it-avoid-multiple-objects-at-once","title":"Does it avoid multiple objects at once?","text":"<p>Yes, the module is capable of avoiding multiple static objects simultaneously. It generates multiple shift lines and calculates an avoidance path that navigates around each object. Details are explained in the How to decide path shape section.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#area-used-when-avoiding","title":"Area used when avoiding","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#which-lanes-are-used-to-avoid-objects","title":"Which lanes are used to avoid objects?","text":"<p>This module is able to use not only the current lane but also adjacent lanes and opposite lanes. Usable lanes can be selected by the configuration file as noted in the shift length calculation section. It is assumed that there are no parked vehicles on the central lane in a situation where there are lanes on the left and right.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#would-it-avoid-objects-inside-intersections","title":"Would it avoid objects inside intersections?","text":"<p>Basically, the module assumes that there are no parked vehicles within intersections. Vehicles that follow the lane or merge into ego lane are non-target objects. Vehicles waiting to make a right/left turn within an intersection can be avoided by expanding the drivable area in the configuration file, as noted in the drivable area expansion section.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#does-it-generate-avoidance-paths-for-any-road-type","title":"Does it generate avoidance paths for any road type?","text":"<p>Drivable area can be expanded in the configuration file, as noted in the drivable area expansion section.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#path-generation_1","title":"Path generation","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#how-is-the-avoidance-path-generated","title":"How is the avoidance path generated?","text":"<p>The avoidance path is generated by modifying the current reference path to navigate around detected static objects. This is done using a rule-based shift line approach that ensures the vehicle remains within safe boundaries and follows the road while avoiding obstacles. Details are explained in the appendix.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#which-way-right-or-left-is-the-avoidance-path-generated","title":"Which way (right or left) is the avoidance path generated?","text":"<p>The behavior of avoiding depends on the target vehicle's center of gravity. If the target object is on the left side of the ego lane the avoidance would be generated on the right side. Currently, avoiding left-shifted obstacles from the left side is not supported (same for right-shifted objects).</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#why-is-an-envelope-polygon-used-for-the-target-object","title":"Why is an envelope polygon used for the target object?","text":"<p>It is employed to reduce the influence of the perception/tracking noise for each target object. The envelope polygon is a rectangle, whose size depends on the object's polygon and buffer parameter and it is always parallel to the reference path. The envelope polygon is created by using the latest one-shot envelope polygon and the previous envelope polygon. Details are explained in How to prevent shift line chattering that is caused by perception noise section.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#what-happens-if-the-module-cannot-find-a-safe-avoidance-path","title":"What happens if the module cannot find a safe avoidance path?","text":"<p>If the module cannot find a safe avoidance path, the vehicle may stop or continue along its current path without performing an avoidance maneuver. If there is a target object and there is enough space to avoid, the ego vehicle would stop at a position where an avoidance path could be generated; this is called the yield manuever. On the other hand, where there is not enough space, this module has nothing to do and the obstacle cruise module would be in charge of the object.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#there-seems-to-be-an-avoidance-path-but-the-vehicle-stops-what-is-happening","title":"There seems to be an avoidance path, but the vehicle stops. What is happening?","text":"<p>This situation occurs when the module is operating in AUTO mode and the target object is ambiguous or when operating in MANUAL mode. The generated avoidance path is presented as a candidate and requires operator approval before execution. If the operator does not approve the path the ego vehicle would stop where it is possible to generate an avoidance path.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#operation","title":"Operation","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#what-are-the-benefits-of-using-manual-mode-over-auto-mode","title":"What are the benefits of using MANUAL mode over AUTO mode?","text":"<p>MANUAL mode allows the operator to have direct control over the approval of avoidance paths, which is particularly useful in situations where sensor data may be unreliable or ambiguous. This mode helps prevent unnecessary or incorrect avoidance maneuvers by requiring human validation before execution. It is recommended for environments where false positives are likely or when the sensor/perception system's performance is limited.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#can-this-module-be-customized-for-specific-vehicle-types-or-environments","title":"Can this module be customized for specific vehicle types or environments?","text":"<p>The module can be customized by adjusting the rules and parameters that define how it identifies and avoids obstacles. The avoidance manuever would not be changed by specific vehicle types.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#appendix-shift-line-generation-pipeline","title":"Appendix: Shift line generation pipeline","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#flow-chart-of-the-process","title":"Flow chart of the process","text":""},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#how-to-decide-the-path-shape","title":"How to decide the path shape","text":"<p>Generate shift points for obstacles with a given lateral jerk. These points are integrated to generate an avoidance path. The detailed process flow for each case corresponding to the obstacle placement are described below. The actual implementation is not separated for each case, but the function corresponding to <code>multiple obstacle case (both directions)</code> is always running.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#one-obstacle-case","title":"One obstacle case","text":"<p>The lateral shift distance to the obstacle is calculated, and then the shift point is generated from the ego vehicle speed and the given lateral jerk as shown in the figure below. A smooth avoidance path is then calculated based on the shift point.</p> <p>Additionally, the following processes are executed in special cases.</p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#lateral-jerk-relaxation-conditions","title":"Lateral jerk relaxation conditions","text":"<ul> <li>If the ego vehicle is close to the avoidance target, the lateral jerk will be relaxed up to the maximum jerk.</li> <li>When returning to the center line after avoidance, if there is not enough distance left to the goal (end of path), the jerk condition will be relaxed as above.</li> </ul>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#minimum-velocity-relaxation-conditions","title":"Minimum velocity relaxation conditions","text":"<p>There is a problem that we cannot know the actual speed during avoidance in advance. This is especially critical when the ego vehicle speed is 0. To solve that, this module provides a parameter for the minimum avoidance speed, which is used for the lateral jerk calculation when the vehicle speed is low.</p> <ul> <li>If the ego vehicle speed is lower than \"nominal\" minimum speed, use the minimum speed in the calculation of the jerk.</li> <li>If the ego vehicle speed is lower than \"sharp\" minimum speed and a nominal lateral jerk is not enough for avoidance (the case where the ego vehicle is stopped close to the obstacle), use the \"sharp\" minimum speed in the calculation of the jerk (it should be lower than \"nominal\" speed).</li> </ul> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#multiple-obstacle-case-one-direction","title":"Multiple obstacle case (one direction)","text":"<p>Generate shift points for multiple obstacles. All of them are merged to generate new shift points along the reference path. The new points are filtered (e.g. remove small-impact shift points), and the avoidance path is computed for the filtered shift points.</p> <p>Merge process of raw shift points: check the shift length on each path point. If the shift points are overlapped, the maximum shift value is selected for the same direction.</p> <p>For the details of the shift point filtering, see filtering for shift points.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#multiple-obstacle-case-both-direction","title":"Multiple obstacle case (both direction)","text":"<p>Generate shift points for multiple obstacles. All of them are merged to generate new shift points. If there are areas where the desired shifts conflict in different directions, the sum of the maximum shift amounts of these areas is used as the final shift amount. The rest of the process is the same as in the case of one direction.</p> <p></p>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#filtering-for-shift-points","title":"Filtering for shift points","text":"<p>The shift points are modified by a filtering process in order to get the expected shape of the avoidance path. It contains the following filters.</p> <ul> <li>Quantization: Quantize the avoidance width in order to ignore small shifts.</li> <li>Small shifts removal: Shifts with small changes with respect to the previous shift point are unified in the previous shift width.</li> <li>Similar gradient removal: Connect two shift points with a straight line, and remove the shift points in between if their shift amount is in the vicinity of the straight line.</li> <li>Remove momentary returns: For shift points that reduce the avoidance width (for going back to the center line), if there is enough long distance in the longitudinal direction, remove them.</li> </ul>"},{"location":"planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/#appendix-all-parameters","title":"Appendix: All parameters","text":"<p>Location of the avoidance specific parameter configuration file: <code>src/autoware/launcher/planning_launch/config/scenario_planning/lane_driving/behavior_planning/behavior_path_planner/autoware_behavior_path_static_obstacle_avoidance_module/static_obstacle_avoidance.param.yaml</code>.</p> Name Type Description Default Range resample_interval_for_planning float Path resample interval for avoidance planning path. 0.3 N/A resample_interval_for_output float Path resample interval for output path. Too short interval increases computational cost for latter modules. 4.0 N/A path_generation_method string Path generation method. shift_line_base ['shift_line_base', 'optimization_base', 'both'] use_lane_type string Drivable lane configuration. opposite_direction_lane ['current_lane', 'same_direction_lane', 'opposite_direction_lane'] use_hatched_road_markings boolean Extend drivable to hatched road marking area. true N/A use_intersection_areas boolean Extend drivable to intersection area. true N/A use_freespace_areas boolean Extend drivable to freespace area. true N/A car.th_moving_speed float Objects with speed greater than this will be judged as moving ones. 1.0 \u22650.0 car.th_moving_time float Objects keep moving longer duration than this will be excluded from avoidance target. 1.0 N/A car.longitudinal_margin float Creates an additional longitudinal gap that will prevent the vehicle from getting to near to the obstacle. 0.0 N/A lateral_margin.soft_margin float Lateral distance between ego and avoidance targets. 0.3 N/A lateral_margin.hard_margin float Lateral distance between ego and avoidance targets. 0.2 N/A lateral_margin.hard_margin_for_parked_vehicle float Lateral distance between ego and avoidance targets. 0.7 N/A car.envelope_buffer_margin float The buffer between raw boundary box of detected objects and enveloped polygon that is used for avoidance path generation. 0.5 N/A car.max_expand_ratio float This value will be applied envelope_buffer_margin according to the distance between the ego and object. 0.0 N/A car.th_error_eclipse_long_radius float This value will be applied to judge whether the eclipse error is to large 0.6 N/A truck.th_moving_speed float Objects with speed greater than this will be judged as moving ones. 1.0 \u22650.0 truck.th_moving_time float Objects keep moving longer duration than this will be excluded from avoidance target. 1.0 N/A truck.longitudinal_margin float Creates an additional longitudinal gap that will prevent the vehicle from getting to near to the obstacle. 0.0 N/A lateral_margin.soft_margin float Lateral distance between ego and avoidance targets. 0.3 N/A lateral_margin.hard_margin float Lateral distance between ego and avoidance targets. 0.2 N/A lateral_margin.hard_margin_for_parked_vehicle float Lateral distance between ego and avoidance targets. 0.7 N/A truck.envelope_buffer_margin float The buffer between raw boundary box of detected objects and enveloped polygon that is used for avoidance path generation. 0.5 N/A truck.max_expand_ratio float This value will be applied envelope_buffer_margin according to the distance between the ego and object. 0.0 N/A truck.th_error_eclipse_long_radius float This value will be applied to judge whether the eclipse error is to large 0.6 N/A bus.th_moving_speed float Objects with speed greater than this will be judged as moving ones. 1.0 \u22650.0 bus.th_moving_time float Objects keep moving longer duration than this will be excluded from avoidance target. 1.0 N/A bus.longitudinal_margin float Creates an additional longitudinal gap that will prevent the vehicle from getting to near to the obstacle. 0.0 N/A lateral_margin.soft_margin float Lateral distance between ego and avoidance targets. 0.3 N/A lateral_margin.hard_margin float Lateral distance between ego and avoidance targets. 0.2 N/A lateral_margin.hard_margin_for_parked_vehicle float Lateral distance between ego and avoidance targets. 0.7 N/A bus.envelope_buffer_margin float The buffer between raw boundary box of detected objects and enveloped polygon that is used for avoidance path generation. 0.5 N/A bus.max_expand_ratio float This value will be applied envelope_buffer_margin according to the distance between the ego and object. 0.0 N/A bus.th_error_eclipse_long_radius float This value will be applied to judge whether the eclipse error is to large 0.6 N/A trailer.th_moving_speed float Objects with speed greater than this will be judged as moving ones. 1.0 \u22650.0 trailer.th_moving_time float Objects keep moving longer duration than this will be excluded from avoidance target. 1.0 N/A trailer.longitudinal_margin float Creates an additional longitudinal gap that will prevent the vehicle from getting to near to the obstacle. 0.0 N/A lateral_margin.soft_margin float Lateral distance between ego and avoidance targets. 0.3 N/A lateral_margin.hard_margin float Lateral distance between ego and avoidance targets. 0.2 N/A lateral_margin.hard_margin_for_parked_vehicle float Lateral distance between ego and avoidance targets. 0.7 N/A trailer.envelope_buffer_margin float The buffer between raw boundary box of detected objects and enveloped polygon that is used for avoidance path generation. 0.5 N/A trailer.max_expand_ratio float This value will be applied envelope_buffer_margin according to the distance between the ego and object. 0.0 N/A trailer.th_error_eclipse_long_radius float This value will be applied to judge whether the eclipse error is to large 0.6 N/A unknown.th_moving_speed float Objects with speed greater than this will be judged as moving ones. 1.0 \u22650.0 unknown.th_moving_time float Objects keep moving longer duration than this will be excluded from avoidance target. 1.0 N/A unknown.longitudinal_margin float Creates an additional longitudinal gap that will prevent the vehicle from getting to near to the obstacle. 0.0 N/A lateral_margin.soft_margin float Lateral distance between ego and avoidance targets. 0.7 N/A lateral_margin.hard_margin float Lateral distance between ego and avoidance targets. -0.2 N/A lateral_margin.hard_margin_for_parked_vehicle float Lateral distance between ego and avoidance targets. -0.2 N/A unknown.envelope_buffer_margin float The buffer between raw boundary box of detected objects and enveloped polygon that is used for avoidance path generation. 0.1 N/A unknown.max_expand_ratio float This value will be applied envelope_buffer_margin according to the distance between the ego and object. 0.0 N/A unknown.th_error_eclipse_long_radius float This value will be applied to judge whether the eclipse error is to large 0.6 N/A motorcycle.th_moving_speed float Objects with speed greater than this will be judged as moving ones. 1.0 \u22650.0 motorcycle.th_moving_time float Objects keep moving longer duration than this will be excluded from avoidance target. 1.0 N/A motorcycle.longitudinal_margin float Creates an additional longitudinal gap that will prevent the vehicle from getting to near to the obstacle. 0.0 N/A lateral_margin.soft_margin float Lateral distance between ego and avoidance targets. 0.7 N/A lateral_margin.hard_margin float Lateral distance between ego and avoidance targets. 0.5 N/A lateral_margin.hard_margin_for_parked_vehicle float Lateral distance between ego and avoidance targets. 0.5 N/A motorcycle.envelope_buffer_margin float The buffer between raw boundary box of detected objects and enveloped polygon that is used for avoidance path generation. 0.5 N/A motorcycle.max_expand_ratio float This value will be applied envelope_buffer_margin according to the distance between the ego and object. 0.0 N/A motorcycle.th_error_eclipse_long_radius float This value will be applied to judge whether the eclipse error is to large 0.6 N/A bicycle.th_moving_speed float Objects with speed greater than this will be judged as moving ones. 1.0 \u22650.0 bicycle.th_moving_time float Objects keep moving longer duration than this will be excluded from avoidance target. 1.0 N/A bicycle.longitudinal_margin float Creates an additional longitudinal gap that will prevent the vehicle from getting to near to the obstacle. 0.0 N/A lateral_margin.soft_margin float Lateral distance between ego and avoidance targets. 0.7 N/A lateral_margin.hard_margin float Lateral distance between ego and avoidance targets. 0.3 N/A lateral_margin.hard_margin_for_parked_vehicle float Lateral distance between ego and avoidance targets. 0.3 N/A bicycle.envelope_buffer_margin float The buffer between raw boundary box of detected objects and enveloped polygon that is used for avoidance path generation. 0.5 N/A bicycle.max_expand_ratio float This value will be applied envelope_buffer_margin according to the distance between the ego and object. 0.0 N/A bicycle.th_error_eclipse_long_radius float This value will be applied to judge whether the eclipse error is to large 0.6 N/A pedestrian.th_moving_speed float Objects with speed greater than this will be judged as moving ones. 1.0 \u22650.0 pedestrian.th_moving_time float Objects keep moving longer duration than this will be excluded from avoidance target. 1.0 N/A pedestrian.longitudinal_margin float Creates an additional longitudinal gap that will prevent the vehicle from getting to near to the obstacle. 0.0 N/A lateral_margin.soft_margin float Lateral distance between ego and avoidance targets. 0.7 N/A lateral_margin.hard_margin float Lateral distance between ego and avoidance targets. 0.5 N/A lateral_margin.hard_margin_for_parked_vehicle float Lateral distance between ego and avoidance targets. 0.5 N/A pedestrian.envelope_buffer_margin float The buffer between raw boundary box of detected objects and enveloped polygon that is used for avoidance path generation. 0.5 N/A pedestrian.max_expand_ratio float This value will be applied envelope_buffer_margin according to the distance between the ego and object. 0.0 N/A pedestrian.th_error_eclipse_long_radius float This value will be applied to judge whether the eclipse error is to large 0.6 N/A target_object.lower_distance_for_polygon_expansion float If the distance between the ego and object is less than this, the expand ratio will be zero. 30.0 N/A target_object.upper_distance_for_polygon_expansion float If the distance between the ego and object is larger than this, the expand ratio will be max_expand_ratio. 100.0 N/A target_type.car boolean Enable avoidance maneuver for CAR. true N/A target_type.truck boolean Enable avoidance maneuver for TRUCK. true N/A target_type.bus boolean Enable avoidance maneuver for BUS. true N/A target_type.trailer boolean Enable avoidance maneuver for TRAILER. true N/A target_type.unknown boolean Enable avoidance maneuver for UNKNOWN. true N/A target_type.bicycle boolean Enable avoidance maneuver for BICYCLE. true N/A target_type.motorcycle boolean Enable avoidance maneuver for MOTORCYCLE. true N/A target_type.pedestrian boolean Enable avoidance maneuver for PEDESTRIAN. true N/A target_filtering.object_check_goal_distance float If the distance between object and goal position is less than this parameter, the module do not return center line. 20.0 N/A target_filtering.object_check_return_pose_distance float If the distance between object and return position is less than this parameter, the module do not return center line. 20.0 N/A target_filtering.max_compensation_time float For the compensation of the detection lost. The object is registered once it is observed as an avoidance target. When the detection loses, the timer will start and the object will be un-registered when the time exceeds this limit. 2.0 N/A target_filtering.unstable_classification_time float If the object is classified as UNKNOWN type for less than unstable_classification_time seconds, it is probably not an unknown type. 2.0 N/A detection_area.static boolean If true, the detection area longitudinal range is calculated based on current ego speed. false N/A detection_area.min_forward_distance float Minimum forward distance to search the avoidance target. 50.0 N/A detection_area.max_forward_distance float Maximum forward distance to search the avoidance target. 150.0 N/A detection_area.backward_distance float Backward distance to search the avoidance target. 10.0 N/A merging_vehicle.th_overhang_distance float Distance threshold to ignore merging/deviating vehicle to/from ego driving lane. The distance represents how the object polygon overlaps ego lane, and it's calculated from polygon overhang point and lane centerline. If the distance is more than this param, the module never avoid the object. (Basically, the ego stops behind of it.) 0.5 N/A parked_vehicle.th_offset_from_centerline float Vehicles around the center line within this distance will be excluded from avoidance target. 1.0 N/A parked_vehicle.th_shiftable_ratio float Vehicles around the center line within this distance will be excluded from avoidance target. 0.8 \u22650.0\u22641.0 parked_vehicle.min_road_shoulder_width float Width considered as a road shoulder if the lane does not have a road shoulder target. 0.5 N/A avoidance_for_ambiguous_vehicle.policy string Ego behavior policy for ambiguous vehicle. true ['auto', 'manual', 'ignore'] avoidance_for_ambiguous_vehicle.closest_distance_to_wait_and_see float Start avoidance maneuver after the distance to ambiguous vehicle is less than this param. 10.0 N/A condition.th_stopped_time float Never avoid object whose stopped time is less than this param. 3.0 N/A condition.th_moving_distance float Never avoid object which moves more than this param. 1.0 N/A traffic_light.front_distance float If the distance between traffic light and vehicle is less than this parameter, this module will ignore it. 100.0 N/A crosswalk.front_distance float If the front distance between crosswalk and vehicle is less than this parameter, this module will ignore it. 30.0 N/A crosswalk.behind_distance float If the back distance between crosswalk and vehicle is less than this parameter, this module will ignore it. 30.0 N/A wait_and_see.target_behaviors array This module doesn't avoid these behaviors vehicle until it gets closer than threshold. ['MERGING', 'DEVIATING'] N/A wait_and_see.th_closest_distance float Threshold to check whether the ego gets close enough the ambiguous vehicle. 10.0 N/A avoidance_for_parking_violation_vehicle.policy string Ego behavior policy for parking violation vehicle. ignore ['auto', 'manual', 'ignore'] condition.th_road_border_distance float Distance threshold from road border to determine parking violation. 0.8 \u22650.0 avoidance_for_adjacent_lane_stop_vehicle.policy string Ego behavior policy for adjacent lane stop vehicle. auto ['auto', 'manual', 'ignore'] intersection.yaw_deviation float Yaw deviation threshold param to judge if the object is not merging or deviating vehicle. 0.349 N/A condition.th_stopped_time float This module delays avoidance maneuver to see vehicle behavior in freespace. 5.0 N/A target_type.car boolean Enable safety_check for CAR. true N/A target_type.truck boolean Enable safety_check for TRUCK. true N/A target_type.bus boolean Enable safety_check for BUS. true N/A target_type.trailer boolean Enable safety_check for TRAILER. true N/A target_type.unknown boolean Enable safety_check for UNKNOWN. false N/A target_type.bicycle boolean Enable safety_check for BICYCLE. true N/A target_type.motorcycle boolean Enable safety_check for MOTORCYCLE. true N/A target_type.pedestrian boolean Enable safety_check for PEDESTRIAN. true N/A safety_check.enable boolean Enable to use safety check feature. true N/A safety_check.check_current_lane boolean Check objects on current driving lane. true N/A safety_check.check_shift_side_lane boolean Check objects on shift side lane. true N/A safety_check.check_other_side_lane boolean Check objects on other side lane. true N/A safety_check.check_unavoidable_object boolean Check collision between ego and unavoidable objects. true N/A safety_check.check_other_object boolean Check collision between ego and non avoidance target objects. true N/A safety_check.check_all_predicted_path boolean Check all prediction path of safety check target objects. true N/A safety_check.safety_check_backward_distance float Backward distance to search the dynamic objects. 100.0 N/A safety_check.hysteresis_factor_expand_rate float Hysteresis factor that be used for chattering prevention. 2.0 N/A safety_check.hysteresis_factor_safe_count integer Hysteresis count that be used for chattering prevention. 10 N/A safety_check.collision_check_yaw_diff_threshold float Max yaw difference between ego and object when doing collision check 3.1416 N/A safety_check.min_velocity float Minimum velocity of the ego vehicle's predicted path. 1.38 N/A safety_check.max_velocity float Maximum velocity of the ego vehicle's predicted path. 50.0 N/A safety_check.time_resolution float Time resolution for the ego vehicle's predicted path. 0.5 N/A safety_check.time_horizon_for_front_object float Time horizon for predicting front objects. 3.0 N/A safety_check.time_horizon_for_rear_object float Time horizon for predicting rear objects. 10.0 N/A safety_check.delay_until_departure float Delay until the ego vehicle departs. 1.0 N/A safety_check.extended_polygon_policy string See https://github.com/autowarefoundation/autoware_universe/pull/6336. along_path ['rectangle', 'along_path'] safety_check.expected_front_deceleration float The front object's maximum deceleration when the front vehicle perform sudden braking. -1.0 N/A safety_check.expected_rear_deceleration float The rear object's maximum deceleration when the rear vehicle perform sudden braking. -1.0 N/A safety_check.rear_vehicle_reaction_time float The reaction time of the rear vehicle driver which starts from the driver noticing the sudden braking of the front vehicle until the driver step on the brake. 2.0 N/A safety_check.rear_vehicle_safety_time_margin float The time buffer for the rear vehicle to come into complete stop when its driver perform sudden braking. 1.0 N/A safety_check.lateral_distance_max_threshold float The lateral distance threshold that is used to determine whether lateral distance between two object is enough and whether lane change is safe. 2.0 N/A safety_check.longitudinal_distance_min_threshold float The longitudinal distance threshold that is used to determine whether longitudinal distance between two object is enough and whether lane change is safe. 3.0 N/A safety_check.longitudinal_velocity_delta_time float The time multiplier that is used to compute the actual gap between vehicle at each predicted points (not RSS distance) 0.0 N/A lateral.th_avoid_execution float The lateral distance deviation threshold between the current path and suggested avoidance point to execute avoidance. 0.09 N/A lateral.th_small_shift_length float The shift lines whose lateral offset is less than this will be applied with other ones. 0.101 N/A lateral.soft_drivable_bound_margin float Keep distance to drivable bound. 0.3 N/A lateral.hard_drivable_bound_margin float Keep distance to drivable bound. 0.3 N/A lateral.max_right_shift_length float Maximum shift length for right direction 5.0 N/A lateral.max_left_shift_length float Maximum shift length for left direction. 5.0 N/A lateral.max_deviation_from_lane float Use in validation phase to check if the avoidance path is in drivable area. 0.2 N/A lateral.ratio_for_return_shift_approval float This parameter is added to allow waiting for the return of shift approval until the occlusion behind the avoid target is clear. 0.5 \u22650.0\u22641.0 longitudinal.min_prepare_time float Avoidance shift starts from point ahead of this time x ego speed at least. 1.0 N/A longitudinal.max_prepare_time float Avoidance shift starts from point ahead of this time x ego speed if possible. 2.0 N/A longitudinal.min_prepare_distance float Minimum prepare distance. 1.0 N/A longitudinal.min_slow_down_speed float Minimum slow speed for avoidance prepare section. 1.38 N/A longitudinal.buf_slow_down_speed float Buffer for controller tracking error. Basically, vehicle always cannot follow velocity profile precisely. Therefore, the module inserts lower speed than target speed that satisfies conditions to avoid object within accel/jerk constraints so that the avoidance path always can be output even if the current speed is a little bit higher than target speed. 0.56 N/A longitudinal.nominal_avoidance_speed float Nominal avoidance speed. 8.33 N/A longitudinal.consider_front_overhang boolean Flag to consider vehicle front overhang in shift line generation logic. True N/A longitudinal.consider_rear_overhang boolean Flag to consider vehicle rear overhang in shift line generation logic. True N/A goal.enable boolean Insert stop point in order to return original lane before reaching goal. true N/A goal.buffer float Buffer distance to return original lane before reaching goal. 3.0 N/A traffic_light.enable boolean Insert stop point in order to return original lane before reaching traffic light. true N/A traffic_light.buffer float Buffer distance to return original lane before reaching traffic light. 3.0 N/A stop.max_distance float Maximum stop distance in the situation where avoidance maneuver is not approved or in yield maneuver. 20.0 N/A stop.stop_buffer float Buffer distance in the situation where avoidance maneuver is not approved or in yield maneuver. 1.0 N/A yield.enable boolean Flag to enable yield maneuver. true N/A yield.enable_during_shifting boolean Flag to enable yield maneuver during shifting. false N/A cancel.enable boolean Flag to enable cancel maneuver. true N/A force.duration_time float force deactivate duration time 2.0 N/A policy.detection_reliability string policy for module behavior. select 'reliable' or 'not_enough'. 'reliable': this module fully trusts the perception results and makes all decisions automatically.  'not_enough': because the perception results are not fully reliable, the system waits for the operator's judgment in situations where long-range perception is required. reliable ['reliable', 'not_enough'] policy.make_approval_request string policy for rtc request. select <code>per_shift_line</code> or <code>per_avoidance_maneuver</code>. <code>per_shift_line</code>: request approval for each shift line. <code>per_avoidance_maneuver</code>: request approval for avoidance maneuver (avoid + return). per_shift_line ['per_shift_line', 'per_avoidance_maneuver'] policy.deceleration string policy for vehicle slow down behavior. select <code>best_effort</code> or <code>reliable</code>. <code>best_effort</code>: slow down deceleration &amp; jerk are limited by constraints but there is a possibility that the vehicle can't stop in front of the vehicle. <code>reliable</code>: insert stop or slow down point with ignoring decel/jerk constraints. make it possible to increase chance to avoid but uncomfortable deceleration maybe happen. best_effort ['reliable', 'best_effort'] policy.lateral_margin string policy for voidance lateral margin. select <code>best_effort</code> or <code>reliable</code>. <code>best_effort</code>: output avoidance path with shorten lateral margin when there is no enough longitudinal margin to avoid. <code>reliable</code>: module output avoidance path with safe (rtc cooperate) state only when the vehicle can avoid with expected lateral margin. best_effort ['reliable', 'best_effort'] policy.use_shorten_margin_immediately boolean if true, module doesn't wait deceleration and outputs avoidance path by best effort margin. true N/A lateral.velocity array Velocity array to decide current lateral accel/jerk limit. [1.0, 1.38, 11.1] N/A lateral.max_accel_values array Avoidance path gets sharp up to this accel limit when there is not enough distance from ego. [0.5, 0.5, 0.5] N/A min_jerk_values.avoid array Minimum lateral jerk constraint when laterally shifting to avoid an obstacle. [0.2, 0.2, 0.2] N/A min_jerk_values.return array Minimum lateral jerk constraint when laterally shifting to return to the original lane. [0.2, 0.2, 0.2] N/A lateral.max_jerk_values array Avoidance path gets sharp up to this jerk limit when there is not enough distance from ego. [1.0, 1.0, 1.0] N/A longitudinal.nominal_deceleration float Nominal deceleration limit. -1.0 N/A longitudinal.nominal_jerk float Nominal jerk limit. 0.5 N/A longitudinal.max_deceleration float Max deceleration limit. -1.5 N/A longitudinal.max_jerk float Max jerk limit. 1.0 N/A longitudinal.max_acceleration float Maximum acceleration during avoidance. 0.5 N/A longitudinal.min_velocity_to_limit_max_acceleration float If the ego speed is faster than this param, the module applies acceleration limit <code>max_acceleration</code>. 2.78 N/A trim.quantize_size float Lateral shift length quantize size. 0.1 N/A trim.th_similar_grad_1 float Lateral shift length threshold to merge similar gradient shift lines. 0.1 N/A trim.th_similar_grad_2 float Lateral shift length threshold to merge similar gradient shift lines. 0.2 N/A trim.th_similar_grad_3 float Lateral shift length threshold to merge similar gradient shift lines. 0.5 N/A debug.enable_other_objects_marker boolean Publish other objects marker. false N/A debug.enable_other_objects_info boolean Publish other objects detail information. false N/A debug.enable_detection_area_marker boolean Publish detection area. false N/A debug.enable_drivable_bound_marker boolean Publish drivable area boundary. false N/A debug.enable_safety_check_marker boolean Publish safety check information. false N/A debug.enable_shift_line_marker boolean Publish shift line information. false N/A debug.enable_lane_marker boolean Publish lane information. false N/A debug.enable_misc_marker boolean Publish misc markers. false N/A"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_blind_spot_module/","title":"Index","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_blind_spot_module/#blind-spot","title":"Blind Spot","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_blind_spot_module/#role","title":"Role","text":"<p>Blind spot module checks possible collisions with bicycles and pedestrians running on its left/right side while turing left/right before junctions.</p> <p></p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_blind_spot_module/#activation-timing","title":"Activation Timing","text":"<p>This function is activated when the lane id of the target path has an intersection label (i.e. the <code>turn_direction</code> attribute is <code>left</code> or <code>right</code>).</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_blind_spot_module/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>Sets a stop line, a pass judge line, a detection area and conflict area based on a map information and a self position.</p> <ul> <li>Stop line : Automatically created based on crossing lane information.</li> </ul> <ul> <li>Pass judge line : A position to judge if stop or not to avoid a rapid brake.</li> </ul> <ul> <li>Detection area : Right/left side area of the self position.</li> </ul> <ul> <li>Conflict area : Right/left side area from the self position to the stop line.</li> </ul> <p>Stop/Go state: When both conditions are met for any of each object, this module state is transited to the \"stop\" state and insert zero velocity to stop the vehicle.</p> <ul> <li>Object is on the detection area</li> <li>Object\u2019s predicted path is on the conflict area</li> </ul> <p>In order to avoid a rapid stop, the \u201cstop\u201d judgement is not executed after the judgment line is passed.</p> <p>Once a \"stop\" is judged, it will not transit to the \"go\" state until the \"go\" judgment continues for a certain period in order to prevent chattering of the state (e.g. 2 seconds).</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_blind_spot_module/#module-parameters","title":"Module Parameters","text":"Parameter Type Description <code>stop_line_margin</code> double [m] a margin that the vehicle tries to stop before stop_line <code>backward_length</code> double [m] distance from closest path point to the edge of beginning point. <code>ignore_width_from_center_line</code> double [m] ignore threshold that vehicle behind is collide with ego vehicle or not <code>max_future_movement_time</code> double [s] maximum time for considering future movement of object <code>adjacent_extend_width</code> double [m] if adjacent lane e.g. bicycle only lane exists, blind_spot area is expanded by this length"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_blind_spot_module/#flowchart","title":"Flowchart","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_crosswalk_module/","title":"Crosswalk","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_crosswalk_module/#crosswalk","title":"Crosswalk","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_crosswalk_module/#role","title":"Role","text":"<p>This module judges whether the ego should stop in front of the crosswalk in order to provide safe passage for crosswalk users, such as pedestrians and bicycles, based on the objects' behavior and surround traffic.</p> <p></p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_crosswalk_module/#flowchart","title":"Flowchart","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_crosswalk_module/#features","title":"Features","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_crosswalk_module/#yield-the-way-to-the-pedestrians","title":"Yield the Way to the Pedestrians","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_crosswalk_module/#target-object","title":"Target Object","text":"<p>The crosswalk module handles objects of the types defined by the following parameters in the <code>object_filtering.target_object</code> namespace.</p> Parameter Unit Type Description <code>unknown</code> [-] bool whether to look and stop by UNKNOWN objects <code>pedestrian</code> [-] bool whether to look and stop by PEDESTRIAN objects <code>bicycle</code> [-] bool whether to look and stop by BICYCLE objects <code>motorcycle</code> [-] bool whether to look and stop by MOTORCYCLE objects <p>In order to handle the crosswalk users crossing the neighborhood but outside the crosswalk, the crosswalk module creates an attention area around the crosswalk, shown as the yellow polygon in the figure. If the object's predicted path collides with the attention area, the object will be targeted for yield.</p> <p></p> <p>The neighborhood is defined by the following parameter in the <code>object_filtering.target_object</code> namespace.</p> Parameter Unit Type Description <code>crosswalk_attention_range</code> [m] double the detection area is defined as -X meters before the crosswalk to +X meters behind the crosswalk"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_crosswalk_module/#stop-position","title":"Stop Position","text":"<p>This function places a stop line to prevent re-transmission based on the following logic:</p> <ol> <li>stop line will be positioned at the closest point among the followings. Note that the envelope of the distances determined here is taken:<ol> <li>The location of an existing stop line (if none exists, then at a distance of <code>stop_distance_from_crosswalk [m]</code> from the crosswalk).</li> <li><code>stop_distance_from_object_preferred [m]</code> before the predicted collision point with a person.</li> </ol> </li> <li>If reaching the stop line determined above requires an deceleration of <code>min_acc_preferred [m/ss]</code> or more, the stop line will be set at the location where the vehicle can stop with a deceleration of <code>min_acc_preferred [m/ss]</code>.</li> <li>If the stop line position determined above is further than a distance of <code>stop_distance_from_crosswalk_limit [m]</code> from the pedestrian crossing, the stop line will be set at <code>stop_distance_from_crosswalk_limit [m]</code> from the pedestrian crossing.</li> <li>If <code>enable_no_stop_decision</code> is enabled, and the deceleration required to stop at the position determined above is greater than <code>no_stop_decision.min_acc</code>, the system will cancel the stop.</li> </ol> 1-1-1 Stop line based on road markings 1-1-2 Virtual stop line 1-2 Preferred stop position from pedestrian location The figure shows how the stop position is determined based on the distance to the crosswalk and current vehicle speed. If the vehicle can stop at the preferred location with preferred deceleration (gray region), the preferred location becomes the stop position. If it cannot stop with preferred deceleration (orange region), the stop position is set where it can stop with preferred deceleration. If it cannot stop at the limit position with preferred deceleration (red region), the limit position becomes the stop position. If it cannot stop at the limit position even with strong deceleration, stopping is cancelled. <p>To decide the stop position, the following parameters are defined.</p> Parameter Type Description <code>stop_position.stop_position_threshold</code> [m] double If the ego vehicle has stopped near the stop line than this value, this module assumes itself to have achieved yielding. <code>stop_position.stop_distance_from_crosswalk</code> [m] double Stop line distance from a pedestrian crossing when no explicit stop line exists in the map. <code>stop_position.stop_distance_from_object_preferred</code> [m] double Preferred distance before a predicted collision for stop line placement. <code>stop_position.stop_distance_from_crosswalk_limit</code> [m] double Minimum stop line distance from a pedestrian crossing. <code>stop_position.min_acc_preferred</code> [m/ss] double Minimum deceleration to reach the initial stop line; if exceeded, the stop line is moved accordingly. <code>no_stop_decision.enable_no_stop_decision</code> bool Enables/disables the cancellation of stopping. <code>no_stop_decision.min_acc</code> [m/ss] double Minimum deceleration to cancel a stop (if enabled)."},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_crosswalk_module/#yield-decision","title":"Yield decision","text":"<p>The module makes a decision to yield only when the pedestrian traffic light is GREEN or UNKNOWN. The decision is based on the following variables, along with the calculation of the collision point.</p> <ul> <li>Time-To-Collision (TTC): The time for the ego to reach the virtual collision point.</li> <li>Time-To-Vehicle (TTV): The time for the object to reach the virtual collision point.</li> </ul> <p>We classify ego behavior at crosswalks into three categories according to the relative relationship between TTC and TTV [1].</p> <ul> <li>A. TTC &gt;&gt; TTV: The object will pass early enough than the ego reach the collision point.<ul> <li>No stop planning.</li> </ul> </li> <li>B. TTC \u2252 TTV: There is a risk of collision.<ul> <li>Stop point is inserted in the ego's path.</li> </ul> </li> <li>C. TTC &lt;&lt; TTV: The ego will pass early enough than the object reach the collision point.<ul> <li>No stop planning.</li> </ul> </li> </ul> <p>The following figure shows the decision result for each TTC and TTV with the parameters, <code>ego_pass_first_margin_x</code> is <code>{0}</code>, <code>ego_pass_first_margin_y</code> is <code>{4}</code>, <code>ego_pass_later_margin_x</code> is <code>{0}</code>, and <code>ego_pass_later_margin_y</code> is <code>{13}</code>.</p> <p>If the red signal is indicating to the corresponding crosswalk, the ego do not yield against the pedestrians.</p> <p>In the <code>pass_judge</code> namespace, the following parameters are defined.</p> Parameter Type Description <code>ego_pass_first_margin_x</code> [[s]] double time to collision margin vector for ego pass first situation (the module judges that ego don't have to stop at TTC + MARGIN &lt; TTV condition) <code>ego_pass_first_margin_y</code> [[s]] double time to vehicle margin vector for ego pass first situation (the module judges that ego don't have to stop at TTC + MARGIN &lt; TTV condition) <code>ego_pass_first_additional_margin</code> [s] double additional time margin for ego pass first situation to suppress chattering <code>ego_pass_later_margin_x</code> [[s]] double time to vehicle margin vector for object pass first situation (the module judges that ego don't have to stop at TTV + MARGIN &lt; TTC condition) <code>ego_pass_later_margin_y</code> [[s]] double time to collision margin vector for object pass first situation (the module judges that ego don't have to stop at TTV + MARGIN &lt; TTC condition) <code>ego_pass_later_additional_margin</code> [s] double additional time margin for object pass first situation to suppress chattering"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_crosswalk_module/#smooth-yield-decision","title":"Smooth Yield Decision","text":"<p>If the object is stopped near the crosswalk but has no intention of walking, a situation can arise in which the ego continues to yield the right-of-way to the object. To prevent such a deadlock situation, the ego will cancel yielding depending on the situation.</p> <p>For the object stopped around the crosswalk but has no intention to walk (*1), after the ego has keep stopping to yield for a specific time (*2), the ego cancels the yield and starts driving.</p> <p>*1: The time is calculated by the interpolation of distance between the object and crosswalk with <code>distance_set_for_no_intention_to_walk</code> and <code>timeout_set_for_no_intention_to_walk</code>.</p> <p>In the <code>pass_judge</code> namespace, the following parameters are defined.</p> Parameter Type Description <code>distance_set_for_no_intention_to_walk</code> [[m]] double key sets to calculate the timeout for no intention to walk with interpolation <code>timeout_set_for_no_intention_to_walk</code> [[s]] double value sets to calculate the timeout for no intention to walk with interpolation <p>*2: In the <code>pass_judge</code> namespace, the following parameters are defined.</p> Parameter Type Description <code>timeout_ego_stop_for_yield</code> [s] double If the ego maintains the stop for this amount of time, then the ego proceeds, assuming it has stopped long time enough."},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_crosswalk_module/#new-object-handling","title":"New Object Handling","text":"<p>Due to the perception's limited performance where the tree or poll is recognized as a pedestrian or the tracking failure in the crowd or occlusion, even if the surrounding environment does not change, the new pedestrian (= the new ID's pedestrian) may suddenly appear unexpectedly. If this happens while the ego is going to pass the crosswalk, the ego will stop suddenly.</p> <p>To deal with this issue, the option <code>disable_yield_for_new_stopped_object</code> is prepared. If true is set, the yield decisions around the crosswalk with a traffic light will ignore the new stopped object.</p> <p>In the <code>pass_judge</code> namespace, the following parameters are defined.</p> Parameter Type Description <code>disable_yield_for_new_stopped_object</code> [-] bool If set to true, the new stopped object will be ignored around the crosswalk with a traffic light"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_crosswalk_module/#obstruction-prevention-on-the-crosswalk","title":"Obstruction Prevention on the Crosswalk","text":"<p>The feature will make the ego not to stop on the crosswalk. When there is a low-speed or stopped vehicle ahead of the crosswalk, and there is not enough space between the crosswalk and the vehicle, the crosswalk module plans to stop before the crosswalk even if there are no pedestrians or bicycles.</p> <p><code>min_acc</code>, <code>min_jerk</code>, and <code>max_jerk</code> are met. If the ego cannot stop before the crosswalk with these parameters, the stop position will move forward.</p> <p></p> <p>In the <code>obstruction_prevention</code> namespace, the following parameters are defined.</p> Parameter Unit Type Description <code>target_vehicle_velocity</code> [m/s] double maximum velocity threshold whether the target vehicle is stopped or not <code>max_target_vehicle_lateral_offset</code> [m] double maximum lateral offset of the target vehicle position <code>required_clearance</code> [m] double clearance to be secured between the ego and the ahead vehicle <code>min_acc</code> [m/ss] double minimum acceleration to stop <code>min_jerk</code> [m/sss] double minimum jerk to stop <code>max_jerk</code> [m/sss] double maximum jerk to stop"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_crosswalk_module/#map-slow-down-behavior","title":"Map Slow Down Behavior","text":"<p>In the current autoware implementation, if no target object is detected around a crosswalk, the ego vehicle will not slow down for the crosswalk. However, it may be desirable to slow down in situations, for example, where there are blind spots. Such a situation can be handled by setting some tags to the related crosswalk as instructed in the lanelet2_format_extension.md document.</p> Parameter Type Description <code>slow_velocity</code> [m/s] double target vehicle velocity when module receive slow down command from FOA <code>max_slow_down_jerk</code> [m/sss] double minimum jerk deceleration for safe brake <code>max_slow_down_accel</code> [m/ss] double minimum accel deceleration for safe brake <code>no_relax_velocity</code> [m/s] double if the current velocity is less than X m/s, ego always stops at the stop position(not relax deceleration constraints)"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_crosswalk_module/#occlusion","title":"Occlusion","text":"<p>This feature makes ego slow down for a crosswalk that is occluded.</p> <p>Occlusion of the crosswalk is determined using the occupancy grid. An occlusion is a square of size <code>min_size</code> of occluded cells (i.e., their values are between <code>free_space_max</code> and <code>occupied_min</code>) of size <code>min_size</code>. If an occlusion is found within range of the crosswalk, then the velocity limit at the crosswalk is set to <code>slow_down_velocity</code> (or more to not break limits set by <code>max_slow_down_jerk</code> and <code>max_slow_down_accel</code>). The range is calculated from the intersection between the ego path and the crosswalk and is equal to the time taken by ego to reach the crosswalk times the <code>occluded_object_velocity</code>. This range is meant to be large when ego is far from the crosswalk and small when ego is close.</p> <p>In order to avoid flickering decisions, a time buffer can be used such that the decision to add (or remove) the slow down is only taken after an occlusion is detected (or not detected) for a consecutive time defined by the <code>time_buffer</code> parameter.</p> <p>To ignore occlusions when the crosswalk has a traffic light, <code>ignore_with_traffic_light</code> should be set to true.</p> <p>To ignore temporary occlusions caused by moving objects, <code>ignore_behind_predicted_objects</code> should be set to true. By default, occlusions behind an object with velocity higher than <code>ignore_velocity_thresholds.default</code> are ignored. This velocity threshold can be specified depending on the object type by specifying the object class label and velocity threshold in the parameter lists <code>ignore_velocity_thresholds.custom_labels</code> and <code>ignore_velocity_thresholds.custom_thresholds</code>. To inflate the masking behind objects, their footprint can be made bigger using <code>extra_predicted_objects_size</code>.</p> <p></p> Parameter Unit Type Description <code>enable</code> [-] bool if true, ego will slow down around crosswalks that are occluded <code>occluded_object_velocity</code> [m/s] double assumed velocity of objects that may come out of the occluded space <code>slow_down_velocity</code> [m/s] double slow down velocity <code>time_buffer</code> [s] double consecutive time with/without an occlusion to add/remove the slowdown <code>min_size</code> [m] double minimum size of an occlusion (square side size) <code>free_space_max</code> [-] double maximum value of a free space cell in the occupancy grid <code>occupied_min</code> [-] double minimum value of an occupied cell in the occupancy grid <code>ignore_with_traffic_light</code> [-] bool if true, occlusions at crosswalks with traffic lights are ignored <code>ignore_behind_predicted_objects</code> [-] bool if true, occlusions behind predicted objects are ignored <code>ignore_velocity_thresholds.default</code> [m/s] double occlusions are only ignored behind objects with a higher or equal velocity <code>ignore_velocity_thresholds.custom_labels</code> [-] string list labels for which to define a non-default velocity threshold (see <code>autoware_perception_msgs::msg::ObjectClassification</code> for all the labels) <code>ignore_velocity_thresholds.custom_thresholds</code> [-] double list velocities of the custom labels <code>extra_predicted_objects_size</code> [m] double extra size added to the objects for masking the occlusions"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_crosswalk_module/#stop-for-parked-vehicles","title":"Stop for parked vehicles","text":"<p>This feature makes ego stop before the crosswalk if there are other vehicles parked in front of the crosswalk, possibly obstructing the view of incoming pedestrians or bicycles.</p> <p>The feature is enabled with the <code>parked_vehicles_stop.enable</code> parameter but is skipped when one of these conditions is true:</p> <ul> <li>the traffic light is red for the crosswalk users: pedestrians are not supposed to cross so ego does not need to stop;</li> <li>the traffic light is red for the ego vehicle: the traffic light module will add a stop if necessary;</li> <li>we are already planning to stop for crosswalk users or stuck vehicles.</li> </ul> <p>Other vehicles are determined to be stopped if their velocity is less or equal to the <code>parked_vehicles_stop.parked_velocity_threshold</code> parameter.</p> <p>A search area is built along the ego path ahead of the crosswalk by a distance set with the <code>parked_vehicles_stop.search_distance</code> parameter.</p> <p>A stop is planned such that ego will be aligned with the furthest parked vehicle along the ego path within the search area. Once ego stops inside the search area for at least the duration set by <code>parked_vehicles_stop.min_ego_stop_duration</code>, the stop is removed and no more stop for parked vehicles will be triggered for that crosswalk.</p> <p>To prevent chattering caused by noise in the detected objects, once an object is selected for stopping, its last state will keep being considered for at least a duration of <code>parked_vehicles_stop.vehicle_permanence_duration</code>, even if the object is no longer detected or if it is no longer parked.</p> Parameter Unit Type Description <code>parked_vehicles_stop.enable</code> [-] bool if true, ego will stop if there are parked vehicles before the crosswalk <code>parked_vehicles_stop.search_distance</code> [m] double distance ahead of the crosswalk where to search for parked vehicles <code>parked_vehicles_stop.min_ego_stop_duration</code> [s] double minimum duration ego should stop before it can continue <code>parked_vehicles_stop.vehicle_permanence_duration</code> [s] double [s] if an object disappears or is no longer classified as parked vehicle, its last state is still used for this duration <code>parked_vehicles_stop.ego_inside_safe_area_margin</code> [m] double margin used to consider if ego is inside the search area. Should be higher than the expected longitudinal error when stopping <code>parked_vehicles_stop.parked_velocity_threshold</code> [m/s] double vehicle are considered to be parked if their velocity is less or equal this threshold"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_crosswalk_module/#others","title":"Others","text":"<p>In the <code>common</code> namespace, the following parameters are defined.</p> Parameter Unit Type Description <code>show_processing_time</code> [-] bool whether to show processing time <code>traffic_light_state_timeout</code> [s] double timeout threshold for traffic light signal <code>enable_rtc</code> [-] bool if true, the scene modules should be approved by (request to cooperate)rtc function. if false, the module can be run without approval from rtc. <code>lost_detection_timeout</code> [s] double Time to keep an object after its detection is lost"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_crosswalk_module/#map-based-forced-rtc","title":"Map-based forced RTC","text":"<p>RTC can be enabled for specific crosswalks in the lanelet map such that even if <code>enable_rtc</code> is set to <code>false</code>, approval will be required for crossing the corresponding crosswalks. The following attribute should be added to the crosswalk lanelet in the map file:</p> <pre><code>&lt;tag k='rtc_approval_required_v1' v='crosswalk' /&gt;\n</code></pre>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_crosswalk_module/#known-issues","title":"Known Issues","text":"<ul> <li>The yield decision may be sometimes aggressive or conservative depending on the case.<ul> <li>The main reason is that the crosswalk module does not know the ego's position in the future. The detailed ego's position will be determined after the whole planning.</li> <li>Currently the module assumes that the ego will move with a constant velocity.</li> </ul> </li> </ul>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_crosswalk_module/#debugging","title":"Debugging","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_crosswalk_module/#visualization-of-debug-markers","title":"Visualization of debug markers","text":"<p><code>/planning/scenario_planning/lane_driving/behavior_planning/behavior_velocity_planner/debug/crosswalk</code> shows the following markers.</p> <p></p> <ul> <li>Yellow polygons<ul> <li>Ego footprints' polygon to calculate the collision check.</li> </ul> </li> <li>Pink polygons<ul> <li>Object footprints' polygon to calculate the collision check.</li> </ul> </li> <li>The color of crosswalks<ul> <li>Considering the traffic light's color, red means the target crosswalk, and white means the ignored crosswalk.</li> </ul> </li> <li>Texts<ul> <li>It shows the module ID, TTC, TTV, and the module state.</li> </ul> </li> </ul>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_crosswalk_module/#visualization-of-time-to-collision","title":"Visualization of Time-To-Collision","text":"<pre><code>ros2 run autoware_behavior_velocity_crosswalk_module time_to_collision_plotter.py\n</code></pre> <p>enables you to visualize the following figure of the ego and pedestrian's time to collision. The label of each plot is <code>&lt;crosswalk module id&gt;-&lt;pedestrian uuid&gt;</code>.</p> <p></p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_crosswalk_module/#trouble-shooting","title":"Trouble Shooting","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_crosswalk_module/#behavior","title":"Behavior","text":"<ul> <li>Q. The ego stopped around the crosswalk even though there were no crosswalk user objects.<ul> <li>A. See Obstruction Prevention on the Crosswalk</li> </ul> </li> <li>Q. The crosswalk virtual wall suddenly appeared resulting in the sudden stop.<ul> <li>A. There may be a crosswalk user started moving when the ego was close to the crosswalk.</li> </ul> </li> <li>Q. The crosswalk module decides to stop even when the pedestrian traffic light is red.<ul> <li>A. The lanelet map may be incorrect. The pedestrian traffic light and the crosswalk have to be related.</li> </ul> </li> <li>Q. In the planning simulation, the crosswalk module does the yield decision to stop on all the crosswalks.<ul> <li>A. This is because the pedestrian traffic light is unknown by default. In this case, the crosswalk does the yield decision for safety.</li> </ul> </li> </ul>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_crosswalk_module/#parameter-tuning","title":"Parameter Tuning","text":"<ul> <li>Q. The ego's yield behavior is too conservative.<ul> <li>A. Tune <code>ego_pass_later_margin</code> described in Yield Decision</li> </ul> </li> <li>Q. The ego's yield behavior is too aggressive.<ul> <li>A. Tune <code>ego_pass_later_margin</code> described in Yield Decision</li> </ul> </li> </ul>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_crosswalk_module/#referencesexternal-links","title":"References/External links","text":"<p>[1] \u4f50\u85e4 \u307f\u306a\u307f, \u65e9\u5742 \u7965\u4e00, \u6e05\u6c34 \u653f\u884c, \u6751\u91ce \u9686\u5f66, \u6a2a\u65ad\u6b69\u884c\u8005\u306b\u5bfe\u3059\u308b\u30c9\u30e9\u30a4\u30d0\u306e\u30ea\u30b9\u30af\u56de\u907f\u884c\u52d5\u306e\u30e2\u30c7\u30eb\u5316, \u81ea\u52d5\u8eca\u6280\u8853\u4f1a\u8ad6\u6587\u96c6, 2013, 44 \u5dfb, 3 \u53f7, p. 931-936.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_detection_area_module/","title":"Index","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_detection_area_module/#detection-area","title":"Detection Area","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_detection_area_module/#role","title":"Role","text":"<p>If pointcloud or predicted objects are detected in a detection area defined on a map, the stop planning will be executed at the predetermined point.</p> <p></p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_detection_area_module/#activation-timing","title":"Activation Timing","text":"<p>This module is activated when there is a detection area on the target lane.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_detection_area_module/#module-parameters","title":"Module Parameters","text":"Parameter Type Description <code>use_dead_line</code> bool [-] weather to use dead line or not <code>state_clear_time</code> double [s] when the vehicle is stopping for certain time without incoming obstacle, move to STOPPED state <code>stop_margin</code> double [m] a margin that the vehicle tries to stop before stop_line <code>dead_line_margin</code> double [m] ignore threshold that vehicle behind is collide with ego vehicle or not <code>unstoppable_policy</code> string [-] policy for handling unstoppable situations: \"go\" (pass through), \"force_stop\" (emergency stop), or \"stop_after_stopline\" (stop after the stop line) <code>max_deceleration</code> double [m/s^2] maximum deceleration used to calculate required braking distance for unstoppable situation handling <code>delay_response_time</code> double [s] delay response time used to calculate required braking distance for unstoppable situation handling <code>hold_stop_margin_distance</code> double [m] parameter for restart prevention (See Algorithm section) <code>distance_to_judge_over_stop_line</code> double [m] parameter for judging that the stop line has been crossed <code>suppress_pass_judge_when_stopping</code> bool [m] parameter for suppressing pass judge when stopping <code>enable_detected_obstacle_logging</code> bool [-] enable/disable logging of detected obstacle positions, time elapsed since last detection, and ego vehicle position when ego-vehicle is in STOP state <code>target_filtering.pointcloud</code> bool [-] whether to stop for pointcloud detection <code>target_filtering.unknown</code> bool [-] whether to stop for UNKNOWN objects area <code>target_filtering.car</code> bool [-] whether to stop for CAR objects area <code>target_filtering.truck</code> bool [-] whether to stop for TRUCK objects area <code>target_filtering.bus</code> bool [-] whether to stop for BUS objects area <code>target_filtering.trailer</code> bool [-] whether to stop for TRAILER objects area <code>target_filtering.motorcycle</code> bool [-] whether to stop for MOTORCYCLE objects area <code>target_filtering.bicycle</code> bool [-] whether to stop for BICYCLE objects area <code>target_filtering.pedestrian</code> bool [-] whether to stop for PEDESTRIAN objects area <code>target_filtering.animal</code> bool [-] whether to stop for ANIMAL objects area <code>target_filtering.hazard</code> bool [-] whether to stop for HAZARD objects area <code>target_filtering.over_drivable</code> bool [-] whether to stop for OVER_DRIVABLE objects area <code>target_filtering.under_drivable</code> bool [-] whether to stop for UNDER_DRIVABLE objects area"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_detection_area_module/#inner-workings-algorithm","title":"Inner-workings / Algorithm","text":"<ol> <li>Gets a detection area and stop line from map information and confirms if there are obstacles in the detection area</li> <li>Inserts stop point l[m] in front of the stop line</li> <li>Calculates required braking distance based on current velocity, <code>max_deceleration</code>, and <code>delay_response_time</code></li> <li>If the vehicle cannot stop before the stop line, applies the configured <code>unstoppable_policy</code></li> <li>Sets velocity as zero at the determined stop point</li> </ol>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_detection_area_module/#detection-logic","title":"Detection Logic","text":"<p>The module uses two detection sources:</p> <ul> <li>Pointcloud detection: Detects any obstacles in the detection area using 3D point cloud data (if <code>target_filtering.pointcloud</code> is enabled)</li> <li>Predicted objects detection: Detects classified objects (vehicles, pedestrians, etc.) in the detection area based on perception module outputs</li> </ul> <p>The module stops the vehicle if either detection source finds an obstacle. For performance optimization, if pointcloud detection finds an obstacle, predicted objects detection is skipped (short-circuit evaluation).</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_detection_area_module/#flowchart","title":"Flowchart","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_detection_area_module/#restart-prevention","title":"Restart prevention","text":"<p>If it needs X meters (e.g. 0.5 meters) to stop once the vehicle starts moving due to the poor vehicle control performance, the vehicle goes over the stopping position that should be strictly observed when the vehicle starts to moving in order to approach the near stop point (e.g. 0.3 meters away).</p> <p>This module has parameter <code>hold_stop_margin_distance</code> in order to prevent from these redundant restart. If the vehicle is stopped within <code>hold_stop_margin_distance</code> meters from stop point of the module (_front_to_stop_line &lt; hold_stop_margin_distance), the module judges that the vehicle has already stopped for the module's stop point and plans to keep stopping current position even if the vehicle is stopped due to other factors.</p> <p> </p> parameters <p> </p> outside the hold_stop_margin_distance <p> </p> inside the hold_stop_margin_distance"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_detection_area_module/#unstoppable-situation-handling","title":"Unstoppable situation handling","text":"<p>When the ego vehicle cannot stop before the stop line with the given <code>max_deceleration</code> and <code>delay_response_time</code>, the module applies the <code>unstoppable_policy</code>:</p> <ul> <li>\"go\" policy: The vehicle is allowed to pass through without stopping. A warning is logged.</li> <li>\"force_stop\" policy: The vehicle performs a stop at the original stop line, even if it cannot stop comfortably. A warning is logged.</li> <li>\"stop_after_stopline\" policy: The stop point is shifted forward beyond the stop line to ensure the vehicle can stop safely within the physical limits.</li> </ul> <p>The required braking distance is calculated as: \\(d*{req} = v \\cdot t*{delay} + v^2/(2a*{max})\\), where \\(v\\) is current velocity, \\(t*{delay}\\) is <code>delay_response_time</code>, and \\(a\\_{max}\\) is <code>max_deceleration</code>.</p> <p>For the \"stop*after_stopline\" policy, if \\(d*{req}\\) exceeds the remaining distance to the stop line \\(d*{stop}\\), the stop point is shifted forward by \\(d*{req} - d\\_{stop}\\). This adjustment is applied only once, when the module transitions from GO to STOP state.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/","title":"Intersection","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#intersection","title":"Intersection","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#role","title":"Role","text":"<p>The intersection module is responsible for safely passing urban intersections by:</p> <ol> <li>checking collisions with upcoming vehicles</li> <li>recognizing the occluded area in the intersection</li> <li>reacting to each color/shape of associated traffic lights</li> </ol> <p>This module is designed to be agnostic to left-hand/right-hand traffic rules and work for crossroads, T-shape junctions, etc. Roundabout is not formally supported in this module.</p> <p></p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#activation-condition","title":"Activation condition","text":"<p>This module is activated when the path contains the lanes with turn_direction tag. More precisely, if the lane_ids of the path contain the ids of those lanes, corresponding instances of intersection module are activated on each lane respectively.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#requirementslimitations","title":"Requirements/Limitations","text":"<ul> <li>The HDMap needs to have the information of turn_direction tag (which should be one of straight, left, right) for all the lanes in intersections and right_of_way tag for specific lanes (refer to RightOfWay section for more details). See autoware_lanelet2_extension document for more detail.</li> <li>WIP(perception requirements/limitations)</li> <li>WIP(sensor visibility requirements/limitations)</li> </ul>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#attention-area","title":"Attention area","text":"<p>The attention area in the intersection is defined as the set of lanes that are conflicting with ego path and their preceding lanes up to <code>common.attention_area_length</code> meters. By default RightOfWay tag is not set, so the attention area covers all the conflicting lanes and its preceding lanes as shown in the first row. RightOfWay tag is used to rule out the lanes that each lane has priority given the traffic light relation and turn_direction priority. In the second row, purple lanes are set as the yield_lane of the ego_lane in the RightOfWay tag.</p> <p></p> <p>intersection_area, which is supposed to be defined on the HDMap, is an area converting the entire intersection.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#in-phaseanti-phase-signal-group","title":"In-phase/Anti-phase signal group","text":"<p>The terms \"in-phase signal group\" and \"anti-phase signal group\" are introduced to distinguish the lanes by the timing of traffic light regulation as shown in below figure.</p> <p></p> <p>The set of intersection lanes whose color is in sync with lane L1 is called the in-phase signal group of L1, and the set of remaining lanes is called the anti-phase signal group.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#how-towhy-set-rightofway-tag","title":"How-to/Why set RightOfWay tag","text":"<p>Ideally RightOfWay tag is unnecessary if ego has perfect knowledge of all traffic signal information because:</p> <ul> <li>it can distinguish which conflicting lanes should be checked because they are GREEN currently and possible collision occur with the vehicles on those lanes</li> <li>it can distinguish which conflicting lanes can be ignored because they are RED currently and there is no chance of collision with the vehicles on those lanes unless they violate the traffic rule</li> </ul> <p>That allows ego to generate the attention area dynamically using the real time traffic signal information. However this ideal condition rarely holds unless the traffic signal information is provided through the infrastructure. Also there maybe be very complicated/bad intersection maps where multiple lanes overlap in a complex manner.</p> <ul> <li>If there is an perfect access to entire traffic light signal, then you can set <code>common.use_map_right_of_way</code> to false and there is no need to set RightOfWay tag on the map. The intersection module will generate the attention area by checking traffic signal and corresponding conflicting lanes. This feature is not implemented yet.</li> <li>If traffic signal information is not perfect, then set <code>common.use_map_right_of_way</code> to true. If you do not want to detect vehicles on the anti-phase signal group lanes, set them as yield_lane for ego lane.</li> <li>Even if there are no traffic lights if the intersection lanes are overlapped in a ugly manner, you may need to set RightOfWay tag. For example if adjacent intersection lanes of the same in-phase group are not sharing the boundary line and overlapped a little bit, you may need to set RightOfWay to each other for them in order to avoid unnecessary stop for vehicle on such unrelated lane.</li> </ul> <p>To help the intersection module care only a set of limited lanes, RightOfWay tag needs to be properly set.</p> <p>Following table shows an example of how to set yield_lanes to each lane in a intersection w/o traffic lights. Since it is not apparent how to uniquely determine signal phase group for a set of intersection lanes in geometric/topological manner, yield_lane needs to be set manually. Straight lanes with traffic lights are exceptionally handled to detect no lanes because commonly it has priority over all the other lanes, so no RightOfWay setting is required.</p> turn direction of right_of_way yield_lane(with traffic light) yield_lane(without traffic light) straight not need to set yield_lane(this case is special) left/right conflicting lanes of in-phase group left(Left hand traffic) all conflicting lanes of the anti-phase group and right conflicting lanes of in-phase group right conflicting lanes of in-phase group right(Left hand traffic) all conflicting lanes of the anti-phase group no yield_lane left(Right hand traffic) all conflicting lanes of the anti-phase group no yield_lane right(Right hand traffic) all conflicting lanes of the anti-phase group and right conflicting lanes of in-phase group left conflicting lanes of in-phase group <p>This setting gives the following <code>attention_area</code> configurations.</p> <p> </p> <p>For complex/bad intersection map like the one illustrated below, additional RightOfWay setting maybe necessary.</p> <p></p> <p>The bad points are:</p> <ol> <li>ego lane is overlapped with adjacent lane of the in-phase group. In this case you need to set this lane as yield_lane additionally because otherwise attention area is generated for its preceding lanes as well, which may cause unwanted stop.</li> <li>ego lane is overlapped with unrelated lane. In this case the lane is right-turn only and there is no chance of collision in theory. But you need to set this lane as yield_lane additionally for the same reason as (1).</li> </ol>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#possible-stop-lines","title":"Possible stop lines","text":"<p>Following figure illustrates important positions used in the intersection module. Note that each solid line represents ego front line position and the corresponding dot represents the actual inserted stop point position for the vehicle frame, namely the center of the rear wheel.</p> <p></p> <p>To precisely calculate stop positions, the path is interpolated at the certain interval of <code>common.path_interpolation_ds</code>.</p> <ul> <li>closest_idx denotes the path point index which is closest to ego position.</li> <li>first_attention_stopline denotes the first path point where ego footprint intersects with the attention_area.</li> <li>If a stopline is associated with the intersection lane on the map, that line is used as default_stopline for collision detection. Otherwise the point which is <code>common.default_stopline_margin</code> meters behind first_attention_stopline is defined as default_stopline instead.</li> <li>occlusion_peeking_stopline is a bit ahead of first_attention_stopline as described later.</li> <li>occlusion_wo_tl_pass_judge_line is the first position where ego footprint intersects with the centerline of the first attention_area lane.</li> </ul>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#target-objects","title":"Target objects","text":"<p>For stuck vehicle detection and collision detection, this module checks car, bus, truck, trailer, motor cycle, and bicycle type objects.</p> <p>Objects that satisfy all of the following conditions are considered as target objects (possible collision objects):</p> <ul> <li>The center of the object is within a certain distance from the attention lane (threshold = <code>common.attention_area_margin</code>) .<ul> <li>(Optional condition) The center of the object is in the intersection area.<ul> <li>To deal with objects that is in the area not covered by the lanelets in the intersection.</li> </ul> </li> </ul> </li> <li>The posture of object is the same direction as the attention lane (threshold = <code>common.attention_area_angle_threshold</code>).</li> <li>Not being in the adjacent lanes of ego.</li> </ul>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#overview-of-decision-process","title":"Overview of decision process","text":"<p>There are several behaviors depending on the scene.</p> behavior scene action Safe Ego detected no occlusion and collision Ego passes the intersection StuckStop The exit of the intersection is blocked by traffic jam Ego stops before the intersection or the boundary of attention area YieldStuck Another vehicle stops to yield ego Ego stops before the intersection or the boundary of attention area NonOccludedCollisionStop Ego detects no occlusion but detects collision Ego stops at default_stopline FirstWaitBeforeOcclusion Ego detected occlusion when entering the intersection Ego stops at default_stopline at first PeekingTowardOcclusion Ego detected occlusion and but no collision within the FOV (after FirstWaitBeforeOcclusion) Ego approaches the boundary of the attention area slowly OccludedCollisionStop Ego detected both occlusion and collision (after FirstWaitBeforeOcclusion) Ego stops immediately FullyPrioritized Ego is fully prioritized by the RED/Arrow signal Ego only cares vehicles still running inside the intersection. Occlusion is ignored OverPassJudgeLine Ego is already inside the attention area and/or cannot stop before the boundary of attention area Ego does not detect collision/occlusion anymore and passes the intersection <p></p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#stuck-vehicle-detection","title":"Stuck Vehicle Detection","text":"<p>If there is any object on the path inside the intersection and at the exit of the intersection (up to <code>stuck_vehicle.stuck_vehicle_detect_dist</code>) lane and its velocity is less than the threshold (<code>stuck_vehicle.stuck_vehicle_velocity_threshold</code>), the object is regarded as a stuck vehicle. If stuck vehicles exist, this module inserts a stopline a certain distance (=<code>default_stopline_margin</code>) before the overlapped region with other lanes. The stuck vehicle detection area is generated based on the planned path, so the stuck vehicle stopline is not inserted if the upstream module generated an avoidance path.</p> <p></p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#yield-stuck-vehicle-detection","title":"Yield stuck vehicle detection","text":"<p>If there is any stopped object on the attention lanelet between the intersection point with ego path and the position which is <code>yield_stuck.distance_threshold</code> before that position, the object is regarded as yielding to ego vehicle. In this case ego is given the right-of-way by the yielding object but this module inserts stopline to prevent entry into the intersection. This scene happens when the object is yielding against ego or the object is waiting before the crosswalk around the exit of the intersection.</p> <p></p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#collision-detection","title":"Collision detection","text":"<p>The following process is performed for the targets objects to determine whether ego can pass the intersection safely. If it is judged that ego cannot pass the intersection with enough margin, this module inserts a stopline on the path.</p> <ol> <li>predict the time \\(t\\) when the object intersects with ego path for the first time from the predicted path time step. Only the predicted whose confidence is greater than <code>collision_detection.min_predicted_path_confidence</code> is used.</li> <li>detect collision between the predicted path and ego's predicted path in the following process<ol> <li>calculate the collision interval of [\\(t\\) - <code>collision_detection.collision_start_margin_time</code>, \\(t\\) + <code>collision_detection.collision_end_margin_time</code>]</li> <li>calculate the passing area of ego during the collision interval from the array of (time, distance) obtained by smoothed velocity profile</li> <li>check if ego passing area and object predicted path interval collides</li> </ol> </li> <li>if collision is detected, the module inserts a stopline</li> <li>if ego is over the pass_judge_line, collision checking is skipped to avoid sudden braking and/or unnecessary stop in the middle of the intersection</li> </ol> <p>The parameters <code>collision_detection.collision_start_margin_time</code> and <code>collision_detection.collision_end_margin_time</code> can be interpreted as follows:</p> <ul> <li>If ego was to pass the intersection earlier than the target object, collision would be detected if the time difference between the two was less than <code>collision_detection.collision_start_margin_time</code>.</li> <li>If ego was to pass the intersection later than the target object, collision would be detected if the time difference between the two was less than <code>collision_detection.collision_end_margin_time</code>.</li> </ul> <p>If collision is detected, the state transits to \"STOP\" immediately. On the other hand, the state does not transit to \"GO\" unless safe judgement continues for a certain period <code>collision_detection.collision_detection_hold_time</code> to prevent the chattering of decisions.</p> <p>Currently, the intersection module uses <code>motion_velocity_smoother</code> feature to precisely calculate ego velocity profile along the intersection lane under longitudinal/lateral constraints. If the flag <code>collision_detection.velocity_profile.use_upstream</code> is true, the target velocity profile of the original path is used. Otherwise the target velocity is set to <code>collision.velocity_profile.default_velocity</code>. In the trajectory smoothing process the target velocity at/before ego trajectory points are set to ego current velocity. The smoothed trajectory is then converted to an array of (time, distance) which indicates the arrival time to each trajectory point on the path from current ego position. You can visualize this array by adding the lane id to <code>debug.ttc</code> and running</p> <pre><code>ros2 run behavior_velocity_intersection_module ttc.py --lane_id &lt;lane_id&gt;\n</code></pre> <p></p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#about-use_upstream_velocity-flag","title":"about use_upstream_velocity flag","text":"<p>There are some use cases where ego should check collision before entering the intersection considering the temporal stop by walkway/crosswalk module around the exit of the intersection, because their stop position can be inside the intersection and it could bother upcoming vehicles. By setting the flag <code>collision_detection.velocity_profile.use_upstream</code> to true and running the walkway/crosswalk module prior to this module, ego velocity profile is calculated considering their velocity and stop positions.</p> <p>As illustrated in below figure if upstream module inserted a stopline, ego position profile will remain there for the infinite time, thus it leads to the judgement that ego cannot exit the intersection during the interval [\\(t\\) - <code>collision_detection.collision_start_margin_time</code>, \\(t\\) + <code>collision_detection.collision_end_margin_time</code>]. In this way this feature considers possible collision for the infinite time if stoplines exist ahead of ego position (practically the prediction horizon is limited so the collision check horizon is bounded). </p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#occlusion-detection","title":"Occlusion detection","text":"<p>If the flag <code>occlusion.enable</code> is true this module checks if there is sufficient field of view (FOV) on the attention area up to <code>occlusion.occlusion_attention_area_length</code>. If FOV is not clear enough ego first makes a brief stop at default_stopline for <code>occlusion.temporal_stop_time_before_peeking</code>, and inserts stopline at occlusion_peeking_stopline, thus slowly creeping toward there.</p> <p>During the creeping if collision is detected this module inserts a stop line in front of ego immediately, and if the FOV gets sufficiently clear the intersection_occlusion wall will disappear. If occlusion is cleared and no collision is detected ego will pass the intersection.</p> <p>The occlusion is detected as the common area of occlusion attention area(which is partially the same as the normal attention area) and the unknown cells of the occupancy grid map. The occupancy grid map is denoised using morphology with the window size of <code>occlusion.denoise_kernel</code>. The occlusion attention area lanes are discretized to line strings and they are used to generate a grid whose each cell represents the distance from ego path along the lane as shown below.</p> <p></p> <p>If the nearest occlusion cell value is below the threshold <code>occlusion.occlusion_required_clearance_distance</code>, it means that the FOV of ego is not clear. It is expected that the occlusion gets cleared as the vehicle approaches the occlusion peeking stop line.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#occlusion-source-estimation-at-intersection-with-traffic-light","title":"Occlusion source estimation at intersection with traffic light","text":"<p>At intersection with traffic light, the whereabout of occlusion is estimated by checking if there are any objects between ego and the nearest occlusion cell. While the occlusion is estimated to be caused by some object (DYNAMICALLY occluded), intersection_wall appears at all times. If no objects are found between ego and the nearest occlusion cell (STATICALLY occluded), after ego stopped for the duration of <code>occlusion.static_occlusion_with_traffic_light_timeout</code> plus <code>occlusion.occlusion_detection_hold_time</code>, occlusion is intentionally ignored to avoid stuck.</p> <p></p> <p>The remaining time is visualized on the intersection_occlusion virtual wall.</p> <p></p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#occlusion-handling-at-intersection-without-traffic-light","title":"Occlusion handling at intersection without traffic light","text":"<p>At intersection without traffic light, if occlusion is detected, ego makes a brief stop at default_stopline and first_attention_stopline respectively. After stopping at the first_attention_area_stopline this module inserts <code>occlusion.absence_traffic_light.creep_velocity</code> velocity between ego and occlusion_wo_tl_pass_judge_line while occlusion is not cleared. If collision is detected, ego immediately stops. Once the occlusion is cleared or ego has passed occlusion_wo_tl_pass_judge_line this module does not detect collision and occlusion because ego footprint is already inside the intersection.</p> <p></p> <p>If the flag <code>occlusion.request_approval_wo_traffic_light</code> is <code>true</code>, <code>intersection_occlusion</code> requests approval from RTC operator. If it is <code>false</code>, it does not request approval after tbe brief stop even if occlusion is not sufficiently cleared.</p> <p>While ego is creeping, yellow intersection_wall appears in front ego.</p> <p></p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#map-based-forced-rtc","title":"Map-based forced RTC","text":"<p>RTC can be enabled for specific intersection lanelets such that even if approval is not required by default, it will be required before crossing the corresponding intersection lanelets. The following attribute should be added to the lanelet in intersection in the map file:</p> <pre><code>&lt;tag k='rtc_approval_required_v1' v='intersection' /&gt;\n</code></pre> <p>The value can be set to <code>intersection</code>, <code>intersection_occlusion</code>, or <code>intersection,intersection_occlusion</code>, to adjust which modules will require approvals.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#traffic-signal-specific-behavior","title":"Traffic signal specific behavior","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#collision-detection_1","title":"Collision detection","text":"<p>TTC parameter varies depending on the traffic light color/shape as follows.</p> traffic light color ttc(start) ttc(end) GREEN <code>collision_detection.not_prioritized.collision_start_margin</code> <code>collision_detection.not_prioritized.collision_end_margin</code> AMBER <code>collision_detection.partially_prioritized.collision_start_end_margin</code> <code>collision_detection.partially_prioritized.collision_start_end_margin</code> RED / Arrow <code>collision_detection.fully_prioritized.collision_start_end_margin</code> <code>collision_detection.fully_prioritized.collision_start_end_margin</code>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#yield-on-green","title":"yield on GREEN","text":"<p>If the traffic light color changed to GREEN and ego approached the entry of the intersection lane within the distance <code>collision_detection.yield_on_green_traffic_light.distance_to_assigned_lanelet_start</code> and there is any object whose distance to its stopline is less than <code>collision_detection.yield_on_green_traffic_light.object_dist_to_stopline</code>, this module commands to stop for the duration of <code>collision_detection.yield_on_green_traffic_light.duration</code> at default_stopline.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#skip-on-amber","title":"skip on AMBER","text":"<p>If the traffic light color is AMBER but the object is expected to stop before its stopline under the deceleration of <code>collision_detection.ignore_on_amber_traffic_light.object_expected_deceleration</code>, collision checking is skipped.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#skip-on-red","title":"skip on RED","text":"<p>If the traffic light color is RED or Arrow signal is turned on, the attention lanes which are not conflicting with ego lane are not used for detection. And even if the object stops with a certain overshoot from its stopline, but its expected stop position under the deceleration of <code>collision_detection.ignore_on_amber_traffic_light.object_expected_deceleration</code> is more than the distance <code>collision_detection.ignore_on_red_traffic_light.object_margin_to_path</code> from collision point, the object is ignored.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#occlusion-detection_1","title":"Occlusion detection","text":"<p>When the traffic light color/shape is RED/Arrow, occlusion detection is skipped.</p> <p></p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#pass-judge-line","title":"Pass Judge Line","text":"<p>Generally it is not tolerable for vehicles that have lower traffic priority to stop in the middle of the unprotected area in intersections, and they need to stop at the stop line beforehand if there will be any risk of collision, which introduces two requirements:</p> <ol> <li>The vehicle must start braking before the boundary of the unprotected area at least by the braking distance if it is supposed to stop</li> <li>The vehicle must recognize upcoming vehicles and check safety beforehand with enough braking distance margin if it is supposed to go<ol> <li>And the SAFE decision must be absolutely certain and remain to be valid for the future horizon so that the safety condition will be always satisfied while ego is driving inside the unprotected area.</li> </ol> </li> <li>(TODO): Since it is almost impossible to make perfectly safe decision beforehand given the limited detection range/velocity tracking performance, intersection module should plan risk-evasive acceleration velocity profile AND/OR relax lateral acceleration limit while ego is driving inside the unprotected area, if the safety decision is \"betrayed\" later due to the following reasons:<ol> <li>The situation turned out to be dangerous later, mainly because velocity tracking was underestimated or the object accelerated beyond TTC margin</li> <li>The situation turned dangerous later, mainly because the object is suddenly detected out of nowhere</li> </ol> </li> </ol> <p>The position which is before the boundary of unprotected area by the braking distance which is obtained by</p> \\[ \\dfrac{v_{\\mathrm{ego}}^{2}}{2a_{\\mathrm{max}}} + v_{\\mathrm{ego}} * t_{\\mathrm{delay}} \\] <p>plus an additional margin <code>common.pass_judge_line_margin</code> is called pass_judge_line, and safety decision must be made before ego passes this position because ego does not stop anymore. pass_judge_line are illustrated in the following figure(2nd_pass_judge_line is deprecated).</p> <p></p> <p>Intersection module will command to GO if</p> <ul> <li>ego is over pass judge line AND</li> <li>ego judged SAFE previously AND</li> </ul> <p>because it is expected to stop or continue stop decision if</p> <ol> <li>ego is before pass_judge_line OR<ol> <li>reason: it has enough braking distance margin</li> </ol> </li> <li>ego judged UNSAFE previously<ol> <li>reason: ego is now trying to stop and should continue stop decision if collision is detected in later calculation</li> </ol> </li> </ol> <p>For the 3rd condition, it is possible that ego stops with some overshoot to the unprotected area while it is trying to stop for collision detection, because ego should keep stop decision while UNSAFE decision is made even if it passed pass_judge_line during deceleration.</p> <p>For the 4th condition, at intersections with 2nd attention lane, even if ego is over the pass_judge_line, still intersection module commands to stop if the most probable collision is expected to happen in the 2nd attention lane.</p> <p>Also if <code>occlusion.enable</code> is true, the position of pass_judge line changes to occlusion_peeking_stopline if ego passed the original pass_judge_line position while ego is peeking. Otherwise ego could inadvertently judge that it passed pass_judge during peeking and then abort peeking.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#data-structure","title":"Data Structure","text":"<p>Each data structure is defined in <code>util_type.hpp</code>.</p> <p></p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#intersectionlanelets","title":"<code>IntersectionLanelets</code>","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#intersectionstoplines","title":"<code>IntersectionStopLines</code>","text":"<p>Each stop lines are generated from interpolated path points to obtain precise positions.</p> <p></p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#targetobject","title":"<code>TargetObject</code>","text":"<p><code>TargetObject</code> holds the object, its belonging lane and corresponding stopline information.</p> <p></p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#module-parameters","title":"Module Parameters","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#common","title":"common","text":"Parameter Type Description <code>.attention_area_length</code> double [m] range for object detection <code>.attention_area_margin</code> double [m] margin for expanding attention area width <code>.attention_area_angle_threshold</code> double [rad] threshold of angle difference between the detected object and lane <code>.use_intersection_area</code> bool [-] flag to use intersection_area for collision detection <code>.default_stopline_margin</code> double [m] margin before_stop_line <code>.pass_judge_line_margin</code> double [m] additional margin for pass_judge_line position from first_attention_stopline <code>.stopline_overshoot_margin</code> double [m] margin for the overshoot from stopline <code>.max_accel</code> double [m/ss] max acceleration for stop <code>.max_jerk</code> double [m/sss] max jerk for stop <code>.delay_response_time</code> double [s] action delay before stop <code>.enable_pass_judge_before_default_stopline</code> bool [-] flag not to stop before default_stopline even if ego is over pass_judge_line"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#stuck_vehicleyield_stuck","title":"stuck_vehicle/yield_stuck","text":"Parameter Type Description <code>stuck_vehicle.turn_direction</code> - [-] turn_direction specifier for stuck vehicle detection <code>stuck_vehicle.stuck_vehicle_detect_dist</code> double [m] length toward from the exit of intersection for stuck vehicle detection <code>stuck_vehicle.stuck_vehicle_velocity_threshold</code> double [m/s] velocity threshold for stuck vehicle detection <code>yield_stuck.distance_threshold</code> double [m/s] distance threshold of yield stuck vehicle from ego path along the lane"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#collision_detection","title":"collision_detection","text":"Parameter Type Description <code>.consider_wrong_direction_vehicle</code> bool [-] flag to detect objects in the wrong direction <code>.collision_detection_hold_time</code> double [s] hold time of collision detection <code>.min_predicted_path_confidence</code> double [-] minimum confidence value of predicted path to use for collision detection <code>.keep_detection_velocity_threshold</code> double [s] ego velocity threshold for continuing collision detection before pass judge line <code>.velocity_profile.use_upstream</code> bool [-] flag to use velocity profile planned by upstream modules <code>.velocity_profile.minimum_upstream_velocity</code> double [m/s] minimum velocity of upstream velocity profile to avoid zero division <code>.velocity_profile.default_velocity</code> double [m/s] constant velocity profile when use_upstream is false <code>.velocity_profile.minimum_default_velocity</code> double [m/s] minimum velocity of default velocity profile to avoid zero division <code>.yield_on_green_traffic_light</code> - [-] description <code>.ignore_amber_traffic_light</code> - [-] description <code>.ignore_on_red_traffic_light</code> - [-] description"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#occlusion","title":"occlusion","text":"Parameter Type Description <code>.enable</code> bool [-] flag to calculate occlusion detection <code>.request_approval_wo_traffic_light</code> bool [-] flag to request RTC approval when occluded without traffic light <code>.occlusion_attention_area_length</code> double [m] the length of attention are for occlusion detection <code>.free_space_max</code> int [-] maximum value of occupancy grid cell to treat at occluded <code>.occupied_min</code> int [-] minimum value of occupancy grid cell to treat at occluded <code>.denoise_kernel</code> double [m] morphology window size for preprocessing raw occupancy grid <code>.attention_lane_crop_curvature_threshold</code> double [m] curvature threshold for trimming curved part of the lane <code>.attention_lane_crop_curvature_ds</code> double [m] discretization interval of centerline for lane curvature calculation <code>.creep_during_peeking.enable</code> bool [-] flag to insert <code>creep_velocity</code> while peeking to intersection occlusion stopline <code>.creep_during_peeking.creep_velocity</code> double [m/s] the command velocity while peeking to intersection occlusion stopline <code>.peeking_offset</code> double [m] the offset of the front of the vehicle into the attention area for peeking to occlusion <code>.occlusion_required_clearance_distance</code> double [m] threshold for the distance to nearest occlusion cell from ego path <code>.possible_object_bbox</code> [double] [m] minimum bounding box size for checking if occlusion polygon is small enough <code>.ignore_parked_vehicle_speed_threshold</code> double [m/s] velocity threshold for checking parked vehicle <code>.occlusion_detection_hold_time</code> double [s] hold time of occlusion detection <code>.temporal_stop_time_before_peeking</code> double [s] temporal stop duration at default_stopline before starting peeking <code>.creep_velocity_without_traffic_light</code> double [m/s] creep velocity to occlusion_wo_tl_pass_judge_line <code>.static_occlusion_with_traffic_light_timeout</code> double [s] the timeout duration for ignoring static occlusion at intersection with traffic light"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#trouble-shooting","title":"Trouble shooting","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#intersection-module-stops-against-unrelated-vehicles","title":"Intersection module stops against unrelated vehicles","text":"<p>In this case, first visualize <code>/planning/scenario_planning/lane_driving/behavior_planning/behavior_velocity_planner/debug/intersection</code> topic and check the <code>attention_area</code> polygon. Intersection module performs collision checking for vehicles running on this polygon, so if it extends to unintended lanes, it needs to have RightOfWay tag.</p> <p>By lowering <code>common.attention_area_length</code> you can check which lanes are conflicting with the intersection lane. Then set part of the conflicting lanes as the yield_lane.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#the-stop-line-of-intersection-is-chattering","title":"The stop line of intersection is chattering","text":"<p>The parameter <code>collision_detection.collision_detection_hold_time</code> suppresses the chattering by keeping UNSAFE decision for this duration until SAFE decision is finally made. The role of this parameter is to account for unstable detection/tracking of objects. By increasing this value you can suppress the chattering. However it could elongate the stopping duration excessively.</p> <p>If the chattering arises from the acceleration/deceleration of target vehicles, increase <code>collision_detection.collision_detection.collision_end_margin_time</code> and/or <code>collision_detection.collision_detection.collision_end_margin_time</code>.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#the-stop-line-is-released-too-fastslow","title":"The stop line is released too fast/slow","text":"<p>If the intersection wall appears too fast, or ego tends to stop too conservatively for upcoming vehicles, lower the parameter <code>collision_detection.collision_detection.collision_start_margin_time</code>. If it lasts too long after the target vehicle passed, then lower the parameter <code>collision_detection.collision_detection.collision_end_margin_time</code>.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#ego-suddenly-stops-at-intersection-with-traffic-light","title":"Ego suddenly stops at intersection with traffic light","text":"<p>If the traffic light color changed from AMBER/RED to UNKNOWN, the intersection module works in the GREEN color mode. So collision and occlusion are likely to be detected again.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#occlusion-is-detected-overly","title":"Occlusion is detected overly","text":"<p>You can check which areas are detected as occlusion by visualizing <code>/planning/scenario_planning/lane_driving/behavior_planning/behavior_velocity_planner/debug/intersection/occlusion_polygons</code>.</p> <p>If you do not want to detect / do want to ignore occlusion far from ego or lower the computational cost of occlusion detection, <code>occlusion.occlusion_attention_area_length</code> should be set to lower value.</p> <p>If you want to care the occlusion nearby ego more cautiously, set <code>occlusion.occlusion_required_clearance_distance</code> to a larger value. Then ego will approach the occlusion_peeking_stopline more closely to assure more clear FOV.</p> <p><code>occlusion.possible_object_bbox</code> is used for checking if detected occlusion area is small enough that no vehicles larger than this size can exist inside. By decreasing this size ego will ignore small occluded area.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#occupancy-grid-map-tuning","title":"occupancy grid map tuning","text":"<p>Refer to the document of autoware_probabilistic_occupancy_grid_map for details. If occlusion tends to be detected at apparently free space, increase <code>occlusion.free_space_max</code> to ignore them.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#in-simple_planning_simulator","title":"in simple_planning_simulator","text":"<p>intersection_occlusion feature is not recommended for use in planning_simulator because the laserscan_based_occupancy_grid_map generates unnatural UNKNOWN cells in 2D manner:</p> <ul> <li>all the cells behind pedestrians are UNKNOWN</li> <li>no ground point clouds are generated</li> </ul> <p>Also many users do not set traffic light information frequently although it is very critical for intersection_occlusion (and in real traffic environment too).</p> <p>For these reasons, <code>occlusion.enable</code> is false by default.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#on-real-vehicle-in-end-to-end-simulator","title":"on real vehicle / in end-to-end simulator","text":"<p>On real vehicle or in end-to-end simulator like AWSIM the following pointcloud_based_occupancy_grid_map configuration is highly recommended:</p> <pre><code>scan_origin_frame: \"velodyne_top\"\n\ngrid_map_type: \"OccupancyGridMapProjectiveBlindSpot\"\nOccupancyGridMapProjectiveBlindSpot:\n  projection_dz_threshold: 0.01 # [m] for avoiding null division\n  obstacle_separation_threshold: 1.0 # [m] fill the interval between obstacles with unknown for this length\n</code></pre> <p>You should set the top lidar link as the <code>scan_origin_frame</code>. In the example it is <code>velodyne_top</code>. The method <code>OccupancyGridMapProjectiveBlindSpot</code> estimates the FOV by running projective ray-tracing from <code>scan_origin</code> to obstacle or up to the ground and filling the cells on the \"shadow\" of the object as UNKNOWN.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#flowchart","title":"Flowchart","text":"<p>WIP</p> <p></p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#merge-from-private","title":"Merge From Private","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#role_1","title":"Role","text":"<p>When an ego enters a public road from a private road (e.g. a parking lot), it needs to face and stop before entering the public road to make sure it is safe.</p> <p>This module is activated when there is an intersection at the private area from which the vehicle enters the public road. The stop line is generated both when the goal is in the intersection lane and when the path goes beyond the intersection lane. The basic behavior is the same as the intersection module, but ego must stop once at the stop line.</p> <p></p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#activation-timing","title":"Activation Timing","text":"<p>This module is activated when the following conditions are met:</p> <ul> <li>ego-lane has a <code>private</code> tag</li> <li>ego-lane has a conflict with other no-private lanelets</li> </ul>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#module-parameters_1","title":"Module Parameters","text":"Parameter Type Description <code>merge_from_private_road/stop_duration_sec</code> double [m] time margin to change state"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#known-issue","title":"Known Issue","text":"<p>If ego go over the stop line for a certain distance, then it will not transit from STOP.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_intersection_module/#test-maps","title":"Test Maps","text":"<p>The intersections lanelet map consist of a variety of intersections including:</p> <ul> <li>4-way crossing with traffic light</li> <li>4-way crossing without traffic light</li> <li>T-shape crossing without traffic light</li> <li>intersection with a loop</li> <li>complicated intersection</li> </ul> <p></p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_no_drivable_lane_module/","title":"Index","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_no_drivable_lane_module/#no-drivable-lane","title":"No Drivable Lane","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_no_drivable_lane_module/#role","title":"Role","text":"<p>This module plans the velocity of the related part of the path in case there is a no drivable lane referring to it.</p> <p>A no drivable lane is a lanelet or more that are out of operation design domain (ODD), i.e., the vehicle must not drive autonomously in this lanelet. A lanelet can be no drivable (out of ODD) due to many reasons, either technical limitations of the SW and/or HW, business requirements, safety considerations, .... etc, or even a combination of those.</p> <p>Some examples of No Drivable Lanes</p> <ul> <li>Closed road intentionally, due to construction work for example</li> <li>Underpass road that goes under a railway, for safety reasons</li> <li>Road with slope/inclination that the vehicle is not be able to drive autonomously due to technical limitations. And lots of other examples.</li> </ul> <p></p> <p>A lanelet becomes invalid by adding a new tag under the relevant lanelet in the map file <code>&lt;tag k=\"no_drivable_lane\" v=\"yes\"/&gt;</code>.</p> <p>The target of this module is to stop the vehicle before entering the no drivable lane (with configurable stop margin) or keep the vehicle stationary if autonomous mode started inside a no drivable lane. Then ask the human driver to take the responsibility of the driving task (Takeover Request / Request to Intervene)</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_no_drivable_lane_module/#activation-timing","title":"Activation Timing","text":"<p>This function is activated when the lane id of the target path has an no drivable lane label (i.e. the <code>no_drivable_lane</code> attribute is <code>yes</code>).</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_no_drivable_lane_module/#module-parameters","title":"Module Parameters","text":"Parameter Type Description <code>stop_margin</code> double [m] margin for ego vehicle to stop before speed_bump <code>print_debug_info</code> bool whether debug info will be printed or not"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_no_drivable_lane_module/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<ul> <li>Get no_drivable_lane attribute on the path from lanelet2 map</li> <li>The no drivable lane state machine starts in <code>INIT</code> state</li> <li>Get the intersection points between path and no drivable lane polygon</li> <li>Assign the state to <code>APPROACHING</code> toward a no drivable lane if:<ul> <li>the distance from front of the ego vehicle till the first intersection point between the ego path and the no drivable lane polygon is more than the <code>stop_margin</code></li> </ul> </li> <li>Assign the state to <code>INSIDE_NO_DRIVABLE_LANE</code> if:<ul> <li>the first point of the ego path is inside the no drivable lane polygon, or</li> <li>the distance from front of the ego vehicle till the first intersection point between the ego path and the no drivable lane polygon is less than the <code>stop_margin</code></li> </ul> </li> <li>Assign the state to <code>STOPPED</code> when the vehicle is completely stopped</li> </ul>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_no_drivable_lane_module/#future-work","title":"Future Work","text":"<ul> <li>As Request to Intervene API is not implemented yet, this will be handled to notify the driver to takeover the driving task responsibility after the vehicle stops due to <code>no_drivable_lane</code></li> <li>Handle the case when the vehicle stops before a no drivable lane but part of its footprint intersects with the no drivable lane polygon.</li> </ul>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_no_stopping_area_module/","title":"Index","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_no_stopping_area_module/#no-stopping-area","title":"No Stopping Area","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_no_stopping_area_module/#role","title":"Role","text":"<p>This module plans to avoid stop in 'no stopping area`.</p> <p></p> <ul> <li>PassThrough case<ul> <li>if ego vehicle go through pass judge point, then ego vehicle can't stop with maximum jerk and acceleration, so this module doesn't insert stop velocity. In this case override or external operation is necessary.</li> </ul> </li> <li>STOP case<ul> <li>If there is a stuck vehicle or stop velocity around <code>no_stopping_area</code>, then vehicle stops inside <code>no_stopping_area</code> so this module makes stop velocity in front of <code>no_stopping_area</code></li> </ul> </li> <li>GO case<ul> <li>else</li> </ul> </li> </ul>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_no_stopping_area_module/#limitation","title":"Limitation","text":"<p>This module allows developers to design vehicle velocity in <code>no_stopping_area</code> module using specific rules. Once ego vehicle go through pass through point, ego vehicle does't insert stop velocity and does't change decision from GO. Also this module only considers dynamic object in order to avoid unnecessarily stop.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_no_stopping_area_module/#modelparameter","title":"ModelParameter","text":"Parameter Type Description <code>state_clear_time</code> double [s] time to clear stop state <code>stuck_vehicle_vel_thr</code> double [m/s] vehicles below this velocity are considered as stuck vehicle. <code>stop_margin</code> double [m] margin to stop line at no stopping area <code>dead_line_margin</code> double [m] if ego pass this position GO <code>stop_line_margin</code> double [m] margin to auto-gen stop line at no stopping area <code>detection_area_length</code> double [m] length of searching polygon <code>stuck_vehicle_front_margin</code> double [m] obstacle stop max distance"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_no_stopping_area_module/#flowchart","title":"Flowchart","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_occlusion_spot_module/","title":"Index","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_occlusion_spot_module/#occlusion-spot","title":"Occlusion Spot","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_occlusion_spot_module/#role","title":"Role","text":"<p>This module plans safe velocity to slow down before reaching collision point that hidden object is darting out from <code>occlusion spot</code> where driver can't see clearly because of obstacles.</p> <p></p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_occlusion_spot_module/#activation-timing","title":"Activation Timing","text":"<p>This module is activated if <code>launch_occlusion_spot</code> becomes true. To make pedestrian first zone map tag is one of the TODOs.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_occlusion_spot_module/#limitation-and-todos","title":"Limitation and TODOs","text":"<p>This module is prototype implementation to care occlusion spot. To solve the excessive deceleration due to false positive of the perception, the logic of detection method can be selectable. This point has not been discussed in detail and needs to be improved.</p> <ul> <li>Make occupancy grid for planning.</li> <li>Make map tag for occlusion spot.</li> <li>About the best safe motion.</li> </ul> <p>TODOs are written in each Inner-workings / Algorithms (see the description below).</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_occlusion_spot_module/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_occlusion_spot_module/#logics-working","title":"Logics Working","text":"<p>There are several types of occlusions, such as \"occlusions generated by parked vehicles\" and \"occlusions caused by obstructions\". In situations such as driving on road with obstacles, where people jump out of the way frequently, all possible occlusion spots must be taken into account. This module considers all occlusion spots calculated from the occupancy grid, but it is not reasonable to take into account all occlusion spots for example, people jumping out from behind a guardrail, or behind cruising vehicle. Therefore currently detection area will be limited to to use predicted object information.</p> <p>Note that this decision logic is still under development and needs to be improved.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_occlusion_spot_module/#detectionarea-polygon","title":"DetectionArea Polygon","text":"<p>This module considers TTV from pedestrian velocity and lateral distance to occlusion spot. TTC is calculated from ego velocity and acceleration and longitudinal distance until collision point using motion velocity smoother. To compute fast this module only consider occlusion spot whose TTV is less than TTC and only consider area within \"max lateral distance\".</p> <p></p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_occlusion_spot_module/#occlusion-spot-occupancy-grid-base","title":"Occlusion Spot Occupancy Grid Base","text":"<p>This module considers any occlusion spot around ego path computed from the occupancy grid. Due to the computational cost occupancy grid is not high resolution and this will make occupancy grid noisy so this module add information of occupancy to occupancy grid map.</p> <p>TODO: consider hight of obstacle point cloud to generate occupancy grid.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_occlusion_spot_module/#collision-free-judgement","title":"Collision Free Judgement","text":"<p>obstacle that can run out from occlusion should have free space until intersection from ego vehicle</p> <p></p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_occlusion_spot_module/#partition-lanelet","title":"Partition Lanelet","text":"<p>By using lanelet information of \"guard_rail\", \"fence\", \"wall\" tag, it's possible to remove unwanted occlusion spot.</p> <p>By using static object information, it is possible to make occupancy grid more accurate.</p> <p>To make occupancy grid for planning is one of the TODOs.</p> <p></p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_occlusion_spot_module/#possible-collision","title":"Possible Collision","text":"<p>obstacle that can run out from occlusion is interrupted by moving vehicle.</p> <p></p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_occlusion_spot_module/#about-safe-motion","title":"About safe motion","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_occlusion_spot_module/#the-concept-of-safe-velocity-and-margin","title":"The Concept of Safe Velocity and Margin","text":"<p>The safe slowdown velocity is calculated from the below parameters of ego emergency braking system and time to collision. Below calculation is included but change velocity dynamically is not recommended for planner.</p> <ul> <li>jerk limit[m/s^3]</li> <li>deceleration limit[m/s2]</li> <li>delay response time[s]</li> <li> <p>time to collision of pedestrian[s]   with these parameters we can briefly define safe motion before occlusion spot for ideal environment.</p> <p></p> </li> </ul> <p>This module defines safe margin to consider ego distance to stop and collision path point geometrically. While ego is cruising from safe margin to collision path point, ego vehicle keeps the same velocity as occlusion spot safe velocity.</p> <p></p> <p>Note: This logic assumes high-precision vehicle speed tracking and margin for decel point might not be the best solution, and override with manual driver is considered if pedestrian really run out from occlusion spot.</p> <p>TODO: consider one of the best choices</p> <ol> <li>stop in front of occlusion spot</li> <li>insert 1km/h velocity in front of occlusion spot</li> <li>slowdown this way</li> <li>etc... .</li> </ol>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_occlusion_spot_module/#maximum-slowdown-velocity","title":"Maximum Slowdown Velocity","text":"<p>The maximum slowdown velocity is calculated from the below parameters of ego current velocity and acceleration with maximum slowdown jerk and maximum slowdown acceleration in order not to slowdown too much.</p> <ul> <li>\\(j_{max}\\) slowdown jerk limit[m/s^3]</li> <li>\\(a_{max}\\) slowdown deceleration limit[m/s2]</li> <li>\\(v_{0}\\) current velocity[m/s]</li> <li>\\(a_{0}\\) current acceleration[m/s]</li> </ul> <p></p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_occlusion_spot_module/#module-parameters","title":"Module Parameters","text":"Parameter Type Description <code>pedestrian_vel</code> double [m/s] maximum velocity assumed pedestrian coming out from occlusion point. <code>pedestrian_radius</code> double [m] assumed pedestrian radius which fits in occlusion spot. Parameter Type Description <code>use_object_info</code> bool [-] whether to reflect object info to occupancy grid map or not. <code>use_partition_lanelet</code> bool [-] whether to use partition lanelet map data. Parameter /debug Type Description <code>is_show_occlusion</code> bool [-] whether to show occlusion point markers. <code>is_show_cv_window</code> bool [-] whether to show open_cv debug window. <code>is_show_processing_time</code> bool [-] whether to show processing time. Parameter /threshold Type Description <code>detection_area_length</code> double [m] the length of path to consider occlusion spot <code>stuck_vehicle_vel</code> double [m/s] velocity below this value is assumed to stop <code>lateral_distance</code> double [m] maximum lateral distance to consider hidden collision Parameter /motion Type Description <code>safety_ratio</code> double [-] safety ratio for jerk and acceleration <code>max_slow_down_jerk</code> double [m/s^3] jerk for safe brake <code>max_slow_down_accel</code> double [m/s^2] deceleration for safe brake <code>non_effective_jerk</code> double [m/s^3] weak jerk for velocity planning. <code>non_effective_acceleration</code> double [m/s^2] weak deceleration for velocity planning. <code>min_allowed_velocity</code> double [m/s] minimum velocity allowed <code>safe_margin</code> double [m] maximum error to stop with emergency braking system. Parameter /detection_area Type Description <code>min_occlusion_spot_size</code> double [m] the length of path to consider occlusion spot <code>slice_length</code> double [m] the distance of divided detection area <code>max_lateral_distance</code> double [m] buffer around the ego path used to build the detection_area area. Parameter /grid Type Description <code>free_space_max</code> double [-] maximum value of a free space cell in the occupancy grid <code>occupied_min</code> double [-] buffer around the ego path used to build the detection_area area."},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_occlusion_spot_module/#flowchart","title":"Flowchart","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_occlusion_spot_module/#rough-overview-of-the-whole-process","title":"Rough overview of the whole process","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_occlusion_spot_module/#detail-process-for-predicted-objectnot-updated","title":"Detail process for predicted object(not updated)","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_occlusion_spot_module/#detail-process-for-occupancy-grid-base","title":"Detail process for Occupancy grid base","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_roundabout_module/","title":"Roundabout Behavior Velocity Module","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_roundabout_module/#roundabout-behavior-velocity-module","title":"Roundabout Behavior Velocity Module","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_roundabout_module/#role","title":"Role","text":"<p>This module is responsible for safely managing entry into roundabouts by performing collision checks against vehicles in the attention area just before entry. Currently, it is designed to work with single-lane roundabouts due to the complexity of multi-lane scenarios.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_roundabout_module/#activation","title":"Activation","text":"<ul> <li>A module instance is launched on a lane that is an entry lanelet of a Roundabout regulatory element on the path.</li> </ul>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_roundabout_module/#requirementslimitations","title":"Requirements/Limitations","text":"<ul> <li>The HDMap needs to have the roundabout regulatory element defined with correct lanelet topology (entry/exit/inner lanes).</li> <li>WIP(perception requirements/limitations)</li> <li>WIP(sensor visibility requirements/limitations)</li> </ul>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_roundabout_module/#attention-area","title":"Attention area","text":"<p>The attention area for a roundabout is defined as the lanelets within the Roundabout regulatory element that conflict with the ego path. The attention area is used to determine which objects are relevant for collision checking when ego is about to enter the roundabout.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_roundabout_module/#stoplines","title":"Stoplines","text":"<p>The module computes the following stoplines:</p> <ul> <li>default_stopline: A stopline placed before the first attention area, with a margin defined by <code>default_stopline_margin</code>. This is used to stop the vehicle before entering the roundabout, if necessary.</li> <li>first_attention_stopline: A stopline placed at the first attention area boundary, which is used to judge whether the vehicle can safely pass through the roundabout.</li> <li>first_pass_judge_line: A line placed one braking distance before the first attention boundary. The module records whether the ego vehicle has safely passed this line for the first time. If a collision is detected after passing this line, the module categorizes the object as <code>too_late_detect</code> or <code>misjudge</code> for diagnostics.</li> </ul>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_roundabout_module/#entry-collision-checking-logic","title":"Entry Collision Checking Logic","text":"<p>The following process is performed for the targets objects to determine whether ego can enter the roundabout safely. If it is judged that ego cannot enter the roundabout with enough margin, this module inserts a stopline on the path.</p> <ol> <li>predict the time \\(t\\) when the object intersects with ego path for the first time from the predicted path time step. Only the predicted whose confidence is greater than <code>collision_detection.min_predicted_path_confidence</code> is used.</li> <li>detect collision between the predicted path and ego's predicted path in the following process<ol> <li>calculate the collision interval of [\\(t\\) - <code>collision_detection.collision_start_margin_time</code>, \\(t\\) + <code>collision_detection.collision_end_margin_time</code>]</li> <li>calculate the passing area of ego during the collision interval from the array of (time, distance) obtained by smoothed velocity profile</li> <li>check if ego passing area and object predicted path interval collides</li> </ol> </li> <li>if collision is detected, the module inserts a stopline</li> <li>if ego is over the pass_judge_line, collision checking is skipped to avoid sudden braking and/or unnecessary stop in the inside of the roundabout.</li> </ol> <p>The parameters <code>collision_detection.collision_start_margin_time</code> and <code>collision_detection.collision_end_margin_time</code> can be interpreted as follows:</p> <ul> <li>If ego was to enter the roundabout earlier than the target object, collision would be detected if the time difference between the two was less than <code>collision_detection.collision_start_margin_time</code>.</li> <li>If ego was to enter the roundabout later than the target object, collision would be detected if the time difference between the two was less than <code>collision_detection.collision_end_margin_time</code>.</li> </ul> <p>If collision is detected, the state transits to \"STOP\" immediately. On the other hand, the state does not transit to \"GO\" unless safe judgement continues for a certain period <code>collision_detection.collision_detection_hold_time</code> to prevent the chattering of decisions.</p> <p>Currently, the roundabout module uses <code>motion_velocity_smoother</code> feature to precisely calculate ego velocity profile along the roundabout lane under longitudinal/lateral constraints. If the flag <code>collision_detection.velocity_profile.use_upstream</code> is true, the target velocity profile of the original path is used. Otherwise the target velocity is set to <code>collision.velocity_profile.default_velocity</code>. In the trajectory smoothing process the target velocity at/before ego trajectory points are set to ego current velocity. The smoothed trajectory is then converted to an array of (time, distance) which indicates the arrival time to each trajectory point on the path from current ego position. You can visualize this array by adding the lane id to <code>debug.ttc</code> and running</p> <pre><code>ros2 run behavior_velocity_roundabout_module ttc.py --lane_id &lt;lane_id&gt;\n</code></pre>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_roundabout_module/#flowchart","title":"Flowchart","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; modifyPathVelocity\n    modifyPathVelocity: modifyPathVelocity\n\n    modifyPathVelocity --&gt; initializeRTCStatus\n    initializeRTCStatus: initializeRTCStatus\n\n    initializeRTCStatus --&gt; modifyPathVelocityDetail\n    modifyPathVelocityDetail: modifyPathVelocityDetail\n\n    modifyPathVelocityDetail --&gt; prepareRoundaboutData\n    prepareRoundaboutData: prepareRoundaboutData\n\n    state prepare_data_check &lt;&lt;choice&gt;&gt;\n    prepareRoundaboutData --&gt; prepare_data_check\n\n    prepare_data_check --&gt; InternalError: prepare data failed\n    prepare_data_check --&gt; updateObjectInfoManagerArea: prepare data success\n\n    updateObjectInfoManagerArea: updateObjectInfoManagerArea\n\n    updateObjectInfoManagerArea --&gt; isOverPassJudgeLinesStatus\n    isOverPassJudgeLinesStatus: isOverPassJudgeLinesStatus\n\n    isOverPassJudgeLinesStatus --&gt; calcRoundaboutPassingTime\n\n    calcRoundaboutPassingTime --&gt; updateObjectInfoManagerCollision\n\n    updateObjectInfoManagerCollision --&gt; detectCollision\n    detectCollision: detectCollision\n\n    detectCollision --&gt; decision_logic\n\n    state \"Decision Logic\" as decision_logic {\n        [*] --&gt; is_permanent_go_check\n\n        state is_permanent_go_check &lt;&lt;choice&gt;&gt;\n        is_permanent_go_check --&gt; permanent_go_logic: true\n        is_permanent_go_check --&gt; normal_logic: false\n\n        state \"Permanent Go Logic\" as permanent_go_logic {\n            [*] --&gt; has_collision_with_margin_1\n\n            state has_collision_with_margin_1 &lt;&lt;choice&gt;&gt;\n            has_collision_with_margin_1 --&gt; can_smoothly_stop_check: true\n            has_collision_with_margin_1 --&gt; has_collision: false\n\n            state can_smoothly_stop_check &lt;&lt;choice&gt;&gt;\n            can_smoothly_stop_check --&gt; Collision_stop: true\n            can_smoothly_stop_check --&gt; has_collision: false\n\n            state has_collision &lt;&lt;choice&gt;&gt;\n            has_collision --&gt; OverPassJudge: OverPassJudge_with_risk\n            has_collision --&gt; OverPassJudge: OverPassJudge_safe\n        }\n\n        state \"Normal Logic\" as normal_logic {\n            [*] --&gt; has_collision_with_margin_2\n\n            state has_collision_with_margin_2 &lt;&lt;choice&gt;&gt;\n            has_collision_with_margin_2 --&gt; Collision_stop: true\n            has_collision_with_margin_2 --&gt; Safe: false\n        }\n\n        Safe\n        Collision_stop\n        OverPassJudge\n\n    }\n    decision_logic --&gt; prepareRTCStatus\n\n    prepareRTCStatus --&gt; reactRTCApproval\n</code></pre>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_roundabout_module/#module-parameters","title":"Module Parameters","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_roundabout_module/#common","title":"common","text":"Parameter Type Description .attention_area_margin double [m] Lateral distance margin for lane membership/object-in-lane checks .attention_area_angle_threshold double [rad] Absolute heading difference threshold between object direction and lane direction .default_stopline_margin double [m] Margin to place default stopline behind the first attention boundary .path_interpolation_ds double [m] Path interpolation step for geometric checks and stopline insertion .enable_pass_judge_before_default_stopline bool [-] If true, allow pass-judge before reaching the default stopline"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_roundabout_module/#collision_detection","title":"collision_detection","text":"Parameter Type Description .collision_detection_hold_time double [s] Debounce hold time to stabilize SAFE/UNSAFE while waiting before entry .min_predicted_path_confidence double [-] Minimum confidence to use a predicted path for collision checking .collision_start_margin_time double [s] Time margin added before object entry time for ego time window .collision_end_margin_time double [s] Time margin added after object exit time for ego time window .target_type.car/bus/truck/trailer/motorcycle/bicycle/unknown bool [-] Enable/disable target object classes .velocity_profile.use_upstream bool [-] Use upstream module\u2019s velocity profile inside roundabout when possible .velocity_profile.minimum_upstream_velocity double [m/s] Lower bound when using upstream velocity profile .velocity_profile.default_velocity double [m/s] Constant reference velocity when not using upstream profile .velocity_profile.minimum_default_velocity double [m/s] Lower bound for default velocity to avoid zero .avoid_collision_by_acceleration.object_time_margin_to_collision_point double [s] Time margin for risk diagnosis to compute required ego acceleration"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_roundabout_module/#debug-enable_rtc","title":"debug / enable_rtc","text":"Parameter Type Description debug.ttc [int64] Internal TTC/diagnostic visualization selector enable_rtc.roundabout bool Enable RTC gating for this module"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_rtc_interface/","title":"Behavior Velocity RTC Interface","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_rtc_interface/#behavior-velocity-rtc-interface","title":"Behavior Velocity RTC Interface","text":"<p>This package provides a behavior velocity interface with RTC, which are used in the <code>behavior_velocity_planner</code> node and modules.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_speed_bump_module/","title":"Index","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_speed_bump_module/#speed-bump","title":"Speed Bump","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_speed_bump_module/#role","title":"Role","text":"<p>This module plans the velocity of the related part of the path in case there is speed bump regulatory element referring to it.</p> <p></p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_speed_bump_module/#activation-timing","title":"Activation Timing","text":"<p>The manager launch speed bump scene module when there is speed bump regulatory element referring to the reference path.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_speed_bump_module/#module-parameters","title":"Module Parameters","text":"Parameter Type Description <code>slow_start_margin</code> double [m] margin for ego vehicle to slow down before speed_bump <code>slow_end_margin</code> double [m] margin for ego vehicle to accelerate after speed_bump <code>print_debug_info</code> bool whether debug info will be printed or not"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_speed_bump_module/#speed-calculation","title":"Speed Calculation","text":"<ul> <li>limits for speed bump height and slow down speed to create a linear equation</li> </ul> Parameter Type Description <code>min_height</code> double [m] minimum height assumption of the speed bump <code>max_height</code> double [m] maximum height assumption of the speed bump <code>min_speed</code> double [m/s] minimum speed assumption of slow down speed <code>max_speed</code> double [m/s] maximum speed assumption of slow down speed"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_speed_bump_module/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<ul> <li>Get speed bump regulatory element on the path from lanelet2 map</li> <li>Calculate <code>slow_down_speed</code> wrt to <code>speed_bump_height</code> specified in regulatory element or   read <code>slow_down_speed</code> tag from speed bump annotation if available</li> </ul> <p>Note: If in speed bump annotation <code>slow_down_speed</code> tag is used then calculating the speed wrt the speed bump height will be ignored. In such case, specified <code>slow_down_speed</code> value in [kph] is being used.</p> <ul> <li>Get the intersection points between path and speed bump polygon</li> <li>Calculate <code>slow_start_point</code> &amp; <code>slow_end_point</code> wrt the intersection points and insert them to   path</li> <li>If <code>slow_start_point</code> or <code>slow_end_point</code> can not be inserted with given/calculated offset values   check if any path point can be virtually assigned as <code>slow_start_point</code> or <code>slow_end_point</code></li> </ul> <p></p> <ul> <li>Assign <code>slow_down_speed</code> to the path points between <code>slow_start_point</code> or <code>slow_end_point</code></li> </ul>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_speed_bump_module/#future-work","title":"Future Work","text":"<ul> <li>In an article here, a bump modeling method   is proposed. Simply it is based on fitting the bump in a circle and a radius calculation is done   with it. Although the velocity calculation is based on just the height of the bump in the recent   implementation, applying this method is intended in the future which will yield more realistic   results.</li> </ul>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_template_module/","title":"Index","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_template_module/#template","title":"Template","text":"<p>A template for behavior velocity modules based on the autoware_behavior_velocity_speed_bump_module.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_template_module/#autoware-behavior-velocity-module-template","title":"Autoware Behavior Velocity Module Template","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_template_module/#scene","title":"<code>Scene</code>","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_template_module/#templatemodule-class","title":"<code>TemplateModule</code> Class","text":"<p>The <code>TemplateModule</code> class serves as a foundation for creating a scene module within the Autoware behavior velocity planner. It defines the core methods and functionality needed for the module's behavior. You should replace the placeholder code with actual implementations tailored to your specific behavior velocity module.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_template_module/#constructor","title":"Constructor","text":"<ul> <li>The constructor for <code>TemplateModule</code> takes the essential parameters to create a module: <code>const int64_t module_id</code>, <code>const rclcpp::Logger &amp; logger</code>, and <code>const rclcpp::Clock::SharedPtr clock</code>. These parameters are supplied by the <code>TemplateModuleManager</code> when registering a new module. Other parameters can be added to the constructor, if required by your specific module implementation.</li> </ul>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_template_module/#modifypathvelocity-method","title":"<code>modifyPathVelocity</code> Method","text":"<ul> <li>This method, defined in the <code>TemplateModule</code> class, is expected to modify the velocity of the input path based on certain conditions. In the provided code, it logs an informational message once when the template module is executing.</li> <li>The specific logic for velocity modification should be implemented in this method based on the module's requirements.</li> </ul>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_template_module/#createdebugmarkerarray-method","title":"<code>createDebugMarkerArray</code> Method","text":"<ul> <li>This method, also defined in the <code>TemplateModule</code> class, is responsible for creating a visualization of debug markers and returning them as a <code>visualization_msgs::msg::MarkerArray</code>. In the provided code, it returns an empty <code>MarkerArray</code>.</li> <li>You should implement the logic to generate debug markers specific to your module's functionality.</li> </ul>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_template_module/#createvirtualwalls-method","title":"<code>createVirtualWalls</code> Method","text":"<ul> <li>The <code>createVirtualWalls</code> method creates virtual walls for the scene and returns them as <code>autoware::motion_utils::VirtualWalls</code>. In the provided code, it returns an empty <code>VirtualWalls</code> object.</li> <li>You should implement the logic to create virtual walls based on your module's requirements.</li> </ul>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_template_module/#manager","title":"<code>Manager</code>","text":"<p>The managing of your modules is defined in manager.hpp and manager.cpp. The managing is handled by two classes:</p> <ul> <li>The <code>TemplateModuleManager</code> class defines the core logic for managing and launching the behavior_velocity_template scenes (defined in behavior_velocity_template_module/src/scene.cpp/hpp). It inherits essential manager attributes from its parent class <code>SceneModuleManagerInterface</code>.</li> <li>The <code>TemplateModulePlugin</code> class provides a way to integrate the <code>TemplateModuleManager</code> into the logic of the Behavior Velocity Planner.</li> </ul>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_template_module/#templatemodulemanager-class","title":"<code>TemplateModuleManager</code> Class","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_template_module/#constructor-templatemodulemanager","title":"Constructor <code>TemplateModuleManager</code>","text":"<ul> <li>This is the constructor of the <code>TemplateModuleManager</code> class, and it takes an <code>rclcpp::Node</code> reference as a parameter.</li> <li>It initializes a member variable <code>dummy_parameter_</code> to 0.0.</li> </ul>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_template_module/#getmodulename-method","title":"<code>getModuleName()</code> Method","text":"<ul> <li>This method is an override of a virtual method from the <code>SceneModuleManagerInterface</code> class.</li> <li>It returns a pointer to a constant character string, which is the name of the module. In this case, it returns \"template\" as the module name.</li> </ul>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_template_module/#launchnewmodules-method","title":"<code>launchNewModules()</code> Method","text":"<ul> <li>This is a private method that takes an argument of type <code>autoware_internal_planning_msgs::msg::PathWithLaneId</code>.</li> <li>It is responsible for launching new modules based on the provided path information (PathWithLaneId). The implementation of this method involves initializing and configuring modules specific to your behavior velocity planner by using the <code>TemplateModule</code> class.</li> <li>In the provided source code, it initializes a <code>module_id</code> to 0 and checks if a module with the same ID is already registered. If not, it registers a new <code>TemplateModule</code> with the module ID. Note that each module managed by the <code>TemplateModuleManager</code> should have a unique ID. The template code registers a single module, so the <code>module_id</code> is set as 0 for simplicity.</li> </ul>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_template_module/#getmoduleexpiredfunction-method","title":"<code>getModuleExpiredFunction()</code> Method","text":"<ul> <li>This is a private method that takes an argument of type <code>autoware_internal_planning_msgs::msg::PathWithLaneId</code>.</li> <li>It returns a <code>std::function&lt;bool(const std::shared_ptr&lt;SceneModuleInterface&gt;&amp;)&gt;</code>. This function is used by the behavior velocity planner to determine whether a particular module has expired or not based on the given path.</li> <li>The implementation of this method is expected to return a function that can be used to check the expiration status of modules.</li> </ul> <p>Please note that the specific functionality of the methods <code>launchNewModules()</code> and <code>getModuleExpiredFunction()</code> would depend on the details of your behavior velocity modules and how they are intended to be managed within the Autoware system. You would need to implement these methods according to your module's requirements.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_template_module/#templatemoduleplugin-class","title":"<code>TemplateModulePlugin</code> Class","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_template_module/#templatemoduleplugin-class_1","title":"<code>TemplateModulePlugin</code> Class","text":"<ul> <li>This class inherits from <code>PluginWrapper&lt;TemplateModuleManager&gt;</code>. It essentially wraps your <code>TemplateModuleManager</code> class within a plugin, which can be loaded and managed dynamically.</li> </ul>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_template_module/#example-usage","title":"<code>Example Usage</code>","text":"<p>In the following example, we take each point of the path, and multiply it by 2. Essentially duplicating the speed. Note that the velocity smoother will further modify the path speed after all the behavior velocity modules are executed.</p> <pre><code>bool TemplateModule::modifyPathVelocity(\n  [[maybe_unused]] PathWithLaneId * path, [[maybe_unused]] StopReason * stop_reason)\n{\n  for (auto &amp; p : path-&gt;points) {\n    p.point.longitudinal_velocity_mps *= 2.0;\n  }\n\n  return false;\n}\n</code></pre>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_traffic_light_module/","title":"Index","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_traffic_light_module/#traffic-light","title":"Traffic Light","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_traffic_light_module/#role","title":"Role","text":"<p>Judgement whether a vehicle can go into an intersection or not by traffic light status, and planning a velocity of the stop if necessary. This module is designed for rule-based velocity decision that is easy for developers to design its behavior. It generates proper velocity for traffic light scene.</p> <p></p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_traffic_light_module/#limitations","title":"Limitations","text":"<p>This module allows developers to design STOP/GO in traffic light module using specific rules. Due to the property of rule-based planning, the algorithm is greatly depends on object detection and perception accuracy considering traffic light. Also, this module only handles STOP/Go at traffic light scene, so rushing or quick decision according to traffic condition is future work.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_traffic_light_module/#activation-timing","title":"Activation Timing","text":"<p>This module is activated when there is traffic light in ego lane.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_traffic_light_module/#algorithm","title":"Algorithm","text":"<ol> <li> <p>Obtains a traffic light mapped to the route and a stop line correspond to the traffic light from a map information.</p> <ul> <li>If a corresponding traffic light signal have never been found, it treats as a signal to pass.</li> </ul> <ul> <li>If a corresponding traffic light signal is found but timed out, it treats as a signal to stop.</li> </ul> </li> <li> <p>Uses the highest reliability one of the traffic light recognition result and if the color of that was not green or corresponding arrow signal, generates a stop point.</p> <ul> <li>If an elapsed time to receive stop signal is less than <code>stop_time_hysteresis</code>, it treats as a signal to pass. This feature is to prevent chattering.</li> </ul> </li> <li> <p>When vehicle current velocity is</p> <ul> <li>higher than <code>yellow_light_stop_velocity</code> m/s \u21d2 pass judge(using next slide formula)</li> </ul> <ul> <li>lower than <code>yellow_light_stop_velocity</code> m/s \u21d2 stop</li> </ul> </li> <li> <p>When it to be judged that vehicle can\u2019t stop before stop line, autoware chooses one of the following behaviors</p> <ul> <li>\"can pass through\" stop line during yellow lamp =&gt; pass</li> </ul> <ul> <li>\"can\u2019t pass through\" stop line during yellow lamp =&gt; emergency stop</li> </ul> </li> </ol>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_traffic_light_module/#dilemma-zone","title":"Dilemma Zone","text":"<ul> <li> <p>yellow lamp line</p> <p>It\u2019s called \u201cyellow lamp line\u201d which shows the distance traveled by the vehicle during yellow lamp.</p> </li> </ul> <ul> <li> <p>dilemma zone</p> <p>It\u2019s called \u201cdilemma zone\u201d which satisfies following conditions: - vehicle can\u2019t pass through stop line during yellow lamp.(right side of the yellow lamp line)</p> <ul> <li> <p>vehicle can\u2019t stop under deceleration and jerk limit.(left side of the pass judge curve)</p> <p>\u21d2emergency stop(relax deceleration and jerk limitation in order to observe the traffic regulation)</p> </li> </ul> </li> </ul> <ul> <li> <p>optional zone</p> <p>It\u2019s called \u201coptional zone\u201d which satisfies following conditions: - vehicle can pass through stop line during yellow lamp.(left side of the yellow lamp line)</p> <ul> <li> <p>vehicle can stop under deceleration and jerk limit.(right side of the pass judge curve)</p> <p>\u21d2 stop(autoware selects the safety choice)</p> </li> </ul> </li> </ul>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_traffic_light_module/#module-parameters","title":"Module Parameters","text":"Parameter Type Description <code>stop_margin</code> double [m] margin before stop point <code>tl_state_timeout</code> double [s] time out for detected traffic light result. <code>stop_time_hysteresis</code> double [s] time threshold to decide stop planning for chattering prevention <code>yellow_lamp_period</code> double [s] time for yellow lamp <code>yellow_light_stop_velocity</code> double [m/s] velocity threshold for always stopping at a yellow light. <code>enable_pass_judge</code> bool [-] whether to use pass judge <code>v2i.use_remaining_time</code> bool [-] whether to use V2I remaining time information for traffic light decision <code>v2i.last_time_allowed_to_pass</code> double [s] relative time against the time of turn to red - safety margin for passing through <code>v2i.velocity_threshold</code> double [m/s] velocity threshold to change decision logic for V2I prediction <code>v2i.required_time_to_departure</code> double [s] required time to departure for low speed scenarios to prevent unsafe passing"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_traffic_light_module/#flowchart","title":"Flowchart","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_traffic_light_module/#known-limits","title":"Known Limits","text":"<ul> <li>tbd.</li> </ul>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_virtual_traffic_light_module/","title":"Index","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_virtual_traffic_light_module/#virtual-traffic-light","title":"Virtual Traffic Light","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_virtual_traffic_light_module/#role","title":"Role","text":"<p>Autonomous vehicles have to cooperate with the infrastructures such as:</p> <ul> <li>Warehouse shutters</li> <li>Traffic lights with V2X support</li> <li>Communication devices at intersections</li> <li>Fleet Management Systems (FMS)</li> </ul> <p>The following items are example cases:</p> <ol> <li> <p>Traffic control by traffic lights with V2X support    </p> </li> <li> <p>Intersection coordination of multiple vehicles by FMS.    </p> </li> </ol> <p>It's possible to make each function individually, however, the use cases can be generalized with these three elements.</p> <ol> <li><code>start</code>: Start a cooperation procedure after the vehicle enters a certain zone.</li> <li><code>stop</code>: Stop at a defined stop line according to the status received from infrastructures.</li> <li><code>end</code>: Finalize the cooperation procedure after the vehicle reaches the exit zone. This should be done within the range of stable communication.</li> </ol> <p>This module sends/receives status from infrastructures and plans the velocity of the cooperation result.</p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_virtual_traffic_light_module/#system-configuration-diagram","title":"System Configuration Diagram","text":"<p>Planner and each infrastructure communicate with each other using common abstracted messages.</p> <ul> <li>Special handling for each infrastructure is not scalable. The interface is defined as an Autoware API.</li> <li>The requirements for each infrastructure are slightly different, but will be handled flexibly.</li> </ul> <p>FMS: Intersection coordination when multiple vehicles are in operation and the relevant lane is occupied</p> <ul> <li>Automatic shutter: Open the shutter when approaching/close it when leaving</li> <li>Manual shutter: Have the driver open and close the shutter.</li> <li>Remote control signal: Have the driver change the signal status to match the direction of travel.</li> <li>Warning light: Activate the warning light</li> </ul> <p>Support different communication methods for different infrastructures</p> <ul> <li>HTTP</li> <li>Bluetooth</li> <li>ZigBee</li> </ul> <p>Have different meta-information for each geographic location</p> <ul> <li>Associated lane ID</li> <li>Hardware ID</li> <li>Communication method</li> </ul> <p>FMS: Fleet Management System</p> <p></p>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_virtual_traffic_light_module/#module-parameters","title":"Module Parameters","text":"Parameter Type Description <code>max_delay_sec</code> double [s] maximum allowed delay for command <code>near_line_distance</code> double [m] threshold distance to stop line to check ego stop. <code>dead_line_margin</code> double [m] threshold distance that this module continue to insert stop line. <code>hold_stop_margin_distance</code> double [m] parameter for restart prevention (See following section) <code>check_timeout_after_stop_line</code> bool [-] check timeout to stop when linkage is disconnected"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_virtual_traffic_light_module/#restart-prevention","title":"Restart prevention","text":"<p>If it needs X meters (e.g. 0.5 meters) to stop once the vehicle starts moving due to the poor vehicle control performance, the vehicle goes over the stopping position that should be strictly observed when the vehicle starts to moving in order to approach the near stop point (e.g. 0.3 meters away).</p> <p>This module has parameter <code>hold_stop_margin_distance</code> in order to prevent from these redundant restart. If the vehicle is stopped within <code>hold_stop_margin_distance</code> meters from stop point of the module (_front_to_stop_line &lt; hold_stop_margin_distance), the module judges that the vehicle has already stopped for the module's stop point and plans to keep stopping current position even if the vehicle is stopped due to other factors.</p> <p> </p> parameters <p> </p> outside the hold_stop_margin_distance <p> </p> inside the hold_stop_margin_distance"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_virtual_traffic_light_module/#flowchart","title":"Flowchart","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_virtual_traffic_light_module/#map-format","title":"Map Format","text":"<ul> <li>To avoid sudden braking, the length between the start line and stop line of a virtual traffic light must be longer than \\(l_{\\mathrm{min}}\\) calculated as follows, assuming that \\(v_0\\) is the velocity when passing the start line and \\(a_{\\mathrm{min}}\\) is minimum acceleration defined in Autoware.</li> </ul> \\[ \\begin{align} l_{\\mathrm{min}} = -\\frac{v_0^2}{2 a_{\\mathrm{min}}} \\end{align} \\]"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_virtual_traffic_light_module/#known-limits","title":"Known Limits","text":"<ul> <li>tbd.</li> </ul>"},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_walkway_module/","title":"Index","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_walkway_module/#walkway","title":"Walkway","text":""},{"location":"planning/behavior_velocity_planner/autoware_behavior_velocity_walkway_module/#role","title":"Role","text":"<p>This module decide to stop before the ego will cross the walkway including crosswalk to enter or exit the private area.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_boundary_departure_prevention_module/","title":"Boundary Departure Prevention Module","text":""},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_boundary_departure_prevention_module/#boundary-departure-prevention-module","title":"Boundary Departure Prevention Module","text":"<p>Warning</p> <p>The Boundary Departure Prevention Module is experimental. It subscribes to the control module\u2019s predicted path and steering report, creating a circular dependency. This violates Autoware\u2019s design principle of forward-only data flow, where control depends on planning, not the other way round. As a result, this module is not officially supported and will remain unofficial for the foreseeable future.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_boundary_departure_prevention_module/#role","title":"Role","text":"<p>This module inserts slow down points and publishes error diagnostics when the ego vehicle is near or about to cross an uncrossable boundary, such as a road border.</p> <p>It also accounts for several types of erroneous behavior that could cause the vehicle to unintentionally cross these boundaries. These behaviors are classified as abnormalities.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_boundary_departure_prevention_module/#abnormalities","title":"Abnormalities","text":"<p>Abnormalities refer to erroneous behaviors at the component level, often caused by noisy or unreliable outputs. In this module, these abnormalities are embedded into the predicted footprints derived from the control module's predicted path, specifically, the MPC (Model Predictive Control) trajectory. Each point along the MPC path is converted into a footprint, and potential deviations due to abnormal conditions are evaluated.</p> <p>The module addresses the following types of abnormalities:</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_boundary_departure_prevention_module/#1-normal-no-abnormality","title":"1. Normal (No Abnormality)","text":"<p>In typical operation, the MPC trajectory may contain small deviations or noise, especially when the vehicle cannot track the planned path perfectly. These deviations are minor and not necessarily the result of a malfunction, but they are still accounted for to ensure safe boundary handling.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_boundary_departure_prevention_module/#2-localization-abnormality","title":"2. Localization Abnormality","text":"<p>Localization errors can cause the ego vehicle to misjudge its position relative to road boundaries such as curbs or road edges. This can happen due to:</p> <ul> <li>Sensor noise or drift, leading to inaccurate pose estimation.</li> <li>Map inaccuracies, where the HD map\u2019s geometry does not precisely align with the real-world boundary.</li> <li>Dynamic uncertainty at higher speeds, where even small errors are magnified due to the vehicle covering more distance in less time, reducing the margin for correction.</li> </ul> <p>These factors can result in the vehicle unintentionally approaching or crossing an uncrossable boundary, even when the planned path appears valid.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_boundary_departure_prevention_module/#how-footprint-expansion-helps-with-localization-and-map-errors","title":"How Footprint Expansion Helps with Localization and Map Errors","text":"<ul> <li>Provides a conservative buffer for localization errors: If the ego vehicle\u2019s estimated position is off due to GPS drift or sensor noise, the expanded footprint increases the likelihood of detecting potential boundary conflicts. While this may lead to conservative (and possibly false) detections, it helps ensure safety by accounting for uncertainty in the vehicle\u2019s true position.</li> <li>Compensates for map inaccuracies: Slight misalignments in Lanelet2 map geometries, such as curb or road edge misplacement, can make it difficult to rely on raw map boundaries. Expanding the footprint ensures that even with these inaccuracies, the module remains cautious and avoids unintentional boundary violations.</li> </ul> Without Abnormality Margins With Localization Abnormality Margin <p>By expanding the footprint, the module introduces a safety margin that accounts for minor localization and mapping uncertainties, especially critical at higher speeds.he expanded footprint creates a small buffer or \"safety margin,\" allowing the vehicle to operate safely despite minor abnormality.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_boundary_departure_prevention_module/#3-steering-abnormality","title":"3. Steering Abnormality","text":"<p>Unexpected steering behavior can cause the vehicle to deviate from its planned trajectory. Instead of using a simple margin, this module simulates a future trajectory based on a bicycle model with modified steering commands to predict potential deviations. This can occur due to:</p> <ul> <li>Actuator faults: such as delayed or stuck steering commands.</li> <li>Software issues: like frozen control outputs or bugs in the steering optimization logic.</li> <li>Unexpected maneuvers: for example, emergency avoidance or unintended sharp turns.</li> </ul> <p>In such cases, the actual motion of the vehicle diverges from the MPC trajectory, increasing the risk of departure.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_boundary_departure_prevention_module/#how-steering-simulation-helps-with-steering-abnormality","title":"How steering simulation helps with steering abnormality","text":"<p>The module simulates a new vehicle trajectory using a bicycle model, where the steering commands are intentionally modified to represent a worst-case scenario. This simulation includes:</p> <ul> <li>Actuator Delay: A delay is introduced to the steering commands to simulate actuator latency.</li> <li>Steering Magnification: The original steering commands from the trajectory are multiplied by a factor to simulate over- or under-steering.</li> <li>Steering Offset: A constant offset is added to the steering commands to simulate a drift.</li> <li>Rate Limiting: The rate of change of the steering angle is limited based on the vehicle's velocity to ensure the simulation is physically realistic.</li> <li>Physical Limits: The final steering angle is clamped to the vehicle's maximum physical steering angle.</li> </ul> <p>This simulated trajectory is then used to create a set of predicted footprints, which are checked for boundary departures. This allows the system to proactively detect and mitigate risks from steering abnormalities.</p> Example steering abnormality trajectories"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_boundary_departure_prevention_module/#4-longitudinal-tracking-abnormality","title":"4. Longitudinal Tracking Abnormality","text":"<p>Sometimes, the actual motion of the vehicle along the longitudinal axis does not match the MPC-predicted trajectory. For instance:</p> <p>The ego vehicle might be ahead or behind the predicted position due to mismatches in acceleration or braking behavior.</p> <p>This discrepancy becomes more problematic when the vehicle is near an uncrossable boundary, as it reduces the reliability of future footprint predictions.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_boundary_departure_prevention_module/#how-longitudinal-expansion-helps-with-tracking-abnormality","title":"How Longitudinal Expansion Helps with Tracking Abnormality","text":"<ul> <li>Accounts for ego being ahead of the predicted pose: During lane changes, avoidance maneuvers, or turns on curved roads, the ego vehicle may move faster than expected or take a slightly different path than predicted. By extending the footprint longitudinally (in the direction of motion), the module accounts for the ego vehicle possibly being ahead of the current MPC trajectory point.</li> <li>Uses speed-scaled margins: The longitudinal margin is scaled based on the current vehicle speed, with an added buffer. At higher speeds, a larger margin is used to reflect the increased risk and reduced reaction time.</li> <li>Captures mismatches during dynamic maneuvers: In situations where heading is changing quickly, like on curved roads or during lateral motion, the ego\u2019s actual position may significantly deviate from the MPC path. The extended footprint covers this discrepancy and helps detect boundary risks even if the predicted path appears safe.</li> </ul>  Without Abnormality Margins   With Longitudinal Tracking Abnormality Margin  <p>This approach helps bridge the gap between prediction and reality. By expanding the footprint in the heading direction, the module ensures safe operation even when there are longitudinal tracking mismatches due to control delay, road surface changes, or other dynamic factors.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_boundary_departure_prevention_module/#types-of-departure","title":"Types of Departure","text":"<p>The Boundary Departure Prevention Module classifies boundary risk into three types, each representing a different level of severity based on the proximity between the predicted footprint (including abnormality margins) and the road boundary.</p> Near-Boundary Approaching Departure and Critical Departure"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_boundary_departure_prevention_module/#1-near-boundary","title":"1. Near-Boundary","text":"<p>The ego vehicle is approaching a road boundary but remains within a tolerable margin. This condition may arise:</p> <ul> <li>On narrow roads or lanes with minimal space.</li> <li>When expanded abnormality margins bring the footprint closer to the edge.</li> <li>During turns or lateral maneuvers near curbs.</li> </ul>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_boundary_departure_prevention_module/#2-approaching-departure","title":"2. Approaching Departure","text":"<p>The ego vehicle is still some distance away from the boundary, but its predicted path will eventually lead to a critical departure if no action is taken. This condition is triggered only when a critical departure is detected at a future point along the path.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_boundary_departure_prevention_module/#3-critical-departure","title":"3. Critical Departure","text":"<p>A portion of the normal (non-expanded) ego footprint crosses an uncrossable boundary. This condition is treated as a safety-critical violation.</p> <p>Unlike near-boundary departure type, abnormality margins are not considered in this judgment. Including them would result in high number of false positives, especially on narrow or constrained roads. Instead, only the actual predicted footprint is used to determine a critical departure. Upon detecting a critical departure:</p> <ul> <li>The module does not insert a stop, but instead, it relies on previously triggered Approaching Departure to have already reduced the vehicle\u2019s speed.</li> <li>The module can publish a diagnostic status, which can be configured to escalate to ERROR level.</li> <li>The ERROR level diagnostic can be connected to an external MRM (Minimum Risk Maneuver) system, which is responsible for issuing a full stop command.</li> </ul>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_boundary_departure_prevention_module/#general-process","title":"General process","text":"<p>The following diagram shows the high-level processing flow of the Boundary Departure Prevention Module. It outlines the steps from checking proximity to the goal, through trajectory and abnormality analysis, to the publication of debug and diagnostic data.</p> <p></p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_boundary_departure_prevention_module/#generating-abnormalities-data","title":"Generating abnormalities data","text":"<p>The diagram below illustrates how the module processes predicted trajectory points to generate footprints with embedded abnormality margins and find their projections relative to nearby map boundaries.</p> <p></p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_boundary_departure_prevention_module/#determining-closest-projection-to-boundaries","title":"Determining Closest Projection to Boundaries","text":"<p>To assess how close the ego vehicle is to nearby uncrossable boundaries, the <code>BoundaryDepartureChecker</code> class calculates the nearest lateral projection between each predicted footprint and the map boundary segments. This is done separately for the left and right sides of the vehicle.</p> <p>Each footprint includes left and right edge segments. These segments are projected onto nearby map segments to compute the shortest lateral distance from the vehicle to the boundary.</p> <p>For each pair of ego-side and boundary segments:</p> <ol> <li>If the segments intersect, the intersection point is used as the projection result.</li> <li>If not, the algorithm checks all endpoint-to-segment projections, from the ego footprint segment to the boundary segment, and vice versa.</li> <li>Among all valid candidates, the one with the shortest lateral distance is selected.</li> </ol> <p>The projection function returns the projected point on the boundary, the corresponding point on the ego segment, and the computed distance.</p> <p>Example of the nearest projections are shown in the following images:</p> Going near the boundary Heading towards and departing from boundaries <ul> <li>Each vehicle box represents a predicted footprint along the path.</li> <li>Red arrows show the closest projection to the left boundary.</li> <li>Purple arrows show the closest projection to the right boundary.</li> </ul>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_boundary_departure_prevention_module/#getting-departure-points-and-finding-critical-departure","title":"Getting Departure Points and finding Critical Departure","text":"<p>Once the closest projections to boundaries are obtained, the module filters and classifies potential departure points to determine the appropriate response.</p> <p>The classification of a potential departure point is determined by its lateral distance from the road boundary and the type of prediction that generated it. The module processes a series of predicted points along a trajectory, from the closest to the farthest.</p> <ul> <li>Critical Departure: This is the most severe type and has the highest work priority. A point is classified as a critical departure if it indicates the vehicle has already crossed the boundary, which is when the lateral distance is less than a minimum threshold, <code>th_dist_to_boundary_m.min</code>. Once a critical departure is found, all other departure points farther along the trajectory are ignored.</li> <li>Near Boundary: If a point is not a critical departure, it is then checked to see if it is a near boundary departure. This classification applies to any point where the lateral distance is less than or equal to a maximum threshold, <code>th_dist_to_boundary_m.max</code>. The module selects the point with the minimum lateral distance from among all prediction types.</li> </ul> <p>The module calculates two key braking distances to inform the departure classification:</p> <ul> <li><code>minimum braking dist</code>: The shortest distance required for the ego vehicle to stop safely without departing the lane. This is calculated using the maximum allowed acceleration and jerk (<code>th_acc_mps2.max</code> and <code>th_jerk_mps3.max</code>).</li> <li><code>maximum braking dist</code>: The longest distance required for a comfortable deceleration before the vehicle gets too close to the boundary. This is calculated using the minimum allowed acceleration and jerk (<code>th_acc_mps2.min</code> and <code>th_jerk_mps3.min</code>).</li> </ul> <p>Following equation is used</p> \\[ d_{\\text{total}} = \\begin{cases} d_1 + d_2, &amp; v_2 \\le 0, \\quad t_2 = \\dfrac{ -a_{\\text{max}} - \\sqrt{a_0^2 - 2 j v_0} }{j} \\\\[12pt] d_1 + d_2 + d_3, &amp; v_2 &gt; 0, \\quad t_2 = \\dfrac{a_{\\text{max}} - a_0}{j} \\end{cases} \\] <p>where</p> <ul> <li>\\(d_1 = v_0 \\cdot t_1\\): distance during delay</li> <li>\\(d_2 = v_0 t_2 + \\dfrac{1}{2} a_0 t_2^2 + \\dfrac{1}{6} j t_2^3\\): distance during jerk-limited deceleration</li> <li>\\(d_3 = -\\dfrac{v_2^2}{2 a_{\\text{max}}}\\): distance under constant deceleration (if needed)</li> <li>\\(v_2 = v_0 + \\dfrac{a_{\\text{max}}^2 - a_0^2}{2j}\\): velocity after jerk ramp</li> </ul> <p>These distances are used to determine if a predicted departure is within a reachable range. Specifically, the module uses these distances to make two key decisions:</p> <ol> <li>A <code>Near Boundary</code> point is only considered for a slowdown if it is within <code>max_braking_dist</code>.</li> <li>A <code>Critical Departure</code> point is only considered for an emergency stop if it is within <code>min_braking_dist</code>.</li> <li>A <code>Critical Departure</code> point is reclassified as an <code>Approaching Departure</code> if it is found beyond <code>min_braking_dist</code>, as it is too far for an immediate emergency response.<ul> <li>Any preceding points are also reclassified to <code>Approaching Departure</code> if they are within <code>max_braking_dist</code> of the critical point.</li> </ul> </li> </ol> <p>The module also filters closely spaced departure points. This process ensures that multiple, minor issues are treated as a single event, simplifying the module's response.</p> Closest points to boundaries Departure points Grouping nearby points Remaining points after merging"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_boundary_departure_prevention_module/#get-and-updating-departure-intervals","title":"Get and updating departure intervals","text":"<p>Path chattering is a problem that occurs when a slowdown command shortens the vehicle's predicted trajectory. This shortened path might temporarily eliminate the very departure point that caused the slowdown, leading the module to believe the risk is gone and to cancel the command. The path then lengthens again, the departure is re-detected, and the cycle repeats.</p> <p>The module uses departure intervals to maintain stable slowdown commands and prevent path chattering. Instead of simply reacting to a single departure point, which could change rapidly from one moment to the next, the module defines a continuous segment of the trajectory where a slowdown is active. This approach avoids a \"fluttering\" effect where the vehicle repeatedly slows down, then accelerates, then slows down again.</p> <p></p> <p>The module merges any new departure points or existing intervals that overlap with each other. For instance, if a new point is detected that is close to an existing slowdown interval, that interval will be extended to include the new point. This ensures the module issues one cohesive command rather than multiple, conflicting ones. This merging process prevents the vehicle from starting and stopping a slowdown for every minor deviation, promoting a smoother and safer ride.</p> Before merge After merge"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_boundary_departure_prevention_module/#handling-unstable-predictions-with-time-buffers","title":"Handling Unstable Predictions with Time Buffers","text":"<p>The module employs a continuous detection mechanism to stabilize its response to boundary predictions, preventing two types of errors: unnecessary deceleration (false positives) and sudden, late deceleration (false negatives).</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_boundary_departure_prevention_module/#1-preventing-unnecessary-actions","title":"1. Preventing Unnecessary Actions","text":"<p>The <code>on_time_buffer_s</code> is a filter that prevents the module from overreacting to fleeting or unstable predictions. The module will only insert a departure into a list if a potential boundary departure is detected continuously for a specific duration.</p> <ul> <li>Near Boundary: A slowdown is initiated only after a potential departure is detected for a duration greater than or equal to <code>on_time_buffer_s.near_boundary</code>. This handles general cases of the vehicle getting close to a boundary, acting as a safeguard against false positives.</li> <li>Critical Departure: For more severe, high-risk situations, a separate check uses <code>on_time_buffer_s.critical_departure</code>. A critical departure is added to a critical departure points list only if a critical departure is consistently detected over this dedicated time period. For further usage of this list, refer to the Diagnostic Section.</li> </ul>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_boundary_departure_prevention_module/#2-preventing-prematurely-ending-actions","title":"2. Preventing Prematurely Ending Actions","text":"<p>The <code>off_time_buffer_s</code> is a filter that prevents the module from prematurely ending a response. Once an action is active, it won't be cleared until the module is confident the risk has passed.</p> <ul> <li>Near Boundary: A slowdown is only cleared when no departure points are detected for a continuous duration greater than or equal to <code>off_time_buffer_s.near_boundary</code>. This prevents a false negative, where a brief absence of a prediction causes the vehicle to accelerate again, even if it's still in a dangerous state.</li> <li>Critical Departure: The module uses <code>off_time_buffer_s.critical_departure</code> to manage a critical departure list. This list is cleared only after a critical departure has not been detected for a continuous period.</li> </ul>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_boundary_departure_prevention_module/#calculate-slow-down","title":"Calculate slow down","text":""},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_boundary_departure_prevention_module/#1-target-velocity-from-lateral-clearance","title":"1. Target Velocity from Lateral Clearance","text":"<p>The target velocity, \\(v_{\\text{target}}\\), is determined based on the vehicle's lateral distance from the boundary. This process ensures the vehicle's speed is reduced as it gets closer to the specified boundary (e.g. <code>road_border</code>).</p> <ul> <li>Concept: The module defines a minimum velocity (\\(v_{\\min}\\)) and a maximum velocity (\\(v_{\\max}\\)). It also sets a safe lateral zone between a minimum distance (\\(d_{\\text{lat,min}}\\)) and a maximum distance (\\(d_{\\text{lat,max}}\\)).</li> <li>How it Works:<ul> <li>If the vehicle is too close to the boundary (at or within \\(d_{\\text{lat,min}}\\)), the module commands the minimum velocity, \\(v_{\\min}\\).</li> <li>If the vehicle is well within its lane (at or beyond \\(d_{\\text{lat,max}}\\)), it is permitted to drive at the maximum velocity, \\(v_{\\max}\\).</li> <li>If the vehicle is between these two distances, its target velocity is scaled between \\(v_{\\min}\\) and \\(v_{\\max}\\). This is achieved by a linear interpolation.</li> </ul> </li> </ul> <p>The target velocity, \\(v_{\\text{target}}\\), is calculated using the following formula:</p> \\[ v_{\\text{target}}(d_{\\text{lat}})= \\begin{cases} v_{\\min}, &amp; d_{\\text{lat}} \\le d_{\\text{lat,min}},\\\\[8pt] v_{\\min} +\\displaystyle \\frac{d_{\\text{lat}}-d_{\\text{lat,min}}}{\\,d_{\\text{lat,max}}-d_{\\text{lat,min}}\\,} \\,(v_{\\max}-v_{\\min}), &amp; d_{\\text{lat,min}}&lt; d_{\\text{lat}} &lt; d_{\\text{lat,max}},\\\\[12pt] v_{\\max}, &amp; d_{\\text{lat}} \\ge d_{\\text{lat,max}}. \\end{cases} \\]"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_boundary_departure_prevention_module/#2-longitudinal-feasibility-and-deceleration-tiers","title":"2. Longitudinal feasibility and deceleration tiers","text":"<p>Once the \\(v_{\\text{target}}\\) is determined, the distance to the start of target interval (i.e.: longitudinal gap, ), is used to decide on the appropriate deceleration profile. The module prioritizes comfort and only uses more aggressive braking when necessary.</p> <p>The module chooses one of three deceleration tiers:</p> <ul> <li>Comfort: The module first checks if it can slow down to the target velocity using a comfortable deceleration profile. This profile is defined by gentle limits on jerk and acceleration (\\((j_{\\text{comfort}}\\le0,\\ a_{\\text{comfort}}\\le0)\\)). If the distance to the start of target interval is sufficient for this gentle braking, the comfort profile is selected.</li> <li>Feasible: If the comfort profile is not sufficient to stop in time, the module finds the least-negative (or most gentle) acceleration that will still allow the vehicle to reach the target velocity within the available gap. This is like pressing the brake pedal a bit harder, but only as much as needed. The jerk remains at the comfortable limit (\\(j_{\\text{brake}} = j_{\\text{comfort}}\\)) to avoid abrupt changes.</li> <li>Hard: If neither of the above profiles is sufficient, the module applies the maximum possible braking (the \"hard\" limits on jerk and acceleration, \\((j_{\\max}\\le0,\\ a_{\\max}\\le0)\\)). This is used in situations where a quick, forceful stop is required to avoid prevent ego from driving onto the road boundary.</li> </ul>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_boundary_departure_prevention_module/#3-commanded-speed-via-an-analytic-s-curve-with-equations","title":"3) Commanded speed via an analytic S-curve (with equations)","text":"<p>This section details how the module calculates the final slow down velocity based on the deceleration profile selected in Section 2. It uses an analytic S-curve to ensure a smooth, comfortable deceleration to the target velocity at the specified distance. An S-curve motion profile provides a smooth transition by controlling the rate of change of acceleration, or jerk.</p> <p>The values for jerk (\\(j_{\\text{brake}}\\)) and braking acceleration (\\(a_{\\text{brake}}\\)) used in the following steps are determined by the longitudinal feasibility tier selected earlier (Comfort, Feasible, or Hard).</p> <p>Note</p> <p>To prevent sudden, unintended deceleration, the module returns the largest safe velocity if the longitudinal gap is very small. This ensures that the vehicle does not brake abruptly when approaching the target point.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_boundary_departure_prevention_module/#1-deceleration-selection","title":"1. Deceleration Selection**","text":"<p>The module begins by selecting the initial active acceleration, \\(a_{\\text{act}}\\), which is the lesser (more negative) of the current acceleration and the braking acceleration value from the selected tier. The jerk, \\(j\\), is also set to the value from the selected tier.</p> \\[ a_{\\text{act}}=\\min(a_{\\text{curr}},\\ a_{\\text{brake}})\\le 0,\\qquad j=j_{\\text{brake}}\\le 0. \\]"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_boundary_departure_prevention_module/#2-jerk-ramp","title":"2. Jerk Ramp","text":"<p>The vehicle enters a jerk phase where its acceleration changes smoothly over time. The equations below describe the vehicle's acceleration, velocity, and distance during this phase.</p> \\[ \\begin{aligned} a(t)&amp;=a_{\\text{act}}+j\\,t,\\\\ v(t)&amp;=v_0+a_{\\text{act}}\\,t+\\tfrac12 j t^2,\\\\ s(t)&amp;=v_0\\,t+\\tfrac12 a_{\\text{act}}\\,t^2+\\tfrac16 j t^3,\\\\[4pt] t_j&amp;=\\frac{a_{\\text{brake}}-a_{\\text{act}}}{j}\\ (\\ge 0),\\\\ v_1&amp;=v(t_j)=v_0+\\tfrac12\\frac{a_{\\text{brake}}^{2}-a_{\\text{act}}^{2}}{j},\\\\ s_j&amp;=s(t_j). \\end{aligned} \\]"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_boundary_departure_prevention_module/#3-waypoint-inside-the-jerk-ramp","title":"3. Waypoint Inside the Jerk Ramp","text":"<p>If the longitudinal distance to the start of the target interval, \\(s_\\star\\), falls within the distance covered during the initial jerk phase (\\(s_\\star \\in [0, s_j]\\)), the module finds the required time and corresponding velocity to reach that point. The slow down velocity, \\(v_{\\text{cmd}}\\), is then set to the greater of the target velocity and the velocity calculated for that point, ensuring a safe and controlled deceleration.</p> \\[v_{\\text{cmd}} = \\max\\left(v_{\\text{target}},\\, v(t^\\star)\\right)\\]"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_boundary_departure_prevention_module/#4-waypoint-after-the-jerk-ramp","title":"4. Waypoint After the Jerk Ramp","text":"<p>If the target point is farther away (\\(s_\\star &gt; s_j\\)), the vehicle will have completed its initial jerk phase. The remaining distance, \\(s_{\\text{rem}}\\), is used to calculate the final velocity. The slow down velocity is determined based on the constant deceleration phase that follows the initial jerk.</p> \\[ \\begin{aligned} \\Delta &amp;= v_1^2 - v_{\\text{target}}^2 + 2\\,a_{\\text{brake}}\\,s_{\\text{rem}}, \\\\ \\text{if } \\Delta &lt; 0 &amp;: \\quad v_{\\text{cmd}} = v_{\\text{target}}, \\\\ \\text{else } \\quad t_a &amp;= \\frac{-v_1 + \\sqrt{\\Delta}}{a_{\\text{brake}}}, \\qquad v_{\\text{cmd}} = \\max\\left(v_{\\text{target}},\\, v_1 + a_{\\text{brake}}\\,t_a\\right) \\end{aligned} \\]"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_boundary_departure_prevention_module/#parameters","title":"Parameters","text":"Name Type Description Default Range boundary_types_to_detect array Linestring boundary tags to be treated as uncrossable. ['road_border'] N/A th_max_lateral_query_num integer Number of nearest boundaries to search from each ego side segment. 5 N/A th_point_merge_distance_m float Distance threshold to merge nearby points [m]. 1.0 N/A th_pt_shift.dist_m float Distance threshold to discard points on shifted paths [m]. 0.2 N/A th_pt_shift.angle_deg float Angle threshold to discard points on shifted paths [deg]. 5.0 N/A th_pt_shift.goal_dist_m float Minimum goal shift distance to trigger module reset [m]. 1.0 N/A th_cutoff_time_s.predicted_path float Cutoff time to consider from predicted path [s]. 3.5 N/A th_cutoff_time_s.near_boundary float Cutoff time to consider departure point as near boundary type [s]. 3.5 N/A th_cutoff_time_s.departure float Cutoff time to consider departure point as critical departure type, anything more than is considered approaching departure [s]. 2.0 N/A on_time_buffer_s.near_boundary float Continuous detection time threshold to insert departure point into departure interval. [s] 0.15 N/A on_time_buffer_s.critical_departure float Continuous detection time required to accept new critical departure point. [s] 0.15 N/A off_time_buffer_s.near_boundary float Continuous detection time threshold to reset departure interval. [s] 0.15 N/A off_time_buffer_s.critical_departure float Continuous detection time threshold to reset critical departure. [s] 0.15 N/A normal.enable boolean Enable basic footprint margin expansion without any condition. true N/A footprint_envelop.lon_m float Longitudinal margin added to footprint [m]. 0.25 \u22650.0 footprint_envelop.lat_m float Lateral margin added to footprint [m]. 0.25 \u22650.0 localization.enable boolean Enable additional margin to handle localization uncertainty. true N/A footprint_envelop.lon_m float Longitudinal margin added to footprint [m]. 0.25 \u22650.0 footprint_envelop.lat_m float Lateral margin added to footprint [m]. 0.25 \u22650.0 longitudinal.enable boolean Enable velocity-based dynamic longitudinal margin adjustment true N/A lon_tracking.scale float Scale factor for margin based on velocity. 1.0 N/A lon_tracking.extra_margin_m float Extra margin added on top of velocity-based margin [m]. 0.0 N/A steering_accelerated.enable boolean Enable footprint obtained from re-simulating the steering of the predicted trajectory false N/A steering_accelerated.steering_rate_velocities_mps array velocities [m/s] used as keys for the steering rate limits mapping. [0.0, 3.5, 9.1, 14.7, 20.3, 25.9, 31.5] N/A steering_accelerated.steering_rate_limits_rps array steering rate limits [rad/s] mapped from the velocities [3.14, 0.777, 0.115, 0.044, 0.023, 0.014, 0.01] N/A steering_accelerated.delay_s float [s] delay to apply to the re-simulated steering 0.0 N/A steering_accelerated.offset_rps float [rad/s] extra steering rate to add to the original steering profile 0.0 N/A steering_accelerated.factor float [-] factor to apply to the original steering profile 1.2 N/A steering_stuck.enable boolean Enable footprint obtained from re-simulating the steering of the predicted trajectory false N/A steering_stuck.steering_rate_velocities_mps array velocities [m/s] used as keys for the steering rate limits mapping. [0.0, 3.5, 9.1, 14.7, 20.3, 25.9, 31.5] N/A steering_stuck.steering_rate_limits_rps array steering rate limits [rad/s] mapped from the velocities [3.14, 0.777, 0.115, 0.044, 0.023, 0.014, 0.01] N/A steering_stuck.delay_s float [s] delay to apply to the re-simulated steering 0.0 N/A steering_stuck.offset_rps float [rad/s] extra steering rate to add to the original steering profile 0.0 N/A steering_stuck.factor float [-] factor to apply to the original steering profile 0.0 N/A steering_sudden_left.enable boolean Enable footprint obtained from re-simulating the steering of the predicted trajectory false N/A steering_sudden_left.steering_rate_velocities_mps array velocities [m/s] used as keys for the steering rate limits mapping. [0.0, 3.5, 9.1, 14.7, 20.3, 25.9, 31.5] N/A steering_sudden_left.steering_rate_limits_rps array steering rate limits [rad/s] mapped from the velocities [3.14, 0.777, 0.115, 0.044, 0.023, 0.014, 0.01] N/A steering_sudden_left.delay_s float [s] delay to apply to the re-simulated steering 0.0 N/A steering_sudden_left.offset_rps float [rad/s] extra steering rate to add to the original steering profile 0.2 N/A steering_sudden_left.factor float [-] factor to apply to the original steering profile 1.0 N/A steering_sudden_right.enable boolean Enable footprint obtained from re-simulating the steering of the predicted trajectory false N/A steering_sudden_right.steering_rate_velocities_mps array velocities [m/s] used as keys for the steering rate limits mapping. [0.0, 3.5, 9.1, 14.7, 20.3, 25.9, 31.5] N/A steering_sudden_right.steering_rate_limits_rps array steering rate limits [rad/s] mapped from the velocities [3.14, 0.777, 0.115, 0.044, 0.023, 0.014, 0.01] N/A steering_sudden_right.delay_s float [s] delay to apply to the re-simulated steering 0.0 N/A steering_sudden_right.offset_rps float [rad/s] extra steering rate to add to the original steering profile -0.2 N/A steering_sudden_right.factor float [-] factor to apply to the original steering profile 1.0 N/A diagnostic.near_boundary integer Diagnostic level when vehicle is near a boundary (0:OK,1:WARN,2:ERROR). 1 N/A diagnostic.approaching_departure integer Diagnostic level when vehicle is moving toward a boundary (0:OK,1:WARN,2:ERROR). 1 N/A diagnostic.critical_departure integer Diagnostic level when vehicle is likely to leave the boundary (0:OK,1:WARN,2:ERROR). 1 N/A enable.slow_down_near_boundary boolean Enable speed reduction when near boundary. false N/A enable.slow_down_before_departure boolean Enable speed reduction just before departure. false N/A enable.stop_before_departure boolean Enable speed reduction if near departure. false N/A th_vel_kmph.min float Minimum speed for the slow down logic [km/h]. 5.0 N/A th_vel_kmph.max float Maximum speed to consider for interpolating deceleration [km/h]. 30.0 N/A th_acc_mps2.min float Comfortable deceleration when slowing down [m/s\u00b2]. -1.0 N/A th_acc_mps2.max float Hardest deceleration allowed when slowing down [m/s\u00b2]. -2.5 N/A th_jerk_mps3.min float Minimum jerk value applied during slowdown interpolation [m/s\u00b3]. -1.0 N/A th_jerk_mps3.max float Maximum jerk value applied during slowdown interpolation [m/s\u00b3]. -1.5 N/A th_trigger.brake_delay_s float Time delay before deceleration starts after triggering slowdown [s]. 1.3 N/A th_trigger.dist_error_m float Allowed stopping error distance from the desired stopping point [m]. 1.0 N/A left.min float Closest lateral distance from the boundary used for velocity and jerk interpolation [m]. 0.01 N/A left.max float Furthest lateral distance from the boundary used for velocity and jerk interpolation [m]. 0.5 N/A right.min float Closest lateral distance from the boundary used for velocity and jerk interpolation [m]. 0.01 N/A right.max float Furthest lateral distance from the boundary used for velocity and jerk interpolation [m]. 0.5 N/A"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_dynamic_obstacle_stop_module/","title":"Index","text":""},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_dynamic_obstacle_stop_module/#dynamic-obstacle-stop","title":"Dynamic Obstacle Stop","text":""},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_dynamic_obstacle_stop_module/#role","title":"Role","text":"<p><code>dynamic_obstacle_stop</code> is a module that stops the ego vehicle from entering the immediate path of a dynamic object.</p> <p>The immediate path of an object is the area that the object would traverse during a given time horizon, assuming constant velocity and heading.</p> <p></p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_dynamic_obstacle_stop_module/#activation-timing","title":"Activation Timing","text":"<p>This module is activated if the launch parameter <code>launch_dynamic_obstacle_stop_module</code> is set to true in the motion planning launch file.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_dynamic_obstacle_stop_module/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The module insert a stop point where the ego trajectory collides with the immediate path of an object. The overall module flow can be summarized with the following 4 steps.</p> <ol> <li>Filter dynamic objects.</li> <li>Calculate immediate path rectangles of the dynamic objects.</li> <li>Find earliest collision where ego collides with an immediate path rectangle.</li> <li>Insert stop point before the collision.</li> </ol> <p>In addition to these 4 steps, 2 mechanisms are in place to make the stop point of this module more stable: an hysteresis and a decision duration buffer.</p> <p>The <code>hysteresis</code> parameter is used when a stop point was already being inserted in the previous iteration and it increases the range where dynamic objects are considered close enough to the ego trajectory to be used by the module.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_dynamic_obstacle_stop_module/#filter-dynamic-objects","title":"Filter dynamic objects","text":"<p>An object is considered by the module only if it meets all of the following conditions:</p> <ul> <li>it is a vehicle (pedestrians are ignored);</li> <li>it is moving at a velocity higher than defined by the <code>minimum_object_velocity</code> parameter;</li> <li>it is not too close to the current position of the ego vehicle;</li> <li>it is not unavoidable (only if parameter <code>ignore_unavoidable_collisions</code> is set to <code>true</code>);</li> <li>it is close to the ego trajectory.</li> </ul> <p>An object is considered unavoidable if it is heading towards the ego vehicle such that even if ego stops, a collision would still occur (assuming the object keeps driving in a straight line).</p> <p>For the last condition, the object is considered close enough if its lateral distance from the ego trajectory is less than the threshold parameter <code>minimum_object_distance_from_ego_trajectory</code> plus half the width of ego and of the object (including the <code>extra_object_width</code> parameter). In addition, the value of the <code>hysteresis</code> parameter is added to the minimum distance if a stop point was inserted in the previous iteration.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_dynamic_obstacle_stop_module/#calculate-immediate-path-rectangles","title":"Calculate immediate path rectangles","text":"<p>For each considered object, a rectangle is created representing its immediate path. The rectangle has the width of the object plus the <code>extra_object_width</code> parameter and its length is the current speed of the object multiplied by the <code>time_horizon</code>.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_dynamic_obstacle_stop_module/#find-earliest-collision","title":"Find earliest collision","text":"<p>We build the ego trajectory footprints as the set of ego footprint polygons projected on each trajectory point. We then calculate the intersections between these ego trajectory footprints and the previously calculated immediate path rectangles. An intersection is ignored if the object is not driving toward ego, i.e., the absolute angle between the object and the trajectory point is larger than \\(\\frac{3 \\pi}{4}\\).</p> <p>The collision point with the lowest arc length when projected on the ego trajectory will be used to calculate the final stop point.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_dynamic_obstacle_stop_module/#insert-stop-point","title":"Insert stop point","text":"<p>Before inserting a stop point, we calculate the range of trajectory arc lengths where it can be inserted. The minimum is calculated to satisfy the acceleration and jerk constraints of the vehicle. If a stop point was inserted in the previous iteration of the module, its arc length is used as the maximum. Finally, the stop point arc length is calculated to be the arc length of the previously found collision point minus the <code>stop_distance_buffer</code> and the ego vehicle longitudinal offset, clamped between the minimum and maximum values.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_dynamic_obstacle_stop_module/#duration-buffer","title":"Duration buffer","text":"<p>To prevent chatter caused by noisy perception, two duration parameters are used.</p> <ul> <li><code>add_stop_duration_buffer</code> represents the duration of consecutive collision detection with an object for the corresponding stop point to be added.</li> <li><code>remove_stop_duration_buffer</code> represents the duration of consecutive non-detection of collision with an object for the corresponding stop point to be removed.</li> </ul> <p>Timers and collision points are tracked for each dynamic object independently.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_dynamic_obstacle_stop_module/#module-parameters","title":"Module Parameters","text":"Parameter Type Description <code>extra_object_width</code> double [m] extra width around detected objects <code>minimum_object_velocity</code> double [m/s] objects with a velocity bellow this value are ignored <code>stop_distance_buffer</code> double [m] extra distance to add between the stop point and the collision point <code>time_horizon</code> double [s] time horizon used for collision checks <code>hysteresis</code> double [m] once a collision has been detected, this hysteresis is used on the collision detection <code>add_stop_duration_buffer</code> double [s] duration where a collision must be continuously detected before a stop decision is added <code>remove_stop_duration_buffer</code> double [s] duration between no collision being detected and the stop decision being remove <code>minimum_object_distance_from_ego_trajectory</code> double [m] minimum distance between the footprints of ego and an object to consider for collision <code>ignore_unavoidable_collisions</code> bool [-] if true, ignore collisions that cannot be avoided by stopping (assuming the obstacle continues going straight)"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_cruise_module/","title":"Obstacle Cruise","text":""},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_cruise_module/#obstacle-cruise","title":"Obstacle Cruise","text":""},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_cruise_module/#role","title":"Role","text":"<p>The <code>obstacle_cruise</code> module does the cruise planning against a dynamic obstacle in front of the ego.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_cruise_module/#activation","title":"Activation","text":"<p>This module is activated if the launch parameter <code>launch_obstacle_cruise_module</code> is set to true.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_cruise_module/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_cruise_module/#obstacle-filtering","title":"Obstacle Filtering","text":"<p>The obstacles meeting the following condition are determined as obstacles for cruising.</p> <ul> <li>The lateral distance from the object to the ego's trajectory is smaller than <code>obstacle_filtering.max_lat_margin</code>.</li> </ul> <ul> <li>The object type is for cruising according to <code>obstacle_filtering.object_type.*</code>.</li> <li>The object is not crossing the ego's trajectory (*1).</li> <li>If the object is inside the trajectory.<ul> <li>The object type is for inside cruising according to <code>obstacle_filtering.object_type.inside.*</code>.</li> <li>The object velocity is larger than <code>obstacle_filtering.obstacle_velocity_threshold_from_cruise</code>.</li> </ul> </li> <li>If the object is outside the trajectory.<ul> <li>The object type is for outside cruising according to <code>obstacle_filtering.object_type.outside.*</code>.</li> <li>The object velocity is larger than <code>obstacle_filtering.outside_obstacle.obstacle_velocity_threshold</code>.</li> <li>The highest confident predicted path collides with the ego's trajectory.</li> <li>Its collision's period is larger than <code>obstacle_filtering.outside_obstacle.ego_obstacle_overlap_time_threshold</code>.</li> </ul> </li> </ul>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_cruise_module/#note","title":"NOTE","text":""},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_cruise_module/#1-crossing-obstacles","title":"*1: Crossing obstacles","text":"<p>Crossing obstacle is the object whose orientation's yaw angle against the ego's trajectory is smaller than <code>obstacle_filtering.crossing_obstacle.obstacle_traj_angle_threshold</code>.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_cruise_module/#yield-for-vehicles-that-might-cut-in-into-the-egos-lane","title":"Yield for vehicles that might cut in into the ego's lane","text":"<p>It is also possible to yield (cruise) behind vehicles in neighbor lanes if said vehicles might cut in the ego vehicle's current lane.</p> <p>The obstacles meeting the following condition are determined as obstacles for yielding (cruising).</p> <ul> <li>The object type is for cruising according to <code>obstacle_filtering.object_type.*</code> and it is moving with a speed greater than <code>obstacle_filtering.yield.stopped_obstacle_velocity_threshold</code>.</li> <li>The object is not crossing the ego's trajectory (*1).</li> <li>There is another object of type <code>obstacle_filtering.object_type.side_stopped</code> stopped in front of the moving obstacle.</li> <li>The lateral distance (using the ego's trajectory as reference) between both obstacles is less than <code>obstacle_filtering.yield.max_lat_dist_between_obstacles</code></li> <li>Both obstacles, moving and stopped, are within <code>obstacle_filtering.yield.lat_distance_threshold</code> and <code>obstacle_filtering.yield.lat_distance_threshold</code> + <code>obstacle_filtering.yield.max_lat_dist_between_obstacles</code> lateral distance from the ego's trajectory respectively.</li> </ul> <p>If the above conditions are met, the ego vehicle will cruise behind the moving obstacle, yielding to it so it can cut in into the ego's lane to avoid the stopped obstacle.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_cruise_module/#cruise-planning","title":"Cruise Planning","text":"<p>The role of the cruise planning is keeping a safe distance with dynamic vehicle objects with smoothed velocity transition. This includes not only cruising a front vehicle, but also reacting a cut-in and cut-out vehicle.</p> <p>The safe distance is calculated dynamically based on the Responsibility-Sensitive Safety (RSS) by the following equation.</p> \\[ d_{rss} = v_{ego} t_{idling} + \\frac{v_{ego}^2}{2 a_{ego}} - \\frac{v_{obstacle}^2}{2 a_{obstacle}} + l_{margin}, \\] <p>assuming that \\(d_{rss}\\) is the calculated safe distance, \\(t_{idling}\\) is the idling time for the ego to detect the front vehicle's deceleration, \\(v_{ego}\\) is the ego's current velocity, \\(v_{obstacle}\\) is the front obstacle's current velocity, \\(a_{ego}\\) is the ego's acceleration, \\(a_{obstacle}\\) is the obstacle's acceleration, and \\(l_{margin}\\) is the safety margin. These values are parameterized as follows. Other common values such as ego's minimum acceleration is defined in <code>common.param.yaml</code>.</p> Parameter Type Description <code>cruise_planning.idling_time</code> double idling time for the ego to detect the front vehicle starting deceleration [s] <code>cruise_planning.min_ego_accel_for_rss</code> double ego's acceleration for RSS [m/ss] <code>cruise_planning.min_object_accel_for_rss</code> double front obstacle's acceleration for RSS [m/ss] <code>cruise_planning.safe_distance_margin</code> double safety margin for RSS [m] <p>The detailed formulation is as follows.</p> \\[ \\begin{align} d_{error} &amp; = d - d_{rss} \\\\ d_{normalized} &amp; = lpf(d_{error} / d_{obstacle}) \\\\ d_{quad, normalized} &amp; = sign(d_{normalized}) *d_{normalized}*d_{normalized} \\\\ v_{pid} &amp; = pid(d_{quad, normalized}) \\\\ v_{add} &amp; = v_{pid} &gt; 0 ? v_{pid}* w_{acc} : v_{pid} \\\\ v_{target} &amp; = max(v_{ego} + v_{add}, v_{min, cruise}) \\end{align} \\] Variable Description <code>d</code> actual distance to obstacle <code>d_{rss}</code> ideal distance to obstacle based on RSS <code>v_{min, cruise}</code> <code>min_cruise_target_vel</code> <code>w_{acc}</code> <code>output_ratio_during_accel</code> <code>lpf(val)</code> apply low-pass filter to <code>val</code> <code>pid(val)</code> apply pid to <code>val</code>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_cruise_module/#algorithm-selection-for-cruise-planner","title":"Algorithm selection for cruise planner","text":"<p>Currently, only a PID-based planner is supported. Each planner will be explained in the following.</p> Parameter Type Description <code>option.planning_algorithm</code> string cruise and stop planning algorithm, selected from \"pid_base\""},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_cruise_module/#pid-based-planner","title":"PID-based planner","text":"<p>In order to keep the safe distance, the target velocity and acceleration is calculated and sent as an external velocity limit to the velocity smoothing package (<code>velocity_smoother</code> by default). The target velocity and acceleration is respectively calculated with the PID controller according to the error between the reference safe distance and the actual distance.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_cruise_module/#optimization-based-planner","title":"Optimization-based planner","text":"<p>under construction</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_cruise_module/#debugging","title":"Debugging","text":""},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_cruise_module/#obstacle-for-cruise","title":"Obstacle for cruise","text":"<p>Orange sphere which is an obstacle for cruise is visualized by <code>obstacles_to_cruise</code> in the <code>~/debug/marker</code> topic.</p> <p>Orange wall which means a safe distance to cruise if the ego's front meets the wall is visualized in the <code>~/debug/cruise/virtual_wall</code> topic.</p> <p></p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_slow_down_module/","title":"Obstacle Slow Down","text":""},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_slow_down_module/#obstacle-slow-down","title":"Obstacle Slow Down","text":""},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_slow_down_module/#role","title":"Role","text":"<p>The <code>obstacle_slow_down</code> module does the slow down planning when there is a static/dynamic obstacle near the trajectory.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_slow_down_module/#activation","title":"Activation","text":"<p>This module is activated if the launch parameter <code>launch_obstacle_slow_down_module</code> is set to true.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_slow_down_module/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_slow_down_module/#obstacle-filtering","title":"Obstacle Filtering","text":"<p>Obstacles meeting the following condition are determined as obstacles for slowing down.</p> <ul> <li>The object type is for slowing down according to <code>obstacle_filtering.object_type.*</code>.</li> <li>The lateral distance from the object to the ego's trajectory is smaller than <code>obstacle_filtering.max_lat_margin</code> and larger than <code>obstacle_filtering.min_lat_margin</code>.<ul> <li>For the stable decision making, <code>obstacle_filtering.lat_hysteresis_margin</code> is applied for the hysteresis of the lateral margin.</li> </ul> </li> <li>The obstacle which meets the condition <code>obstacle_filtering.successive_num_to_entry_slow_down_condition</code> times in a row will be a target obstacle</li> <li>The obstacle which was previously the target obstacle but does not meet the condition <code>obstacle_filtering.successive_num_to_entry_slow_down_condition</code> times in a row will not be a target obstacle.</li> </ul>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_slow_down_module/#slow-down-planning","title":"Slow Down Planning","text":"<p>The role of the slow down planning is inserting slow down velocity in the trajectory where the trajectory points are close to the obstacles. The parameters can be customized depending on the obstacle type, making it possible to adjust the slow down behavior depending if the obstacle is a pedestrian, bicycle, car, etc. Each obstacle type has a <code>static</code> and a <code>moving</code> parameter set, so it is possible to customize the slow down response of the ego vehicle according to the obstacle type and if it is moving or not. If an obstacle is determined to be moving, the corresponding <code>moving</code> set of parameters will be used to compute the vehicle slow down, otherwise, the <code>static</code> parameters will be used. The <code>static</code> and <code>moving</code> separation is useful for customizing the ego vehicle slow down behavior to, for example, slow down more significantly when passing stopped vehicles that might cause occlusion or that might suddenly open its doors.</p> <p>An obstacle is classified as <code>static</code> if its total speed is less than the <code>moving_object_speed_threshold</code> parameter. Furthermore, a hysteresis based approach is used to avoid chattering, it uses the <code>moving_object_hysteresis_range</code> parameter range and the obstacle's previous state (<code>moving</code> or <code>static</code>) to determine if the obstacle is moving or not. In other words, if an obstacle was previously classified as <code>static</code>, it will not change its classification to <code>moving</code> unless its total speed is greater than <code>moving_object_speed_threshold</code> + <code>moving_object_hysteresis_range</code>. Likewise, an obstacle previously classified as <code>moving</code>, will only change to <code>static</code> if its speed is lower than <code>moving_object_speed_threshold</code> - <code>moving_object_hysteresis_range</code>.</p> <p>The closest point on the obstacle to the ego's trajectory is calculated. Then, the slow down velocity is calculated by linear interpolation with the distance between the point and trajectory as follows.</p> <p></p> Variable Description <code>v_{out}</code> calculated velocity for slow down <code>v_{min}</code> <code>min_ego_velocity</code> <code>v_{max}</code> <code>max_ego_velocity</code> <code>l_{min}</code> <code>min_lat_margin</code> <code>l_{max}</code> <code>max_lat_margin</code> <code>l'_{max}</code> <code>obstacle_filtering.max_lat_margin</code> <p>The parameters <code>min/max_ego_velocity</code> and <code>min/max_lat_margin</code> can be adjusted based on the obstacle type, its side relative to the ego trajectory, and whether it is moving or not. Default values must be provided at launch (<code>object_type_specified_params.default...</code>), but each parameter can be overridden for specific cases (obstacle type, left or right, moving or not). For example, the <code>min_ego_velocity</code> can be modified for moving cars on the left by setting a different value with parameter <code>object_type_specified_params.default.left.moving.min_ego_velocity</code>.</p> <p>The calculated velocity is inserted in the trajectory where the obstacle is inside the area with <code>obstacle_filtering.max_lat_margin</code>. More precisely, the velocity is inserted <code>&lt;slow down velocity&gt;</code> * <code>slow_down_planning.time_margin_on_target_velocity</code> meters behind the obstacle.</p> <p></p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_slow_down_module/#sudden-deceleration-suppression","title":"Sudden deceleration suppression","text":"<p>When the slow down point is inserted, the deceleration and jerk is supposed to be higher than <code>slow_down_planning.slow_down_min_acc</code> and <code>slow_down_planning.slow_down_min_jerk</code> respectively. If the slow down point does not follow this condition, the velocity will be increased and the slow down point does not change.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_slow_down_module/#stable-slow-down-planning","title":"Stable slow down planning","text":"<p>The following low-pass filters will be applied.</p> <ul> <li><code>slow_down_planning.lpf_gain_slow_down_vel</code><ul> <li>slow down velocity</li> </ul> </li> <li><code>slow_down_planning.lpf_gain_lateral_distance</code><ul> <li>lateral distance of obstacles to the ego's trajectory to calculate the target velocity</li> </ul> </li> <li><code>slow_down_planning.lpf_gain_dist_to_slow_down</code><ul> <li>distance to the slow down point</li> </ul> </li> </ul>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_slow_down_module/#debugging","title":"Debugging","text":""},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_slow_down_module/#obstacle-for-slow-down","title":"Obstacle for slow down","text":"<p>Yellow sphere which is an obstacle for slow_down is visualized by <code>obstacles_to_slow_down</code> in the <code>~/debug/marker</code> topic.</p> <p>Yellow wall which means a safe distance to slow_down if the ego's front meets the wall is visualized in the <code>~/debug/slow_down/virtual_wall</code> topic.</p> <p></p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_velocity_limiter_module/","title":"Obstacle Velocity Limiter","text":""},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_velocity_limiter_module/#obstacle-velocity-limiter","title":"Obstacle Velocity Limiter","text":""},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_velocity_limiter_module/#purpose","title":"Purpose","text":"<p>This node limits the velocity when driving in the direction of an obstacle. For example, it allows to reduce the velocity when driving close to a guard rail in a curve.</p> Without this node With this node"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_velocity_limiter_module/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>Using a parameter <code>min_ttc</code> (minimum time to collision), the node set velocity limits such that no collision with an obstacle would occur, even without new control inputs for a duration of <code>min_ttc</code>.</p> <p>To achieve this, the motion of the ego vehicle is simulated forward in time at each point of the trajectory to create a corresponding footprint. If the footprint collides with some obstacle, the velocity at the trajectory point is reduced such that the new simulated footprint do not have any collision.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_velocity_limiter_module/#simulated-motion-footprint-and-collision-distance","title":"Simulated Motion, Footprint, and Collision Distance","text":"<p>The motion of the ego vehicle is simulated at each trajectory point using the <code>heading</code>, <code>velocity</code>, and <code>steering</code> defined at the point. Footprints are then constructed from these simulations and checked for collision. If a collision is found, the distance from the trajectory point is used to calculate the adjusted velocity that would produce a collision-free footprint. Parameter <code>simulation.distance_method</code> allow to switch between an exact distance calculation and a less expensive approximation using a simple euclidean distance.</p> <p>Two models can be selected with parameter <code>simulation.model</code> for simulating the motion of the vehicle: a simple particle model and a more complicated bicycle model.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_velocity_limiter_module/#particle-model","title":"Particle Model","text":"<p>The particle model uses the constant heading and velocity of the vehicle at a trajectory point to simulate the future motion. The simulated forward motion corresponds to a straight line and the footprint to a rectangle.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_velocity_limiter_module/#footprint","title":"Footprint","text":"<p>The rectangle footprint is built from 2 lines parallel to the simulated forward motion and at a distance of half the vehicle width.</p> <p></p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_velocity_limiter_module/#distance","title":"Distance","text":"<p>When a collision point is found within the footprint, the distance is calculated as described in the following figure.</p> <p></p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_velocity_limiter_module/#bicycle-model","title":"Bicycle Model","text":"<p>The bicycle model uses the constant heading, velocity, and steering of the vehicle at a trajectory point to simulate the future motion. The simulated forward motion corresponds to an arc around the circle of curvature associated with the steering. Uncertainty in the steering can be introduced with the <code>simulation.steering_offset</code> parameter which will generate a range of motion from a left-most to a right-most steering. This results in 3 curved lines starting from the same trajectory point. A parameter <code>simulation.nb_points</code> is used to adjust the precision of these lines, with a minimum of <code>2</code> resulting in straight lines and higher values increasing the precision of the curves.</p> <p>By default, the steering values contained in the trajectory message are used. Parameter <code>trajectory_preprocessing.calculate_steering_angles</code> allows to recalculate these values when set to <code>true</code>.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_velocity_limiter_module/#footprint_1","title":"Footprint","text":"<p>The footprint of the bicycle model is created from lines parallel to the left and right simulated motion at a distance of half the vehicle width. In addition, the two points on the left and right of the end point of the central simulated motion are used to complete the polygon.</p> <p></p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_velocity_limiter_module/#distance_1","title":"Distance","text":"<p>The distance to a collision point is calculated by finding the curvature circle passing through the trajectory point and the collision point.</p> <p></p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_velocity_limiter_module/#obstacle-detection","title":"Obstacle Detection","text":"<p>Obstacles are represented as points or linestrings (i.e., sequence of points) around the obstacles and are constructed from an occupancy grid, a pointcloud, or the lanelet map. The lanelet map is always checked for obstacles but the other source is switched using parameter <code>obstacles.dynamic_source</code>.</p> <p>To efficiently find obstacles intersecting with a footprint, they are stored in a R-tree. Two trees are used, one for the obstacle points, and one for the obstacle linestrings (which are decomposed into segments to simplify the R-tree).</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_velocity_limiter_module/#obstacle-masks","title":"Obstacle masks","text":""},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_velocity_limiter_module/#dynamic-obstacles","title":"Dynamic obstacles","text":"<p>Moving obstacles such as other cars should not be considered by this module. These obstacles are detected by the perception modules and represented as polygons. Obstacles inside these polygons are ignored.</p> <p>Only dynamic obstacles with a velocity above parameter <code>obstacles.dynamic_obstacles_min_vel</code> are removed.</p> <p>To deal with delays and precision errors, the polygons can be enlarged with parameter <code>obstacles.dynamic_obstacles_buffer</code>.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_velocity_limiter_module/#obstacles-outside-of-the-safety-envelope","title":"Obstacles outside of the safety envelope","text":"<p>Obstacles that are not inside any forward simulated footprint are ignored if parameter <code>obstacles.filter_envelope</code> is set to true. The safety envelope polygon is built from all the footprints and used as a positive mask on the occupancy grid or pointcloud.</p> <p>This option can reduce the total number of obstacles which reduces the cost of collision detection. However, the cost of masking the envelope is usually too high to be interesting.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_velocity_limiter_module/#obstacles-on-the-ego-path","title":"Obstacles on the ego path","text":"<p>If parameter <code>obstacles.ignore_obstacles_on_path</code> is set to <code>true</code>, a polygon mask is built from the trajectory and the vehicle dimension. Any obstacle in this polygon is ignored.</p> <p>The size of the polygon can be increased using parameter <code>obstacles.ignore_extra_distance</code> which is added to the vehicle lateral offset.</p> <p>This option is a bit expensive and should only be used in case of noisy dynamic obstacles where obstacles are wrongly detected on the ego path, causing unwanted velocity limits.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_velocity_limiter_module/#lanelet-map","title":"Lanelet Map","text":"<p>Information about static obstacles can be stored in the Lanelet map using the value of the <code>type</code> tag of linestrings. If any linestring has a <code>type</code> with one of the value from parameter <code>obstacles.static_map_tags</code>, then it will be used as an obstacle.</p> <p>Obstacles from the lanelet map are not impacted by the masks.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_velocity_limiter_module/#occupancy-grid","title":"Occupancy Grid","text":"<p>Masking is performed by iterating through the cells inside each polygon mask using the <code>autoware::grid_map_utils::PolygonIterator</code> function. A threshold is then applied to only keep cells with an occupancy value above parameter <code>obstacles.occupancy_grid_threshold</code>. Finally, the image is converted to an image and obstacle linestrings are extracted using the opencv function <code>findContour</code>.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_velocity_limiter_module/#pointcloud","title":"Pointcloud","text":"<p>Masking is performed using the <code>pcl::CropHull</code> function. Points from the pointcloud are then directly used as obstacles.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_velocity_limiter_module/#velocity-adjustment","title":"Velocity Adjustment","text":"<p>If a collision is found, the velocity at the trajectory point is adjusted such that the resulting footprint would no longer collide with an obstacle: \\(velocity = \\frac{dist\\_to\\_collision}{min\\_ttc}\\)</p> <p>To prevent sudden deceleration of the ego vehicle, the parameter <code>max_deceleration</code> limits the deceleration relative to the current ego velocity. For a trajectory point occurring at a duration <code>t</code> in the future (calculated from the original velocity profile), the adjusted velocity cannot be set lower than \\(v_{current} - t * max\\_deceleration\\).</p> <p>Furthermore, a parameter <code>min_adjusted_velocity</code> provides a lower bound on the modified velocity.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_velocity_limiter_module/#trajectory-preprocessing","title":"Trajectory preprocessing","text":"<p>The node only modifies part of the input trajectory, starting from the current ego position. Parameter <code>trajectory_preprocessing.start_distance</code> is used to adjust how far ahead of the ego position the velocities will start being modified. Parameters <code>trajectory_preprocessing.max_length</code> and <code>trajectory_preprocessing.max_duration</code> are used to control how much of the trajectory will see its velocity adjusted.</p> <p>To reduce computation cost at the cost of precision, the trajectory can be downsampled using parameter <code>trajectory_preprocessing.downsample_factor</code>. For example a value of <code>1</code> means all trajectory points will be evaluated while a value of <code>10</code> means only 1/10th of the points will be evaluated.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_velocity_limiter_module/#parameters","title":"Parameters","text":"Name Type Description <code>min_ttc</code> float [s] required minimum time with no collision at each point of the trajectory assuming constant heading and velocity. <code>distance_buffer</code> float [m] required distance buffer with the obstacles. <code>min_adjusted_velocity</code> float [m/s] minimum adjusted velocity this node can set. <code>max_deceleration</code> float [m/s\u00b2] maximum deceleration an adjusted velocity can cause. <code>trajectory_preprocessing.start_distance</code> float [m] controls from which part of the trajectory (relative to the current ego pose) the velocity is adjusted. <code>trajectory_preprocessing.max_length</code> float [m] controls the maximum length (starting from the <code>start_distance</code>) where the velocity is adjusted. <code>trajectory_preprocessing.max_distance</code> float [s] controls the maximum duration (measured from the <code>start_distance</code>) where the velocity is adjusted. <code>trajectory_preprocessing.downsample_factor</code> int trajectory downsampling factor to allow tradeoff between precision and performance. <code>trajectory_preprocessing.calculate_steering_angle</code> bool if true, the steering angles of the trajectory message are not used but are recalculated. <code>simulation.model</code> string model to use for forward simulation. Either \"particle\" or \"bicycle\". <code>simulation.distance_method</code> string method to use for calculating distance to collision. Either \"exact\" or \"approximation\". <code>simulation.steering_offset</code> float offset around the steering used by the bicycle model. <code>simulation.nb_points</code> int number of points used to simulate motion with the bicycle model. <code>obstacles.dynamic_source</code> string source of dynamic obstacle used for collision checking. Can be \"occupancy_grid\", \"point_cloud\", or \"static_only\" (no dynamic obstacle). <code>obstacles.occupancy_grid_threshold</code> int value in the occupancy grid above which a cell is considered an obstacle. <code>obstacles.dynamic_obstacles_buffer</code> float buffer around dynamic obstacles used when masking an obstacle in order to prevent noise. <code>obstacles.dynamic_obstacles_min_vel</code> float velocity above which to mask a dynamic obstacle. <code>obstacles.static_map_tags</code> string list linestring of the lanelet map with this tags are used as obstacles. <code>obstacles.filter_envelope</code> bool wether to use the safety envelope to filter the dynamic obstacles source."},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_velocity_limiter_module/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>The velocity profile produced by this node is not meant to be a realistic velocity profile and can contain sudden jumps of velocity with no regard for acceleration and jerk. This velocity profile is meant to be used as an upper bound on the actual velocity of the vehicle.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_velocity_limiter_module/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":"<p>The critical case for this node is when an obstacle is falsely detected very close to the trajectory such that the corresponding velocity suddenly becomes very low. This can cause a sudden brake and two mechanisms can be used to mitigate these errors.</p> <p>Parameter <code>min_adjusted_velocity</code> allow to set a minimum to the adjusted velocity, preventing the node to slow down the vehicle too much. Parameter <code>max_deceleration</code> allow to set a maximum deceleration (relative to the current ego velocity) that the adjusted velocity would incur.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_velocity_limiter_module/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_velocity_limiter_module/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_obstacle_velocity_limiter_module/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_out_of_lane_module/","title":"Out of Lane","text":""},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_out_of_lane_module/#out-of-lane","title":"Out of Lane","text":""},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_out_of_lane_module/#role","title":"Role","text":"<p>There are cases where the ego vehicle footprint goes out of the driving lane, for example when taking a narrow turn with a large vehicle. The <code>out_of_lane</code> module adds deceleration and stop points to the ego trajectory in order to prevent collisions from occurring in these out of lane cases.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_out_of_lane_module/#activation","title":"Activation","text":"<p>This module is activated if the launch parameter <code>launch_out_of_lane_module</code> is set to true.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_out_of_lane_module/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>This module calculates if out of lane collisions occur and insert stop point before the collisions if necessary.</p> <p>The algorithm assumes the input ego trajectory contains accurate <code>time_from_start</code> values in order to calculate accurate time to collisions with the predicted objects.</p> <p>Next we explain the inner-workings of the module in more details.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_out_of_lane_module/#1-ego-trajectory-footprints","title":"1. Ego trajectory footprints","text":"<p>In this first step, the ego footprint is projected at each trajectory point and its size is modified based on the <code>ego.extra_..._offset</code> parameters.</p> <p>The length of the trajectory used for generating the footprints is limited by the <code>max_arc_length</code> parameter.</p> <p></p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_out_of_lane_module/#2-other-lanelets","title":"2. Other lanelets","text":"<p>In the second step, we calculate the lanelets where collisions should be avoided. We consider all lanelets around the ego vehicle that are not crossed by the trajectory linestring (sequence of trajectory points) or their preceding lanelets.</p> <p></p> <p>In the debug visualization, these other lanelets are shown as blue polygons.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_out_of_lane_module/#3-out-of-lane-areas","title":"3. Out of lane areas","text":"<p>Next, for each trajectory point, we create the corresponding out of lane areas by intersection the other lanelets (from step 2) with the trajectory point footprint (from step 1). Each area is associated with the lanelets overlapped by the area and with the corresponding ego trajectory point.</p> <p></p> <p>In the debug visualization, the out of lane area polygon is connected to the corresponding trajectory point by a line.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_out_of_lane_module/#4-predicted-objects-filtering","title":"4. Predicted objects filtering","text":"<p>We filter objects and their predicted paths with the following conditions:</p> <ul> <li>ignore objects with a speed bellow the <code>minimum_velocity</code> parameter;</li> <li>ignore objects coming from behind the ego vehicle if parameter <code>ignore_behind_ego</code> is set to true;</li> <li>ignore predicted paths whose confidence value is bellow the <code>predicted_path_min_confidence</code> parameter;</li> <li>cut the points of predicted paths going beyond the stop line of a red traffic light if parameter <code>cut_predicted_paths_beyond_red_lights</code> is set to <code>true</code>.</li> </ul> <code>cut_predicted_paths_beyond_red_lights = false</code> <code>cut_predicted_paths_beyond_red_lights = true</code> <p>In the debug visualization, the filtered predicted paths are shown in green and the stop lines of red traffic lights are shown in red.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_out_of_lane_module/#5-time-to-collisions","title":"5. Time to collisions","text":"<p>For each out of lane area, we calculate the times when a dynamic object will overlap the area based on its filtered predicted paths. To make it more likely to detect collision in the other lanes, the width of the dynamic object can be increased using the parameter <code>objects.extra_width</code>.</p> <p>In the case where parameter <code>mode</code> is set to <code>threshold</code> and the calculated time is less than <code>threshold.time_threshold</code> parameter, then we decide to avoid the out of lane area.</p> <p>In the case where parameter <code>mode</code> is set to <code>ttc</code>, we calculate the time to collision by comparing the predicted time of the object with the <code>time_from_start</code> field contained in the trajectory point. If the time to collision is bellow the <code>ttc.threshold</code> parameter value, we decide to avoid the out of lane area.</p> <p></p> <p>In the debug visualization, the ttc (in seconds) is displayed on top of its corresponding trajectory point. The color of the text is red if the collision should be avoided and green otherwise.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_out_of_lane_module/#validation-using-the-lanelet-map","title":"Validation using the lanelet map","text":"<p>When parameter <code>object.validate_predicted_paths_on_lanelets</code> is set to <code>true</code>, an additional check is performed before considering a collision between the out of lane area and a predicted path.</p> <p>First, the possible sequences of lanelets followed by the predicted path are calculated such that:</p> <ul> <li>each lanelet in the sequence follows the previous one (according to the lanelet map);</li> <li>the sequence fully contains the predicted path.</li> </ul> <p>Then, for a collision to be considered, one of the \"other lanelets\" overlapped by the out of lane area must be inside the calculated sequence.</p> <p>This feature allows the module to ignore collisions caused by nonsensical predicted paths, or by predicted paths that \"merge\" into the ego lane and that should be handled by other modules. Indeed, lanelets that merge into the ego lane (i.e., precede a trajectory lanelet) are not included in the \"other lanelets\".</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_out_of_lane_module/#6-calculate-the-stop-or-slowdown-point","title":"6. Calculate the stop or slowdown point","text":"<p>First, the minimum stopping distance of the ego vehicle is calculated based on the jerk and deceleration constraints set by the velocity smoother parameters.</p> <p>We then search for the furthest pose along the trajectory where the ego footprint stays inside of the ego lane (calculate in step 2) and constraint the search to be between the minimum stopping distance and the 1st trajectory point with a collision to avoid (as determined in the previous step). The search is done by moving backward along the trajectory with a distance step set by the <code>action.precision</code> parameter.</p> <p>We first do this search for a footprint expanded with the <code>ego.extra_..._offset</code>, <code>action.longitudinal_distance_buffer</code> and <code>action.lateral_distance_buffer</code> parameters. If no valid pose is found, we search again while only considering the extra offsets but without considering the distance buffers. If still no valid pose is found, we use the base ego footprint without any offset. In case no pose is found, we fallback to using the pose before the detected collision without caring if it is out of lane or not.</p> <p>Whether it is decided to slow down or stop is determined by the distance between the ego vehicle and the trajectory point to avoid. If this distance is bellow the <code>actions.slowdown.threshold</code>, a velocity of <code>actions.slowdown.velocity</code> will be used. If the distance is bellow the <code>actions.stop.threshold</code>, a velocity of <code>0</code>m/s will be used.</p> <p>In addition, if parameter <code>action.use_map_stop_lines</code> is set to <code>true</code>, then the stop point may be moved to the earliest stop line preceding the stop point where ego can comfortably stop. Stop lines are defined in the vector map and must be attached to one of the lanelet followed by the ego trajectory.</p> <p></p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_out_of_lane_module/#about-stability-of-the-stopslowdown-pose","title":"About stability of the stop/slowdown pose","text":"<p>As the input trajectory can change significantly between iterations, it is expected that the decisions of this module will also change. To make the decision more stable, a stop or slowdown pose is used for a minimum duration set by the <code>action.min_duration</code> parameter. If during that time a new pose closer to the ego vehicle is generated, then it replaces the previous one. Otherwise, the stop or slowdown pose will only be discarded after no out of lane collision is detection for the set duration.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_out_of_lane_module/#module-parameters","title":"Module Parameters","text":"Parameter Type Description <code>mode</code> string [-] mode used to consider a dynamic object. Candidates: threshold, intervals, ttc <code>skip_if_already_overlapping</code> bool [-] if true, do not run this module when ego already overlaps another lane <code>max_arc_length</code> double [m] maximum trajectory arc length that is checked for out_of_lane collisions Parameter /threshold Type Description <code>time_threshold</code> double [s] consider objects that will reach an overlap within this time Parameter /ttc Type Description <code>threshold</code> double [s] consider objects with an estimated time to collision bellow this value while ego is on the overlap Parameter /objects Type Description <code>minimum_velocity</code> double [m/s] ignore objects with a velocity lower than this value <code>predicted_path_min_confidence</code> double [-] minimum confidence required for a predicted path to be considered <code>cut_predicted_paths_beyond_red_lights</code> bool [-] if true, predicted paths are cut beyond the stop line of red traffic lights <code>ignore_behind_ego</code> bool [-] if true, objects behind the ego vehicle are ignored <code>validate_predicted_paths_on_lanelets</code> bool [-] if true, an out of lane collision is only considered if the predicted path fully follows a sequence of lanelets that include the out of lane lanelet Parameter /action Type Description <code>use_map_stop_lines</code> bool [-] if true, try to stop at stop lines defined in the vector map <code>precision</code> double [m] precision when inserting a stop pose in the trajectory <code>longitudinal_distance_buffer</code> double [m] safety distance buffer to keep in front of the ego vehicle <code>lateral_distance_buffer</code> double [m] safety distance buffer to keep on the side of the ego vehicle <code>min_duration</code> double [s] minimum duration needed before a decision can be canceled <code>slowdown.distance_threshold</code> double [m] insert a slow down when closer than this distance from an overlap <code>slowdown.velocity</code> double [m] slow down velocity <code>stop.distance_threshold</code> double [m] insert a stop when closer than this distance from an overlap Parameter /ego Type Description <code>extra_front_offset</code> double [m] extra front distance to add to the ego footprint <code>extra_rear_offset</code> double [m] extra rear distance to add to the ego footprint <code>extra_left_offset</code> double [m] extra left distance to add to the ego footprint <code>extra_right_offset</code> double [m] extra right distance to add to the ego footprint"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_road_user_stop_module/","title":"Road User Stop","text":""},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_road_user_stop_module/#road-user-stop","title":"Road User Stop","text":""},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_road_user_stop_module/#role","title":"Role","text":"<p>The <code>road_user_stop</code> module stops the ego vehicle when pedestrians, cyclists, or other road users are detected on or near the road within the ego's planned trajectory. This module ensures safe interaction with vulnerable road users by maintaining appropriate stopping distances and making decisions about when to stop versus when to continue driving.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_road_user_stop_module/#activation","title":"Activation","text":"<p>This module is activated if the launch parameter <code>launch_road_user_stop_module</code> is set to true.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_road_user_stop_module/#parameters","title":"Parameters","text":""},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_road_user_stop_module/#option-parameters","title":"Option Parameters","text":"Name Unit Type Description Default value suppress_sudden_stop [-] bool Enable to suppress sudden stop true"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_road_user_stop_module/#stop-planning-parameters","title":"Stop Planning Parameters","text":""},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_road_user_stop_module/#longitudinal-margin-parameters","title":"Longitudinal Margin Parameters","text":"Name Unit Type Description Default value default_margin [m] double Default longitudinal margin to obstacle 5.0 terminal_margin [m] double Stop margin at the goal position 3.0 minimum_margin [m] double Minimum stop margin for behavior decision 3.0"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_road_user_stop_module/#opposing-traffic-parameters","title":"Opposing Traffic Parameters","text":"Name Unit Type Description Default value stop_margin [m] double Ideal stop-margin from moving opposing obstacle when ego stops 10.0 max_negative_velocity [m/s] double Maximum velocity of opposing traffic to consider stop planning -0.1 min_velocity_for_stop_planning [m/s] double Minimum velocity of ego to consider stop planning 2.77 effective_deceleration [m/s^2] double Higher value brings final stop-margin closer to ideal value 4.0"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_road_user_stop_module/#general-stop-planning-parameters","title":"General Stop Planning Parameters","text":"Name Unit Type Description Default value hold_stop_velocity_threshold [m/s] double The maximum ego velocity to hold stopping 0.01 hold_stop_distance_threshold [m] double The ego keeps stopping if distance to stop changes within threshold 0.3 limit_min_acc [m/s^2] double Overwrite the deceleration limit (usually from common_param.yaml) -2.5 sudden_object_acc_threshold [m/s^2] double If stop achievable by smaller deceleration, not \"sudden stop\" -1.0 sudden_object_dist_threshold [m] double If stop distance is longer than this, not considered \"sudden stop\" 1000.0 abandon_to_stop [-] bool Give up stopping when cannot avoid run over within decel limit false"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_road_user_stop_module/#stop-on-curve-parameters","title":"Stop on Curve Parameters","text":"Name Unit Type Description Default value enable_approaching [-] bool Enable approaching behavior on curved paths false additional_stop_margin [m] double Additional stop margin for obstacles on curves 3.0 min_stop_margin [m] double Minimum stop margin on curves 6.0"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_road_user_stop_module/#obstacle-filtering-parameters","title":"Obstacle Filtering Parameters","text":""},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_road_user_stop_module/#object-type-parameters","title":"Object Type Parameters","text":"Name Unit Type Description Default value pedestrian [-] bool Enable detection of pedestrians true bicycle [-] bool Enable detection of bicycles true motorcycle [-] bool Enable detection of motorcycles false unknown [-] bool Enable detection of unknown objects false"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_road_user_stop_module/#general-filtering-parameters","title":"General Filtering Parameters","text":"Name Unit Type Description Default value trajectory_lateral_margin [m] double Lateral margin from ego trajectory to detect objects 1.0"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_road_user_stop_module/#intersection-parameters","title":"Intersection Parameters","text":"Name Unit Type Description Default value exclude [-] bool If true, exclude objects inside intersection lanelets from detection false"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_road_user_stop_module/#crosswalk-parameters","title":"Crosswalk Parameters","text":"Name Unit Type Description Default value exclude [-] bool Exclude objects near crosswalks true margin [m] double Margin distance from crosswalk 1.0"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_road_user_stop_module/#opposing-traffic-detection-parameters","title":"Opposing Traffic Detection Parameters","text":"Name Unit Type Description Default value enable [-] bool Enable wrong-way object detection true angle_threshold [deg] double Angle threshold for wrong-way detection 150.0 min_speed_threshold [m/s] double Minimum speed for wrong-way detection 0.5"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_road_user_stop_module/#temporal-filtering-parameters","title":"Temporal Filtering Parameters","text":"Name Unit Type Description Default value min_detection_duration [s] double Minimum duration for object detection 0.1 lost_object_retention_duration [s] double Duration to keep tracking objects after they are lost or exit the detection area 2.0 polygon_expansion_length [m] double Distance to expand object polygon outward from centroid when object was previously inside detection area 0.5"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_road_user_stop_module/#stopped-object-tracking-parameters","title":"Stopped Object Tracking Parameters","text":"Name Unit Type Description Default value stopped_velocity_threshold [m/s] double Velocity threshold to consider object as stopped 0.5 stopped_position_tolerance [m] double Distance threshold for stopped position movement 0.3 ego_reached_wall_distance_threshold [m] double Distance threshold to consider ego reached virtual wall 0.5 stopped_duration_after_ego_arrival [s] double Duration threshold for stopped object after ego reaches virtual wall 3.0"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_road_user_stop_module/#limitations","title":"Limitations","text":"<ul> <li>If object tracking is lost for an extended period or object classification is misidentified, the module may not function properly and may fail to stop as expected</li> </ul>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_run_out_module/","title":"Run Out","text":""},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_run_out_module/#run-out","title":"Run Out","text":""},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_run_out_module/#role","title":"Role","text":"<p>The <code>run_out</code> module adds deceleration and stop points to the ego trajectory in order to prevent collisions with objects that are moving towards the ego vehicle path.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_run_out_module/#activation","title":"Activation","text":"<p>This module is activated if the launch parameter <code>launch_run_out_module</code> is set to true.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_run_out_module/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>This module calculates the times when the ego vehicle and the objects are predicted to overlap each other's trajectories. These times are then used to decide whether to stop before the overlap or not.</p> <p>Next we explain the inner-workings of the module in more details.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_run_out_module/#1-ego-trajectory-footprint","title":"1. Ego trajectory footprint","text":"<p>In this first step, the trajectory footprint is constructed from the corner points of the vehicle. 4 linestrings are constructed from the 4 corners (front left, front right, rear left, rear right) projected at each trajectory point.</p> <p>At this step, the footprint size can be adjusted using the <code>ego.lateral_margin</code> and <code>ego.longitudinal_margin</code> parameters.</p> <p>The following figures show the 4 corner linestrings calculated for the red trajectory.</p> front left front right rear left rear right <p>These can be visualized on the debug markers with the <code>ego_footprint_(front|rear)_(left|right)</code> namespaces.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_run_out_module/#2-extracting-map-filtering-data","title":"2. Extracting map filtering data","text":"<p>In the second step, we extract geometric information from the vector map that will be used to filter dynamic objects. For each object classification label, we prepare the following sets of geometries based on the parameters defined for that label (<code>objects.{CLASSIFICATION_LABEL}</code>):</p> <ul> <li>polygons to ignore objects (<code>ignore.polygon_types</code> and <code>ignore.lanelet_subtypes</code>);<ul> <li>polygons for the ego trajectory footprint are also added if <code>ignore.if_on_ego_trajectory</code> is set to <code>true</code>.</li> </ul> </li> <li>polygons to ignore collisions (<code>ignore_collisions.polygon_types</code> and <code>ignore_collisions.lanelet_subtypes</code>);</li> <li>segments to cut predicted paths (<code>cut_predicted_paths.polygon_types</code>, <code>cut_predicted_paths.linestring_types</code>, and <code>cut_predicted_paths.lanelet_subtypes</code>).<ul> <li>the rear segment of the current ego footprint is also added if <code>cut_predicted_paths.if_crossing_ego_from_behind</code> is set to <code>true</code>.</li> </ul> </li> <li>segments to strictly cut predicted paths (<code>cut_predicted_paths.strict_polygon_types</code>, <code>cut_predicted_paths.strict_linestring_types</code>, and <code>cut_predicted_paths.strict_lanelet_subtypes</code>).<ul> <li>strict cutting means that the cut is always applied, regardless of any preserved distance or duration.</li> </ul> </li> </ul> <p>Polygon subtypes can also be considered if the parameter value is in the format <code>\"type.subtype\"</code>.</p> <p>The following figure shows an example where the polygons to ignore objects are shown in blue, to ignore collisions in green, and to cut predicted paths in red.</p> <p></p> <p>These geometries can be visualized on the debug markers with the <code>filtering_data_(ignore_objects|ignore_collisions|cut_predicted_paths)</code> namespaces. The classification label corresponding to the published debug markers can be selected with parameter <code>debug.object_label</code>.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_run_out_module/#3-dynamic-objects-filtering","title":"3. Dynamic objects filtering","text":"<p>In this step, objects and their predicted paths are filtered based on its classification label and the corresponding parameters <code>objects.{CLASSIFICATION_LABEL}</code>.</p> <p>An object is ignored if one of the following condition is true:</p> <ul> <li>its classification label is not in the list defined by the <code>objects.target_labels</code> parameter;</li> <li>its velocity is bellow the <code>ignore.stopped_velocity_threshold</code> and <code>ignore.if_stopped</code> is set to <code>true</code>;</li> <li>its current footprint is inside one of the polygons prepared in the previous step.</li> </ul> <p>However, if it was decided to stop for the object in the previous iteration, or if a collision was detected with the object, then it cannot be ignored.</p> <p>If an object is not ignored, its predicted path footprints are generated similarly to the ego footprint First, we only keep predicted paths that have a confidence value above the <code>confidence_filtering.threshold</code> parameter. If, <code>confidence_filtering.only_use_highest</code> is set to <code>true</code> then for each object only the predicted paths that have the higher confidence value are kept. Next, the remaining predicted paths are cut according to the segments prepared in the previous step.</p> <p>To guarantee that parts of the predicted paths are never ignored, parameters <code>preserved_duration</code> and <code>preserved_distance</code> can be used to set a minimum duration and/or distance that cannot be cut or ignored. This is not applied in the case of the strict cutting.</p> <p>The following figures shows an example where crosswalks are used to ignore pedestrians and to cut their predicted paths.</p> debug markers (<code>objects_footprints</code>) objects of interest <p>The result of the filtering can be visualized on the debug markers with the <code>objects_footprints</code> namespace which shows in yellow which predicted path will be used for collision checking in the next step.</p> <p>In addition, the objects of interests markers shows which objects are not ignored and the color will correspond to the decision made towards that object (green for nothing, yellow for slowdown, and red for stop).</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_run_out_module/#4-collision-detection","title":"4. Collision detection","text":"<p>Now that we prepared the ego trajectory footprint, the dynamic objects, and their predicted paths, we will calculate the times when they are predicted to collide.</p> <p>The following operations are performed for each object that was not ignored in the previous iteration.</p> <p>First, we calculate the intersections between each pair of linestrings between the ego and object footprints. For each intersection, we calculate the corresponding point, the time when ego and the object are predicted to reach that point, and the location of that point on the ego footprint (e.g., on the rear left linestring).</p> <p>All these intersections are then combined into intervals representing when the overlap between the ego trajectory and object predicted paths starts and ends. An overlap is represented by the entering and exiting intersections for both ego and the object.</p> <p>These overlaps calculated for all the object's predicted paths are then combined if overlapping in time (including the <code>collision.time_overlap_tolerance</code> parameter). and classified into the following collision types:</p> <ul> <li><code>ignored_collision</code> if one of the following condition is true:<ul> <li>parameter <code>collision.ignore_conditions.if_ego_arrives_first.enable</code> is set to <code>true</code> and:<ul> <li>ego enters the overlap at least <code>time_margin</code> seconds before the object.<ul> <li><code>time_margin</code> is calculated based on the time when ego enters the overlap, which is used to interpolate the margin using the mapping between <code>margin.ego_enter_times</code> and <code>margin.time_margins</code>.</li> </ul> </li> <li>ego does not overlap the object's path by more than <code>max_overlap_duration</code> seconds.</li> </ul> </li> <li>parameter <code>collision.ignore_conditions.if_ego_arrives_first.enable</code> is set to <code>true</code> and:<ul> <li>ego cannot stop before entering the interval by using the deceleration limit set with <code>deceleration_limit</code>.</li> </ul> </li> </ul> </li> <li><code>collision</code> if ego and the object are predicted to be in the overlap at the same time.<ul> <li>the time distance between the time intervals of ego and the objects must be smaller than the <code>time_margin</code> parameter (a distance of 0 means that the intervals overlap).</li> </ul> </li> <li><code>pass_first_no_collision</code> if ego is predicted to exit the overlap before the object enters it.</li> <li><code>no_collision</code> in all other cases.</li> </ul> <p>In the case where a collision is detected, the corresponding collision time is calculated based on the yaw difference between the object and the ego vehicle at the first intersection point.</p> <ul> <li>default: collision time is set to the time when ego enters the overlap.</li> <li>object yaw is within <code>collision.same_direction_angle_threshold</code> of the ego yaw:<ul> <li>if the object is faster than ego, no collision will happen and the type is changed to <code>no_collision</code>.</li> <li>the collision time is increased based on the velocity difference.</li> </ul> </li> <li>object yaw is within <code>collision.opposite_direction_angle_threshold</code> of the opposite of the ego yaw:<ul> <li>the collision time is increased based on the estimated time when ego will collide will the object after entering the overlap.</li> </ul> </li> </ul> <p>The following figure shows the collision points in red and a table showing each overlap found and the corresponding collision type, ego and object time intervals, and predicted ego collision time.</p> <p></p> <p>The collisions points and the table can be visualized on the debug markers with the <code>collisions_points</code> and <code>collisions_table</code> namespaces.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_run_out_module/#5-decisions","title":"5. Decisions","text":"<p>We will now make the decision towards each object on whether to stop, slowdown, or do nothing. For each object, we consider what decision each collision require, and keep the one with highest priority as follows:</p> <ul> <li><code>stop</code> types have higher priority than <code>slowdown</code> types.</li> <li>for two decisions of the same type, the one whose predicted collision time is earlier has higher priority.</li> </ul> <p>Once a decision is made, the history of the object is updated and will allow to know, for each previous time step, what was the decision made and the type of collision identified.</p> <p>To decide the type of a collision, we use the decision history of the object and first check if it satisfies the following conditions to stop:</p> <ul> <li>if the current collision type is <code>collision</code> and collisions with the object have been identified for a consecutive duration of at least <code>stop.on_time_buffer</code> seconds.</li> <li>if the previous decision was <code>stop</code> and the time since the last identified collision with the object was less than <code>stop.off_time_buffer</code> seconds ago.</li> </ul> <p>If the condition to stop is not met, we check the following conditions to slowdown as follows:</p> <ul> <li>if the current collision type is <code>collision</code> and collisions with the object have been identified for a consecutive duration of at least <code>slowdown.on_time_buffer</code> seconds.</li> <li>if the previous decision was <code>slowdown</code> and the time since the last identified collision with the object was less than <code>slowdown.off_time_buffer</code> seconds ago.</li> </ul> <p></p> <p>The decision table can be visualized on the debug markers with the <code>decisions</code> namespace.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_run_out_module/#6-calculate-the-stop-or-slowdowns","title":"6. Calculate the stop or slowdowns","text":"<p>Finally, for each object, we calculate how the velocity profile will be modified based on the decision made:</p> <ul> <li><code>stop</code>: insert a <code>0</code> velocity ahead of the predicted collision point by the distance set in the <code>stop.distance_buffer</code> parameter.<ul> <li>to prevent the stop pose from jumping, the previous stop pose is reused if its arc length position is before the newly calculated stop pose and the arc length difference between them is smaller than the <code>stop.reuse_margin</code> parameter.</li> </ul> </li> <li><code>slowdown</code>: insert a \\(V_{slow}\\) velocity between the collision point and the point ahead of collision point by the distance set in the <code>slowdown.distance_buffer</code> parameter.<ul> <li>\\(V_{slow}\\) is calculated as the maximum between the safe velocity and the comfortable velocity.<ul> <li>safe velocity: velocity required to be able to stop over the <code>distance_buffer</code> assuming a deceleration as set by the   <code>stop.deceleration_limit</code> parameter.</li> <li>comfortable velocity: velocity ego would reach assuming it constantly decelerates at the   <code>slowdown.deceleration_limit</code> parameter until the slowdown point.</li> </ul> </li> </ul> </li> </ul> <p>The slowdowns and stops inserted in the trajectory are visualized with the virtual walls.</p> <p></p> <p>If an inserted <code>stop</code> point requires a stronger deceleration than set by the <code>stop.deceleration_limit</code> parameter, then an ERROR diagnostic is published to indicate that the stop in unfeasible.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_run_out_module/#use-of-rtree-for-fast-spatial-queries","title":"Use of Rtree for fast spatial queries","text":"<p>In step 1, each segment of the 4 linestrings of the ego trajectory footprint are stored in a Rtree along with the corresponding trajectory point index. This allows to efficiently find intersections with an object's predicted path along with the corresponding ego trajectory segment from which the interpolated <code>time_from_start</code> can be calculated.</p> <p>In step 2, the polygons and linestrings used for filtering the objects are stored in Rtree objects to efficiently find whether an object is inside a polygon or if its predicted path intersects a linestring.</p> <p>For more information about Rtree, see https://beta.boost.org/doc/libs/1_82_0/libs/geometry/doc/html/geometry/spatial_indexes/introduction.html</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_run_out_module/#accounting-for-prediction-inaccuracies","title":"Accounting for prediction inaccuracies","text":"<p>When calculating predicted collisions between ego and the objects, we assume that the input ego trajectory contains accurate <code>time_from_start</code> values. Similarly, accurate predicted paths are expected to be provided for the objects.</p> <p>To allow for errors in these predictions, margins around the time intervals can be added using the parameters <code>collision.time_margin</code>. Higher values of this parameter will make it more likely to detect a collision and generate a stop.</p> <p>The time buffers <code>on_time_buffer</code> and <code>off_time_buffer</code> allow to delay the addition or removal of the decisions to stop/slowdown. Higher values prevent incorrect decisions in case of noisy object predictions, but also increase the reaction time, possibly causing stronger decelerations once a decision is made.</p>"},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_run_out_module/#module-parameters","title":"Module Parameters","text":""},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_run_out_module/#flow-diagram","title":"Flow Diagram","text":""},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_run_out_module/#debugging-and-tuning-guide","title":"Debugging and Tuning Guide","text":""},{"location":"planning/motion_velocity_planner/autoware_motion_velocity_run_out_module/#ego-does-not-stop-for-the-incoming-object","title":"Ego does not stop for the incoming object","text":"<p>Possible reasons to investigate:</p> <ul> <li>the object classification label is not in the <code>objects.target_labels</code>;</li> <li>the object is inside an ignore polygon (<code>objects.LABEL.ignore.polygon_types</code> or <code>lanelet_subtypes</code>);</li> <li>the object is on the ego trajectory (<code>objects.LABEL.ignore.if_on_ego_trajectory</code>) or behind ego (<code>if_behind_ego</code>);</li> <li>the predicted path of the object is cut (<code>objects.LABEL.cut_predicted_paths.polygon_types</code>, <code>lanelet_subtypes</code>, or <code>linestring_types</code>);</li> <li>the collision is ignored;<ul> <li>ego does not have time to stop (<code>ignore_conditions.if_ego_arrives_first_and_cannot_stop</code>);<ul> <li><code>deceleration_limit</code> can be increased.</li> </ul> </li> <li>ego is predicted to pass before the object (<code>ignore_conditions.if_ego_arrives_first</code>), including the time margin (calculated from <code>margin.ego_enter_times</code> and <code>margin.time_margins</code>);<ul> <li><code>time_margins</code> can be increased.</li> </ul> </li> </ul> </li> <li>the collision is not detected;<ul> <li><code>collision.time_margin</code> can be increased.</li> <li>the ego footprint can be made larger (<code>ego.lateral_margin</code> and <code>ego.longitudinal.margin</code>).</li> </ul> </li> </ul>"},{"location":"planning/planning_validator/autoware_planning_validator/","title":"Planning Validator","text":""},{"location":"planning/planning_validator/autoware_planning_validator/#planning-validator","title":"Planning Validator","text":"<p>The <code>autoware_planning_validator</code> node is the last module executed in the planning component, it responsible for checking the validity of the planning trajectory before it is published to control component. The status of the validation can be viewed in the <code>/diagnostics</code> and <code>/validation_status</code> topics. When an invalidity is detected, the <code>autoware_planning_validator</code> will process the trajectory following the selected option: \"0. publish the trajectory as it is\", \"1. stop publishing the trajectory\", \"2. publish the last validated trajectory\".</p> <p>Note: The planning validator automatically suppresses validation errors during manual driving mode. When the vehicle is in autonomous mode, all validation checks are performed normally. During manual driving, validation errors are suppressed and only debug logs are output.</p> <p>The <code>autoware_planning_validator</code> node loads multiple plugins modules, each responsible for running specific validation checks on the planning trajectory:</p> <ul> <li>Latency Checker: The <code>autoware_planning_validator_latency_checker</code> is responsible for checking the validity of planning trajectory age</li> <li>Trajectory Checker: The <code>autoware_planning_validator_trajectory_checker</code> is responsible for checking the validity of planning trajectory shape</li> <li>Intersection Collision Checker: The <code>autoware_planning_validator_intersection_collision_checker</code> is responsible for verifying planning trajectory does not result in collision at intersections</li> <li>Rear Collision Checker: The <code>autoware_planning_validator_rear_collision_checker</code> is responsible for verifying planning trajectory does not result in collision with rear vehicles</li> </ul> <p></p>"},{"location":"planning/planning_validator/autoware_planning_validator/#inputsoutputs","title":"Inputs/Outputs","text":""},{"location":"planning/planning_validator/autoware_planning_validator/#inputs","title":"Inputs","text":"<p>The <code>autoware_planning_validator</code> takes in the following inputs:</p> Name Type Description <code>~/input/kinematics</code> nav_msgs/Odometry ego pose and twist <code>~/input/acceleration</code> geometry_msgs/AccelWithCovarianceStamped current acceleration of the ego vehicle <code>~/input/trajectory</code> autoware_planning_msgs/Trajectory target trajectory to be validated in this node <code>~/input/route</code> autoware_planning_msgs/LaneletRoute route information <code>~/input/lanelet_map_bin</code> autoware_map_msgs/LaneletMapBin lanelet vector map information <code>~/input/pointcloud</code> sensor_msgs/PointCloud2 obstacle pointcloud with ground removed <code>~/input/operational_mode_state</code> autoware_adapi_v1_msgs/OperationModeState current operation mode state (autonomous/manual) <code>~/input/traffic_signals</code> autoware_perception_msgs/TrafficLightGroupArray recognized traffic signal information"},{"location":"planning/planning_validator/autoware_planning_validator/#outputs","title":"Outputs","text":"<p>It outputs the following:</p> Name Type Description <code>~/output/trajectory</code> autoware_planning_msgs/Trajectory validated trajectory <code>~/output/validation_status</code> planning_validator/PlanningValidatorStatus validator status to inform the reason why the trajectory is valid/invalid <code>/diagnostics</code> diagnostic_msgs/DiagnosticStatus diagnostics to report errors"},{"location":"planning/planning_validator/autoware_planning_validator/#parameters","title":"Parameters","text":"<p>The following parameters can be set for the <code>autoware_planning_validator</code>:</p> Name Type Description Default value <code>default_handling_type</code> int set default handling type for when invalidity is detected. 0: publish invalid traj as it is, 1: publish last valid traj, 2: publish last valid traj with soft stop 0 <code>publish_diag</code> bool if true, diagnostics msg is published. true <code>diag_error_count_threshold</code> int Number of consecutive invalid trajectories to set the Diag to ERROR. (Fe.g, threshold = 1 means, even if the trajectory is invalid, the Diag will not be ERROR if the next trajectory is valid.) 0 <code>display_on_terminal</code> bool show error msg on terminal true <code>soft_stop_deceleration</code> double deceleration value to be used for soft stop action. [m/ss] -1.0 <code>soft_stop_jerk_lim</code> double jerk limit value to be used for soft stop action. [m/sss] 0.3 <code>th_traffic_light_timeout</code> double timeout threshold [s] to discard outdated traffic light information during validation 0.5"},{"location":"planning/planning_validator/autoware_planning_validator_intersection_collision_checker/","title":"Intersection Collision Checker","text":""},{"location":"planning/planning_validator/autoware_planning_validator_intersection_collision_checker/#intersection-collision-checker","title":"Intersection Collision Checker","text":"<p>The <code>intersection_collision_checker</code> is a plugin module of <code>autoware_planning_validator</code> node. It is responsible for validating the planning trajectory at intersections by verifying that it does NOT lead to a collision with other road vehicles.</p> <p>The check is executed only when:</p> <ul> <li>Ego is approaching a <code>turn_direction</code> lane</li> <li>Ego trajectory intersects with lanes other than <code>route_lanelets</code></li> </ul>"},{"location":"planning/planning_validator/autoware_planning_validator_intersection_collision_checker/#inner-workings","title":"Inner Workings","text":"<p>The intersection_collision_checker checks for collisions using pointcloud data and route information. It identifies target lanes at intersections and extracts pcd objects withing target lanes, and performs simplistic tracking and velocity estimation of pcd objects for each target lane. Times to arrival are computed for Ego and pcd objects, and the difference in the arrival time is used to judge if a collision is imminent.</p>"},{"location":"planning/planning_validator/autoware_planning_validator_intersection_collision_checker/#flowchart","title":"Flowchart","text":"<p>The following diagram illustrates the overall flow of module implementation:</p> <p></p>"},{"location":"planning/planning_validator/autoware_planning_validator_intersection_collision_checker/#ego-trajectory","title":"Ego Trajectory","text":"<p>The intersection_collision_checker module utilizes the ego trajectory subscribed to by planning_validator node, it uses the resampled trajectory to avoid clustered trajectory points in low velocity areas. The for each trajectory point the time_from_start is computed with respect to the ego's front pose and the ego's back pose. This information is later used to estimate the ego's entry and exit times for each target lanelet.</p>"},{"location":"planning/planning_validator/autoware_planning_validator_intersection_collision_checker/#target-lanelets","title":"Target Lanelets","text":"<p>The module applies slightly different logic for acquiring target lanes for right and left turn intersections. In case of right turn intersection, the aim is to check all lanes crossing/overlapping with the egos intended trajectory. In case of left turn, the aim is check no vehicles are coming along the destination lane (lane ago turning into).</p> <p>Warning</p> <p>Target lane selection logic applies only for Left-hand traffic (LHT). The module should be improved to be driving side agnostic.</p>"},{"location":"planning/planning_validator/autoware_planning_validator_intersection_collision_checker/#right-turn","title":"Right Turn","text":"<p>To get the target lanelets for a right-turn intersection:</p> <ul> <li>Use ego\u2019s turn-direction lane to define the search space (bounding box enclosing the turn-direction lane).</li> <li>Get all lanelets within the bounding box as candidate lanelets.</li> <li>Filter out the following lanelets:<ul> <li>Lanelets that are <code>route_lanelets</code></li> <li>Lanelets with a \"time to reach\" exceeding the time horizon</li> <li>Lanelets that have the <code>turn_direction</code> attribute and are not <code>STRAIGHT</code> (if parameter <code>right_turn.check_turn_lanes</code> is FALSE)</li> <li>Lanelets that are determined to be crossing lanes (if parameter <code>right_turn.check_crossing_lanes</code> is FALSE)</li> <li>Lanelets that are excluded based on traffic signal context (if parameter <code>right_turn.check_traffic_signal</code> is TRUE and the right-turn arrow signal is active)</li> </ul> </li> <li>Remaining lanelets are then processed to:<ul> <li>Compute the overlap point between the ego trajectory and the target lanelet</li> <li>Compute ego\u2019s time to arrive and leave the overlap point</li> </ul> </li> </ul> <p>The image below shows the target lanelets at a right-turn intersection. (<code>right_turn.check_turn_lanes</code> set to FALSE)</p> <p></p>"},{"location":"planning/planning_validator/autoware_planning_validator_intersection_collision_checker/#left-turn","title":"Left Turn","text":"<p>To get the target lanelets for a left-turn intersection:</p> <ul> <li>Use ego\u2019s turn-direction lanelet(s) to get the next lanelet, the \"destination_lanelet,\" following the turn.</li> <li>Get all lanelets preceding the \"destination_lanelet\" and filter out:<ul> <li>Lanelets that are <code>route_lanelets</code></li> <li>Lanelets with a \"time to reach\" exceeding the time horizon</li> <li>Lanelets that have the <code>turn_direction</code> attribute and are not <code>STRAIGHT</code> (if parameter <code>left_turn.check_turn_lanes</code> is FALSE)</li> <li>Lanelets that are excluded based on traffic signal context (if parameter <code>left_turn.check_traffic_signal</code> is TRUE and the signal is green or amber, giving priority to the left-turn movement)</li> </ul> </li> <li>Remaining lanelets are then processed to:<ul> <li>Compute the overlap point between the ego trajectory and the target lanelet</li> <li>Compute ego\u2019s time to arrive and leave the overlap point</li> </ul> </li> </ul> <p>Target lanelets are then expanded, if necessary, up to <code>detection_range</code>.</p> <p>The image below shows the target lanelets at a left-turn intersection. (<code>left_turn.check_turn_lanes</code> set to TRUE)</p> <p></p>"},{"location":"planning/planning_validator/autoware_planning_validator_intersection_collision_checker/#collision-check","title":"Collision Check","text":"<p>After target lanes are determined, The next step is to identify pcd objects and perform velocity estimation and tracking for each target lane, and determine possibility of collision.</p> <p>First the object pointcloud is filtered and transformed to map frame. Then the logic described in the following diagram is applied for each target lane to get the nearest pcd object:</p> <p></p> <p>For each tracked object, the velocity estimation is done with linear regression, using the last N samples of distance measurements and time stamps. If any of the following conditions are met the tracking information is reset and the object is handled as a new object:</p> <ul> <li>Computed raw velocity exceeds threshold (parameterized) -&gt; Indicates a large jump in pcd object position.</li> <li>Computed acceleration exceeds threshold (parameterized) -&gt; Indicates a large change in estimated velocity.</li> </ul>"},{"location":"planning/planning_validator/autoware_planning_validator_intersection_collision_checker/#parameters","title":"Parameters","text":"Name Unit Type Description Default value <code>enable</code> [-] bool Flag to enable/disable the check globally true <code>detection_range</code> [m] double Range of detection from ego position, pointcloud points beyond this range are filtered out 50.0 <code>ttc_threshold</code> [s] double Threshold value for the difference between ego and object reach times to trigger and a stop 1.0 <code>ego_deceleration</code> [m/ss] double Ego deceleration relate used to estimate ego stopping time 1.0 <code>min_time_horizon</code> [s] double Minimum time horizon to check ahead along ego trajectory 10.0 <code>on_time_buffer</code> [s] double Continuous collision detection time required to judge as unsafe 0.5 <code>off_time_buffer</code> [s] double Continuous no collision detection time required to clear unsafe decision 1.0 <code>filter.min_velocity</code> [m/s] double Minimum velocity threshold to determine moving object 1.0 <code>filter.moving_time</code> [s] double Minimum duration object needs to satisfy min velocity condition to classify as moving 1.0"},{"location":"planning/planning_validator/autoware_planning_validator_intersection_collision_checker/#target-lanes-parameters","title":"Target Lanes Parameters","text":"Name Unit Type Description Default value <code>right_turn.enable</code> [-] bool Flag to enable/disable the check at right turns true <code>right_turn.check_crossing_lanes</code> [-] bool Flag to enable/disable checking crossing lanes true <code>right_turn.check_turn_lanes</code> [-] bool Flag to enable/disable checking turning lanes true <code>right_turn.crossing_lane_angle_th</code> [rad] double Angle threshold for determining crossing lanes 0.785398 <code>right_turn.check_traffic_signal</code> [-] bool Use traffic light context for right-turn validation true <code>left_turn.enable</code> [-] bool Flag to enable/disable the check at left turns true <code>left_turn.check_turn_lanes</code> [-] bool Flag to enable/disable checking turning lanes true <code>left_turn.check_traffic_signal</code> [-] bool Use traffic light context for left-turn validation true"},{"location":"planning/planning_validator/autoware_planning_validator_intersection_collision_checker/#pointcloud-parameters","title":"Pointcloud Parameters","text":"Name Unit Type Description Default value <code>pointcloud.height_buffer</code> [m] double Height offset to add above ego vehicle height when filtering pointcloud points 0.5 <code>pointcloud.min_height</code> [m] double Minimum height threshold for filtering pointcloud points 0.5 <code>pointcloud.voxel_grid_filter.x</code> [m] double x value for voxel leaf size 0.2 <code>pointcloud.voxel_grid_filter.y</code> [m] double y value for voxel leaf size 0.2 <code>pointcloud.voxel_grid_filter.z</code> [m] double z value for voxel leaf size 0.2 <code>pointcloud.voxel_grid_filter.min_size</code> [-] int min number of points per voxel leaf 3 <code>pointcloud.clustering.tolerance</code> [m] double Distance tolerance between two points in a cluster 0.5 <code>pointcloud.clustering.min_height</code> [m] double Minimum height of a cluster to be considered as a target 0.5 <code>pointcloud.clustering.min_size</code> [-] int Minimum number of points in a cluster to be considered as a target 10 <code>pointcloud.clustering.max_size</code> [-] int Maximum number of points in a cluster to be considered as a target 10000 <code>pointcloud.velocity_estimation.max_acceleration</code> [s] double Max acceleration threshold above which object tracking is reset 20.0 <code>pointcloud.velocity_estimation.max_velocity</code> [s] double Max velocity threshold above which object tracking is reset 25.0 <code>pointcloud.velocity_estimation.observation_time</code> [s] double Minimum tracking time for a pointcloud object to be considered reliable 0.3 <code>pointcloud.velocity_estimation.max_history_time</code> [s] double Maximum duration since last object update above which object will be discarded 0.5 <code>pointcloud.velocity_estimation.buffer_size</code> [-] int Number of data samples to keep for object velocity estimation 10 <code>pointcloud.latency</code> [s] double Time delay used to compensate for latency in pointcloud data 0.3"},{"location":"planning/planning_validator/autoware_planning_validator_rear_collision_checker/","title":"Rear Collision Checker","text":""},{"location":"planning/planning_validator/autoware_planning_validator_rear_collision_checker/#rear-collision-checker","title":"Rear Collision Checker","text":"<p>The <code>rear_collision_checker</code> is a plugin module of the <code>autoware_planning_validator</code> node. It validates the planned trajectory by verifying that it does not lead to a collision with other road users (primarily vehicles, but also pedestrian/cyclists/motorcycles) approaching from lateral/rear directions. In particular, it is designed to detect potential rear-end collisions and entrapment risks when the ego vehicle is making a right/left turn or merging into an adjacent lane.</p>"},{"location":"planning/planning_validator/autoware_planning_validator_rear_collision_checker/#inner-workings","title":"Inner Workings","text":"<p>The module operates by:</p> <ol> <li> <p>Identifying conflict lanes</p> <ul> <li>Uses the current route and lanelet topology to determine lanes that the ego vehicle\u2019s trajectory may intersect with during right/left turns or lane changes.</li> <li>Focuses on lanes where vehicles, bicycles, or motorcycles could approach from behind or from a lateral direction, posing a potential rear-end or entrapment risk.</li> <li>Considers the <code>turn_direction</code> of the approaching lanelets to filter out irrelevant lanes.</li> </ul> </li> <li> <p>Filtering perception data</p> <ul> <li>Subscribes to obstacle pointcloud and first filters by the conflict region, keeping only points that fall within relevant spatial bounds.</li> <li>Performs clustering on the filtered points to form obstacle candidates.</li> <li>Associates clusters with nearby lanelets and computes the nearest face of each obstacle along the lane direction.</li> <li>Applies configurable range gates (forward/backward, lateral, height) and basic outlier rejection to discard distant or irrelevant clusters.</li> </ul> </li> <li> <p>Estimating motion</p> <ul> <li>For each selected object, determines its motion relative to the lane direction.</li> <li>Estimates speed and direction based on frame-to-frame position changes along the lane\u2019s centerline.</li> <li>Maintains a per-lane tracking list to stabilize velocity estimation and reduce noise from perception flicker.</li> </ul> </li> <li> <p>Calculating metrics</p> <ul> <li>Depending on the selected metric (e.g., TTC, RSS), the calculation method and decision logic differ. The chosen metric is computed, and the result is used to determine whether there is a potential collision risk.</li> </ul> </li> <li> <p>Judging collision risk</p> <ul> <li>Evaluates the selected metric results against predefined thresholds to determine whether a collision is likely.</li> <li>Applies hysteresis logic:<ul> <li><code>on_time_buffer</code>: Hazardous state must persist for this duration before marking the trajectory unsafe.</li> <li><code>off_time_buffer</code>: Safe state must persist for this duration before clearing the unsafe flag.</li> </ul> </li> <li>If <code>check_on_unstoppable=false</code>, the module will skip collision warnings in cases where the ego vehicle cannot realistically stop before the conflict area (to prevent unnecessary alerts when stopping is already infeasible).</li> </ul> </li> </ol>"},{"location":"planning/planning_validator/autoware_planning_validator_rear_collision_checker/#flowchart","title":"Flowchart","text":"<p>WIP</p>"},{"location":"planning/planning_validator/autoware_planning_validator_rear_collision_checker/#general-parameters","title":"General Parameters","text":"Name Unit Type Description Default value <code>on_time_buffer</code> [s] double Time buffer before enabling detection after a relevant condition is met 0.5 <code>off_time_buffer</code> [s] double Time buffer before disabling detection after the condition clears 1.5 <code>check_on_unstoppable</code> [-] bool If <code>true</code>, the module continues collision checking even when the ego vehicle cannot stop before entering a potential collision area, and outputs an ERROR if a collision risk is detected. If <code>false</code>, checking is skipped in such cases to avoid unnecessary warnings. false"},{"location":"planning/planning_validator/autoware_planning_validator_rear_collision_checker/#pointcloud-preprocess","title":"Pointcloud Preprocess","text":"Name Unit Type Description Default value <code>pointcloud.range.dead_zone</code> [m] double Distance in front of the ego vehicle ignored for collision detection 0.3 <code>pointcloud.range.buffer</code> [m] double Additional margin around detection range 1.0 <code>pointcloud.crop_box_filter.x.max</code> [m] double Maximum X coordinate for cropped detection range 30.0 <code>pointcloud.crop_box_filter.x.min</code> [m] double Minimum X coordinate for cropped detection range -100.0 <code>pointcloud.crop_box_filter.z.max</code> [m] double Maximum Z coordinate for cropped detection range -1.0 <code>pointcloud.crop_box_filter.z.min</code> [m] double Minimum Z coordinate for cropped detection range 0.3 <code>pointcloud.voxel_grid_filter.x</code> [m] double Voxel grid filter size in X direction 0.1 <code>pointcloud.voxel_grid_filter.y</code> [m] double Voxel grid filter size in Y direction 0.1 <code>pointcloud.voxel_grid_filter.z</code> [m] double Voxel grid filter size in Z direction 0.5 <code>pointcloud.clustering.cluster_tolerance</code> [m] double Maximum distance between points to be considered part of the same cluster 0.15 <code>pointcloud.clustering.min_cluster_height</code> [m] double Minimum height of a cluster to be considered valid 0.1 <code>pointcloud.clustering.min_cluster_size</code> [points] int Minimum number of points in a valid cluster 5 <code>pointcloud.clustering.max_cluster_size</code> [points] int Maximum number of points in a valid cluster 10000 <code>pointcloud.velocity_estimation.observation_time</code> [s] double Time window used for velocity estimation 0.3 <code>pointcloud.velocity_estimation.max_acceleration</code> [m/s^2] double Maximum allowed acceleration in velocity estimation 10.0 <code>pointcloud.latency</code> [s] double Assumed system latency for point cloud processing 0.3"},{"location":"planning/planning_validator/autoware_planning_validator_rear_collision_checker/#object-filtering","title":"Object Filtering","text":"Name Unit Type Description Default value <code>filter.min_velocity</code> [m/s] double Minimum velocity threshold for objects to be considered moving 1.0 <code>filter.moving_time</code> [s] double Minimum time duration an object must be moving to be considered as such 0.5"},{"location":"planning/planning_validator/autoware_planning_validator_rear_collision_checker/#ttc","title":"TTC","text":"Name Unit Type Description Default value <code>time_to_collision.margin</code> [s] double Additional margin added to time-to-collision calculation 2.0"},{"location":"planning/planning_validator/autoware_planning_validator_rear_collision_checker/#ego-behavior","title":"Ego Behavior","text":"Name Unit Type Description Default value <code>ego.reaction_time</code> [s] double Reaction time of the ego vehicle 1.2 <code>ego.min_velocity</code> [m/s] double Minimum considered velocity of the ego vehicle 1.38 <code>ego.max_velocity</code> [m/s] double Maximum considered velocity of the ego vehicle 16.6 <code>ego.max_acceleration</code> [m/s^2] double Maximum considered acceleration of the ego vehicle 1.5 <code>ego.max_deceleration</code> [m/s^2] double Maximum considered deceleration of the ego vehicle (negative value) -4.0 <code>ego.max_positive_jerk</code> [m/s^3] double Maximum allowed positive jerk for the ego vehicle 5.0 <code>ego.max_negative_jerk</code> [m/s^3] double Maximum allowed negative jerk for the ego vehicle -5.0 <code>ego.nominal_deceleration</code> [m/s^2] double Nominal deceleration used for calculations -1.5 <code>ego.nominal_positive_jerk</code> [m/s^3] double Nominal positive jerk used for calculations 0.6 <code>ego.nominal_negative_jerk</code> [m/s^3] double Nominal negative jerk used for calculations -0.6"},{"location":"planning/planning_validator/autoware_planning_validator_rear_collision_checker/#collision-check-for-blind-spot","title":"Collision Check For Blind Spot","text":"Name Unit Type Description Default value <code>blind_spot.lookahead_time</code> [s] double Lookahead time for blind spot detection 4.0 <code>blind_spot.metric</code> [-] string Metric used for blind spot detection (<code>ttc</code>, etc.) ttc <code>blind_spot.check.front</code> [-] bool Whether to check for blind spot in the front false <code>blind_spot.check.left</code> [-] bool Whether to check for blind spot on the left true <code>blind_spot.check.right</code> [-] bool Whether to check for blind spot on the right false <code>blind_spot.check.yaw_th</code> [rad] double Yaw threshold for blind spot detection 0.78 <code>blind_spot.offset.inner</code> [m] double Inner offset for blind spot detection zone 0.1 <code>blind_spot.offset.outer</code> [m] double Outer offset for blind spot detection zone 0.3 <code>blind_spot.participants.reaction_time</code> [s] double Reaction time of participants in blind spot detection 1.2 <code>blind_spot.participants.max_velocity</code> [m/s] double Maximum velocity of participants in blind spot detection 5.5 <code>blind_spot.participants.max_deceleration</code> [m/s^2] double Maximum deceleration of participants in blind spot detection -2.0"},{"location":"planning/planning_validator/autoware_planning_validator_rear_collision_checker/#collision-check-for-adjacent-lane","title":"Collision Check For Adjacent Lane","text":"Name Unit Type Description Default value <code>adjacent_lane.lookahead_time</code> [s] double Lookahead time for adjacent lane collision detection 4.0 <code>adjacent_lane.metric</code> [-] string Metric used for adjacent lane collision detection (<code>rss</code>, etc.) rss <code>adjacent_lane.check.front</code> [-] bool Whether to check for adjacent lane collision in the front true <code>adjacent_lane.offset.left</code> [m] double Left offset for adjacent lane collision detection -0.5 <code>adjacent_lane.offset.right</code> [m] double Right offset for adjacent lane collision detection -0.5 <code>adjacent_lane.participants.reaction_time</code> [s] double Reaction time of participants in adjacent lane collision detection 1.2 <code>adjacent_lane.participants.max_velocity</code> [m/s] double Maximum velocity of participants in adjacent lane collision detection 16.6 <code>adjacent_lane.participants.max_deceleration</code> [m/s^2] double Maximum deceleration of participants in adjacent lane collision detection -2.0"},{"location":"planning/planning_validator/autoware_planning_validator_trajectory_checker/","title":"Trajectory Checker","text":""},{"location":"planning/planning_validator/autoware_planning_validator_trajectory_checker/#trajectory-checker","title":"Trajectory Checker","text":"<p>The <code>intersection_collision_checker</code> is a plugin module of <code>autoware_planning_validator</code> node. It is responsible for validating the planning trajectory before it is published.</p>"},{"location":"planning/planning_validator/autoware_planning_validator_trajectory_checker/#supported-features","title":"Supported features","text":"<p>The following features are supported for trajectory validation and can have thresholds set by parameters:</p> <ul> <li>Invalid field : e.g. Inf, Nan</li> <li>Trajectory points interval : invalid if any of the distance of trajectory points is too large</li> <li>Curvature : invalid if the trajectory has too sharp turns that is not feasible for the given vehicle kinematics</li> <li>Relative angle : invalid if the yaw angle changes too fast in the sequence of trajectory points</li> <li> <p>Lateral acceleration : invalid if the expected lateral acceleration/deceleration is too large. Lateral acceleration is calculated using the formula:</p> \\[ a_{lat} = v_{lon}^2 * \\kappa \\] <p>Where \\(v_{lon}\\) is longitudinal velocity and \\(\\kappa\\) is curvature. Since the acceleration embedded in path points is perpendicular to the derived lateral acceleration, projections are not considered. The velocity and acceleration assigned to each point are directed toward the next path point.</p> </li> </ul> <ul> <li> <p>Lateral jerk : invalid if the rate of change of lateral acceleration is too large. Lateral jerk is calculated using the formula:</p> \\[ j_{lat} = v_{lon}^3 * \\frac{d\\kappa}{ds} + 3 * v_{lon}^2 * a_{lon} * \\kappa \\] <p>Where \\(v_{lon}\\) is longitudinal velocity, \\(\\kappa\\) is curvature, \\(a_{lon}\\) is longitudinal acceleration, and \\(\\frac{d\\kappa}{ds}\\) is the rate of curvature change with respect to distance. In this implementation, the curvature change (\\(\\frac{d\\kappa}{ds}\\)) is not considered, simplifying the calculation to only the second term. The lateral jerk represents how quickly the lateral acceleration changes, which affects ride comfort and vehicle stability.</p> </li> </ul> <ul> <li>Longitudinal acceleration/deceleration : invalid if the acceleration/deceleration in the trajectory point is too large</li> <li>Steering angle : invalid if the expected steering value is too large estimated from trajectory curvature</li> <li>Steering angle rate : invalid if the expected steering rate value is too large</li> <li>Velocity deviation : invalid if the planning velocity is too far from the ego velocity</li> <li>Distance deviation : invalid if the ego is too far from the trajectory</li> <li>Longitudinal distance deviation : invalid if the trajectory is too far from ego in longitudinal direction</li> <li>Forward trajectory length : invalid if the trajectory length is not enough to stop within a given deceleration</li> <li>Yaw deviation : invalid if the difference between the ego yaw and closest trajectory yaw is too large.<ul> <li>in general planning is not responsible for keeping a low yaw deviation, so this metric is checked only when the closest trajectory yaw changes by more than <code>th_trajectory_yaw_shift</code> between successive trajectories.</li> </ul> </li> <li>Trajectory Shift : invalid if the lat./long. distance between two consecutive trajectories near the Ego exceed the thresholds.</li> </ul>"},{"location":"planning/planning_validator/autoware_planning_validator_trajectory_checker/#parameters","title":"Parameters","text":"<p>The following parameters are used to configure the different validation checks performed by <code>trajectory_checker</code>:</p>"},{"location":"planning/planning_validator/autoware_planning_validator_trajectory_checker/#interval-check","title":"Interval Check","text":"Name Unit Type Description Default value <code>interval.enable</code> [-] bool flag to enable/disable interval validation check true <code>interval.threshold</code> [m] double max valid distance between two consecutive trajectory points 100.0 <code>interval.handling_type</code> [-] int specify handling type for invalid interval (optional parameter, if not specified will use default of planning validator) unspecified <code>interval.override_error_diag</code> [-] bool if true, will override error diag from other checks (optional parameter, if not specified will assume FALSE) unspecified"},{"location":"planning/planning_validator/autoware_planning_validator_trajectory_checker/#curvature-check","title":"Curvature Check","text":"Name Unit Type Description Default value <code>curvature.enable</code> [-] bool flag to enable/disable curvature validation check true <code>curvature.threshold</code> [1/m] double max valid value for the trajectory curvature 2.0 <code>curvature.handling_type</code> [-] int specify handling type for invalid curvature (optional parameter, if not specified will use default of planning validator) unspecified <code>curvature.override_error_diag</code> [-] bool if true, will override error diag from other checks (optional parameter, if not specified will assume FALSE) unspecified"},{"location":"planning/planning_validator/autoware_planning_validator_trajectory_checker/#relative-angle-check","title":"Relative Angle Check","text":"Name Unit Type Description Default value <code>relative_angle.enable</code> [-] bool flag to enable/disable relative angle validation check true <code>relative_angle.threshold</code> [rad] double max valid angle difference between two consecutive trajectory points 2.0 <code>relative_angle.handling_type</code> [-] int specify handling type for invalid relative_angle (optional parameter, if not specified will use default of planning validator) unspecified <code>relative_angle.override_error_diag</code> [-] bool if true, will override error diag from other checks (optional parameter, if not specified will assume FALSE) unspecified"},{"location":"planning/planning_validator/autoware_planning_validator_trajectory_checker/#lateral-acceleration-check","title":"Lateral Acceleration Check","text":"Name Unit Type Description Default value <code>acceleration.enable</code> [-] bool flag to enable/disable lateral acceleration validation check true <code>acceleration.threshold</code> [m/ss] double max valid value for the lateral acceleration along the trajectory 9.8 <code>acceleration.handling_type</code> [-] int specify handling type for invalid acceleration (optional parameter, if not specified will use default of planning validator) unspecified <code>acceleration.override_error_diag</code> [-] bool if true, will override error diag from other checks (optional parameter, if not specified will assume FALSE) unspecified"},{"location":"planning/planning_validator/autoware_planning_validator_trajectory_checker/#max-longitudinal-acceleration-check","title":"Max Longitudinal Acceleration Check","text":"Name Unit Type Description Default value <code>acceleration.enable</code> [-] bool flag to enable/disable max longitudinal acceleration validation check true <code>acceleration.threshold</code> [m/ss] double max valid value for the longitudinal acceleration along the trajectory 9.8 <code>acceleration.handling_type</code> [-] int specify handling type for invalid acceleration (optional parameter, if not specified will use default of planning validator) unspecified <code>acceleration.override_error_diag</code> [-] bool if true, will override error diag from other checks (optional parameter, if not specified will assume FALSE) unspecified"},{"location":"planning/planning_validator/autoware_planning_validator_trajectory_checker/#min-longitudinal-acceleration-check","title":"Min Longitudinal Acceleration Check","text":"Name Unit Type Description Default value <code>acceleration.enable</code> [-] bool flag to enable/disable min longitudinal acceleration validation check true <code>acceleration.threshold</code> [m/ss] double min valid value for the longitudinal acceleration along the trajectory -9.8 <code>acceleration.handling_type</code> [-] int specify handling type for invalid acceleration (optional parameter, if not specified will use default of planning validator) unspecified <code>acceleration.override_error_diag</code> [-] bool if true, will override error diag from other checks (optional parameter, if not specified will assume FALSE) unspecified"},{"location":"planning/planning_validator/autoware_planning_validator_trajectory_checker/#lateral-jerk-check","title":"Lateral Jerk Check","text":"Name Unit Type Description Default value <code>lateral_jerk.enable</code> [-] bool flag to enable/disable lateral jerk validation check true <code>lateral_jerk.threshold</code> [m/sss] double max valid value for the lateral jerk along the trajectory 7.0 <code>lateral_jerk.handling_type</code> [-] int specify handling type for invalid lateral jerk (optional parameter, if not specified will use default of planning validator) unspecified <code>lateral_jerk.override_error_diag</code> [-] bool if true, will override error diag from other checks (optional parameter, if not specified will assume FALSE) unspecified"},{"location":"planning/planning_validator/autoware_planning_validator_trajectory_checker/#steering-check","title":"Steering Check","text":"Name Unit Type Description Default value <code>steering.enable</code> [-] bool flag to enable/disable steering validation check true <code>steering.threshold</code> [rad] double max valid steering value along the trajectory 1.414 <code>steering.handling_type</code> [-] int specify handling type for invalid steering (optional parameter, if not specified will use default of planning validator) unspecified <code>steering.override_error_diag</code> [-] bool if true, will override error diag from other checks (optional parameter, if not specified will assume FALSE) unspecified"},{"location":"planning/planning_validator/autoware_planning_validator_trajectory_checker/#steering-rate-check","title":"Steering Rate Check","text":"Name Unit Type Description Default value <code>steering_rate.enable</code> [-] bool flag to enable/disable steering rate validation check true <code>steering_rate.threshold</code> [rad/s] double max valid steering rate along the trajectory 10.0 <code>steering_rate.handling_type</code> [-] int specify handling type for invalid steering rate (optional parameter, if not specified will use default of planning validator) unspecified <code>steering_rate.override_error_diag</code> [-] bool if true, will override error diag from other checks (optional parameter, if not specified will assume FALSE) unspecified"},{"location":"planning/planning_validator/autoware_planning_validator_trajectory_checker/#distance-deviation-check","title":"Distance Deviation Check","text":"Name Unit Type Description Default value <code>distance_deviation.enable</code> [-] bool flag to enable/disable distance deviation validation check true <code>distance_deviation.threshold</code> [m] double max valid lateral distance between ego and the nearest trajectory segment 100.0 <code>distance_deviation.handling_type</code> [-] int specify handling type for invalid dist deviation (optional parameter, if not specified will use default of planning validator) unspecified <code>distance_deviation.override_error_diag</code> [-] bool if true, will override error diag from other checks (optional parameter, if not specified will assume FALSE) unspecified"},{"location":"planning/planning_validator/autoware_planning_validator_trajectory_checker/#longitudinal-distance-deviation-check","title":"Longitudinal Distance Deviation Check","text":"Name Unit Type Description Default value <code>lon_distance_deviation.enable</code> [-] bool flag to enable/disable longitudinal distance deviation validation check true <code>lon_distance_deviation.threshold</code> [m] double max valid longitudinal distance between ego and nearest trajectory point 2.0 <code>lon_distance_deviation.handling_type</code> [-] int specify handling type for invalid lon. dist deviation (optional parameter, if not specified will use default of planning validator) unspecified <code>lon_distance_deviation.override_error_diag</code> [-] bool if true, will override error diag from other checks (optional parameter, if not specified will assume FALSE) unspecified"},{"location":"planning/planning_validator/autoware_planning_validator_trajectory_checker/#velocity-deviation-check","title":"Velocity Deviation Check","text":"Name Unit Type Description Default value <code>velocity_deviation.enable</code> [-] bool flag to enable/disable velocity deviation validation check true <code>velocity_deviation.threshold</code> [m/s] double max valid velocity deviation between ego and nearest trajectory point 100.0 <code>velocity_deviation.handling_type</code> [-] int specify handling type for invalid velocity deviation (optional parameter, if not specified will use default of planning validator) unspecified <code>velocity_deviation.override_error_diag</code> [-] bool if true, will override error diag from other checks (optional parameter, if not specified will assume FALSE) unspecified"},{"location":"planning/planning_validator/autoware_planning_validator_trajectory_checker/#yaw-deviation-check","title":"Yaw Deviation Check","text":"Name Unit Type Description Default value <code>yaw_deviation.enable</code> [-] bool flag to enable/disable yaw deviation validation check true <code>yaw_deviation.threshold</code> [rad] double max valid yaw deviation between ego and nearest trajectory point 1.5708 <code>yaw_deviation.th_trajectory_yaw_shift</code> [rad] double minimum change in nearest yaw value between the previous and current trajectory to trigger check 0.1 <code>yaw_deviation.handling_type</code> [-] int specify handling type for invalid yaw deviation (optional parameter, if not specified will use default of planning validator) unspecified <code>yaw_deviation.override_error_diag</code> [-] bool if true, will override error diag from other checks (optional parameter, if not specified will assume FALSE) unspecified"},{"location":"planning/planning_validator/autoware_planning_validator_trajectory_checker/#forward-trajectory-length-check","title":"Forward Trajectory Length Check","text":"Name Unit Type Description Default value <code>forward_trajectory_length.enable</code> [-] bool flag to enable/disable forward length validation check true <code>forward_trajectory_length.acceleration</code> [m/ss] double acceleration value used to calculate required trajectory length. -5.0 <code>forward_trajectory_length.margin</code> [m] double margin of the required length not to raise an error when ego slightly exceeds the end point. 2.0 <code>forward_trajectory_length.handling_type</code> [-] int specify handling type for invalid forward length (optional parameter, if not specified will use default of planning validator) unspecified <code>forward_trajectory_length.override_error_diag</code> [-] bool if true, will override error diag from other checks (optional parameter, if not specified will assume FALSE) unspecified"},{"location":"planning/planning_validator/autoware_planning_validator_trajectory_checker/#trajectory-shift-check","title":"Trajectory Shift Check","text":"Name Unit Type Description Default value <code>trajectory_shift.enable</code> [-] bool flag to enable/disable trajectory shift validation check true <code>trajectory_shift.lat_shift_th</code> [m] double max valid lateral distance between two consecutive trajectories (measured at nearest points to ego) 0.5 <code>trajectory_shift.forward_shift_th</code> [m] double max valid Longitudinal distance between two consecutive trajectories (measured at nearest point to ego) 1.0 <code>trajectory_shift.backward_shift_th</code> [m] double min valid longitudinal distance between two consecutive trajectories (measured at nearest point to ego) 0.1 <code>trajectory_shift.handling_type</code> [-] int specify handling type for invalid trajectory shift (optional parameter, if not specified will use default of planning validator) 2 <code>trajectory_shift.override_error_diag</code> [-] bool if true, will override error diag from other checks (optional parameter, if not specified will assume FALSE) true"},{"location":"planning/sampling_based_planner/autoware_bezier_sampler/","title":"B\u00e9zier sampler","text":""},{"location":"planning/sampling_based_planner/autoware_bezier_sampler/#bezier-sampler","title":"B\u00e9zier sampler","text":"<p>Implementation of b\u00e9zier curves and their generation following the sampling strategy from https://ieeexplore.ieee.org/document/8932495</p>"},{"location":"planning/sampling_based_planner/autoware_frenet_planner/","title":"Frenet planner","text":""},{"location":"planning/sampling_based_planner/autoware_frenet_planner/#frenet-planner","title":"Frenet planner","text":"<p>Trajectory generation in Frenet frame.</p>"},{"location":"planning/sampling_based_planner/autoware_frenet_planner/#description","title":"Description","text":"<p>Original paper</p>"},{"location":"planning/sampling_based_planner/autoware_path_sampler/","title":"Path Sampler","text":""},{"location":"planning/sampling_based_planner/autoware_path_sampler/#path-sampler","title":"Path Sampler","text":""},{"location":"planning/sampling_based_planner/autoware_path_sampler/#purpose","title":"Purpose","text":"<p>This package implements a node that uses sampling based planning to generate a drivable trajectory.</p>"},{"location":"planning/sampling_based_planner/autoware_path_sampler/#feature","title":"Feature","text":"<p>This package is able to:</p> <ul> <li>make the trajectory smooth;</li> <li>keep the trajectory inside the drivable area;</li> <li>avoid static obstacles;</li> <li>stop if no valid trajectory can be generated.</li> </ul> <p>Note that the velocity is just taken over from the input path.</p>"},{"location":"planning/sampling_based_planner/autoware_path_sampler/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"planning/sampling_based_planner/autoware_path_sampler/#input","title":"input","text":"Name Type Description <code>~/input/path</code> autoware_planning_msgs/msg/Path Reference path and the corresponding drivable area <code>~/input/odometry</code> nav_msgs/msg/Odometry Current state of the ego vehicle <code>~/input/objects</code> autoware_perception_msgs/msg/PredictedObjects objects to avoid"},{"location":"planning/sampling_based_planner/autoware_path_sampler/#output","title":"output","text":"Name Type Description <code>~/output/trajectory</code> autoware_planning_msgs/msg/Trajectory generated trajectory that is feasible to drive and collision-free"},{"location":"planning/sampling_based_planner/autoware_path_sampler/#algorithm","title":"Algorithm","text":"<p>Sampling based planning is decomposed into 3 successive steps:</p> <ol> <li>Sampling: candidate trajectories are generated.</li> <li>Pruning: invalid candidates are discarded.</li> <li>Selection: the best remaining valid candidate is selected.</li> </ol>"},{"location":"planning/sampling_based_planner/autoware_path_sampler/#sampling","title":"Sampling","text":"<p>Candidate trajectories are generated based on the current ego state and some target state. 2 sampling algorithms are currently implemented: sampling with b\u00e9zier curves or with polynomials in the frenet frame.</p>"},{"location":"planning/sampling_based_planner/autoware_path_sampler/#pruning","title":"Pruning","text":"<p>The validity of each candidate trajectory is checked using a set of hard constraints.</p> <ul> <li>collision: ensure no collision with static obstacles;</li> <li>curvature: ensure smooth curvature;</li> <li>drivable area: ensure the trajectory stays within the drivable area.</li> </ul>"},{"location":"planning/sampling_based_planner/autoware_path_sampler/#selection","title":"Selection","text":"<p>Among the valid candidate trajectories, the best one is determined using a set of soft constraints (i.e., objective functions).</p> <ul> <li>curvature: prefer smoother trajectories;</li> <li>length: prefer longer trajectories;</li> <li>lateral deviation: prefer trajectories close to the reference path.</li> </ul> <p>Each soft constraint is associated with a weight to allow tuning of the preferences.</p>"},{"location":"planning/sampling_based_planner/autoware_path_sampler/#limitations","title":"Limitations","text":"<p>The quality of the candidates generated with polynomials in frenet frame greatly depend on the reference path. If the reference path is not smooth, the resulting candidates will probably be undriveable.</p> <p>Failure to find a valid trajectory current results in a suddenly stopping trajectory.</p>"},{"location":"planning/sampling_based_planner/autoware_path_sampler/#comparison-with-the-autoware_path_optimizer","title":"Comparison with the <code>autoware_path_optimizer</code>","text":"<p>The <code>autoware_path_optimizer</code> uses an optimization based approach, finding the optimal solution of a mathematical problem if it exists. When no solution can be found, it is often hard to identify the issue due to the intermediate mathematical representation of the problem.</p> <p>In comparison, the sampling based approach cannot guarantee an optimal solution but is much more straightforward, making it easier to debug and tune.</p>"},{"location":"planning/sampling_based_planner/autoware_path_sampler/#how-to-tune-parameters","title":"How to Tune Parameters","text":"<p>The sampling based planner mostly offers a trade-off between the consistent quality of the trajectory and the computation time. To guarantee that a good trajectory is found requires generating many candidates which linearly increases the computation time.</p> <p>TODO</p>"},{"location":"planning/sampling_based_planner/autoware_path_sampler/#drivability-in-narrow-roads","title":"Drivability in narrow roads","text":""},{"location":"planning/sampling_based_planner/autoware_path_sampler/#computation-time","title":"Computation time","text":""},{"location":"planning/sampling_based_planner/autoware_path_sampler/#robustness","title":"Robustness","text":""},{"location":"planning/sampling_based_planner/autoware_path_sampler/#other-options","title":"Other options","text":""},{"location":"planning/sampling_based_planner/autoware_path_sampler/#how-to-debug","title":"How To Debug","text":"<p>TODO</p>"},{"location":"planning/sampling_based_planner/autoware_sampler_common/","title":"Sampler Common","text":""},{"location":"planning/sampling_based_planner/autoware_sampler_common/#sampler-common","title":"Sampler Common","text":"<p>Common functions for sampling based planners. This includes classes for representing paths and trajectories, hard and soft constraints, conversion between cartesian and frenet frames, ...</p>"},{"location":"sensing/autoware_calibration_status_classifier/","title":"autoware_calibration_status_classifier","text":""},{"location":"sensing/autoware_calibration_status_classifier/#autoware_calibration_status_classifier","title":"autoware_calibration_status_classifier","text":""},{"location":"sensing/autoware_calibration_status_classifier/#purpose","title":"Purpose","text":"<p>The <code>autoware_calibration_status_classifier</code> package provides real-time LiDAR-camera calibration validation using deep learning inference. It detects miscalibration between LiDAR and camera sensors by analyzing projected point clouds overlaid on camera images through a neural network-based approach.</p>"},{"location":"sensing/autoware_calibration_status_classifier/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The calibration status detection system operates through the following pipeline:</p>"},{"location":"sensing/autoware_calibration_status_classifier/#1-data-preprocessing-cuda-accelerated","title":"1. Data Preprocessing (CUDA-accelerated)","text":"<ul> <li>Image Undistortion: Corrects camera distortion</li> <li>Point Cloud Projection: Projects 3D LiDAR points onto undistorted 2D image plane - adds depth and intensity information</li> <li>Morphological Dilation: Enhances point visibility for neural network input</li> </ul>"},{"location":"sensing/autoware_calibration_status_classifier/#2-neural-network-inference-tensorrt","title":"2. Neural Network Inference (TensorRT)","text":"<ul> <li>Input Format: 5-channel normalized data (RGB + depth + intensity)</li> <li>Architecture: Deep neural network trained on calibrated/miscalibrated data</li> <li>Output: Binary classification with confidence scores for calibration status</li> </ul>"},{"location":"sensing/autoware_calibration_status_classifier/#3-runtime-modes","title":"3. Runtime Modes","text":"<ul> <li>MANUAL: On-demand validation via service calls</li> <li>PERIODIC: Regular validation at configurable intervals</li> <li>ACTIVE: Continuous monitoring with synchronized sensor data</li> </ul>"},{"location":"sensing/autoware_calibration_status_classifier/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"sensing/autoware_calibration_status_classifier/#input","title":"Input","text":"Name Type Description <code>~/input/velocity</code> <code>prerequisite.velocity_source</code> parameter Vehicle velocity (multiple message types supported) <code>input.cloud_topics</code> <code>sensor_msgs::msg::PointCloud2</code> LiDAR point cloud data <code>input.image_topics</code> <code>sensor_msgs::msg::Image</code> Camera image data (BGR8 format) Camera info topics <code>sensor_msgs::msg::CameraInfo</code> Camera intrinsic parameters and distortion coefficients"},{"location":"sensing/autoware_calibration_status_classifier/#output","title":"Output","text":"Name Type Description <code>/diagnostics</code> <code>diagnostic_msgs::msg::DiagnosticArray</code> ROS diagnostics with calibration status <code>~/validate_calibration_srv</code> <code>std_srvs::srv::Trigger</code> Manual validation service (MANUAL mode) Preview image topics <code>sensor_msgs::msg::Image</code> Visualization images with projected points"},{"location":"sensing/autoware_calibration_status_classifier/#services","title":"Services","text":"Name Type Description <code>~/input/validate_calibration_srv</code> <code>std_srvs::srv::Trigger</code> Manual calibration validation request"},{"location":"sensing/autoware_calibration_status_classifier/#parameters","title":"Parameters","text":""},{"location":"sensing/autoware_calibration_status_classifier/#node-parameters","title":"Node Parameters","text":"Name Type Description Default Range trt_precision string A precision of TensorRT engine. fp16 ['fp16', 'fp32'] cloud_capacity integer Capacity of the point cloud buffer (should be set to at least the maximum theoretical number of points). 2000000 \u22651 onnx_path string Path to the ONNX model file used for calibration status verification. \\((var model_path)/\\)(var model_name).onnx N/A runtime_mode string The mode of calibration status node, which determines when the calibration status verification is performed. 'manual' triggers by service, 'periodic' runs at a fixed interval, and 'active' runs continuously. active ['manual', 'periodic', 'active'] period float The period duration in seconds. For manual mode it determines the timeout duration for the service request. For periodic mode it determines how often the calibration status is verified. 5.0 N/A queue_size integer The queue size in synchronized callbacks. 5 N/A input.cloud_topics array List of LiDAR topics to subscribe for calibration status verification. If only a single LiDAR topic is specified with multiple camera topics, all the camera topics will be validated against that single LiDAR topic. If multiple LiDAR topics are specified with multiple camera topics, each camera topic will be validated against the corresponding LiDAR topic in the list (both lists size must match). If multiple LiDAR topics are specified with a single camera topic, the single camera topic will be validated against all the LiDAR topics in the list. ['/sensing/lidar/concatenated/pointcloud'] N/A input.image_topics array List of camera topics to subscribe for calibration status verification. Same rules as for cloud_topics apply here. ['/sensing/camera/camera8/image_raw'] N/A input.approx_deltas array The approximate time deltas in seconds for each topic pair (LiDAR and camera) for Approximate Time Synchronization Policy. The length of this array must be equal to maximum length of cloud_topics and image_topics arrays or be a single value which will be applied to all topic pairs. ['0.1'] N/A input.already_rectified array List of flags indicating whether each camera topic provides already rectified images. The length of this array must be equal to maximum length of image_topics array or be a single value which will be applied to all camera topics. [True] N/A prerequisite.check_velocity boolean Flag to enable or disable velocity check for calibration status verification. If enabled, the node will subscribe to the specified velocity source topics and verify that the vehicle is moving at maximum velocity defined by velocity_threshold. True N/A prerequisite.velocity_source string The msg type to subscribe for velocity check. Available options are 'twist', 'twist_stamped', 'twist_with_cov', 'twist_with_cov_stamped', 'odometry'. twist_with_cov_stamped ['twist', 'twist_with_cov', 'twist_stamped', 'twist_with_cov_stamped', 'odometry'] prerequisite.velocity_threshold float The maximum velocity threshold in meters per second for the vehicle to be considered moving. If the vehicle's velocity is below this threshold, the calibration status verification will not be performed. 5.0 N/A prerequisite.check_objects boolean Flag to enable or disable object check for calibration status verification. If enabled, the node will subscribe to the specified object source topics and verify that the number of objects is within the defined limit. True N/A prerequisite.objects_limit integer The maximum number of objects allowed for the vehicle to be considered in a valid state for calibration status verification. If the number of detected objects exceeds this limit, the calibration status verification will not be performed. 100 \u22650 miscalibration_confidence_threshold float The confidence threshold for determining miscalibration. Positive values shifts the decision boundary towards detecting miscalibration, while negative values shifts it towards detecting proper calibration. A value of 0.0 means that the decision is based solely on the model's output without any bias. 0.0 N/A"},{"location":"sensing/autoware_calibration_status_classifier/#network-parameters","title":"Network Parameters","text":"Name Type Description Default Range max_depth float Maximum depth for projected LiDAR points in the camera frame. Points with depth values greater than this are considered invalid. 128.0 N/A dilation_size integer The size of the dilation kernel used for point cloud processing, 1 means 3x3 kernel, 2 means 5x5 kernel, etc. 1 N/A height array The height of the input image for optimization profile of the ML model, specified as [min, opt, max]. [1080, 1860, 2160] N/A width array The width of the input image for the optimization profile of the ML model, specified as [min, opt, max]. [1920, 2880, 3840] N/A"},{"location":"sensing/autoware_calibration_status_classifier/#assumptions-known-limits","title":"Assumptions / Known Limits","text":"<ul> <li>Input images must be in BGR8 format (8-bit per channel)</li> <li>Input point clouds should contain intensity information (XYZIRC format)</li> </ul>"},{"location":"sensing/autoware_calibration_status_classifier/#usage-example","title":"Usage Example","text":"<pre><code>ros2 launch autoware_calibration_status_classifier calibration_status_classifier.launch.xml\n</code></pre>"},{"location":"sensing/autoware_calibration_status_classifier/#future-extensions-unimplemented-parts","title":"Future Extensions / Unimplemented Parts","text":"<ul> <li>Manual runtime mode with detailed response (custom srv)</li> <li>Replace filter for objects on the scene counter to objects within the camera FOV counter (raytracing)</li> <li>Multithreading for multiple camera-LiDAR pairs</li> <li>More filters (e.g. yaw rate)</li> <li>cuda_blackboard support</li> <li>Replace custom kernels with NPP functions where applicable</li> </ul>"},{"location":"sensing/autoware_calibration_status_classifier/#references","title":"References","text":"<ul> <li>AWML - Calibration Status Classification</li> </ul>"},{"location":"sensing/autoware_cuda_pointcloud_preprocessor/","title":"autoware_cuda_pointcloud_preprocessor","text":""},{"location":"sensing/autoware_cuda_pointcloud_preprocessor/#autoware_cuda_pointcloud_preprocessor","title":"autoware_cuda_pointcloud_preprocessor","text":""},{"location":"sensing/autoware_cuda_pointcloud_preprocessor/#purpose","title":"Purpose","text":"<p>The pointcloud preprocessing implemented in <code>autoware_pointcloud_preprocessor</code> has been thoroughly tested in autoware. However, the latency it introduces does not scale well with modern LiDAR devices due to the high number of points they introduce.</p> <p>To alleviate this issue, this package reimplements most of the pipeline presented in <code>autoware_pointcloud_preprocessor</code> leveraging the use of GPGPUs. In particular, this package makes use of CUDA to provide accelerated versions of the already established implementations, while also maintaining compatibility with normal ROS nodes/topics. </p>"},{"location":"sensing/autoware_cuda_pointcloud_preprocessor/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>A detailed description of each filter's algorithm is available in the following links.</p> Filter Name Description Detail cuda_pointcloud_preprocessor Implements the cropping, distortion correction, and outlier filtering (ring-based) of the <code>autoware_pointcloud_preprocessor</code>'s CPU versions. link cuda_concatenate_and_time_sync_node Implements pointcloud concatenation an synchronization following <code>autoware_pointcloud_preprocessor</code>'s CPU implementation. link cuda_voxel_grid_downsample_filter Implements voxel downsample filtering of the <code>autoware_pointcloud_preprocessor</code>'s CPU version link cuda_polar_voxel_outlier_filter Implements polar voxel outlier filtering of the <code>autoware_pointcloud_preprocessor</code>'s CPU version link"},{"location":"sensing/autoware_cuda_pointcloud_preprocessor/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":"<p>The subsample filters implemented in <code>autoware_pointcloud_preprocessor</code> will have similar counterparts in this package.</p>"},{"location":"sensing/autoware_cuda_pointcloud_preprocessor/docs/cuda-concatenate-data/","title":"cuda_concatenate_and_time_synchronize_node","text":""},{"location":"sensing/autoware_cuda_pointcloud_preprocessor/docs/cuda-concatenate-data/#cuda_concatenate_and_time_synchronize_node","title":"cuda_concatenate_and_time_synchronize_node","text":"<p>This package is a cuda accelerated version of the one available in autoware_cuda_pointcloud_preprocessor. As this node is templated, the overall design, algorithm, inputs, and outputs are the same.</p> <p>The only change, corresponds to the pointcloud topics, which instead of using the standard <code>sensor_msgs::msg::PointCloud2</code> message type, they use the <code>cuda_blackboard</code> mechanism.</p>"},{"location":"sensing/autoware_cuda_pointcloud_preprocessor/docs/cuda-pointcloud-preprocessor/","title":"Macro Rendering Error","text":""},{"location":"sensing/autoware_cuda_pointcloud_preprocessor/docs/cuda-pointcloud-preprocessor/#macro-rendering-error","title":"Macro Rendering Error","text":"<p>File: <code>sensing/autoware_cuda_pointcloud_preprocessor/docs/cuda-pointcloud-preprocessor.md</code></p> <p>FileNotFoundError: [Errno 2] No such file or directory: 'sensing/autoware_cuda_pointcloud_preprocessor/schema/cuda_pointcloud_preprocessor.schema.schema.json'</p> <pre><code>Traceback (most recent call last):\n  File \"/opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages/mkdocs_macros/plugin.py\", line 703, in render\n    return md_template.render(**page_variables)\n  File \"/opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages/jinja2/environment.py\", line 1295, in render\n    self.environment.handle_exception()\n  File \"/opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages/jinja2/environment.py\", line 942, in handle_exception\n    raise rewrite_traceback_stack(source=source)\n  File \"&lt;template&gt;\", line 40, in top-level template code\n  File \"/home/runner/work/autoware.universe/autoware.universe/mkdocs_macros.py\", line 71, in json_to_markdown\n    with open(json_schema_file_path) as f:\nFileNotFoundError: [Errno 2] No such file or directory: 'sensing/autoware_cuda_pointcloud_preprocessor/schema/cuda_pointcloud_preprocessor.schema.schema.json'\n</code></pre>"},{"location":"sensing/autoware_cuda_pointcloud_preprocessor/docs/cuda-polar-voxel-outlier-filter/","title":"cuda_polar_voxel_outlier_filter","text":""},{"location":"sensing/autoware_cuda_pointcloud_preprocessor/docs/cuda-polar-voxel-outlier-filter/#cuda_polar_voxel_outlier_filter","title":"cuda_polar_voxel_outlier_filter","text":""},{"location":"sensing/autoware_cuda_pointcloud_preprocessor/docs/cuda-polar-voxel-outlier-filter/#purpose","title":"Purpose","text":"<p>This node is a CUDA accelerated version of the <code>PolarVoxelOutlierFilter</code> available in autoware_cuda_pointcloud_preprocessor.</p>"},{"location":"sensing/autoware_cuda_pointcloud_preprocessor/docs/cuda-polar-voxel-outlier-filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>This node is an alternative implementation to <code>autoware::pointcloud_preprocessor::PolarVoxelOutlierFilterComponent</code>, which filters outliers based on voxels in polar coordinate space instead of Cartesian coordinate space.</p>"},{"location":"sensing/autoware_cuda_pointcloud_preprocessor/docs/cuda-polar-voxel-outlier-filter/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"sensing/autoware_cuda_pointcloud_preprocessor/docs/cuda-polar-voxel-outlier-filter/#input","title":"Input","text":"Name Type Description <code>~/input/pointcloud</code> <code>sensor_msgs::msg::PointCloud2</code> Input pointcloud's topic. <code>~/input/pointcloud/cuda</code> <code>negotiated_interfaces/msg/NegotiatedTopicsInfo</code> Input pointcloud's type negotiation topic"},{"location":"sensing/autoware_cuda_pointcloud_preprocessor/docs/cuda-polar-voxel-outlier-filter/#output","title":"Output","text":"Name Type Description <code>~/output/pointcloud</code> <code>sensor_msgs::msg::PointCloud2</code> Processed pointcloud's topic <code>~/output/pointcloud/cuda</code> <code>negotiated_interfaces/msg/NegotiatedTopicsInfo</code> Processed pointcloud's negotiation topic"},{"location":"sensing/autoware_cuda_pointcloud_preprocessor/docs/cuda-polar-voxel-outlier-filter/#additional-debug-topics","title":"Additional Debug Topics","text":"Name Type Description <code>~/debug/filter_ratio</code> <code>autoware_internal_debug_msgs::msg::Float32Stamped</code> Ratio of output to input points <code>~/debug/visibility</code> <code>autoware_internal_debug_msgs::msg::Float32Stamped</code> Ratio of voxels passing secondary return threshold test (PointXYZIRCAEDT only) <code>~/debug/pointcloud_noise</code> <code>sensor_msgs::msg::PointCloud2</code> Processed pointcloud's topic which is categorized as outlier <code>~/debug/pointcloud_noise/cuda</code> <code>negotiated_interfaces/msg/NegotiatedTopicsInfo</code> Negotiation topic"},{"location":"sensing/autoware_cuda_pointcloud_preprocessor/docs/cuda-polar-voxel-outlier-filter/#parameters","title":"Parameters","text":"<p>See the original implementation in autoware_cuda_pointcloud_preprocessor for the detail.</p>"},{"location":"sensing/autoware_cuda_pointcloud_preprocessor/docs/cuda-polar-voxel-outlier-filter/#core-parameters-schema-based","title":"Core Parameters (Schema-based)","text":"Name Type Description Default Range radial_resolution_m float The resolution in radial direction [m]. 0.5 \u22650 azimuth_resolution_rad float The resolution in azimuth direction [rad]. The supplied value is modified to ensure consistent voxel sizes across the whole azimuth range. 0.0175 \u22650\u22646.283185307179586 elevation_resolution_rad float The resolution in elevation direction [rad]. The supplied value is modified to ensure consistent voxel sizes across the whole elevation range. 0.0175 \u22650\u22646.283185307179586 voxel_points_threshold integer The minimum number of points required per voxel. 2 \u22651 min_radius_m float The minimum radius to consider [m]. 0.5 \u22650 max_radius_m float The maximum radius to consider [m]. 300.0 \u22650 intensity_threshold integer The maximum intensity threshold for secondary returns (primary returns are not affected). 2 \u22650\u2264255 visibility_estimation_max_range_m float The maximum range to consider for visibility estimation [m] (limits visibility calculations to reliable sensor range). 20.0 \u22650 visibility_estimation_min_azimuth_rad float The minimum range to consider for visibility estimation [rad] (limits visibility calculations to reliable sensor range). 0.0 \u22650.0\u22646.28 visibility_estimation_max_azimuth_rad float The maximum range to consider for visibility estimation [rad] (limits visibility calculations to reliable sensor range). 6.28 \u22650.0\u22646.28 visibility_estimation_min_elevation_rad float The minimum range to consider for visibility estimation [rad] (limits visibility calculations to reliable sensor range). -1.57 \u2265-1.57\u22641.57 visibility_estimation_max_elevation_rad float The maximum range to consider for visibility estimation [rad] (limits visibility calculations to reliable sensor range). 1.57 \u2265-1.57\u22641.57 use_return_type_classification boolean Whether to use return type classification (if false, all returns are processed). true N/A filter_secondary_returns boolean Whether to filter secondary returns (if true, only primary returns are kept). false N/A secondary_noise_threshold float Threshold for classifying primary vs secondary returns. 4 \u22650 visibility_estimation_max_secondary_voxel_count integer Maximum secondary voxel count for visibility estimation calculations. 500 \u22650 primary_return_types array List of return type values considered as primary returns (all others are secondary). [1, 6, 8, 10] N/A visibility_estimation_only boolean Whether to run filter for visibility estimation only without publishing point cloud outputs (for monitoring/diagnostic purposes only). false N/A publish_noise_cloud boolean Whether to generate and publish noise point cloud for debugging (ignored when visibility_estimation_only=true). false N/A filter_ratio_error_threshold float The filter ratio threshold for error diagnostics (ratio of output/input points). 0.5 \u22650\u22641 filter_ratio_warn_threshold float The filter ratio threshold for warning diagnostics (ratio of output/input points). 0.7 \u22650\u22641 visibility_error_threshold float The visibility threshold for error diagnostics. 0.8 \u22650\u22641 visibility_warn_threshold float The visibility threshold for warning diagnostics. 0.9 \u22650\u22641 publish_area_marker boolean Publish a marker to visualize region used for visibility estimation false N/A num_frames_hysteresis_transition float The number of frames to be required to transition judgement 1 \u22651 immediate_report_error boolean Whether to report error state immediately if a single frame categorized as error false N/A immediate_relax_state boolean Whether to relax state if observed state is better than the current one false N/A"},{"location":"sensing/autoware_cuda_pointcloud_preprocessor/docs/cuda-polar-voxel-outlier-filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>Due to differences in floating-point arithmetic between CPUs and GPUs, the outputs of <code>autoware::pointcloud_preprocessor::PolarVoxelOutlierFilterComponent</code> and this filter may not be identical.</p> <p>Adding compiler options, such as the following, can reduce numerical discrepancies, though a slight performance impact can also be introduced, and it is still difficult to acquire complete identical results.</p> <pre><code>list(APPEND CUDA_NVCC_FLAGS \"--fmad=false\")\n</code></pre> <p>To prioritize performance, these compiler options are not enabled.</p>"},{"location":"sensing/autoware_cuda_pointcloud_preprocessor/docs/cuda-voxel-grid-downsample/","title":"Macro Rendering Error","text":""},{"location":"sensing/autoware_cuda_pointcloud_preprocessor/docs/cuda-voxel-grid-downsample/#macro-rendering-error","title":"Macro Rendering Error","text":"<p>File: <code>sensing/autoware_cuda_pointcloud_preprocessor/docs/cuda-voxel-grid-downsample.md</code></p> <p>FileNotFoundError: [Errno 2] No such file or directory: 'sensing/autoware_cuda_pointcloud_preprocessor/schema/cuda_voxel_grid_downsample_filter.schema.schema.json'</p> <pre><code>Traceback (most recent call last):\n  File \"/opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages/mkdocs_macros/plugin.py\", line 703, in render\n    return md_template.render(**page_variables)\n  File \"/opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages/jinja2/environment.py\", line 1295, in render\n    self.environment.handle_exception()\n  File \"/opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages/jinja2/environment.py\", line 942, in handle_exception\n    raise rewrite_traceback_stack(source=source)\n  File \"&lt;template&gt;\", line 31, in top-level template code\n  File \"/home/runner/work/autoware.universe/autoware.universe/mkdocs_macros.py\", line 71, in json_to_markdown\n    with open(json_schema_file_path) as f:\nFileNotFoundError: [Errno 2] No such file or directory: 'sensing/autoware_cuda_pointcloud_preprocessor/schema/cuda_voxel_grid_downsample_filter.schema.schema.json'\n</code></pre>"},{"location":"sensing/autoware_cuda_utils/","title":"autoware_cuda_utils","text":""},{"location":"sensing/autoware_cuda_utils/#autoware_cuda_utils","title":"autoware_cuda_utils","text":""},{"location":"sensing/autoware_cuda_utils/#purpose","title":"Purpose","text":"<p>This package contains a library of common functions related to CUDA.</p>"},{"location":"sensing/autoware_image_diagnostics/","title":"image_diagnostics","text":""},{"location":"sensing/autoware_image_diagnostics/#image_diagnostics","title":"image_diagnostics","text":""},{"location":"sensing/autoware_image_diagnostics/#purpose","title":"Purpose","text":"<p>The <code>image_diagnostics</code> is a node that check the status of the input raw image.</p>"},{"location":"sensing/autoware_image_diagnostics/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>Below figure shows the flowchart of image diagnostics node. Each image is divided into small blocks for block state assessment.</p> <p></p> <p>Each small image block state is assessed as below figure.</p> <p></p> <p>After all image's blocks state are evaluated, the whole image status is summarized as below.</p> <p></p>"},{"location":"sensing/autoware_image_diagnostics/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"sensing/autoware_image_diagnostics/#input","title":"Input","text":"Name Type Description <code>input/raw_image</code> <code>sensor_msgs::msg::Image</code> raw image"},{"location":"sensing/autoware_image_diagnostics/#output","title":"Output","text":"Name Type Description <code>image_diag/debug/gray_image</code> <code>sensor_msgs::msg::Image</code> gray image <code>image_diag/debug/dft_image</code> <code>sensor_msgs::msg::Image</code> discrete Fourier transformation image <code>image_diag/debug/diag_block_image</code> <code>sensor_msgs::msg::Image</code> each block state colorization <code>image_diag/image_state_diag</code> <code>autoware_internal_debug_msgs::msg::Int32Stamped</code> image diagnostics status value <code>/diagnostics</code> <code>diagnostic_msgs::msg::DiagnosticArray</code> diagnostics"},{"location":"sensing/autoware_image_diagnostics/#parameters","title":"Parameters","text":""},{"location":"sensing/autoware_image_diagnostics/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<ul> <li>This is proof of concept for image diagnostics and the algorithms still under further improvement.</li> </ul>"},{"location":"sensing/autoware_image_diagnostics/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"sensing/autoware_image_diagnostics/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"sensing/autoware_image_diagnostics/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"sensing/autoware_image_diagnostics/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":"<ul> <li>Consider more specific image distortion/occlusion type, for instance raindrop or dust.</li> </ul> <ul> <li>Consider degraded visibility under fog or rain condition from optical point of view</li> </ul>"},{"location":"sensing/autoware_image_transport_decompressor/","title":"image_transport_decompressor","text":""},{"location":"sensing/autoware_image_transport_decompressor/#image_transport_decompressor","title":"image_transport_decompressor","text":""},{"location":"sensing/autoware_image_transport_decompressor/#purpose","title":"Purpose","text":"<p>The <code>image_transport_decompressor</code> is a node that decompresses images.</p>"},{"location":"sensing/autoware_image_transport_decompressor/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"sensing/autoware_image_transport_decompressor/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"sensing/autoware_image_transport_decompressor/#input","title":"Input","text":"Name Type Description <code>~/input/compressed_image</code> <code>sensor_msgs::msg::CompressedImage</code> compressed image"},{"location":"sensing/autoware_image_transport_decompressor/#output","title":"Output","text":"Name Type Description <code>~/output/raw_image</code> <code>sensor_msgs::msg::Image</code> decompressed image"},{"location":"sensing/autoware_image_transport_decompressor/#parameters","title":"Parameters","text":"Name Type Description Default Range encoding string The image encoding to use for the decompressed image default N/A"},{"location":"sensing/autoware_image_transport_decompressor/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"sensing/autoware_image_transport_decompressor/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"sensing/autoware_image_transport_decompressor/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"sensing/autoware_image_transport_decompressor/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"sensing/autoware_image_transport_decompressor/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"sensing/autoware_imu_corrector/","title":"autoware_imu_corrector","text":""},{"location":"sensing/autoware_imu_corrector/#autoware_imu_corrector","title":"autoware_imu_corrector","text":""},{"location":"sensing/autoware_imu_corrector/#imu_corrector","title":"imu_corrector","text":"<p><code>imu_corrector_node</code> is a node that correct imu data.</p> <ol> <li>Correct yaw rate offset \\(b\\) by reading the parameter.</li> <li>Correct yaw rate standard deviation \\(\\sigma\\) by reading the parameter.</li> <li>Correct yaw rate scale \\(s\\) using NDT pose as a ground truth.</li> </ol> <p>Mathematically, we assume the following equation:</p> \\[ \\tilde{\\omega}(t) = s(t) * \\omega(t) + b(t) + n(t) \\] <p>where \\(\\tilde{\\omega}\\) denotes observed angular velocity, \\(\\omega\\) denotes true angular velocity, \\(b\\) denotes an offset, \\(s\\) denotes the scale, and \\(n\\) denotes a gaussian noise. We also assume that \\(n\\sim\\mathcal{N}(0, \\sigma^2)\\).</p>"},{"location":"sensing/autoware_imu_corrector/#input","title":"Input","text":"Name Type Description <code>~input</code> <code>sensor_msgs::msg::Imu</code> raw imu data"},{"location":"sensing/autoware_imu_corrector/#output","title":"Output","text":"Name Type Description <code>~output</code> <code>sensor_msgs::msg::Imu</code> corrected imu data"},{"location":"sensing/autoware_imu_corrector/#parameters","title":"Parameters","text":"Name Type Description <code>angular_velocity_offset_x</code> double roll rate offset in imu_link [rad/s] <code>angular_velocity_offset_y</code> double pitch rate offset imu_link [rad/s] <code>angular_velocity_offset_z</code> double yaw rate offset imu_link [rad/s] <code>angular_velocity_stddev_xx</code> double roll rate standard deviation imu_link [rad/s] <code>angular_velocity_stddev_yy</code> double pitch rate standard deviation imu_link [rad/s] <code>angular_velocity_stddev_zz</code> double yaw rate standard deviation imu_link [rad/s] <code>acceleration_stddev</code> double acceleration standard deviation imu_link [m/s^2] <p>Note: The angular velocity offset values introduce a fixed compensation that is not considered in the gyro bias estimation. If the <code>on_off_correction.correct_for_dynamic_bias</code> flag and the <code>on_off_correction.correct_for_static_bias</code> flags are enabled, automatically the <code>correct_for_static_bias</code> will be disabled to avoid correction errors.</p>"},{"location":"sensing/autoware_imu_corrector/#gyro_bias_estimator","title":"gyro_bias_estimator","text":"<p><code>gyro_bias_validator</code> is a node that validates the bias of the gyroscope. It subscribes to the <code>sensor_msgs::msg::Imu</code> topic and validate if the bias of the gyroscope is within the specified range.</p> <p>Note that the node calculates bias from the gyroscope data by averaging the data only when the vehicle is stopped.</p>"},{"location":"sensing/autoware_imu_corrector/#input_1","title":"Input","text":"Name Type Description <code>~/input/imu_raw</code> <code>sensor_msgs::msg::Imu</code> raw imu data <code>~/input/pose</code> <code>geometry_msgs::msg::PoseWithCovarianceStamped</code> ndt pose <code>~/input/odometry</code> <code>nav_msgs::msg::Odometry</code> odometry data <p>Note that the input pose is assumed to be accurate enough. For example when using NDT, we assume that the NDT is appropriately converged.</p> <p>Currently, it is possible to use methods other than NDT as a <code>pose_source</code> for Autoware, but less accurate methods are not suitable for IMU bias estimation.</p> <p>In the future, with careful implementation for pose errors, the IMU bias estimated by NDT could potentially be used not only for validation but also for online calibration.</p> <p>The Extended Kalman Filter (EKF) is used for scale estimation. The NDT pose is used as ground truth, and we assume it's accurate enough to provide long-term convergence for the correct scale observation.</p>"},{"location":"sensing/autoware_imu_corrector/#output_1","title":"Output","text":"Name Type Description <code>~/output/gyro_bias</code> <code>geometry_msgs::msg::Vector3Stamped</code> bias of the gyroscope [rad/s] <code>~/output/gyro_scale</code> <code>geometry_msgs::msg::Vector3Stamped</code> estimated scale of the gyroscope [no units]"},{"location":"sensing/autoware_imu_corrector/#parameters-bias-estimation","title":"Parameters (Bias estimation)","text":"<p>Note that this node also uses <code>angular_velocity_offset_x</code>, <code>angular_velocity_offset_y</code>, <code>angular_velocity_offset_z</code> parameters from <code>imu_corrector.param.yaml</code>.</p> Name Type Description <code>gyro_bias_threshold</code> double threshold of the bias of the gyroscope [rad/s] <code>timer_callback_interval_sec</code> double seconds about the timer callback function [sec] <code>diagnostics_updater_interval_sec</code> double period of the diagnostics updater [sec] <code>straight_motion_ang_vel_upper_limit</code> double upper limit of yaw angular velocity, beyond which motion is not considered straight [rad/s]"},{"location":"sensing/autoware_imu_corrector/#parameters-scale-estimation","title":"Parameters (Scale estimation)","text":"Name Type Description <code>estimate_scale_init</code> double Initial value for scale estimation <code>min_allowed_scale</code> double Minimum allowed scale value <code>max_allowed_scale</code> double Maximum allowed scale value <code>threshold_to_estimate_scale</code> double Minimum yaw rate required to estimate scale <code>percentage_scale_rate_allow_correct</code> double Allowed percentage change with respect to current scale for correction <code>alpha</code> double Filter coefficient for scale (complementary filter) <code>delay_gyro_ms</code> int Delay applied to gyro data in milliseconds <code>samples_filter_pose_rate</code> int Number of samples for pose rate filtering <code>samples_filter_gyro_rate</code> int Number of samples for gyro rate filtering <code>alpha_gyro</code> double Filter coefficient for gyro rate <code>buffer_size_gyro</code> int Buffer size for gyro data <code>alpha_ndt_rate</code> double Filter coefficient for NDT rate <code>ekf_rate.max_variance_p</code> double Maximum allowed variance for EKF rate estimation <code>ekf_rate.variance_p_after</code> double Variance after initialization for EKF rate estimation <code>ekf_rate.process_noise_q</code> double Process noise for EKF rate estimation <code>ekf_rate.process_noise_q_after</code> double Process noise after initialization for EKF rate estimation <code>ekf_rate.measurement_noise_r</code> double Measurement noise for EKF rate estimation <code>ekf_rate.measurement_noise_r_after</code> double Measurement noise after initialization for EKF rate estimation <code>ekf_rate.samples_to_init</code> int Number of samples to initialize EKF rate estimation <code>ekf_rate.min_covariance</code> double Minimum covariance for EKF rate estimation <code>ekf_angle.process_noise_q_angle</code> double Process noise for EKF angle estimation <code>ekf_angle.variance_p_angle</code> double Initial variance for EKF angle estimation <code>ekf_angle.measurement_noise_r_angle</code> double Measurement noise for EKF angle estimation <code>ekf_angle.min_covariance_angle</code> double Minimum covariance for EKF angle estimation <code>ekf_angle.decay_coefficient</code> double Decay coefficient for EKF angle estimation"},{"location":"sensing/autoware_imu_corrector/#imu-scalebias-injection","title":"IMU scale/bias injection","text":"<p>In order to test the result of the scale and bias estimation for the gyro, an optional artificial scale and bias can be injected into the raw IMU data using the parameters below. The IMU scale can be observed through an output that can be remapped to be the input of the 'imu_corrector'.</p>"},{"location":"sensing/autoware_imu_corrector/#output_2","title":"Output","text":"Name Type Description <code>~/output/imu_scaled</code> <code>sensor_msgs::msg::Imu</code> IMU data after scale correction"},{"location":"sensing/autoware_imu_corrector/#parameters_1","title":"Parameters","text":"Name Type Description <code>modify_imu_scale</code> bool Enable or disable scale injection <code>scale_on_purpose</code> double Value to inject as scale <code>bias_on_purpose</code> double Value to inject as bias <code>drift_scale</code> double Value to add to the scale value every loop, to simulate scale drift <code>drift_bias</code> double Value to add to the bias value every loop, to simulate bias drift"},{"location":"sensing/autoware_imu_corrector/#imu-correction-control","title":"IMU Correction control","text":"<p>These parameters control how gyroscope bias and scale are corrected. Only one bias-correction method should be enabled at a time. If both flags are enabled, static bias correction is automatically disabled.</p>"},{"location":"sensing/autoware_imu_corrector/#static-bias-correction-correct_for_static_bias","title":"Static Bias Correction (<code>correct_for_static_bias</code>)","text":"<ul> <li>Offset values must be precomputed and stored in the configuration file under the parameter:   <code>angular_velocity_offset_[x, y, z]</code></li> <li>Applies these offsets directly to the raw gyroscope data:   <code>~/input/imu_raw</code>   -Should be used only if the gyroscope offset doesn't change along the time.</li> </ul>"},{"location":"sensing/autoware_imu_corrector/#dynamic-bias-correction-correct_for_dynamic_bias","title":"Dynamic Bias Correction (<code>correct_for_dynamic_bias</code>)","text":"<ul> <li>Uses bias estimates published on:   <code>~/output/gyro_bias</code></li> <li>Bias is estimated with the help of the odometry data:   <code>~/input/odometry</code></li> <li>The estimated bias is applied to correct the raw gyroscope data:   <code>~/input/imu_raw</code>   -Should be used when the gyroscope offset is changing along the time and odometry data is accessible.</li> </ul>"},{"location":"sensing/autoware_imu_corrector/#parameters_2","title":"Parameters","text":"Name Type Description <code>on_off_correction.correct_for_static_bias</code> bool Enable or disable static bias correction (default: true) <code>on_off_correction.correct_for_dynamic_bias</code> bool Enable or disable dynamic bias correction (default: false) <code>on_off_correction.correct_for_scale</code> bool Enable or disable scale correction (default: false)"},{"location":"sensing/autoware_pcl_extensions/","title":"autoware_pcl_extensions","text":""},{"location":"sensing/autoware_pcl_extensions/#autoware_pcl_extensions","title":"autoware_pcl_extensions","text":""},{"location":"sensing/autoware_pcl_extensions/#purpose","title":"Purpose","text":"<p>The <code>autoware_pcl_extensions</code> is a pcl extension library. The voxel grid filter in this package works with a different algorithm than the original one.</p>"},{"location":"sensing/autoware_pcl_extensions/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"sensing/autoware_pcl_extensions/#original-algorithm-1","title":"Original Algorithm [1]","text":"<ol> <li>create a 3D voxel grid over the input pointcloud data</li> <li>calculate centroid in each voxel</li> <li>all the points are approximated with their centroid</li> </ol>"},{"location":"sensing/autoware_pcl_extensions/#extended-algorithm","title":"Extended Algorithm","text":"<ol> <li>create a 3D voxel grid over the input pointcloud data</li> <li>calculate centroid in each voxel</li> <li>all the points are approximated with the closest point to their centroid</li> </ol>"},{"location":"sensing/autoware_pcl_extensions/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"sensing/autoware_pcl_extensions/#parameters","title":"Parameters","text":""},{"location":"sensing/autoware_pcl_extensions/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"sensing/autoware_pcl_extensions/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"sensing/autoware_pcl_extensions/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"sensing/autoware_pcl_extensions/#optional-referencesexternal-links","title":"(Optional) References/External links","text":"<p>[1] https://pointclouds.org/documentation/tutorials/voxel_grid.html</p>"},{"location":"sensing/autoware_pcl_extensions/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/","title":"autoware_pointcloud_preprocessor","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/#autoware_pointcloud_preprocessor","title":"autoware_pointcloud_preprocessor","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/#purpose","title":"Purpose","text":"<p>The <code>autoware_pointcloud_preprocessor</code> is a package that includes the following filters:</p> <ul> <li>removing outlier points</li> <li>cropping</li> <li>concatenating pointclouds</li> <li>correcting distortion</li> <li>downsampling</li> <li>densifying pointclouds</li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>Detail description of each filter's algorithm is in the following links.</p> Filter Name Description Detail concatenate_data subscribe multiple pointclouds and concatenate them into a pointcloud link crop_box_filter remove points within a given box link distortion_corrector compensate pointcloud distortion caused by ego vehicle's movement during 1 scan link downsample_filter downsampling input pointcloud link outlier_filter remove points caused by hardware problems, rain drops and small insects as a noise link passthrough_filter remove points on the outside of a range in given field (e.g. x, y, z, intensity) link pointcloud_accumulator accumulate pointclouds for a given amount of time link pointcloud_densifier enhance sparse point clouds by using information from previous frames link vector_map_filter remove points on the outside of lane by using vector map link vector_map_inside_area_filter remove points inside of vector map area that has given type by parameter link"},{"location":"sensing/autoware_pointcloud_preprocessor/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/#input","title":"Input","text":"Name Type Description <code>~/input/points</code> <code>sensor_msgs::msg::PointCloud2</code> reference points <code>~/input/indices</code> <code>pcl_msgs::msg::Indices</code> reference indices"},{"location":"sensing/autoware_pointcloud_preprocessor/#output","title":"Output","text":"Name Type Description <code>~/output/points</code> <code>sensor_msgs::msg::PointCloud2</code> filtered points"},{"location":"sensing/autoware_pointcloud_preprocessor/#parameters","title":"Parameters","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/#node-parameters","title":"Node Parameters","text":"Name Type Default Value Description <code>input_frame</code> string \" \" input frame id <code>output_frame</code> string \" \" output frame id <code>max_queue_size</code> int 5 max queue size of input/output topics <code>use_indices</code> bool false flag to use pointcloud indices <code>latched_indices</code> bool false flag to latch pointcloud indices <code>approximate_sync</code> bool false flag to use approximate sync option"},{"location":"sensing/autoware_pointcloud_preprocessor/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p><code>autoware::pointcloud_preprocessor::Filter</code> is implemented based on pcl_perception [1] because of this issue.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/#measuring-the-performance","title":"Measuring the performance","text":"<p>In Autoware, point cloud data from each LiDAR sensor undergoes preprocessing in the sensing pipeline before being input into the perception pipeline. The preprocessing stages are illustrated in the diagram below:</p> <p></p> <p>Each stage in the pipeline incurs a processing delay. Mostly, we've used <code>ros2 topic delay /topic_name</code> to measure the time between the message header and the current time. This approach works well for small-sized messages. However, when dealing with large point cloud messages, this method introduces an additional delay. This is primarily because accessing these large point cloud messages externally impacts the pipeline's performance.</p> <p>Our sensing/perception nodes are designed to run within composable node containers, leveraging intra-process communication. External subscriptions to these messages (like using ros2 topic delay or rviz2) impose extra delays and can even slow down the pipeline by subscribing externally. Therefore, these measurements will not be accurate.</p> <p>To mitigate this issue, we've adopted a method where each node in the pipeline reports its pipeline latency time. This approach ensures the integrity of intra-process communication and provides a more accurate measure of delays in the pipeline.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/#benchmarking-the-pipeline","title":"Benchmarking The Pipeline","text":"<p>The nodes within the pipeline report the pipeline latency time, indicating the duration from the sensor driver's pointcloud output to the node's output. This data is crucial for assessing the pipeline's health and efficiency.</p> <p>When running Autoware, you can monitor the pipeline latency times for each node in the pipeline by subscribing to the following ROS 2 topics:</p> <ul> <li><code>/sensing/lidar/LidarX/crop_box_filter_self/debug/pipeline_latency_ms</code></li> <li><code>/sensing/lidar/LidarX/crop_box_filter_mirror/debug/pipeline_latency_ms</code></li> <li><code>/sensing/lidar/LidarX/distortion_corrector/debug/pipeline_latency_ms</code></li> <li><code>/sensing/lidar/LidarX/ring_outlier_filter/debug/pipeline_latency_ms</code></li> <li><code>/sensing/lidar/concatenate_data_synchronizer/debug/sensing/lidar/LidarX/pointcloud/pipeline_latency_ms</code></li> </ul> <p>These topics provide the pipeline latency times, giving insights into the delays at various stages of the pipeline from the sensor output of LidarX to each subsequent node.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/#referencesexternal-links","title":"References/External links","text":"<p>[1] https://github.com/ros-perception/perception_pcl/blob/ros2/pcl_ros/src/pcl_ros/filters/filter.cpp</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/blockage-diag/","title":"blockage_diag","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/blockage-diag/#blockage_diag","title":"blockage_diag","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/blockage-diag/#purpose","title":"Purpose","text":"<p>To ensure the performance of LiDAR and safety for autonomous driving, the abnormal condition diagnostics feature is needed. LiDAR blockage is abnormal condition of LiDAR when some unwanted objects stitch to and block the light pulses and return signal. This node's purpose is to detect the existing of blockage on LiDAR and its related size and location.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/blockage-diag/#inner-workings-algorithmsblockage-detection","title":"Inner-workings / Algorithms(Blockage detection)","text":"<p>This node bases on the no-return region and its location to decide if it is a blockage.</p> <p></p> <p>The logic is showed as below</p> <p></p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/blockage-diag/#inner-workings-algorithmsdust-detection","title":"Inner-workings /Algorithms(Dust detection)","text":"<p>About dust detection, morphological processing is implemented. If the lidar's ray cannot be acquired due to dust in the lidar area where the point cloud is considered to return from the ground, black pixels appear as noise in the depth image. The area of noise is found by erosion and dilation these black pixels.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/blockage-diag/#inputs-outputs","title":"Inputs / Outputs","text":"<p>This implementation inherits <code>autoware::pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/blockage-diag/#input","title":"Input","text":"Name Type Description <code>~/input/pointcloud_raw_ex</code> <code>sensor_msgs::msg::PointCloud2</code> The raw point cloud data is used to detect the no-return region"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/blockage-diag/#output","title":"Output","text":"Name Type Description <code>~/output/blockage_diag/debug/blockage_mask_image</code> <code>sensor_msgs::msg::Image</code> The mask image of detected blockage <code>~/output/blockage_diag/debug/ground_blockage_ratio</code> <code>autoware_internal_debug_msgs::msg::Float32Stamped</code> The area ratio of blockage region in ground region <code>~/output/blockage_diag/debug/sky_blockage_ratio</code> <code>autoware_internal_debug_msgs::msg::Float32Stamped</code> The area ratio of blockage region in sky region <code>~/output/blockage_diag/debug/lidar_depth_map</code> <code>sensor_msgs::msg::Image</code> The depth map image of input point cloud <code>~/output/blockage_diag/debug/single_frame_dust_mask</code> <code>sensor_msgs::msg::Image</code> The mask image of detected dusty area in latest single frame <code>~/output/blockage_diag/debug/multi_frame_dust_mask</code> <code>sensor_msgs::msg::Image</code> The mask image of continuous detected dusty area <code>~/output/blockage_diag/debug/blockage_dust_merged_image</code> <code>sensor_msgs::msg::Image</code> The merged image of blockage detection(red) and multi frame dusty area detection(yellow) results <code>~/output/blockage_diag/debug/ground_dust_ratio</code> <code>autoware_internal_debug_msgs::msg::Float32Stamped</code> The ratio of dusty area divided by area where ray usually returns from the ground."},{"location":"sensing/autoware_pointcloud_preprocessor/docs/blockage-diag/#parameters","title":"Parameters","text":"Name Type Description Default Range blockage_ratio_threshold float The threshold of blockage area ratio. If the blockage value exceeds this threshold, the diagnostic state will be set to ERROR. 0.1 \u22650 blockage_count_threshold float The threshold of number continuous blockage frames 50 \u22650 blockage_buffering_frames integer The number of buffering about blockage detection [range:1-200] 2 \u22651\u2264200 blockage_buffering_interval integer The interval of buffering about blockage detection 1 \u22650 enable_dust_diag boolean enable dust diagnostic false N/A publish_debug_image boolean publish debug image false N/A dust_ratio_threshold float The threshold of dusty area ratio 0.2 \u22650 dust_count_threshold integer The threshold of number continuous frames include dusty area 10 \u22650 dust_kernel_size integer The kernel size of morphology processing in dusty area detection 2 \u22650 dust_buffering_frames integer The number of buffering about dusty area detection [range:1-200] 10 \u22651\u2264200 dust_buffering_interval integer The interval of buffering about dusty area detection 1 \u22650 max_distance_range float Maximum view range for the LiDAR 200.0 \u22650 horizontal_resolution float The horizontal resolution of depth map image [deg/pixel] 0.4 \u22650 blockage_kernel integer The kernel size of morphology processing the detected blockage area 10 \u22650 angle_range array The effective range of LiDAR [0.0, 360.0] N/A vertical_bins integer The LiDAR channel 40 \u22650 is_channel_order_top2down boolean If the lidar channels are indexed from top to down true N/A horizontal_ring_id integer The id of horizontal ring of the LiDAR 18 \u22650"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/blockage-diag/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<ol> <li>Only Hesai Pandar40P and Hesai PandarQT were tested. For a new LiDAR, it is necessary to check order of channel id in    vertical distribution manually and modify the code.</li> <li>About dusty area detection, False positives occur when there are water puddles on the road surface due to rain, etc.    Also, the area of the ray to the sky is currently undetectable.</li> </ol>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/blockage-diag/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/blockage-diag/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/blockage-diag/#referencesexternal-links","title":"References/External links","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/blockage-diag/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/concatenate-data/","title":"concatenate_and_time_synchronize_node","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/concatenate-data/#concatenate_and_time_synchronize_node","title":"concatenate_and_time_synchronize_node","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/concatenate-data/#purpose","title":"Purpose","text":"<p>The <code>concatenate_and_time_synchronize_node</code> is a node designed to combine and synchronize multiple point clouds into a single, unified point cloud. By integrating data from multiple LiDARs, this node significantly enhances the sensing range and coverage of autonomous vehicles, enabling more accurate perception of the surrounding environment. Synchronization ensures that point clouds are aligned temporally, reducing errors caused by mismatched timestamps.</p> <p>For example, consider a vehicle equipped with three LiDAR sensors mounted on the left, right, and top positions. Each LiDAR captures data from its respective field of view, as shown below:</p> Left Top Right <p>After processing the data through the <code>concatenate_and_time_synchronize_node</code>, the outputs from all LiDARs are combined into a single comprehensive point cloud that provides a complete view of the environment:</p> <p></p> <p>This resulting point cloud allows autonomous systems to detect obstacles, map the environment, and navigate more effectively, leveraging the complementary fields of view from multiple LiDAR sensors.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/concatenate-data/#inner-workings-algorithms","title":"Inner Workings / Algorithms","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/concatenate-data/#step-1-match-and-create-collector","title":"Step 1: Match and Create Collector","text":"<p>When a point cloud arrives, its timestamp is checked, and an offset is subtracted to get the reference timestamp. The node then checks if there is an existing collector with the same reference timestamp. If such a collector exists, the point cloud is added to it. If no such collector exists, a new collector is created with the reference timestamp.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/concatenate-data/#step-2-trigger-the-timer","title":"Step 2: Trigger the Timer","text":"<p>Once a collector is created, a timer for that collector starts counting down (this value is defined by <code>timeout_sec</code>). The collector begins to concatenate the point clouds either when all point clouds defined in <code>input_topics</code> have been collected or when the timer counts down to zero.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/concatenate-data/#step-3-concatenate-the-point-clouds","title":"Step 3: Concatenate the Point Clouds","text":"<p>The concatenation process involves merging multiple point clouds into a single, concatenated point cloud. The timestamp of the concatenated point cloud will be the earliest timestamp from the input point clouds. By setting the parameter <code>is_motion_compensated</code> to <code>true</code>, the node will consider the timestamps of the input point clouds and utilize the <code>twist</code> information from <code>geometry_msgs::msg::TwistWithCovarianceStamped</code> to compensate for motion, aligning the point cloud to the selected (earliest) timestamp.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/concatenate-data/#step-4-publish-the-point-cloud","title":"Step 4: Publish the Point Cloud","text":"<p>After concatenation, the concatenated point cloud is published, and the collector is deleted to free up resources.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/concatenate-data/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/concatenate-data/#input","title":"Input","text":"Name Type Description <code>~/input/twist</code> <code>geometry_msgs::msg::TwistWithCovarianceStamped</code> Twist information adjusts the point cloud scans based on vehicle motion, allowing LiDARs with different timestamps to be synchronized for concatenation. <code>~/input/odom</code> <code>nav_msgs::msg::Odometry</code> Vehicle odometry adjusts the point cloud scans based on vehicle motion, allowing LiDARs with different timestamps to be synchronized for concatenation. <p>By setting the <code>input_twist_topic_type</code> parameter to <code>twist</code> or <code>odom</code>, the subscriber will subscribe to either <code>~/input/twist</code> or <code>~/input/odom</code>. If the user doesn't want to use the twist information or vehicle odometry to compensate for motion, set <code>is_motion_compensated</code> to <code>false</code>.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/concatenate-data/#output","title":"Output","text":"Name Type Description <code>~/output/points</code> <code>sensor_msgs::msg::Pointcloud2</code> Concatenated point clouds <code>~/output/info</code> <code>autoware_sensing_msgs::msg::ConcatenatedPointCloudInfo</code> Information about the concatenated point cloud, including extents of source point clouds and their statuses"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/concatenate-data/#core-parameters","title":"Core Parameters","text":"Name Type Description Default Range debug_mode boolean Flag to enable debug mode to display additional logging information. False N/A rosbag_length float This value determine if the rosbag has started from the beginning again. The value should be set smaller than the actual length of the bag. 10.0 \u22650.0 maximum_queue_size integer Maximum size of the queue for the Keep Last policy in QoS history. 5 \u22651 timeout_sec float Timer's timeout duration in seconds when collectors are created. Collectors will concatenate the point clouds when timeout_sec reaches zero. 0.1 \u22650.001 is_motion_compensated boolean Flag to indicate if motion compensation is enabled. True N/A publish_synchronized_pointcloud boolean Flag to indicate if synchronized point cloud should be published. True N/A keep_input_frame_in_synchronized_pointcloud boolean Flag to indicate if input frame should be kept in synchronized point cloud. True N/A publish_previous_but_late_pointcloud boolean Flag to indicate if a concatenated point cloud should be published if its timestamp is earlier than the previous published concatenated point cloud. False N/A synchronized_pointcloud_postfix string Postfix for the topic name of the synchronized point cloud. pointcloud N/A input_twist_topic_type string Type of the input twist topic. twist ['twist', 'odom'] input_topics array List of input point cloud topics. ['cloud_topic1', 'cloud_topic2', 'cloud_topic3'] N/A output_frame string Output frame. base_link N/A matching_strategy.type string Set it to <code>advanced</code> if you can synchronize your LiDAR sensor; otherwise, set it to <code>naive</code>. advanced ['naive', 'advanced'] matching_strategy.lidar_timestamp_offsets array List of LiDAR timestamp offsets in seconds (relative to the earliest LiDAR timestamp). The offsets should be provided in the same order as the input topics. [0.0, 0.0, 0.0] N/A matching_strategy.lidar_timestamp_noise_window array List of LiDAR timestamp noise windows in seconds. The noise values should be specified in the same order as the input_topics. [0.01, 0.01, 0.01] N/A"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/concatenate-data/#concatenation-strategies","title":"Concatenation Strategies","text":"<p>The <code>concatenate_and_time_synchronize_node</code> supports different concatenation strategies through the <code>matching_strategy.type</code> parameter, designed to handle different LiDAR synchronization scenarios:</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/concatenate-data/#naive-strategy-matching_strategytype-naive","title":"Naive Strategy (<code>matching_strategy.type: \"naive\"</code>)","text":"<p>The naive strategy is designed for scenarios where LiDAR sensors are not synchronized or when precise timestamp alignment is not critical. This strategy:</p> <ul> <li>Direct concatenation: Point clouds are concatenated directly without complex timestamp matching</li> <li>Simple collection: Point clouds are collected and merged as they arrive within the timeout window</li> <li>No offset compensation: Does not use <code>lidar_timestamp_offsets</code> or <code>lidar_timestamp_noise_window</code> parameters</li> <li>Faster processing: Reduced computational overhead due to simplified logic</li> </ul> <p>When to use naive strategy:</p> <ul> <li>LiDAR sensors are not hardware-synchronized</li> <li>Timestamp differences between sensors are negligible for your application</li> <li>You prioritize processing speed over precise temporal alignment</li> <li>Simple concatenation without complex matching logic is sufficient</li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/concatenate-data/#parameter-settings","title":"Parameter Settings","text":"<p>Set the <code>type</code> parameter of <code>matching_strategy</code> to <code>\"naive\"</code> to concatenate the point clouds directly.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/concatenate-data/#advanced-strategy-matching_strategytype-advanced","title":"Advanced Strategy (<code>matching_strategy.type: \"advanced\"</code>)","text":"<p>The advanced strategy is designed for scenarios where LiDAR sensors are synchronized and precise timestamp alignment is crucial. This strategy:</p> <ul> <li>Precise timestamp matching: Uses reference timestamps with offset compensation</li> <li>Noise tolerance: Accounts for timestamp jitter using <code>lidar_timestamp_noise_window</code></li> <li>Offset correction: Applies <code>lidar_timestamp_offsets</code> to align different LiDAR timing</li> <li>Robust collection: Handles temporal variations and ensures proper point cloud grouping</li> </ul> <p>When to use advanced strategy:</p> <ul> <li>LiDAR sensors are hardware-synchronized</li> <li>Precise temporal alignment is critical for your application</li> <li>You have measured timestamp offsets between your LiDAR sensors</li> <li>You want to handle timestamp noise and jitter effectively</li> </ul> <p>Required parameters for advanced strategy:</p> <ul> <li><code>lidar_timestamp_offsets</code>: Array of time offsets for each LiDAR sensor</li> <li><code>lidar_timestamp_noise_window</code>: Time window to handle timestamp jitter</li> <li><code>timeout_sec</code>: Maximum wait time for point cloud collection</li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/concatenate-data/#parameter-settings_1","title":"Parameter Settings","text":"<p>Set the <code>type</code> parameter of <code>matching_strategy</code> to <code>\"advanced\"</code> and configure the <code>timeout_sec</code>, <code>lidar_timestamp_offsets</code> and <code>lidar_timestamp_noise_window</code> parameters accordingly.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/concatenate-data/#timeout_sec","title":"timeout_sec","text":"<p>When network issues occur or when point clouds experience delays in the previous processing pipeline, some point clouds may be delayed or dropped. To address this, the <code>timeout_sec</code> parameter is used. Once the timer is created, it will start counting down from <code>timeout_sec</code>. If the timer reaches zero, the collector will not wait for delayed or dropped point clouds but will concatenate the remaining point clouds in the collector directly. The figure below demonstrates how <code>timeout_sec</code> works with <code>concatenate_and_time_sync_node</code> when <code>timeout_sec</code> is set to <code>0.12</code> (120 ms).</p> <p></p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/concatenate-data/#lidar_timestamp_offsets","title":"lidar_timestamp_offsets","text":"<p>Since different vehicles have varied designs for LiDAR scanning, the timestamps of each LiDAR may differ. Users need to know the offsets between each LiDAR and set the values in <code>lidar_timestamp_offsets</code>.</p> <p>To monitor the timestamps of each LiDAR, run the following command:</p> <pre><code>ros2 topic echo \"pointcloud_topic\" --field header\n</code></pre> <p>The timestamps should increase steadily by approximately 100 ms, as per the Autoware default. You should see output like this:</p> <pre><code>nanosec: 156260951\nnanosec: 257009560\nnanosec: 355444581\n</code></pre> <p>This pattern indicates a LiDAR timestamp of 0.05.</p> <p>If there are three LiDARs (left, right, top), and the timestamps for the left, right, and top point clouds are <code>0.01</code>, <code>0.05</code>, and <code>0.09</code> seconds respectively, the parameters should be set as [0.0, 0.04, 0.08]. This reflects the timestamp differences between the current point cloud and the point cloud with the earliest timestamp. Note that the order of the <code>lidar_timestamp_offsets</code> corresponds to the order of the <code>input_topics</code>.</p> <p>The figure below demonstrates how <code>lidar_timestamp_offsets</code> works with <code>concatenate_and_time_sync_node</code>.</p> <p></p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/concatenate-data/#lidar_timestamp_noise_window","title":"lidar_timestamp_noise_window","text":"<p>Additionally, due to the mechanical design of LiDARs, there may be some jitter in the timestamps of each scan, as shown in the image below. For example, if the scan frequency is set to 10 Hz (scanning every 100 ms), the timestamps between each scan might not be exactly 100 ms apart. To handle this noise, the <code>lidar_timestamp_noise_window</code> parameter is provided.</p> <p>Users can use this tool to visualize the noise between each scan.</p> <p></p> <p>From the example above, the noise ranges from 0 to 8 ms, so the user should set <code>lidar_timestamp_noise_window</code> to <code>0.008</code>.</p> <p>The figure below demonstrates how <code>lidar_timestamp_noise_window</code> works with the <code>concatenate_and_time_sync_node</code>. If the green <code>X</code> is within the range of the red triangles, it indicates that the point cloud matches the reference timestamp of the collector.</p> <p></p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/concatenate-data/#meta-information-topic","title":"Meta Information Topic","text":"<p>The concatenation node publishes detailed meta information about the concatenation process through the <code>~/output/info</code> topic. For detailed information about the <code>ConcatenatedPointCloudInfo</code> message structure, please refer to the autoware_msgs repository documentation.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/concatenate-data/#handling-serialized-configuration","title":"Handling Serialized Configuration","text":"<p>The <code>matching_strategy_config</code> field contains serialized configuration data for the matching strategy. If a strategy has its own configuration, it requires serialization and deserialization implementation based on the <code>StrategyConfig</code> class defined in cloud_info.hpp.</p> <p>Here's how to work with serialized configuration for the Advanced strategy:</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/concatenate-data/#serialization-example","title":"Serialization Example","text":"<pre><code>auto cfg = StrategyAdvancedConfig(reference_timestamp_min, reference_timestamp_max);\nConcatenationInfoManager::set_config(cfg.serialize(), concatenation_info_msg);\n</code></pre>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/concatenate-data/#deserialization-example","title":"Deserialization Example","text":"<pre><code>std::vector&lt;uint8_t&gt; raw_cfg = concat_info_msg-&gt;matching_strategy_config;\nauto cfg = StrategyAdvancedConfig(raw_cfg);\n</code></pre>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/concatenate-data/#launch","title":"Launch","text":"<pre><code># The launch file will read the parameters from the concatenate_and_time_sync_node.param.yaml\nros2 launch autoware_pointcloud_preprocessor concatenate_and_time_sync_node.launch.xml\n</code></pre>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/concatenate-data/#test","title":"Test","text":"<pre><code># build autoware_pointcloud_preprocessor\ncolcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE=Release --packages-up-to autoware_pointcloud_preprocessor\n\n# test autoware_pointcloud_preprocessor\ncolcon test --packages-select autoware_pointcloud_preprocessor --event-handlers console_cohesion+\n</code></pre>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/concatenate-data/#debug-and-diagnostics","title":"Debug and Diagnostics","text":"<p>To verify whether the node has successfully concatenated the point clouds, the user can examine rqt or the <code>/diagnostics</code> topic using the following command:</p> <pre><code>ros2 topic echo /diagnostics\n</code></pre> <p>Below is an example output when the point clouds are concatenated successfully:</p> <ul> <li>Each point cloud has a value of <code>True</code>.</li> <li>The <code>Pointcloud concatenation succeeded</code> is <code>True</code>.</li> <li>The <code>level</code> value is <code>\\0</code>. (diagnostic_msgs::msg::DiagnosticStatus::OK)</li> </ul> <pre><code>header:\n  stamp:\n    sec: 1722492015\n    nanosec: 848508777\n  frame_id: ''\nstatus:\n- level: \"\\0\"\n  name: 'concatenate_and_time_sync_node: concat_status'\n  message: Concatenated pointcloud is published and include all topics\n  hardware_id: concatenate_data_checker\n  values:\n  - key: Concatenated pointcloud timestamp\n    value: '1718260240.159229994'\n  - key: Minimum reference timestamp\n    value: '1718260240.149230003'\n  - key: Maximum reference timestamp\n    value: '1718260240.169229984'\n  - key: Timestamp: /sensing/lidar/left/pointcloud_before_sync\n    value: '1718260240.159229994'\n  - key: Concatenated: /sensing/lidar/left/pointcloud_before_sync\n    value: 'True'\n  - key: Timestamp: /sensing/lidar/right/pointcloud_before_sync\n    value: '1718260240.194104910'\n  - key: Concatenated: /sensing/lidar/right/pointcloud_before_sync\n    value: 'True'\n  - key: Timestamp: /sensing/lidar/top/pointcloud_before_sync\n    value: '1718260240.234578133'\n  - key: Concatenated: /sensing/lidar/top/pointcloud_before_sync\n    value: 'True'\n  - key: Pointcloud concatenation succeeded\n    value: 'True'\n</code></pre> <p>Below is an example when point clouds fail to concatenate successfully.</p> <ul> <li>Some point clouds might have values of <code>False</code>.</li> <li>The <code>Pointcloud concatenation succeeded</code> is <code>False</code>.</li> <li>The <code>level</code> value is <code>\\x02</code>. (diagnostic_msgs::msg::DiagnosticStatus::ERROR)</li> </ul> <pre><code>header:\n  stamp:\n    sec: 1722492663\n    nanosec: 344942959\n  frame_id: ''\nstatus:\n- level: \"\\x02\"\n  name: 'concatenate_and_time_sync_node: concat_status'\n  message: Concatenated pointcloud is published but miss some topics\n  hardware_id: concatenate_data_checker\n  values:\n  - key: Concatenated pointcloud timestamp\n    value: '1718260240.859827995'\n  - key: Minimum reference timestamp\n    value: '1718260240.849828005'\n  - key: Maximum reference timestamp\n    value: '1718260240.869827986'\n  - key: Timestamp: /sensing/lidar/left/pointcloud_before_sync/timestamp\n    value: '1718260240.859827995'\n  - key: Concatenated: /sensing/lidar/left/pointcloud_before_sync\n    value: 'True'\n  - key: Timestamp: /sensing/lidar/right/pointcloud_before_sync/timestamp\n    value: '1718260240.895193815'\n  - key: Concatenated: /sensing/lidar/right/pointcloud_before_sync\n    value: 'True'\n  - key: Concatenated: /sensing/lidar/top/pointcloud_before_sync\n    value: 'False'\n  - key: Pointcloud concatenation succeeded\n    value: 'False'\n</code></pre>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/concatenate-data/#node-separation-options","title":"Node separation options","text":"<p>There is also an option to separate the concatenate_and_time_sync_node into two nodes: one for <code>time synchronization</code> and another for <code>concatenate pointclouds</code> (See this PR).</p> <p>Note that the <code>concatenate_pointclouds</code> and <code>time_synchronizer_nodelet</code> are using the old design of the concatenate node.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/concatenate-data/#assumptions-known-limits","title":"Assumptions / Known Limits","text":"<ul> <li>If <code>is_motion_compensated</code> is set to <code>false</code>, the <code>concatenate_and_time_sync_node</code> will directly concatenate the point clouds without applying for motion compensation. This can save several milliseconds depending on the number of LiDARs being concatenated. Therefore, if the timestamp differences between point clouds are negligible, the user can set <code>is_motion_compensated</code> to <code>false</code> and omit the need for twist or odometry input for the node.</li> <li>As mentioned above, the user should clearly understand how their LiDAR's point cloud timestamps are managed to set the parameters correctly. If the user does not synchronize the point clouds, please set <code>matching_strategy.type</code> to <code>naive</code>.</li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/crop-box-filter/","title":"crop_box_filter","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/crop-box-filter/#crop_box_filter","title":"crop_box_filter","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/crop-box-filter/#purpose","title":"Purpose","text":"<p>The <code>crop_box_filter</code> is a node that removes points with in a given box region. This filter is used to remove the points that hit the vehicle itself.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/crop-box-filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p><code>pcl::CropBox</code> is used, which filters all points inside a given box.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/crop-box-filter/#inputs-outputs","title":"Inputs / Outputs","text":"<p>This implementation inherit <code>autoware::pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/crop-box-filter/#parameters","title":"Parameters","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/crop-box-filter/#node-parameters","title":"Node Parameters","text":"<p>This implementation inherit <code>autoware::pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/crop-box-filter/#core-parameters","title":"Core Parameters","text":"Name Type Description Default Range min_x float minimum x-coordinate value for crop range in meters -1.0 N/A min_y float minimum y-coordinate value for crop range in meters -1.0 N/A min_z float minimum z-coordinate value for crop range in meters -1.0 N/A max_x float maximum x-coordinate value for crop range in meters 1.0 N/A max_y float maximum y-coordinate value for crop range in meters 1.0 N/A max_z float maximum z-coordinate value for crop range in meters 1.0 N/A negative boolean if true, remove points within the box from the pointcloud; otherwise, remove points outside the box. false N/A processing_time_threshold_sec float Threshold in seconds. If the processing time of the node exceeds this value, a diagnostic warning will be issued. 0.01 N/A"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/crop-box-filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/crop-box-filter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/crop-box-filter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/crop-box-filter/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/crop-box-filter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/distortion-corrector/","title":"distortion_corrector","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/distortion-corrector/#distortion_corrector","title":"distortion_corrector","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/distortion-corrector/#purpose","title":"Purpose","text":"<p>The <code>distortion_corrector</code> is a node that compensates for pointcloud distortion caused by the ego-vehicle's movement during one scan.</p> <p>Since the LiDAR sensor scans by rotating an internal laser, the resulting point cloud will be distorted if the ego-vehicle moves during a single scan (as shown by the figure below). The node corrects this by interpolating sensor data using the odometry of the ego-vehicle.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/distortion-corrector/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The node uses twist information (linear and angular velocity) from the <code>~/input/twist</code> topic to correct each point in the point cloud. If the user sets <code>use_imu</code> to true, the node will replace the twist's angular velocity with the angular velocity from IMU.</p> <p>The node supports two different modes of distortion correction: 2D distortion correction and 3D distortion correction. The main difference is that the 2D distortion corrector only utilizes the x-axis of linear velocity and the z-axis of angular velocity to correct the point positions. On the other hand, the 3D distortion corrector utilizes all linear and angular velocity components to correct the point positions.</p> <p>Please note that the processing time difference between the two distortion methods is significant; the 3D corrector takes 50% more time than the 2D corrector. Therefore, it is recommended that in general cases, users should set <code>use_3d_distortion_correction</code> to <code>false</code>. However, in scenarios such as a vehicle going over speed bumps, using the 3D corrector can be beneficial.</p> <p></p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/distortion-corrector/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/distortion-corrector/#input","title":"Input","text":"Name Type Description <code>~/input/pointcloud</code> <code>sensor_msgs::msg::PointCloud2</code> Topic of the distorted pointcloud. <code>~/input/twist</code> <code>geometry_msgs::msg::TwistWithCovarianceStamped</code> Topic of the twist information. <code>~/input/imu</code> <code>sensor_msgs::msg::Imu</code> Topic of the IMU data."},{"location":"sensing/autoware_pointcloud_preprocessor/docs/distortion-corrector/#output","title":"Output","text":"Name Type Description <code>~/output/pointcloud</code> <code>sensor_msgs::msg::PointCloud2</code> Topic of the undistorted pointcloud"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/distortion-corrector/#parameters","title":"Parameters","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/distortion-corrector/#core-parameters","title":"Core Parameters","text":"Name Type Description Default Range base_frame string The undistortion algorithm is based on a base frame, which must be the same as the twist frame. base_link N/A use_imu boolean Use IMU angular velocity, otherwise, use twist angular velocity. true N/A use_3d_distortion_correction boolean Use 3d distortion correction algorithm, otherwise, use 2d distortion correction algorithm. false N/A update_azimuth_and_distance boolean Flag to update the azimuth and distance values of each point after undistortion. If set to false, the azimuth and distance values will remain unchanged after undistortion, resulting in a mismatch with the updated x, y, z coordinates. false N/A processing_time_threshold_sec float Threshold in seconds. If the processing time of the node exceeds this value, a diagnostic warning will be issued. 0.01 N/A timestamp_mismatch_fraction_threshold float Threshold for the fraction of points that lack corresponding twist or IMU data within the allowed timestamp tolerance. 0.01 N/A"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/distortion-corrector/#launch","title":"Launch","text":"<pre><code>ros2 launch autoware_pointcloud_preprocessor distortion_corrector.launch.xml\n</code></pre>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/distortion-corrector/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<ul> <li>The node requires time synchronization between the topics from lidars, twist, and IMU.</li> <li>If you want to use a 3D distortion corrector without IMU, please check that the linear and angular velocity fields of your twist message are not empty.</li> <li>The node updates the per-point azimuth and distance values based on the undistorted XYZ coordinates when the input point cloud is in the sensor frame (not in the <code>base_link</code>) and the <code>update_azimuth_and_distance</code> parameter is set to <code>true</code>. The azimuth values are calculated using a modified version of OpenCV's <code>cv::fastAtan2</code> function.</li> <li>Please note that updating the azimuth and distance fields increases the execution time by approximately 20%. Additionally, due to the <code>cv::fastAtan2</code> algorithm's has a maximum error of 0.3 degrees, there is a possibility of changing the beam order for high azimuth resolution LiDAR.</li> <li>LiDARs from different vendors have different azimuth coordinates, as shown in the images below. Currently, the coordinate systems listed below have been tested, and the node will update the azimuth based on the input coordinate system.<ul> <li><code>velodyne</code>: (x: 0 degrees, y: 270 degrees)</li> <li><code>hesai</code>: (x: 90 degrees, y: 0 degrees)</li> <li><code>others</code>: (x: 0 degrees, y: 90 degrees) and (x: 270 degrees, y: 0 degrees)</li> </ul> </li> </ul> Velodyne azimuth coordinate Hesai azimuth coordinate"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/distortion-corrector/#referencesexternal-links","title":"References/External links","text":"<p>https://docs.opencv.org/3.4/db/de0/group__core__utils.html#ga7b356498dd314380a0c386b059852270</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/downsample-filter/","title":"downsample_filter","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/downsample-filter/#downsample_filter","title":"downsample_filter","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/downsample-filter/#purpose","title":"Purpose","text":"<p>The <code>downsample_filter</code> is a node that reduces the number of points.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/downsample-filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/downsample-filter/#approximate-downsample-filter","title":"Approximate Downsample Filter","text":"<p><code>pcl::VoxelGridNearestCentroid</code> is used. The algorithm is described in autoware_pcl_extensions</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/downsample-filter/#random-downsample-filter","title":"Random Downsample Filter","text":"<p><code>pcl::RandomSample</code> is used, which points are sampled with uniform probability.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/downsample-filter/#voxel-grid-downsample-filter","title":"Voxel Grid Downsample Filter","text":"<p><code>pcl::VoxelGrid</code> is used, which points in each voxel are approximated with their centroid.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/downsample-filter/#pickup-based-voxel-grid-downsample-filter","title":"Pickup Based Voxel Grid Downsample Filter","text":"<p>This algorithm samples a single actual point existing within the voxel, not the centroid. The computation cost is low compared to Centroid Based Voxel Grid Filter.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/downsample-filter/#inputs-outputs","title":"Inputs / Outputs","text":"<p>These implementations inherit <code>autoware::pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/downsample-filter/#parameters","title":"Parameters","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/downsample-filter/#note-parameters","title":"Note Parameters","text":"<p>These implementations inherit <code>autoware::pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/downsample-filter/#core-parameters","title":"Core Parameters","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/downsample-filter/#approximate-downsample-filter_1","title":"Approximate Downsample Filter","text":"Name Type Description Default Range voxel_size_x float voxel size along the x-axis [m] 0.3 \u22650.0 voxel_size_y float voxel size along the y-axis [m] 0.3 \u22650.0 voxel_size_z float voxel size along the z-axis [m] 0.1 \u22650.0"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/downsample-filter/#random-downsample-filter_1","title":"Random Downsample Filter","text":"Name Type Description Default Range sample_num integer number of indices to be sampled 1500 \u22650"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/downsample-filter/#voxel-grid-downsample-filter_1","title":"Voxel Grid Downsample Filter","text":"Name Type Description Default Range voxel_size_x float the voxel size along x-axis [m] 0.3 \u22650 voxel_size_y float the voxel size along y-axis [m] 0.3 \u22650 voxel_size_z float the voxel size along z-axis [m] 0.1 \u22650"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/downsample-filter/#pickup-based-voxel-grid-downsample-filter_1","title":"Pickup Based Voxel Grid Downsample Filter","text":"Name Type Description Default Range voxel_size_x float voxel size along the x-axis [m] 1 &gt;0.0 voxel_size_y float voxel size along the y-axis [m] 1 &gt;0.0 voxel_size_z float voxel size along the z-axis [m] 1 &gt;0.0"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/downsample-filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>This implementation uses the <code>robin_hood.h</code> hashing library by martinus, available under the MIT License at martinus/robin-hood-hashing on GitHub. Special thanks to martinus for this contribution.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/downsample-filter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/downsample-filter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/downsample-filter/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/downsample-filter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/dual-return-outlier-filter/","title":"dual_return_outlier_filter","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/dual-return-outlier-filter/#dual_return_outlier_filter","title":"dual_return_outlier_filter","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/dual-return-outlier-filter/#purpose","title":"Purpose","text":"<p>The purpose is to remove point cloud noise such as fog and rain and publish visibility as a diagnostic topic.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/dual-return-outlier-filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>This node can remove rain and fog by considering the light reflected from the object in two stages according to the attenuation factor. The <code>dual_return_outlier_filter</code> is named because it removes noise using data that contains two types of return values separated by attenuation factor, as shown in the figure below.</p> <p></p> <p>Therefore, in order to use this node, the sensor driver must publish custom data including <code>return_type</code>. please refer to PointXYZIRCAEDT data structure.</p> <p>Another feature of this node is that it publishes visibility as a diagnostic topic. With this function, for example, in heavy rain, the sensing module can notify that the processing performance has reached its limit, which can lead to ensuring the safety of the vehicle.</p> <p>In some complicated road scenes where normal objects also reflect the light in two stages, for instance plants, leaves, some plastic net etc, the visibility faces some drop in fine weather condition. To deal with that, optional settings of a region of interest (ROI) are added.</p> <ol> <li><code>Fixed_xyz_ROI</code> mode: Visibility estimation based on the weak points in a fixed cuboid surrounding region of ego-vehicle, defined by x, y, z in base_link perspective.</li> <li><code>Fixed_azimuth_ROI</code> mode: Visibility estimation based on the weak points in a fixed surrounding region of ego-vehicle, defined by azimuth and distance of LiDAR perspective.</li> </ol> <p>When select 2 fixed ROI modes, due to the range of weak points is shrink, the sensitivity of visibility is decrease so that a trade of between <code>weak_first_local_noise_threshold</code> and <code>visibility_threshold</code> is needed.</p> <p></p> <p>The figure below describe how the node works. </p> <p>The below picture shows the ROI options.</p> <p></p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/dual-return-outlier-filter/#inputs-outputs","title":"Inputs / Outputs","text":"<p>This implementation inherits <code>autoware::pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/dual-return-outlier-filter/#output","title":"Output","text":"Name Type Description <code>/dual_return_outlier_filter/frequency_image</code> <code>sensor_msgs::msg::Image</code> The histogram image that represent visibility <code>/dual_return_outlier_filter/visibility</code> <code>autoware_internal_debug_msgs::msg::Float32Stamped</code> A representation of visibility with a value from 0 to 1 <code>/dual_return_outlier_filter/pointcloud_noise</code> <code>sensor_msgs::msg::Pointcloud2</code> The pointcloud removed as noise"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/dual-return-outlier-filter/#parameters","title":"Parameters","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/dual-return-outlier-filter/#node-parameters","title":"Node Parameters","text":"<p>This implementation inherits <code>autoware::pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/dual-return-outlier-filter/#core-parameters","title":"Core Parameters","text":"Name Type Description Default Range x_max float Maximum of x in meters for <code>Fixed_xyz_ROI</code> mode 18.0 N/A x_min float Minimum of x in meters for <code>Fixed_xyz_ROI</code> mode -12.0 N/A y_max float Maximum of y in meters for <code>Fixed_xyz_ROI</code> mode 2.0 N/A y_min float Minimum of y in meters for <code>Fixed_xyz_ROI</code> mode -2.0 N/A z_max float Maximum of z in meters for <code>Fixed_xyz_ROI</code> mode 10.0 N/A z_min float Minimum of z in meters for <code>Fixed_xyz_ROI</code> mode 0.0 N/A min_azimuth_deg float The left limit of azimuth in degrees for <code>Fixed_azimuth_ROI</code> mode 135.0 \u22650\u2264360 max_azimuth_deg float The right limit of azimuth in degrees for <code>Fixed_azimuth_ROI</code> mode 225.0 \u22650\u2264360 max_distance float The limit distance in meters for <code>Fixed_azimuth_ROI</code> mode 12.0 \u22650.0 vertical_bins integer The number of vertical bins for the visibility histogram 128 \u22651 max_azimuth_diff float The azimuth difference threshold in degrees for ring_outlier_filter 50.0 \u22650.0 weak_first_distance_ratio float The maximum allowed ratio between consecutive weak point distances 1.004 \u22650.0 general_distance_ratio float The maximum allowed ratio between consecutive normal point distances 1.03 \u22650.0 weak_first_local_noise_threshold integer If the number of outliers among weak points is less than the weak_first_local_noise_threshold in the (max_azimuth - min_azimuth) / horizontal_bins interval, all points within the interval will not be filtered out. 2 \u22650 roi_mode string roi mode Fixed_xyz_ROI ['Fixed_xyz_ROI', 'No_ROI', 'Fixed_azimuth_ROI'] visibility_error_threshold float When the proportion of white pixels in the binary histogram falls below this parameter the diagnostic status becomes ERR 0.5 \u22650.0\u22641.0 visibility_warn_threshold float When the proportion of white pixels in the binary histogram falls below this parameter the diagnostic status becomes WARN 0.7 \u22650.0\u22641.0"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/dual-return-outlier-filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>Not recommended for use as it is under development. Input data must be PointXYZIRCAEDT type data including <code>return_type</code>.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/dual-return-outlier-filter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/dual-return-outlier-filter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/dual-return-outlier-filter/#referencesexternal-links","title":"References/External links","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/dual-return-outlier-filter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/outlier-filter/","title":"outlier_filter","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/outlier-filter/#outlier_filter","title":"outlier_filter","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/outlier-filter/#purpose","title":"Purpose","text":"<p>The <code>outlier_filter</code> is a package for filtering outlier of points.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/outlier-filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"Filter Name Description Detail radius search 2d outlier filter A method of removing point cloud noise based on the number of points existing within a certain radius link ring outlier filter A method of operating scan in chronological order and removing noise based on the rate of change in the distance between points link voxel grid outlier filter A method of removing point cloud noise based on the number of points existing within a voxel link polar voxel outlier filter A method of removing point cloud noise using polar coordinate voxels, optimized for LiDAR sensor characteristics link dual return outlier filter (under development) A method of removing rain and fog by considering the light reflected from the object in two stages according to the attenuation factor. link"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/outlier-filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/outlier-filter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/outlier-filter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/outlier-filter/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/outlier-filter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/passthrough-filter/","title":"passthrough_filter","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/passthrough-filter/#passthrough_filter","title":"passthrough_filter","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/passthrough-filter/#purpose","title":"Purpose","text":"<p>The <code>passthrough_filter</code> is a node that removes points on the outside of a range in a given field (e.g. x, y, z, intensity, ring, etc).</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/passthrough-filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/passthrough-filter/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/passthrough-filter/#input","title":"Input","text":"Name Type Description <code>~/input/points</code> <code>sensor_msgs::msg::PointCloud2</code> reference points <code>~/input/indices</code> <code>pcl_msgs::msg::Indices</code> reference indices"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/passthrough-filter/#output","title":"Output","text":"Name Type Description <code>~/output/points</code> <code>sensor_msgs::msg::PointCloud2</code> filtered points"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/passthrough-filter/#parameters","title":"Parameters","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/passthrough-filter/#core-parameters","title":"Core Parameters","text":"Name Type Description Default Range filter_limit_min integer minimum allowed field value 0 \u22650 filter_limit_max integer maximum allowed field value 127 \u22650 filter_field_name string filtering field name channel N/A keep_organized boolean flag to keep indices structure false N/A filter_limit_negative boolean flag to return whether the data is inside limit or not false N/A"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/passthrough-filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/passthrough-filter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/passthrough-filter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/passthrough-filter/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/passthrough-filter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/pointcloud-accumulator/","title":"pointcloud_accumulator","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/pointcloud-accumulator/#pointcloud_accumulator","title":"pointcloud_accumulator","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/pointcloud-accumulator/#purpose","title":"Purpose","text":"<p>The <code>pointcloud_accumulator</code> is a node that accumulates pointclouds for a given amount of time.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/pointcloud-accumulator/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/pointcloud-accumulator/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/pointcloud-accumulator/#input","title":"Input","text":"Name Type Description <code>~/input/points</code> <code>sensor_msgs::msg::PointCloud2</code> reference points"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/pointcloud-accumulator/#output","title":"Output","text":"Name Type Description <code>~/output/points</code> <code>sensor_msgs::msg::PointCloud2</code> filtered points"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/pointcloud-accumulator/#parameters","title":"Parameters","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/pointcloud-accumulator/#core-parameters","title":"Core Parameters","text":"Name Type Description Default Range accumulation_time_sec float accumulation period [s] 2 \u22650 pointcloud_buffer_size integer buffer size 50 \u22650"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/pointcloud-accumulator/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/pointcloud-accumulator/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/pointcloud-accumulator/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/pointcloud-accumulator/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/pointcloud-accumulator/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/pointcloud-densifier/","title":"PointCloud Densifier","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/pointcloud-densifier/#pointcloud-densifier","title":"PointCloud Densifier","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/pointcloud-densifier/#purpose","title":"Purpose","text":"<p>The <code>pointcloud_densifier</code> enhances sparse point cloud data by leveraging information from previous LiDAR frames, creating a denser representation especially for long-range points. This is particularly useful for improving perception of distant objects where LiDAR data tends to be sparse.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/pointcloud-densifier/#inner-workings-algorithm","title":"Inner-workings / Algorithm","text":"<p>The algorithm works as follows:</p> <ol> <li> <p>ROI Filtering: First filters the input point cloud to only keep points in a specific region of interest (ROI),    typically focused on the distant area in front of the vehicle.</p> </li> <li> <p>Occupancy Grid Creation: Creates a 2D occupancy grid from the filtered points to track which areas contain valid    points in the current frame.</p> </li> <li> <p>Previous Frame Integration: Transforms points from previous frames into the current frame's coordinate system    using TF transformations.</p> </li> <li> <p>Selective Point Addition: Adds points from previous frames only if they fall into grid cells that are occupied    in the current frame. This ensures that only relevant points are added, avoiding ghost points from dynamic objects.</p> </li> <li> <p>Combined Output: Returns a combined point cloud that includes both the current frame's points and selected    points from previous frames.</p> </li> </ol>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/pointcloud-densifier/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/pointcloud-densifier/#input","title":"Input","text":"Name Type Description <code>input</code> <code>sensor_msgs::msg::PointCloud2</code> Input point cloud"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/pointcloud-densifier/#output","title":"Output","text":"Name Type Description <code>output</code> <code>sensor_msgs::msg::PointCloud2</code> Densified point cloud output"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/pointcloud-densifier/#parameters","title":"Parameters","text":"Name Type Default Value Description <code>num_previous_frames</code> int 1 Number of previous frames to consider <code>x_min</code> double 80.0 Minimum x coordinate of ROI in meters <code>x_max</code> double 200.0 Maximum x coordinate of ROI in meters <code>y_min</code> double -20.0 Minimum y coordinate of ROI in meters <code>y_max</code> double 20.0 Maximum y coordinate of ROI in meters <code>grid_resolution</code> double 0.3 Resolution of occupancy grid in meters"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/pointcloud-densifier/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<ul> <li>The filter assumes that the TF tree contains valid transformations between coordinate frames from previous point clouds to the current frame.</li> <li>Performance depends on the number of previous frames used - more frames increase density but also processing time.</li> <li>The filter performs best on static elements in the scene, as dynamic objects may create artifacts if they move between frames.</li> <li>The accuracy of the densification depends on the quality of the TF transformations and ego-vehicle motion estimation.</li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/pointcloud-densifier/#usage","title":"Usage","text":"<p>The pointcloud_densifier can be launched using:</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/","title":"Polar Voxel Outlier Filter","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#polar-voxel-outlier-filter","title":"Polar Voxel Outlier Filter","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#overview","title":"Overview","text":"<p>The Polar Voxel Outlier Filter is a point cloud outlier filtering algorithm that operates in polar coordinate space for LiDAR data processing. This filter supports both simple occupancy-based filtering and advanced two-criteria filtering with return type classification for enhanced noise removal.</p> <p>Key Features:</p> <ul> <li>Flexible filtering modes with configurable return type classification</li> <li>Automatic format detection between PointXYZIRC and PointXYZIRCAEDT</li> <li>Two-criteria filtering using primary and secondary return analysis (when enabled)</li> <li>Range-aware visibility estimation for improved diagnostic accuracy</li> <li>Comprehensive diagnostics with filter ratio and visibility metrics</li> <li>Visibility estimation only mode for diagnostic-only operation without point cloud output</li> <li>Optional debug support with noise point cloud publishing for analysis</li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#purpose","title":"Purpose","text":"<p>The purpose is to remove point cloud noise such as insects and rain using a polar coordinate voxel grid approach optimized for LiDAR sensor characteristics. This filter provides configurable filtering methods:</p> <ol> <li>Simple Mode: Basic occupancy filtering (any return type counts equally)</li> <li>Advanced Mode: Two-criteria filtering with return type classification for enhanced accuracy</li> <li>Visibility Estimation Only Mode: Diagnostic-only operation for monitoring without data processing overhead</li> </ol> <p>The advanced mode concept is that when two returns exist, the first return is more likely to represent a region of noise (rain, fog, smoke, and so on).</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#key-differences-from-cartesian-voxel-grid-filter","title":"Key Differences from Cartesian Voxel Grid Filter","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#coordinate-system","title":"Coordinate System","text":"<ul> <li>Cartesian Voxel Grid: Divides 3D space into regular cubic voxels using (x, y, z) coordinates</li> <li>Polar Voxel Grid: Divides 3D space into polar voxels using (radius, azimuth, elevation) coordinates</li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#advantages-of-polar-voxelization","title":"Advantages of Polar Voxelization","text":"<ol> <li>Natural LiDAR Representation: LiDAR sensors naturally scan in polar patterns, making polar voxels more aligned with the data structure</li> <li>Adaptive Resolution: Automatically provides higher angular resolution at closer distances and lower resolution at far distances</li> <li>Range-Aware Filtering: Can apply different filtering strategies based on distance from sensor</li> <li>Range-Aware Visibility: Configurable range limit for accurate visibility estimation</li> <li>Azimuthal Uniformity: Maintains consistent azimuthal coverage regardless of distance</li> <li>Configurable Return Type Classification: Optional return type analysis for enhanced filtering</li> <li>Diagnostic-Only Operation: Visibility estimation without point cloud processing overhead</li> </ol>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#point-cloud-format-support","title":"Point Cloud Format Support","text":"<p>This filter supports point clouds with return type information (required for advanced mode) and automatically detects between two formats:</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#pointxyzirc-format","title":"PointXYZIRC Format","text":"<ul> <li>Usage: Point format with (x, y, z, intensity, return_type, channel) fields</li> <li>Processing: Computes polar coordinates (radius, azimuth, elevation) from Cartesian coordinates</li> <li>Return Type: Uses return_type field for classification (when enabled)</li> <li>Performance: Good performance with coordinate conversion overhead</li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#pointxyzircaedt-format","title":"PointXYZIRCAEDT Format","text":"<ul> <li>Usage: Point clouds with pre-computed polar coordinate fields and return type</li> <li>Fields: (x, y, z, intensity, return_type, channel, azimuth, elevation, distance, time_stamp)</li> <li>Detection: Automatically detects when polar coordinate fields are present</li> <li>Processing: Uses pre-computed polar coordinates directly, no conversion needed</li> <li>Performance: Faster processing as it avoids trigonometric calculations</li> </ul> <pre><code># PointXYZIRC: Computes polar coordinates from Cartesian\n# - x, y, z       (float32): Cartesian coordinates\n# - intensity     (float32): Point intensity\n# - return_type   (uint8):   Return type classification\n# - channel       (uint16):  Channel information\n\n# PointXYZIRCAEDT: Uses pre-computed polar coordinates\n# - x, y, z       (float32): Cartesian coordinates\n# - intensity     (float32): Point intensity\n# - return_type   (uint8):   Return type classification\n# - channel       (uint16):  Channel information\n# - azimuth       (float32): Pre-computed azimuth angle\n# - elevation     (float32): Pre-computed elevation angle\n# - distance      (float32): Pre-computed radius\n# - time_stamp    (uint32):  Point timestamp\n</code></pre> <p>Note: The filter automatically detects the format and uses the appropriate processing path. Return type classification can be enabled or disabled based on requirements.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#coordinate-conversion","title":"Coordinate Conversion","text":"<p>For PointXYZIRC format: Each point (x, y, z) is converted to polar coordinates:</p> <ul> <li>Radius: <code>r = sqrt(x\u00b2 + y\u00b2 + z\u00b2)</code></li> <li>Azimuth: <code>\u03b8 = atan2(y, x)</code></li> <li>Elevation: <code>\u03c6 = atan2(z, sqrt(x\u00b2 + y\u00b2))</code></li> </ul> <p>For PointXYZIRCAEDT format: Uses pre-computed polar coordinates directly from the point fields:</p> <ul> <li>Radius: <code>r = point.distance</code></li> <li>Azimuth: <code>\u03b8 = point.azimuth</code></li> <li>Elevation: <code>\u03c6 = point.elevation</code></li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#voxel-index-calculation","title":"Voxel Index Calculation","text":"<p>Each point is assigned to a voxel based on:</p> <ul> <li>Radius Index: <code>floor(radius / radial_resolution_m)</code></li> <li>Azimuth Index: <code>floor(azimuth / azimuth_resolution_rad)</code></li> <li>Elevation Index: <code>floor(elevation / elevation_resolution_rad)</code></li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#return-type-classification","title":"Return Type Classification","text":"<p>When <code>use_return_type_classification=true</code>, points are classified using the <code>return_type</code> field:</p> <ul> <li>Primary Returns: Return types specified in <code>primary_return_types</code> parameter (default: [1,6,8,10])</li> <li>Secondary Returns: All other return types not specified as primary</li> <li>Classification: Used for advanced two-criteria filtering</li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#range-aware-visibility-estimation","title":"Range-Aware Visibility Estimation","text":"<p>The visibility metric is calculated only for voxels within the configured range:</p> <ul> <li>Range Filtering: Only voxels with maximum radius \u2264 <code>visibility_estimation_max_range_m</code>, <code>visibility_estimation_min_azimuth_rad</code> \u2264 azimuth \u2264 <code>visibility_estimation_max_azimuth_rad</code>, and <code>visibility_estimation_min_elevation_rad</code> \u2264 elevation \u2264 <code>visibility_estimation_max_elevation_rad</code> are considered</li> <li>Visibility Estimation Tuning: Reported visibility value is tuned using the <code>visibility_estimation_max_secondary_voxel_count</code> parameter</li> <li>Reliability: Excludes potentially unreliable distant measurements from visibility calculations</li> <li>Configurable: Allows adjustment based on sensor characteristics and requirements</li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#filtering-methodology","title":"Filtering Methodology","text":"<p>The filter uses different algorithms based on the <code>use_return_type_classification</code> parameter and can operate in two output modes:</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#normal-mode-visibility_estimation_onlyfalse","title":"Normal Mode (<code>visibility_estimation_only=false</code>)","text":"<ol> <li>Format Detection: Automatically detects PointXYZIRC vs PointXYZIRCAEDT</li> <li>Coordinate Processing: Uses appropriate coordinate source</li> <li>Voxel Processing: Groups points into polar voxels</li> <li>Filtering Logic: Applies simple or advanced filtering</li> <li>Output Generation: Creates filtered point cloud</li> <li>Diagnostics: Publishes filter ratio and visibility metrics</li> <li>Optional Noise Cloud: Publishes filtered-out points if enabled</li> </ol>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#visibility-estimation-only-mode-visibility_estimation_onlytrue","title":"Visibility Estimation Only Mode (<code>visibility_estimation_only=true</code>)","text":"<ol> <li>Format Detection: Same as normal mode</li> <li>Coordinate Processing: Same as normal mode</li> <li>Voxel Processing: Same as normal mode</li> <li>Filtering Logic: Same as normal mode (for accurate diagnostics)</li> <li>Output Generation: SKIPPED - creates empty output for interface compatibility</li> <li>Diagnostics: ALWAYS PUBLISHED - full visibility and filter ratio metrics</li> <li>Noise Cloud: SKIPPED - no noise cloud generation regardless of <code>publish_noise_cloud</code> setting</li> </ol> <p>Use Cases for Visibility Estimation Only Mode:</p> <ul> <li>Environmental monitoring: Track visibility conditions without processing overhead</li> <li>Sensor health monitoring: Monitor LiDAR performance without data pipeline impact</li> <li>Algorithm validation: Test filtering parameters without output processing</li> <li>Diagnostic-only applications: Pure monitoring without data transformation</li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#simple-mode-use_return_type_classificationfalse","title":"Simple Mode (<code>use_return_type_classification=false</code>)","text":"<ol> <li>Format Detection: Automatically detects PointXYZIRC vs PointXYZIRCAEDT</li> <li>Coordinate Processing:<ul> <li>PointXYZIRC: Computes polar coordinates from Cartesian</li> <li>PointXYZIRCAEDT: Uses pre-computed polar coordinates</li> </ul> </li> <li>Voxel Binning: Points are grouped into polar voxels</li> <li>Simple Thresholding: Voxels with \u2265 <code>voxel_points_threshold</code> points (any return type) are kept</li> <li>Output: Filtered point cloud with basic noise removal (unless visibility-only mode)</li> </ol>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#advanced-mode-use_return_type_classificationtrue","title":"Advanced Mode (<code>use_return_type_classification=true</code>)","text":"<ol> <li>Format Detection: Automatically detects PointXYZIRC vs PointXYZIRCAEDT</li> <li>Return Type Validation: Ensures return_type field is present</li> <li>Coordinate Processing:<ul> <li>PointXYZIRC: Computes polar coordinates from Cartesian</li> <li>PointXYZIRCAEDT: Uses pre-computed polar coordinates</li> </ul> </li> <li>Return Type Classification: Points are classified as primary or secondary returns</li> <li>Two-Criteria Filtering:<ul> <li>Criterion 1: Primary returns \u2265 <code>voxel_points_threshold</code></li> <li>Criterion 2: Secondary returns \u2264 <code>secondary_noise_threshold</code></li> <li>Both criteria must be satisfied for a voxel to be kept</li> </ul> </li> <li>Range-Aware Visibility: Visibility calculation limited to voxels within <code>visibility_estimation_max_range_m</code>, <code>visibility_estimation_(min|max)_azimuth_rad</code>, and <code>visibility_estimation_(min|max)_elevation_rad</code>, and secondary voxel count limited by <code>visibility_estimation_max_secondary_voxel_count</code></li> <li>Secondary Return Filtering: Optional exclusion of secondary returns from output</li> <li>Output: Filtered point cloud with enhanced noise removal (unless visibility-only mode)</li> </ol>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#advanced-two-criteria-filtering","title":"Advanced Two-Criteria Filtering","text":"<p>When enabled, for each voxel both criteria must be satisfied:</p> <ul> <li>Primary Return Threshold: <code>primary_count &gt;= voxel_points_threshold</code></li> <li>Secondary Return Threshold: <code>secondary_count &lt;= secondary_noise_threshold</code></li> <li>Final Decision: <code>valid_voxel = (primary_threshold_met AND secondary_threshold_met)</code></li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#key-features","title":"Key Features","text":"<ul> <li>Flexible Architecture: Configurable between simple and advanced filtering</li> <li>Format-Optimized Processing: Automatic selection of optimal coordinate source</li> <li>Range-Aware Diagnostics: Visibility estimation limited to reliable sensor range</li> <li>Secondary Voxel Limiting: Configurable limit on secondary voxels for visibility estimation</li> <li>Visibility-Only Mode: Diagnostic operation without point cloud output</li> <li>Comprehensive Diagnostics: Mode-specific filter ratio and visibility metrics</li> <li>Debug Support: Optional noise cloud publishing for analysis and tuning</li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#return-type-management-advanced-mode-only","title":"Return Type Management (Advanced Mode Only)","text":"<ul> <li>Primary Returns: Configurable list of return types (default: [1,6,8,10])</li> <li>Secondary Returns: All return types not specified as primary</li> <li>Dynamic Classification: Runtime configurable through parameter updates</li> <li>Output Filtering: Optional exclusion of secondary returns from final output</li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#inputs-outputs","title":"Inputs / Outputs","text":"<p>This implementation inherits <code>autoware::pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#input-requirements","title":"Input Requirements","text":"<ul> <li>Supported Formats: PointXYZIRC or PointXYZIRCAEDT</li> <li>Return Type Field: Required only when <code>use_return_type_classification=true</code></li> <li>Invalid Inputs: Point clouds without return_type field will be rejected in advanced mode</li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#additional-debug-topics","title":"Additional Debug Topics","text":"Name Type Description <code>~/polar_voxel_outlier_filter/debug/filter_ratio</code> <code>autoware_internal_debug_msgs::msg::Float32Stamped</code> Ratio of output to input points (always published) <code>~/polar_voxel_outlier_filter/debug/visibility</code> <code>autoware_internal_debug_msgs::msg::Float32Stamped</code> Ratio of voxels passing secondary return threshold test (advanced mode only, range-limited) <code>~/polar_voxel_outlier_filter/debug/pointcloud_noise</code> <code>sensor_msgs::msg::PointCloud2</code> Filtered-out points for debugging (when enabled and not in visibility-only mode)"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#parameters","title":"Parameters","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#node-parameters","title":"Node Parameters","text":"<p>This implementation inherits <code>autoware::pointcloud_preprocessor::Filter</code> class, please refer README.</p> Name Type Description Default Range radial_resolution_m float The resolution in radial direction [m]. 0.5 \u22650 azimuth_resolution_rad float The resolution in azimuth direction [rad]. The supplied value is modified to ensure consistent voxel sizes across the whole azimuth range. 0.0175 \u22650\u22646.283185307179586 elevation_resolution_rad float The resolution in elevation direction [rad]. The supplied value is modified to ensure consistent voxel sizes across the whole elevation range. 0.0175 \u22650\u22646.283185307179586 voxel_points_threshold integer The minimum number of points required per voxel. 2 \u22651 min_radius_m float The minimum radius to consider [m]. 0.5 \u22650 max_radius_m float The maximum radius to consider [m]. 300.0 \u22650 intensity_threshold integer The maximum intensity threshold for secondary returns (primary returns are not affected). 2 \u22650\u2264255 visibility_estimation_max_range_m float The maximum range to consider for visibility estimation [m] (limits visibility calculations to reliable sensor range). 20.0 \u22650 visibility_estimation_min_azimuth_rad float The minimum range to consider for visibility estimation [rad] (limits visibility calculations to reliable sensor range). 0.0 \u22650.0\u22646.28 visibility_estimation_max_azimuth_rad float The maximum range to consider for visibility estimation [rad] (limits visibility calculations to reliable sensor range). 6.28 \u22650.0\u22646.28 visibility_estimation_min_elevation_rad float The minimum range to consider for visibility estimation [rad] (limits visibility calculations to reliable sensor range). -1.57 \u2265-1.57\u22641.57 visibility_estimation_max_elevation_rad float The maximum range to consider for visibility estimation [rad] (limits visibility calculations to reliable sensor range). 1.57 \u2265-1.57\u22641.57 use_return_type_classification boolean Whether to use return type classification (if false, all returns are processed). true N/A filter_secondary_returns boolean Whether to filter secondary returns (if true, only primary returns are kept). false N/A secondary_noise_threshold float Threshold for classifying primary vs secondary returns. 4 \u22650 visibility_estimation_max_secondary_voxel_count integer Maximum secondary voxel count for visibility estimation calculations. 500 \u22650 primary_return_types array List of return type values considered as primary returns (all others are secondary). [1, 6, 8, 10] N/A visibility_estimation_only boolean Whether to run filter for visibility estimation only without publishing point cloud outputs (for monitoring/diagnostic purposes only). false N/A publish_noise_cloud boolean Whether to generate and publish noise point cloud for debugging (ignored when visibility_estimation_only=true). false N/A filter_ratio_error_threshold float The filter ratio threshold for error diagnostics (ratio of output/input points). 0.5 \u22650\u22641 filter_ratio_warn_threshold float The filter ratio threshold for warning diagnostics (ratio of output/input points). 0.7 \u22650\u22641 visibility_error_threshold float The visibility threshold for error diagnostics. 0.8 \u22650\u22641 visibility_warn_threshold float The visibility threshold for warning diagnostics. 0.9 \u22650\u22641 publish_area_marker boolean Publish a marker to visualize region used for visibility estimation false N/A num_frames_hysteresis_transition float The number of frames to be required to transition judgement 1 \u22651 immediate_report_error boolean Whether to report error state immediately if a single frame categorized as error false N/A immediate_relax_state boolean Whether to relax state if observed state is better than the current one false N/A"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#parameter-interactions","title":"Parameter Interactions","text":"<ul> <li>use_return_type_classification: Must be <code>true</code> to enable advanced two-criteria filtering</li> <li>filter_secondary_returns: When <code>true</code>, only primary returns appear in output (advanced mode only)</li> <li>secondary_noise_threshold: Only used when <code>use_return_type_classification=true</code></li> <li>visibility_estimation_max_secondary_voxel_count: Only used when <code>use_return_type_classification=true</code>, limits secondary voxel counting in visibility calculations</li> <li>primary_return_types: Only used when <code>use_return_type_classification=true</code></li> <li>visibility_estimation_max_range_m: Limits visibility calculation to reliable sensor range (advanced mode only)</li> <li>visibility_estimation_only: When <code>true</code>, skips point cloud output generation but still calculates and publishes diagnostics</li> <li>publish_noise_cloud: When <code>false</code>, improves performance by skipping noise cloud generation (ignored when <code>visibility_estimation_only=true</code>)</li> <li>Diagnostics: Visibility is only published when return type classification is enabled</li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#configuration-examples","title":"Configuration Examples","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#visibility-only-configuration","title":"Visibility-Only Configuration","text":"<pre><code># Run filter for diagnostics only - no point cloud processing\nvisibility_estimation_only: true\nuse_return_type_classification: true\nvoxel_points_threshold: 2\nsecondary_noise_threshold: 4\nvisibility_estimation_max_secondary_voxel_count: 500\nvisibility_estimation_max_range_m: 20.0\nprimary_return_types: [1, 6, 8, 10]\nradial_resolution_m: 0.5\nazimuth_resolution_rad: 0.0175\nelevation_resolution_rad: 0.0175\n# publish_noise_cloud is ignored in visibility-only mode\n</code></pre>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#simple-mode-configuration","title":"Simple Mode Configuration","text":"<pre><code># Basic occupancy filtering - any return type counts equally\nuse_return_type_classification: false\nvoxel_points_threshold: 2 # Total points threshold\nradial_resolution_m: 0.5\nazimuth_resolution_rad: 0.0175 # ~1 degree\nelevation_resolution_rad: 0.0175 # ~1 degree\nvisibility_estimation_only: false # Normal filtering mode\npublish_noise_cloud: true # Enable for debugging\n</code></pre>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#advanced-mode-configuration","title":"Advanced Mode Configuration","text":"<pre><code># Two-criteria filtering with return type classification\nuse_return_type_classification: true\nvoxel_points_threshold: 2 # Primary return threshold\nsecondary_noise_threshold: 4 # Secondary return threshold\nvisibility_estimation_max_secondary_voxel_count: 500 # Max secondary voxels for visibility\nprimary_return_types: [1, 6, 8, 10] # Primary return types\nfilter_secondary_returns: false # Include secondary returns in output\nradial_resolution_m: 0.5\nazimuth_resolution_rad: 0.0175 # ~1 degree\nelevation_resolution_rad: 0.0175 # ~1 degree\nvisibility_estimation_max_range_m: 20.0 # Range limit for visibility calculation\nvisibility_estimation_min_azimuth_rad: 0.78\nvisibility_estimation_max_azimuth_rad: 2.35\nvisibility_estimation_min_elevation_rad: -0.26\nvisibility_estimation_max_elevation_rad: 1.04\nvisibility_estimation_only: false # Normal filtering mode\npublish_noise_cloud: true\n</code></pre>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#debug-configuration","title":"Debug Configuration","text":"<pre><code># Enable debugging and monitoring\nuse_return_type_classification: true\nvisibility_estimation_max_range_m: 20.0 # Configured range for urban environments\nvisibility_estimation_min_azimuth_rad: 0.78\nvisibility_estimation_max_azimuth_rad: 2.35\nvisibility_estimation_min_elevation_rad: -0.26\nvisibility_estimation_max_elevation_rad: 1.04\nvisibility_estimation_max_secondary_voxel_count: 500 # Allow secondary voxels in visibility calculation\nvisibility_estimation_only: false # Normal filtering with debug output\npublish_noise_cloud: true # Enable noise cloud for analysis\nfilter_ratio_error_threshold: 0.5\nfilter_ratio_warn_threshold: 0.7\nvisibility_error_threshold: 0.8\nvisibility_warn_threshold: 0.9\n</code></pre>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#sensor-specific-configuration-examples","title":"Sensor-Specific Configuration Examples","text":"<pre><code># Long-range highway LiDAR\nvisibility_estimation_max_range_m: 200.0\nvisibility_estimation_max_secondary_voxel_count: 500\nradial_resolution_m: 0.5\nvoxel_points_threshold: 2\nvisibility_estimation_only: false\n\n# Urban short-range LiDAR\nvisibility_estimation_max_range_m: 20.0\nvisibility_estimation_max_secondary_voxel_count: 500\nradial_resolution_m: 0.5\nvoxel_points_threshold: 2\nvisibility_estimation_only: false\n\n# High-resolution near-field processing\nvisibility_estimation_max_range_m: 20.0\nvisibility_estimation_max_secondary_voxel_count: 500\nradial_resolution_m: 0.5\nazimuth_resolution_rad: 0.0175 # ~1 degree\nelevation_resolution_rad: 0.0175 # ~1 degree\nvisibility_estimation_only: false\n\n# Environmental monitoring only\nvisibility_estimation_max_range_m: 50.0\nvisibility_estimation_max_secondary_voxel_count: 500\nradial_resolution_m: 1.0 # Coarser resolution for performance\nvisibility_estimation_only: true # Diagnostics only\n</code></pre>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<ul> <li>Simple mode: Works with any point cloud format, basic occupancy filtering only</li> <li>Advanced mode: Requires return_type field for enhanced filtering</li> <li>Visibility-only mode: Runs full filtering algorithm but produces no point cloud output</li> <li>Supported formats: PointXYZIRC and PointXYZIRCAEDT only</li> <li>Finite coordinates required: Automatically filters out NaN/Inf points</li> <li>Return type dependency: Advanced filtering effectiveness depends on accurate return type classification</li> <li>Visibility range dependency: Visibility accuracy depends on appropriate <code>visibility_estimation_max_range_m</code>, <code>visibility_estimation_(min|max)_azimuth_rad</code>, and <code>visibility_estimation_(min|max)_elevation_rad</code> settings. Besides, for azimuth and elevation range, the following definitions are assumed<ul> <li>azimuth: starts with the y-axis, increasing in counter-corkscrew rule around the z-axis. The range domain is \\(<code>[0, 2\\pi]</code>\\)</li> <li>elevation: starts with the x-axis, increasing in counter-corkscrew rule around the y-axis. The range domain is \\(<code>[-\\frac{\\pi}{2}, \\frac{pi}{2}]</code>\\)</li> </ul> </li> <li>Secondary voxel limiting: Visibility estimation can be tuned via <code>visibility_estimation_max_secondary_voxel_count</code></li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#error-detection-and-handling","title":"Error detection and handling","text":"<p>The filter includes robust error handling:</p> <ul> <li>Mode-specific validation: Checks return_type field presence in advanced mode</li> <li>Input validation: Checks for null point clouds</li> <li>Coordinate validation: Filters invalid points (NaN, Inf values) automatically</li> <li>Range validation: Points outside configured radius ranges are excluded</li> <li>Parameter validation: Ensures <code>visibility_estimation_max_range_m</code> &gt; 0 and <code>visibility_estimation_max_secondary_voxel_count</code> \u2265 0</li> <li>Dynamic parameter validation: Runtime parameter updates with validation</li> <li>Mode compatibility: Validates parameter combinations for different operating modes</li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#usage","title":"Usage","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#launch-the-filter","title":"Launch the Filter","text":"<pre><code># Example launch file integration for simple mode:\n# &lt;node pkg=\"autoware_pointcloud_preprocessor\" exec=\"polar_voxel_outlier_filter_node\" name=\"polar_voxel_filter\"&gt;\n#   &lt;param name=\"use_return_type_classification\" value=\"false\"/&gt;\n#   &lt;param name=\"radial_resolution_m\" value=\"1.0\"/&gt;\n#   &lt;param name=\"azimuth_resolution_rad\" value=\"0.0349\"/&gt;\n#   &lt;param name=\"voxel_points_threshold\" value=\"3\"/&gt;\n#   &lt;param name=\"visibility_estimation_only\" value=\"false\"/&gt;\n# &lt;/node&gt;\n\n# Example launch file integration for advanced mode:\n# &lt;node pkg=\"autoware_pointcloud_preprocessor\" exec=\"polar_voxel_outlier_filter_node\" name=\"polar_voxel_filter\"&gt;\n#   &lt;param name=\"use_return_type_classification\" value=\"true\"/&gt;\n#   &lt;param name=\"radial_resolution_m\" value=\"0.5\"/&gt;\n#   &lt;param name=\"azimuth_resolution_rad\" value=\"0.0175\"/&gt;\n#   &lt;param name=\"voxel_points_threshold\" value=\"2\"/&gt;\n#   &lt;param name=\"secondary_noise_threshold\" value=\"4\"/&gt;\n#   &lt;param name=\"visibility_estimation_max_secondary_voxel_count\" value=\"500\"/&gt;\n#   &lt;param name=\"primary_return_types\" value=\"[1,6,8,10]\"/&gt;\n#   &lt;param name=\"visibility_estimation_max_range_m\" value=\"20.0\"/&gt;\n#   &lt;param name=\"visibility_estimation_min_azimuth_rad\" value=\"0.78\"/&gt;\n#   &lt;param name=\"visibility_estimation_max_azimuth_rad\" value=\"2.35\"/&gt;\n#   &lt;param name=\"visibility_estimation_min_elevation_rad\" value=\"-0.26\"/&gt;\n#   &lt;param name=\"visibility_estimation_max_elevation_rad\" value=\"1.04\"/&gt;\n#   &lt;param name=\"visibility_estimation_only\" value=\"false\"/&gt;\n# &lt;/node&gt;\n\n# Example launch file integration for visibility-only mode:\n# &lt;node pkg=\"autoware_pointcloud_preprocessor\" exec=\"polar_voxel_outlier_filter_node\" name=\"polar_voxel_filter\"&gt;\n#   &lt;param name=\"use_return_type_classification\" value=\"true\"/&gt;\n#   &lt;param name=\"visibility_estimation_only\" value=\"true\"/&gt;\n#   &lt;param name=\"visibility_estimation_max_range_m\" value=\"20.0\"/&gt;\n#   &lt;param name=\"visibility_estimation_min_azimuth_rad\" value=\"0.78\"/&gt;\n#   &lt;param name=\"visibility_estimation_max_azimuth_rad\" value=\"2.35\"/&gt;\n#   &lt;param name=\"visibility_estimation_min_elevation_rad\" value=\"-0.26\"/&gt;\n#   &lt;param name=\"visibility_estimation_max_elevation_rad\" value=\"1.04\"/&gt;\n#   &lt;param name=\"visibility_estimation_max_secondary_voxel_count\" value=\"500\"/&gt;\n# &lt;/node&gt;\n</code></pre>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#ros-2-topics","title":"ROS 2 Topics","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#inputoutput","title":"Input/Output","text":"<ul> <li>Input: <code>/input</code> (sensor_msgs/PointCloud2) - Must have return_type field for advanced mode</li> <li>Output: <code>/output</code> (sensor_msgs/PointCloud2) - Empty in visibility-only mode</li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#debug-topics","title":"Debug Topics","text":"<ul> <li>Filter Ratio: <code>~/polar_voxel_outlier_filter/debug/filter_ratio</code> (autoware_internal_debug_msgs/Float32Stamped) - Always published</li> <li>Visibility: <code>~/polar_voxel_outlier_filter/debug/visibility</code> (autoware_internal_debug_msgs/Float32Stamped) - Advanced mode only, range-limited</li> <li>Noise Cloud: <code>~/polar_voxel_outlier_filter/debug/pointcloud_noise</code> (sensor_msgs/PointCloud2) - Not published in visibility-only mode</li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#programmatic-usage","title":"Programmatic Usage","text":"<pre><code>#include \"autoware/pointcloud_preprocessor/outlier_filter/polar_voxel_outlier_filter_node.hpp\"\n\n// Create node\nauto node = std::make_shared&lt;autoware::pointcloud_preprocessor::PolarVoxelOutlierFilterComponent&gt;(options);\n\n// The filter automatically detects point cloud format and applies filtering based on configuration:\n// - Simple mode (use_return_type_classification=false): Basic occupancy filtering\n// - Advanced mode (use_return_type_classification=true): Two-criteria filtering with return type analysis\n// - Visibility-only mode (visibility_estimation_only=true): Diagnostics without point cloud output\n// - Both modes support PointXYZIRC and PointXYZIRCAEDT formats\n// - Advanced mode uses range-limited visibility estimation with configurable secondary voxel limiting\n</code></pre>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#performance-characterization","title":"Performance characterization","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#computational-complexity","title":"Computational Complexity","text":"<ul> <li>Time Complexity: O(n) where n is the number of input points</li> <li>Space Complexity: O(v) where v is the number of occupied voxels</li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#performance-impact-by-mode","title":"Performance Impact by Mode","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#visibility-estimation-only-mode","title":"Visibility Estimation Only Mode","text":"<ul> <li>Computational: Runs full filtering algorithm for accurate diagnostics</li> <li>Memory: Minimal memory usage - no output point cloud allocation</li> <li>I/O: Publishes diagnostics only, no point cloud output</li> <li>Use Case: Optimal for monitoring applications without data processing needs</li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#normal-mode","title":"Normal Mode","text":"<ul> <li>PointXYZIRCAEDT: Optimal performance with pre-computed coordinates</li> <li>PointXYZIRC: Good performance with coordinate conversion overhead</li> <li>Output Processing: Full point cloud generation and optional noise cloud</li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#simple-mode","title":"Simple Mode","text":"<ul> <li>PointXYZIRCAEDT: Fast processing with pre-computed coordinates</li> <li>PointXYZIRC: Good performance with coordinate conversion overhead</li> <li>No return type analysis: Reduced computational overhead</li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#advanced-mode","title":"Advanced Mode","text":"<ul> <li>PointXYZIRCAEDT: Optimal performance with pre-computed coordinates and return type analysis</li> <li>PointXYZIRC: Good performance with coordinate conversion and return type analysis</li> <li>Enhanced filtering: Additional return type classification and range-aware visibility processing</li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#memory-usage","title":"Memory Usage","text":"<ul> <li>Visibility-only mode: Significantly reduced memory footprint</li> <li>Hash-based voxel storage: Efficiently handles sparse voxel occupancy</li> <li>Single-pass processing: Minimal memory overhead regardless of mode</li> <li>Mode-specific outputs: Memory allocation optimized per mode</li> <li>Range filtering: Additional hash map for visibility calculation (advanced mode only)</li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#optimization-tips","title":"Optimization Tips","text":"<ol> <li>Use visibility estimation only mode for pure monitoring applications</li> <li>Choose appropriate mode based on requirements:<ul> <li>Normal mode for data processing needs</li> <li>Visibility-only mode for environmental/sensor monitoring</li> </ul> </li> <li>Use PointXYZIRCAEDT format when available for optimal performance</li> <li>Combine modes dynamically - switch at runtime based on operational needs</li> <li>Tune voxel resolutions based on your use case</li> <li>Configure return type mappings to match your sensor (advanced mode)</li> <li>Set appropriate visibility range (<code>visibility_estimation_max_range_m</code>) for your sensor and environment</li> <li>Tune secondary voxel limiting (<code>visibility_estimation_max_secondary_voxel_count</code>) for visibility estimation accuracy</li> <li>Monitor diagnostics for real-time performance assessment in both modes</li> </ol>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#diagnostics-and-monitoring","title":"Diagnostics and Monitoring","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#filter-ratio-diagnostics","title":"Filter Ratio Diagnostics","text":"<ul> <li>Published for all modes: Overall filtering effectiveness (output/input ratio)</li> <li>Configurable thresholds: Error/warning levels for automated monitoring</li> <li>Real-time feedback: Immediate filtering performance assessment</li> <li>Visibility-only mode: Shows theoretical filter effectiveness without actual filtering</li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#visibility-diagnostics","title":"Visibility Diagnostics","text":"<ul> <li>Advanced mode only: Uses return type classification data</li> <li>Range-limited metric: Only considers voxels within <code>visibility_estimation_max_range_m</code>, <code>visibility_estimation_(min|max)_azimuth_rad</code> and <code>visibility_estimation_(min|max)_elevation_rad</code></li> <li>Secondary voxel limiting: Controlled by <code>visibility_estimation_max_secondary_voxel_count</code> parameter</li> <li>Voxel-based metric: Percentage of range-limited voxels passing secondary threshold test</li> <li>Environmental indicator: Useful for detecting sensor conditions within reliable range</li> <li>Diagnostic context: Status messages include the configured visibility estimation range and secondary voxel limits</li> <li>Available in all modes: Published even in visibility-only mode for monitoring</li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#debug-features","title":"Debug Features","text":"<ul> <li>Noise point cloud: All filtered-out points for analysis (when enabled and not in visibility-only mode)</li> <li>Runtime parameter updates: Dynamic threshold and range adjustment</li> <li>Mode-specific logging: Debug messages tailored to filtering mode</li> <li>Range-aware diagnostics: Visibility calculations clearly indicate the estimation range</li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#use-cases-and-configuration-guidelines","title":"Use Cases and Configuration Guidelines","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#visibility-estimation-only-mode-use-cases","title":"Visibility Estimation Only Mode Use Cases","text":"<ol> <li>Environmental Monitoring: Track atmospheric conditions without processing overhead</li> <li>Sensor Health Monitoring: Monitor LiDAR performance and visibility conditions</li> <li>Algorithm Validation: Test and tune filtering parameters without output generation</li> <li>Pure Diagnostics: Applications that only need visibility and filter ratio metrics</li> <li>Resource-Constrained Systems: Minimize computational load while maintaining monitoring</li> <li>Weather Station Integration: Automated visibility reporting for meteorological systems</li> </ol>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#simple-mode-use-cases","title":"Simple Mode Use Cases","text":"<ol> <li>Legacy System Integration: Basic filtering without return type requirements</li> <li>Performance-Critical Applications: When computational resources are limited</li> <li>Unknown Return Type Reliability: When sensor return type information is questionable</li> <li>Basic Noise Removal: Simple occupancy-based filtering requirements</li> </ol>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#advanced-mode-use-cases","title":"Advanced Mode Use Cases","text":"<ol> <li>Modern LiDAR Processing: Enhanced filtering with reliable return type information</li> <li>Environmental Monitoring: Range-aware visibility estimation and weather condition detection</li> <li>High-Quality Filtering: Two-criteria approach for superior noise removal</li> <li>Autonomous Vehicle Applications: Safety-critical filtering with comprehensive diagnostics</li> <li>Range-Specific Analysis: Different visibility requirements for near vs. far field</li> </ol>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#parameter-tuning-guidelines","title":"Parameter Tuning Guidelines","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#visibility-estimation-only-mode-configuration","title":"Visibility Estimation Only Mode Configuration","text":"<ul> <li>For environmental monitoring: Enable advanced mode with appropriate visibility range</li> <li>For sensor monitoring: Use return type classification for detailed analysis</li> <li>For performance: Larger voxel resolutions to reduce computation</li> <li>For accuracy: Smaller voxel resolutions for precise visibility estimation</li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#simple-mode-configuration_1","title":"Simple Mode Configuration","text":"<ul> <li>For dense environments: Smaller voxel resolutions, higher point thresholds</li> <li>For sparse data: Larger voxel resolutions, lower point thresholds</li> <li>For performance: Larger resolutions, disable noise cloud publishing</li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#advanced-mode-configuration_1","title":"Advanced Mode Configuration","text":"<ul> <li>For aggressive noise removal: Lower secondary noise threshold (0-2)</li> <li>For conservative filtering: Higher secondary noise threshold (3-5)</li> <li>For strict visibility estimation: Set <code>visibility_estimation_max_secondary_voxel_count</code> to a lower number</li> <li>For lenient visibility estimation: Allow higher secondary voxel counts (several hundred)</li> <li>For primary-only output: Enable <code>filter_secondary_returns</code></li> <li>For sensor-specific optimization: Adjust <code>primary_return_types</code> based on sensor characteristics</li> <li>For range-specific visibility: Set <code>visibility_estimation_max_range_m</code>, <code>visibility_estimation_(min|max)_azimuth_rad</code>, and <code>visibility_estimation_(min|max)_elevation_rad</code> based on sensor effective range and application requirements</li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#mode-selection-guidelines","title":"Mode Selection Guidelines","text":"<ul> <li>Choose visibility-only mode when:<ul> <li>Only diagnostic information is needed</li> <li>Computational resources are limited</li> <li>Running parallel monitoring alongside main processing</li> <li>Testing filter parameters without affecting downstream systems</li> </ul> </li> </ul> <ul> <li>Choose normal mode when:<ul> <li>Filtered point cloud output is required</li> <li>Integration into data processing pipelines</li> <li>Real-time filtering for perception systems</li> <li>Full functionality including noise cloud debugging</li> </ul> </li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#visibility-range-guidelines","title":"Visibility Range Guidelines","text":"<ul> <li>Urban environments: 30-80m (shorter ranges for reliable near-field analysis)</li> <li>Highway applications: 100-200m (longer ranges for high-speed scenarios)</li> <li>Parking/loading: 10-30m (very short ranges for precise near-field monitoring)</li> <li>Sensor specifications: Match to sensor's reliable detection range</li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#comparison-table","title":"Comparison Table","text":"Aspect Simple Mode Advanced Mode Visibility-Only Mode Filtering Method Basic occupancy Two-criteria with return type Same as selected mode Return Type Required No Yes Depends on mode selected Computational Cost Low Moderate Moderate (no output) Filtering Quality Good Excellent N/A (no output) Visibility Metrics None Range-aware with voxel limiting Range-aware with voxel limiting Configuration Simple Advanced Advanced (diagnostics-focused) Use Case Basic filtering Enhanced noise removal Monitoring/diagnostics Environmental Adapt Limited Comprehensive Comprehensive Range Awareness Basic Configurable Configurable Point Cloud Output Yes Yes No (empty)"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#migration-guide","title":"Migration Guide","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#enabling-visibility-estimation-only-mode","title":"Enabling Visibility Estimation Only Mode","text":"<p>To enable diagnostic-only operation:</p> <ol> <li>Set parameter: <code>visibility_estimation_only: true</code></li> <li>Configure diagnostics: Ensure proper visibility and filter ratio thresholds</li> <li>Verify mode: Check logs for \"visibility estimation only\" confirmation</li> <li>Monitor diagnostics: Use published metrics for monitoring</li> <li>No output expectation: Downstream nodes should handle empty point clouds</li> </ol>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#dynamic-mode-switching","title":"Dynamic Mode Switching","text":"<p>The mode can be changed at runtime:</p> <pre><code># Switch to visibility-only mode\nros2 param set /polar_voxel_filter visibility_estimation_only true\n\n# Switch back to normal mode\nros2 param set /polar_voxel_filter visibility_estimation_only false\n</code></pre>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#enabling-advanced-mode","title":"Enabling Advanced Mode","text":"<p>To enable advanced filtering on existing systems:</p> <ol> <li>Ensure return type field: Verify input point clouds have return_type field</li> <li>Set parameter: <code>use_return_type_classification: true</code></li> <li>Configure return types: Set <code>primary_return_types</code> for your sensor</li> <li>Set visibility range: Configure <code>visibility_estimation_max_range_m</code>, <code>visibility_estimation_(min|max)_azimuth_rad</code>, and <code>visibility_estimation_(min|max)_elevation_rad</code> for your application</li> <li>Configure secondary voxel limiting: Set <code>visibility_estimation_max_secondary_voxel_count</code> based on requirements</li> <li>Tune thresholds: Adjust <code>secondary_noise_threshold</code> based on requirements</li> <li>Monitor diagnostics: Use range-aware visibility metrics for performance assessment</li> </ol>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#disabling-advanced-mode","title":"Disabling Advanced Mode","text":"<p>To use simple mode for basic filtering:</p> <ol> <li>Set parameter: <code>use_return_type_classification: false</code></li> <li>Configure threshold: Set <code>voxel_points_threshold</code> for total point count</li> <li>Remove advanced parameters: Return type and visibility range parameters will be ignored</li> <li>Simplified monitoring: Only filter ratio diagnostics available</li> </ol>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#compatibility-considerations","title":"Compatibility Considerations","text":"<ul> <li>Output interface: Empty point clouds maintain topic compatibility in visibility-only mode</li> <li>Diagnostic topics: Same diagnostic information regardless of mode</li> <li>Parameter compatibility: All filtering parameters work in both normal and visibility-only modes</li> <li>Performance impact: Mode changes take effect on next filter call</li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/polar-voxel-outlier-filter/#updating-existing-configurations","title":"Updating Existing Configurations","text":"<p>For systems already using advanced mode, add the new parameters:</p> <pre><code># Add to existing configuration:\nvisibility_estimation_only: false # Default for normal operation\nvisibility_estimation_max_secondary_voxel_count: 500 # Updated default\nprimary_return_types: [1, 6, 8, 10] # Updated to include return type 8\n</code></pre> <p>This approach provides maximum flexibility with range-aware visibility estimation, configurable secondary voxel limiting, and diagnostic-only operation while maintaining optimal performance for all use cases! \ud83c\udfaf\u2728</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/","title":"radius_search_2d_outlier_filter","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#radius_search_2d_outlier_filter","title":"radius_search_2d_outlier_filter","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#purpose","title":"Purpose","text":"<p>The purpose is to remove point cloud noise such as insects and rain.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>RadiusOutlierRemoval filter which removes all indices in its input cloud that don\u2019t have at least some number of neighbors within a certain range.</p> <p>The description above is quoted from [1]. <code>pcl::search::KdTree</code> [2] is used to implement this package.</p> <p></p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#inputs-outputs","title":"Inputs / Outputs","text":"<p>This implementation inherits <code>autoware::pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#parameters","title":"Parameters","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#node-parameters","title":"Node Parameters","text":"<p>This implementation inherits <code>autoware::pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#core-parameters","title":"Core Parameters","text":"Name Type Description Default Range min_neighbors integer If points in the circle centered on reference point is less than min_neighbors, a reference point is judged as outlier 5 \u22650 search_radius float Searching number of points included in search_radius 0.2 \u22650"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>Since the method is to count the number of points contained in the cylinder with the direction of gravity as the direction of the cylinder axis, it is a prerequisite that the ground has been removed.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#referencesexternal-links","title":"References/External links","text":"<p>[1] https://pcl.readthedocs.io/projects/tutorials/en/latest/remove_outliers.html</p> <p>[2] https://pcl.readthedocs.io/projects/tutorials/en/latest/kdtree_search.html#kdtree-search</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/radius-search-2d-outlier-filter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/ring-outlier-filter/","title":"ring_outlier_filter","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/ring-outlier-filter/#ring_outlier_filter","title":"ring_outlier_filter","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/ring-outlier-filter/#purpose","title":"Purpose","text":"<p>The purpose is to remove point cloud noise such as insects and rain.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/ring-outlier-filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>A method of operating scan in chronological order and removing noise based on the rate of change in the distance between points</p> <p></p> <p>Another feature of this node is that it calculates visibility score based on outlier pointcloud and publish score as a topic.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/ring-outlier-filter/#visibility-score-calculation-algorithm","title":"visibility score calculation algorithm","text":"<p>The pointcloud is divided into vertical bins (rings) and horizontal bins (azimuth divisions). The algorithm starts by splitting the input point cloud into separate rings based on the ring value of each point. Then, for each ring, it iterates through the points and calculates the frequency of points within each horizontal bin. The frequency is determined by incrementing a counter for the corresponding bin based on the point's azimuth value. The frequency values are stored in a frequency image matrix, where each cell represents a specific ring and azimuth bin. After calculating the frequency image, the algorithm applies a noise threshold to create a binary image. Points with frequency values above the noise threshold are considered valid, while points below the threshold are considered noise. Finally, the algorithm calculates the visibility score by counting the number of non-zero pixels in the frequency image and dividing it by the total number of pixels (vertical bins multiplied by horizontal bins).</p> <p></p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/ring-outlier-filter/#inputs-outputs","title":"Inputs / Outputs","text":"<p>This implementation inherits <code>autoware::pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/ring-outlier-filter/#parameters","title":"Parameters","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/ring-outlier-filter/#node-parameters","title":"Node Parameters","text":"<p>This implementation inherits <code>autoware::pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/ring-outlier-filter/#core-parameters","title":"Core Parameters","text":"Name Type Description Default Range distance_ratio float distance_ratio 1.03 \u22650.0 object_length_threshold float object_length_threshold 0.05 \u22650.0 max_rings_num integer max_rings_num 128 \u22651 max_points_num_per_ring integer Set this value large enough such that HFoV / resolution &lt; max_points_num_per_ring 4000 \u22650 publish_outlier_pointcloud boolean Flag to publish outlier pointcloud and visibility score. Due to performance concerns, please set to false during experiments. false N/A min_azimuth_deg float The left limit of azimuth for visibility score calculation 0.0 \u22650.0 max_azimuth_deg float The right limit of azimuth for visibility score calculation 360.0 \u22650.0\u2264360.0 max_distance float The limit distance for visibility score calculation 12.0 \u22650.0 vertical_bins integer The number of vertical bin for visibility histogram 128 \u22651 horizontal_bins integer The number of horizontal bin for visibility histogram 36 \u22651 noise_threshold integer The threshold value for distinguishing noise from valid points in the frequency image 2 \u22650 processing_time_threshold_sec float Threshold in seconds. If the processing time of the node exceeds this value, a diagnostic warning will be issued. 0.01 N/A"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/ring-outlier-filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>This nodes requires that the points of the input point cloud are in chronological order and that individual points follow the memory layout specified by PointXYZIRCAEDT.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/ring-outlier-filter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/ring-outlier-filter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/ring-outlier-filter/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/ring-outlier-filter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/vector-map-filter/","title":"vector_map_filter","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/vector-map-filter/#vector_map_filter","title":"vector_map_filter","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/vector-map-filter/#purpose","title":"Purpose","text":"<p>The <code>vector_map_filter</code> is a node that removes points on the outside of lane by using vector map.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/vector-map-filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/vector-map-filter/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/vector-map-filter/#input","title":"Input","text":"Name Type Description <code>~/input/points</code> <code>sensor_msgs::msg::PointCloud2</code> reference points <code>~/input/vector_map</code> <code>autoware_map_msgs::msg::LaneletMapBin</code> vector map"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/vector-map-filter/#output","title":"Output","text":"Name Type Description <code>~/output/points</code> <code>sensor_msgs::msg::PointCloud2</code> filtered points"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/vector-map-filter/#parameters","title":"Parameters","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/vector-map-filter/#core-parameters","title":"Core Parameters","text":"Name Type Description Default Range voxel_size_x float voxel size along x-axis [m] 0.04 \u22650 voxel_size_y float voxel size along y-axis [m] 0.04 \u22650"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/vector-map-filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/vector-map-filter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/vector-map-filter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/vector-map-filter/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/vector-map-filter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/vector-map-inside-area-filter/","title":"vector_map_inside_area_filter","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/vector-map-inside-area-filter/#vector_map_inside_area_filter","title":"vector_map_inside_area_filter","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/vector-map-inside-area-filter/#purpose","title":"Purpose","text":"<p>The <code>vector_map_inside_area_filter</code> is a node that removes points inside the vector map area that has given type by parameter.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/vector-map-inside-area-filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<ul> <li>Get the vector map area that has given type by parameter of <code>polygon_type</code></li> <li>Extract the vector map area that intersects with the bounding box of input points to reduce the calculation cost</li> <li>Create the 2D polygon from the extracted vector map area</li> <li>Remove input points inside the polygon</li> <li>If the z value is used for filtering, remove points that are below the z threshold</li> </ul>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/vector-map-inside-area-filter/#inputs-outputs","title":"Inputs / Outputs","text":"<p>This implementation inherits <code>autoware::pointcloud_preprocessor::Filter</code> class, so please see also README.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/vector-map-inside-area-filter/#input","title":"Input","text":"Name Type Description <code>~/input</code> <code>sensor_msgs::msg::PointCloud2</code> input points <code>~/input/vector_map</code> <code>autoware_map_msgs::msg::LaneletMapBin</code> vector map used for filtering points"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/vector-map-inside-area-filter/#output","title":"Output","text":"Name Type Description <code>~/output</code> <code>sensor_msgs::msg::PointCloud2</code> filtered points"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/vector-map-inside-area-filter/#core-parameters","title":"Core Parameters","text":"Name Type Description Default Range polygon_type string polygon type to be filtered no_obstacle_segmentation_area N/A use_z_filter boolean use z value for filtering false N/A z_threshold float z threshold for filtering 0.0 N/A"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/vector-map-inside-area-filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/voxel-grid-outlier-filter/","title":"voxel_grid_outlier_filter","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#voxel_grid_outlier_filter","title":"voxel_grid_outlier_filter","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#purpose","title":"Purpose","text":"<p>The purpose is to remove point cloud noise such as insects and rain.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>Removing point cloud noise based on the number of points existing within a voxel. The radius_search_2d_outlier_filter is better for accuracy, but this method has the advantage of low calculation cost.</p> <p></p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#inputs-outputs","title":"Inputs / Outputs","text":"<p>This implementation inherits <code>autoware::pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#parameters","title":"Parameters","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#node-parameters","title":"Node Parameters","text":"<p>This implementation inherits <code>autoware::pointcloud_preprocessor::Filter</code> class, please refer README.</p>"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#core-parameters","title":"Core Parameters","text":"Name Type Description Default Range voxel_size_x float the voxel size along x-axis [m] 0.3 \u22650 voxel_size_y float the voxel size along y-axis [m] 0.3 \u22650 voxel_size_z float the voxel size along z-axis [m] 0.1 \u22650 voxel_points_threshold integer the minimum number of points in each voxel 2 \u22651"},{"location":"sensing/autoware_pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#optional-referencesexternal-links","title":"(Optional) References/External links","text":""},{"location":"sensing/autoware_pointcloud_preprocessor/docs/voxel-grid-outlier-filter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"sensing/autoware_radar_objects_adapter/","title":"Macro Rendering Error","text":""},{"location":"sensing/autoware_radar_objects_adapter/#macro-rendering-error","title":"Macro Rendering Error","text":"<p>File: <code>sensing/autoware_radar_objects_adapter/README.md</code></p> <p>KeyError: 'description'</p> <pre><code>Traceback (most recent call last):\n  File \"/opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages/mkdocs_macros/plugin.py\", line 703, in render\n    return md_template.render(**page_variables)\n  File \"/opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages/jinja2/environment.py\", line 1295, in render\n    self.environment.handle_exception()\n  File \"/opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages/jinja2/environment.py\", line 942, in handle_exception\n    raise rewrite_traceback_stack(source=source)\n  File \"&lt;template&gt;\", line 37, in top-level template code\n  File \"/home/runner/work/autoware.universe/autoware.universe/mkdocs_macros.py\", line 73, in json_to_markdown\n    return format_json(data)\n  File \"/home/runner/work/autoware.universe/autoware.universe/mkdocs_macros.py\", line 64, in format_json\n    markdown_table = tabulate(extract_parameter_info(parameters), headers=\"keys\", tablefmt=\"github\")\n  File \"/home/runner/work/autoware.universe/autoware.universe/mkdocs_macros.py\", line 57, in extract_parameter_info\n    params.extend(extract_parameter_info(v[\"properties\"], k + \".\"))\n  File \"/home/runner/work/autoware.universe/autoware.universe/mkdocs_macros.py\", line 52, in extract_parameter_info\n    param[\"Description\"] = v[\"description\"]\nKeyError: 'description'\n</code></pre>"},{"location":"sensing/autoware_radar_scan_to_pointcloud2/","title":"radar_scan_to_pointcloud2","text":""},{"location":"sensing/autoware_radar_scan_to_pointcloud2/#radar_scan_to_pointcloud2","title":"radar_scan_to_pointcloud2","text":""},{"location":"sensing/autoware_radar_scan_to_pointcloud2/#radar_scan_to_pointcloud2_node","title":"radar_scan_to_pointcloud2_node","text":"<ul> <li>Convert from <code>radar_msgs::msg::RadarScan</code> to <code>sensor_msgs::msg::PointCloud2</code></li> <li>Calculation cost O(n)<ul> <li>n: The number of radar return</li> </ul> </li> </ul>"},{"location":"sensing/autoware_radar_scan_to_pointcloud2/#input-topics","title":"Input topics","text":"Name Type Description input/radar radar_msgs::msg::RadarScan RadarScan"},{"location":"sensing/autoware_radar_scan_to_pointcloud2/#output-topics","title":"Output topics","text":"Name Type Description output/amplitude_pointcloud sensor_msgs::msg::PointCloud2 PointCloud2 radar pointcloud whose intensity is amplitude. output/doppler_pointcloud sensor_msgs::msg::PointCloud2 PointCloud2 radar pointcloud whose intensity is doppler velocity."},{"location":"sensing/autoware_radar_scan_to_pointcloud2/#parameters","title":"Parameters","text":"Name Type Description publish_amplitude_pointcloud bool Whether publish radar pointcloud whose intensity is amplitude. Default is <code>true</code>. publish_doppler_pointcloud bool Whether publish radar pointcloud whose intensity is doppler velocity. Default is <code>false</code>."},{"location":"sensing/autoware_radar_scan_to_pointcloud2/#how-to-launch","title":"How to launch","text":"<pre><code>ros2 launch autoware_radar_scan_to_pointcloud2 radar_scan_to_pointcloud2.launch.xml\n</code></pre>"},{"location":"sensing/autoware_radar_static_pointcloud_filter/","title":"radar_static_pointcloud_filter","text":""},{"location":"sensing/autoware_radar_static_pointcloud_filter/#radar_static_pointcloud_filter","title":"radar_static_pointcloud_filter","text":""},{"location":"sensing/autoware_radar_static_pointcloud_filter/#radar_static_pointcloud_filter_node","title":"radar_static_pointcloud_filter_node","text":"<p>Extract static/dynamic radar pointcloud by using doppler velocity and ego motion. Calculation cost is O(n). <code>n</code> is the number of radar pointcloud.</p>"},{"location":"sensing/autoware_radar_static_pointcloud_filter/#input-topics","title":"Input topics","text":"Name Type Description input/radar radar_msgs::msg::RadarScan RadarScan input/odometry nav_msgs::msg::Odometry Ego vehicle odometry topic"},{"location":"sensing/autoware_radar_static_pointcloud_filter/#output-topics","title":"Output topics","text":"Name Type Description output/static_radar_scan radar_msgs::msg::RadarScan static radar pointcloud output/dynamic_radar_scan radar_msgs::msg::RadarScan dynamic radar pointcloud"},{"location":"sensing/autoware_radar_static_pointcloud_filter/#parameters","title":"Parameters","text":"Name Type Description doppler_velocity_sd double Standard deviation for radar doppler velocity. [m/s]"},{"location":"sensing/autoware_radar_static_pointcloud_filter/#how-to-launch","title":"How to launch","text":"<pre><code>ros2 launch autoware_radar_static_pointcloud_filter radar_static_pointcloud_filter.launch.xml\n</code></pre>"},{"location":"sensing/autoware_radar_static_pointcloud_filter/#algorithm","title":"Algorithm","text":""},{"location":"sensing/autoware_radar_threshold_filter/","title":"radar_threshold_filter","text":""},{"location":"sensing/autoware_radar_threshold_filter/#radar_threshold_filter","title":"radar_threshold_filter","text":""},{"location":"sensing/autoware_radar_threshold_filter/#radar_threshold_filter_node","title":"radar_threshold_filter_node","text":"<p>Remove noise from radar return by threshold.</p> <ul> <li>Amplitude filter: Low amplitude consider noise</li> <li>FOV filter: Pointcloud from radar's FOV edge occur perturbation</li> <li>Range filter: Too near pointcloud often occur noise</li> </ul> <p>Calculation cost is O(n). <code>n</code> is the number of radar return.</p>"},{"location":"sensing/autoware_radar_threshold_filter/#input-topics","title":"Input topics","text":"Name Type Description input/radar radar_msgs/msg/RadarScan.msg Radar pointcloud data"},{"location":"sensing/autoware_radar_threshold_filter/#output-topics","title":"Output topics","text":"Name Type Description output/radar radar_msgs/msg/RadarScan.msg Filtered radar pointcloud"},{"location":"sensing/autoware_radar_threshold_filter/#parameters","title":"Parameters","text":"Name Type Description Default Range node_params.is_amplitude_filter boolean If true, amplitude filtering is enabled. true N/A node_params.amplitude_min float Minimum amplitude threshold. -10.0 N/A node_params.amplitude_max float Maximum amplitude threshold. 100.0 N/A node_params.is_range_filter boolean If true, range filtering is enabled. false N/A node_params.range_min float Minimum range threshold. 20.0 N/A node_params.range_max float Maximum range threshold. 300.0 N/A node_params.is_azimuth_filter boolean If true, azimuth filtering is enabled. true N/A node_params.azimuth_min float Minimum azimuth threshold (radians). -1.2 N/A node_params.azimuth_max float Maximum azimuth threshold (radians). 1.2 N/A node_params.is_z_filter boolean If true, Z filtering is enabled. false N/A node_params.z_min float Minimum Z threshold (meters). -2.0 N/A node_params.z_max float Maximum Z threshold (meters). 5.0 N/A node_params.max_queue_size integer Maximum size of the queue. 5 N/A"},{"location":"sensing/autoware_radar_threshold_filter/#how-to-launch","title":"How to launch","text":"<pre><code>ros2 launch autoware_radar_threshold_filter radar_threshold_filter.launch.xml\n</code></pre>"},{"location":"sensing/autoware_radar_tracks_noise_filter/","title":"Macro Rendering Error","text":""},{"location":"sensing/autoware_radar_tracks_noise_filter/#macro-rendering-error","title":"Macro Rendering Error","text":"<p>File: <code>sensing/autoware_radar_tracks_noise_filter/README.md</code></p> <p>FileNotFoundError: [Errno 2] No such file or directory: 'sensing/autoware_radar_tracks_noise_filter/schema/</p>"},{"location":"sensing/livox/autoware_livox_tag_filter/","title":"livox_tag_filter","text":""},{"location":"sensing/livox/autoware_livox_tag_filter/#livox_tag_filter","title":"livox_tag_filter","text":""},{"location":"sensing/livox/autoware_livox_tag_filter/#purpose","title":"Purpose","text":"<p>The <code>livox_tag_filter</code> is a node that removes noise from pointcloud by using the following tags:</p> <ul> <li>Point property based on spatial position</li> <li>Point property based on intensity</li> <li>Return number</li> </ul>"},{"location":"sensing/livox/autoware_livox_tag_filter/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"sensing/livox/autoware_livox_tag_filter/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"sensing/livox/autoware_livox_tag_filter/#input","title":"Input","text":"Name Type Description <code>~/input</code> <code>sensor_msgs::msg::PointCloud2</code> reference points"},{"location":"sensing/livox/autoware_livox_tag_filter/#output","title":"Output","text":"Name Type Description <code>~/output</code> <code>sensor_msgs::msg::PointCloud2</code> filtered points"},{"location":"sensing/livox/autoware_livox_tag_filter/#parameters","title":"Parameters","text":""},{"location":"sensing/livox/autoware_livox_tag_filter/#node-parameters","title":"Node Parameters","text":"Name Type Description <code>ignore_tags</code> vector ignored tags (See the following table)"},{"location":"sensing/livox/autoware_livox_tag_filter/#tag-parameters","title":"Tag Parameters","text":"Bit Description Options 0~1 Point property based on spatial position 00: Normal 01: High confidence level of the noise 10: Moderate confidence level of the noise 11: Low confidence level of the noise 2~3 Point property based on intensity 00: Normal 01: High confidence level of the noise 10: Moderate confidence level of the noise 11: Reserved 4~5 Return number 00: return 0 01: return 1 10: return 2 11: return 3 6~7 Reserved <p>You can download more detail description about the livox from external link [1].</p>"},{"location":"sensing/livox/autoware_livox_tag_filter/#assumptions-known-limits","title":"Assumptions / Known limits","text":""},{"location":"sensing/livox/autoware_livox_tag_filter/#optional-error-detection-and-handling","title":"(Optional) Error detection and handling","text":""},{"location":"sensing/livox/autoware_livox_tag_filter/#optional-performance-characterization","title":"(Optional) Performance characterization","text":""},{"location":"sensing/livox/autoware_livox_tag_filter/#optional-referencesexternal-links","title":"(Optional) References/External links","text":"<p>[1] https://www.livoxtech.com/downloads</p>"},{"location":"sensing/livox/autoware_livox_tag_filter/#optional-future-extensions-unimplemented-parts","title":"(Optional) Future extensions / Unimplemented parts","text":""},{"location":"simulator/autoware_carla_interface/","title":"autoware_carla_interface","text":""},{"location":"simulator/autoware_carla_interface/#autoware_carla_interface","title":"autoware_carla_interface","text":""},{"location":"simulator/autoware_carla_interface/#ros-2-autoware-universe-bridge-for-carla-simulator","title":"ROS 2 / Autoware Universe bridge for CARLA simulator","text":"<p>Thanks to https://github.com/gezp for ROS 2 Humble support for CARLA Communication. This ros package enables communication between Autoware and CARLA for autonomous driving simulation.</p>"},{"location":"simulator/autoware_carla_interface/#supported-environment","title":"Supported Environment","text":"ubuntu ros carla autoware 22.04 humble 0.9.15 Main"},{"location":"simulator/autoware_carla_interface/#setup","title":"Setup","text":""},{"location":"simulator/autoware_carla_interface/#install","title":"Install","text":""},{"location":"simulator/autoware_carla_interface/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Install CARLA 0.9.15: Follow the CARLA Installation Guide</p> </li> <li> <p>Install CARLA Python Package: Install CARLA 0.9.15 ROS 2 Humble communication package</p> <ul> <li>Option A: Install the wheel using pip</li> <li>Option B: Add the egg file to your <code>PYTHONPATH</code></li> </ul> </li> <li> <p>Download CARLA Lanelet2 Maps: Get the y-axis inverted maps from CARLA Autoware Contents</p> </li> </ol>"},{"location":"simulator/autoware_carla_interface/#map-setup","title":"Map Setup","text":"<ol> <li>Download the maps (y-axis inverted version) to an arbitrary location</li> <li>Create the map folder structure in <code>$HOME/autoware_map</code>:<ul> <li>Rename <code>point_cloud/Town01.pcd</code> \u2192 <code>$HOME/autoware_map/Town01/pointcloud_map.pcd</code></li> <li>Rename <code>vector_maps/lanelet2/Town01.osm</code> \u2192 <code>$HOME/autoware_map/Town01/lanelet2_map.osm</code></li> </ul> </li> <li> <p>Create <code>$HOME/autoware_map/Town01/map_projector_info.yaml</code> with:</p> <pre><code>projector_type: Local\n</code></pre> </li> </ol>"},{"location":"simulator/autoware_carla_interface/#build","title":"Build","text":"<pre><code>colcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE=Release\n</code></pre>"},{"location":"simulator/autoware_carla_interface/#run","title":"Run","text":"<ol> <li> <p>Run carla, change map, spawn object if you need     </p> <pre><code>cd CARLA\n./CarlaUE4.sh -prefernvidia -quality-level=Low -RenderOffScreen\n</code></pre> </li> <li> <p>Run Autoware with CARLA</p> <pre><code>ros2 launch autoware_launch e2e_simulator.launch.xml \\\n    map_path:=$HOME/autoware_map/Town01 \\\n    vehicle_model:=sample_vehicle \\\n    sensor_model:=carla_sensor_kit \\\n    simulator_type:=carla \\\n    carla_map:=Town01\n</code></pre> </li> <li> <p>Set initial pose (Init by GNSS)</p> </li> <li>Set goal position</li> <li>Wait for planning</li> <li>Engage</li> </ol>"},{"location":"simulator/autoware_carla_interface/#viewing-multi-camera-view-in-rviz","title":"Viewing Multi-Camera View in RViz","text":"<p>The <code>carla_sensor_kit</code> includes 6 cameras providing 360-degree coverage (Front, Front-Left, Front-Right, Back, Back-Left, Back-Right). A multi-camera combiner node automatically combines all camera feeds into a single 2x3 grid view.</p> <p>To view the combined camera feed in RViz:</p> <ol> <li>In the Displays panel (left side), click the \"Add\" button</li> <li>Select the \"By topic\" tab</li> <li>Navigate to <code>/sensing/camera/all_cameras/image_raw</code></li> <li>Select \"Image\" display type</li> <li>Click OK</li> </ol> <p></p> <p>The combined view shows all 6 cameras with labels: FL (Front-Left), F (Front), FR (Front-Right), BL (Back-Left), B (Back), BR (Back-Right).</p> <p>Note: If you don't need the multi-camera combiner (to save CPU resources), you can comment out the following line in <code>launch/autoware_carla_interface.launch.xml</code>:</p> <pre><code>&lt;!-- Multi-camera combiner for RViz visualization --&gt;\n&lt;!-- &lt;node pkg=\"autoware_carla_interface\" exec=\"multi_camera_combiner\" output=\"screen\"/&gt; --&gt;\n</code></pre>"},{"location":"simulator/autoware_carla_interface/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The <code>InitializeInterface</code> class is key to setting up both the CARLA world and the ego vehicle. It fetches configuration parameters through the <code>autoware_carla_interface.launch.xml</code>.</p> <p>The main simulation loop runs within the <code>carla_ros2_interface</code> class. This loop ticks simulation time inside the CARLA simulator at <code>fixed_delta_seconds</code> time, where data is received and published as ROS 2 messages at frequencies defined in <code>self.sensor_frequencies</code>.</p> <p>Ego vehicle commands from Autoware are processed through the <code>autoware_raw_vehicle_cmd_converter</code>, which calibrates these commands for CARLA. The calibrated commands are then fed directly into CARLA control via <code>CarlaDataProvider</code>.</p>"},{"location":"simulator/autoware_carla_interface/#configurable-parameters-for-world-loading","title":"Configurable Parameters for World Loading","text":"<p>All the key parameters can be configured in <code>autoware_carla_interface.launch.xml</code>.</p> Name Type Default Value Description <code>host</code> string \"localhost\" Hostname for the CARLA server <code>port</code> int \"2000\" Port number for the CARLA server <code>timeout</code> int 20 Timeout for the CARLA client <code>ego_vehicle_role_name</code> string \"ego_vehicle\" Role name for the ego vehicle <code>vehicle_type</code> string \"vehicle.toyota.prius\" Blueprint ID of the vehicle to spawn. The Blueprint ID of vehicles can be found in CARLA Blueprint ID <code>spawn_point</code> string None Coordinates for spawning the ego vehicle (None is random). Format = [x, y, z, roll, pitch, yaw] <code>carla_map</code> string \"Town01\" Name of the map to load in CARLA <code>sync_mode</code> bool True Boolean flag to set synchronous mode in CARLA <code>fixed_delta_seconds</code> double 0.05 Time step for the simulation (related to client FPS) <code>use_traffic_manager</code> bool False Boolean flag to set traffic manager in CARLA <code>max_real_delta_seconds</code> double 0.05 Parameter to limit the simulation speed below <code>fixed_delta_seconds</code> <code>sensor_kit_name</code> string \"carla_sensor_kit_description\" Name of the sensor kit package to use for sensor configuration. Should be the *_description package containing config/sensor_kit_calibration.yaml <code>sensor_mapping_file</code> string \"$(find-pkg-share autoware_carla_interface)/config/sensor_mapping.yaml\" Path to sensor mapping YAML configuration file <code>config_file</code> string \"$(find-pkg-share autoware_carla_interface)/raw_vehicle_cmd_converter.param.yaml\" Control mapping file to be used in <code>autoware_raw_vehicle_cmd_converter</code>. Current control are calibrated based on <code>vehicle.toyota.prius</code> Blueprints ID in CARLA. Changing the vehicle type may need a recalibration."},{"location":"simulator/autoware_carla_interface/#sensor-configuration","title":"Sensor Configuration","text":"<p>The interface uses the <code>carla_sensor_kit</code> which provides 6 cameras for 360-degree coverage, LiDAR, IMU, and GNSS sensors. Sensor configurations are dynamically loaded from Autoware sensor kit calibration files through two configuration files:</p>"},{"location":"simulator/autoware_carla_interface/#1-sensor-kit-calibration-from-autoware-sensor-kit","title":"1. Sensor Kit Calibration (from Autoware sensor kit)","text":"<p>Located in <code>&lt;sensor_kit_name&gt;_description/config/sensor_kit_calibration.yaml</code></p> <p>Defines sensor positions and orientations relative to <code>base_link</code> (rear axle center). Example:</p> <pre><code>sensor_kit_base_link:\n  CAM_FRONT/camera_link:\n    x: 2.225\n    y: 0.000\n    z: 1.600\n    roll: 0.000\n    pitch: 0.000\n    yaw: 0.000 # Angles in radians\n</code></pre>"},{"location":"simulator/autoware_carla_interface/#2-sensor-mapping-carla-specific","title":"2. Sensor Mapping (CARLA-specific)","text":"<p>Located in <code>config/sensor_mapping.yaml</code></p> <p>Maps Autoware sensors to CARLA sensor types and parameters. Key sections:</p> <ul> <li><code>default_sensor_kit_name</code>: Default sensor kit to use (e.g., <code>carla_sensor_kit_description</code>)</li> <li><code>sensor_mappings</code>: Maps each sensor to CARLA type and ROS topics</li> <li><code>enabled_sensors</code>: List of sensors to spawn in CARLA</li> <li><code>vehicle_config</code> (optional): Vehicle parameters like wheelbase</li> </ul> <p>Example sensor mapping:</p> <pre><code>sensor_mappings:\n  CAM_FRONT/camera_link:\n    carla_type: sensor.camera.rgb\n    id: CAM_FRONT\n    ros_config:\n      frame_id: CAM_FRONT/camera_optical_link\n      topic_image: /sensing/camera/CAM_FRONT/image_raw\n      topic_info: /sensing/camera/CAM_FRONT/camera_info\n      frequency_hz: 11\n      qos_profile: reliable\n    parameters:\n      image_size_x: 1600\n      image_size_y: 900\n      fov: 70.0\n</code></pre> <p>For CARLA sensor parameters, see CARLA Sensor Reference.</p>"},{"location":"simulator/autoware_carla_interface/#world-loading","title":"World Loading","text":"<p>The <code>carla_ros.py</code> sets up the CARLA world:</p> <ol> <li> <p>Client Connection:</p> <pre><code>client = carla.Client(self.local_host, self.port)\nclient.set_timeout(self.timeout)\n</code></pre> </li> <li> <p>Load the Map:</p> <p>Map loaded in CARLA world with map according to <code>carla_map</code> parameter.</p> <pre><code>client.load_world(self.map_name)\nself.world = client.get_world()\n</code></pre> </li> <li> <p>Spawn Ego Vehicle:</p> <p>Vehicle are spawn according to <code>vehicle_type</code>, <code>spawn_point</code>, and <code>agent_role_name</code> parameter.</p> <pre><code>spawn_point = carla.Transform()\npoint_items = self.spawn_point.split(\",\")\nif len(point_items) == 6:\n   spawn_point.location.x = float(point_items[0])\n   spawn_point.location.y = float(point_items[1])\n   spawn_point.location.z = float(point_items[2]) + 2\n   spawn_point.rotation.roll = float(point_items[3])\n   spawn_point.rotation.pitch = float(point_items[4])\n   spawn_point.rotation.yaw = float(point_items[5])\nCarlaDataProvider.request_new_actor(self.vehicle_type, spawn_point, self.agent_role_name)\n</code></pre> </li> </ol>"},{"location":"simulator/autoware_carla_interface/#traffic-light-recognition","title":"Traffic Light Recognition","text":"<p>The maps provided by the Carla Simulator (Carla Lanelet2 Maps) currently lack proper traffic light components for Autoware and have different latitude and longitude coordinates compared to the pointcloud map. To enable traffic light recognition, follow the steps below to modify the maps.</p> <ul> <li> <p>Options to Modify the Map</p> <ul> <li>A. Create a New Map from Scratch</li> <li>Use the TIER IV Vector Map Builder to create a new map.</li> </ul> <ul> <li>B. Modify the Existing Carla Lanelet2 Maps</li> <li>Adjust the longitude and latitude of the Carla Lanelet2 Maps to align with the PCD (origin).<ul> <li>Use this tool to modify the coordinates.</li> <li>Snap Lanelet with PCD and add the traffic lights using the TIER IV Vector Map Builder.</li> </ul> </li> </ul> </li> </ul> <ul> <li>When using the TIER IV Vector Map Builder, you must convert the PCD format from <code>binary_compressed</code> to <code>ascii</code>. You can use <code>pcl_tools</code> for this conversion.</li> <li>For reference, an example of Town01 with added traffic lights at one intersection can be downloaded here.</li> </ul>"},{"location":"simulator/autoware_carla_interface/#tips","title":"Tips","text":"<ul> <li>Misalignment might occurs during initialization, pressing <code>init by gnss</code> button should fix it.</li> <li>Changing the <code>fixed_delta_seconds</code> can increase the simulation tick (default 0.05 s), some sensor params in <code>sensor_mapping.yaml</code> need to be adjusted when it is changed (example: LIDAR rotation frequency should match the FPS).</li> </ul>"},{"location":"simulator/autoware_carla_interface/#known-issues-and-future-works","title":"Known Issues and Future Works","text":"<ul> <li>Testing on procedural maps (Adv Digital Twin): Currently unable to test due to failures in creating the Adv Digital Twin map.</li> <li>Traffic light recognition: The default CARLA Lanelet2 maps lack proper traffic light regulatory elements. See the \"Traffic Light Recognition\" section above for workarounds.</li> </ul>"},{"location":"simulator/autoware_dummy_perception_publisher/","title":"dummy_perception_publisher","text":""},{"location":"simulator/autoware_dummy_perception_publisher/#dummy_perception_publisher","title":"dummy_perception_publisher","text":""},{"location":"simulator/autoware_dummy_perception_publisher/#purpose","title":"Purpose","text":"<p>This node publishes the result of the dummy detection with the type of perception. The node uses a plugin architecture to support different object movement strategies.</p>"},{"location":"simulator/autoware_dummy_perception_publisher/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"simulator/autoware_dummy_perception_publisher/#plugin-architecture","title":"Plugin Architecture","text":"<p>The node uses a plugin-based architecture to handle different types of object movement. This allows for flexible and extensible movement behaviors without modifying the core node logic.</p>"},{"location":"simulator/autoware_dummy_perception_publisher/#available-plugins","title":"Available Plugins","text":"<ol> <li>StraightLineObjectMovementPlugin: Moves objects in straight lines based on their initial velocity and direction.</li> <li>PredictedObjectMovementPlugin: Moves objects along predicted paths from perception predictions, providing more realistic movement patterns.</li> </ol>"},{"location":"simulator/autoware_dummy_perception_publisher/#plugin-base-class","title":"Plugin Base Class","text":"<p>All movement plugins inherit from <code>DummyObjectMovementBasePlugin</code> which provides:</p> <ul> <li>Object management (add, delete operations)</li> <li>Associated action type handling</li> <li>Common interface for object movement</li> </ul>"},{"location":"simulator/autoware_dummy_perception_publisher/#object-action-handling","title":"Object Action Handling","text":"<ul> <li>ADD: New objects are created and they move in a straight line, acceleration and deceleration parameters can be used.</li> <li>MODIFY: Handled directly by the node, bypassing plugin movement logic. Immediately replaces the object's position information across all plugins.</li> <li>DELETE: The specified object is removed from all plugins.</li> <li>DELETEALL: Clears all objects from all plugins.</li> <li>PREDICT: New objects are created, they move in a straight line for a set time and then the predictions extracted from the perception module are used to dictate where the objects will move to. NOTE: for ease of calculation, acceleration is not taken into account when calculating the object's position, only its initial speed.</li> </ul>"},{"location":"simulator/autoware_dummy_perception_publisher/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"simulator/autoware_dummy_perception_publisher/#input","title":"Input","text":"Name Type Description <code>/tf</code> <code>tf2_msgs/TFMessage</code> TF (self-pose) <code>input/object</code> <code>tier4_simulation_msgs::msg::DummyObject</code> dummy detection objects <code>predicted_objects</code> <code>autoware_perception_msgs::msg::PredictedObjects</code> predicted objects (used by PredictedObjectMovementPlugin)"},{"location":"simulator/autoware_dummy_perception_publisher/#output","title":"Output","text":"Name Type Description <code>output/dynamic_object</code> <code>tier4_perception_msgs::msg::DetectedObjectsWithFeature</code> dummy detection objects <code>output/points_raw</code> <code>sensor_msgs::msg::PointCloud2</code> point cloud of objects <code>output/debug/ground_truth_objects</code> <code>autoware_perception_msgs::msg::TrackedObjects</code> ground truth objects"},{"location":"simulator/autoware_dummy_perception_publisher/#parameters","title":"Parameters","text":""},{"location":"simulator/autoware_dummy_perception_publisher/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Explanation <code>visible_range</code> double 100.0 sensor visible range [m] <code>detection_successful_rate</code> double 0.8 sensor detection rate. (min) 0.0 - 1.0(max) <code>enable_ray_tracing</code> bool true if True, use ray tracking <code>use_object_recognition</code> bool true if True, publish objects topic <code>use_base_link_z</code> bool true if True, node uses z coordinate of ego base_link <code>publish_ground_truth</code> bool false if True, publish ground truth objects <code>use_fixed_random_seed</code> bool false if True, use fixed random seed <code>random_seed</code> int 0 random seed <code>object_centric_pointcloud</code> bool false if True, generate object-centric point clouds <code>angle_increment</code> double 0.004363323 angle increment for ray tracing (0.25\u00b0 in radians)"},{"location":"simulator/autoware_dummy_perception_publisher/#predictedobjectmovementplugin-parameters","title":"PredictedObjectMovementPlugin Parameters","text":"Name Type Default Value Explanation <code>min_predicted_path_keep_duration</code> double 3.0 minimum time (seconds) to keep using same prediction <code>switch_time_threshold</code> double 2.0 time threshold (seconds) to switch from straight-line to predicted path"},{"location":"simulator/autoware_dummy_perception_publisher/#common-remapping-parameters","title":"Common Remapping Parameters","text":"<p>The plugin uses <code>CommonParameters</code> for both vehicle and pedestrian object types. Each parameter is prefixed with either <code>vehicle.</code> or <code>pedestrian.</code>:</p> Parameter Name Type Explanation <code>max_remapping_distance</code> double maximum distance (meters) for remapping validation <code>max_speed_difference_ratio</code> double maximum speed difference ratio tolerance <code>min_speed_ratio</code> double minimum speed ratio relative to dummy object speed <code>max_speed_ratio</code> double maximum speed ratio relative to dummy object speed <code>speed_check_threshold</code> double speed threshold (m/s) above which speed checks apply <code>path_selection_strategy</code> string path selection strategy: \"highest_confidence\" or \"random\""},{"location":"simulator/autoware_fault_injection/","title":"fault_injection","text":""},{"location":"simulator/autoware_fault_injection/#fault_injection","title":"fault_injection","text":""},{"location":"simulator/autoware_fault_injection/#purpose","title":"Purpose","text":"<p>This package is used to convert pseudo system faults from PSim to Diagnostics and notify Autoware. The component diagram is as follows:</p> <p></p>"},{"location":"simulator/autoware_fault_injection/#test","title":"Test","text":"<pre><code>source install/setup.bash\ncd fault_injection\nlaunch_test test/test_fault_injection_node.test.py\n</code></pre>"},{"location":"simulator/autoware_fault_injection/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"simulator/autoware_fault_injection/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"simulator/autoware_fault_injection/#input","title":"Input","text":"Name Type Description <code>~/input/simulation_events</code> <code>tier4_simulation_msgs::msg::SimulationEvents</code> simulation events"},{"location":"simulator/autoware_fault_injection/#output","title":"Output","text":"Name Type Description <code>/diagnostics</code> <code>diagnostic_msgs::msg::DiagnosticArray</code> Dummy diagnostics"},{"location":"simulator/autoware_fault_injection/#parameters","title":"Parameters","text":"<p>None.</p>"},{"location":"simulator/autoware_fault_injection/#node-parameters","title":"Node Parameters","text":"<p>None.</p>"},{"location":"simulator/autoware_fault_injection/#core-parameters","title":"Core Parameters","text":"<p>None.</p>"},{"location":"simulator/autoware_fault_injection/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"simulator/autoware_learning_based_vehicle_model/","title":"Learned Model","text":""},{"location":"simulator/autoware_learning_based_vehicle_model/#learned-model","title":"Learned Model","text":"<p>This is the design document for the Python learned model used in the <code>simple_planning_simulator</code> package.</p>"},{"location":"simulator/autoware_learning_based_vehicle_model/#purpose-use-cases","title":"Purpose / Use cases","text":"<p>This library creates an interface between models in Python and PSIM (C++). It is used to quickly deploy learned Python models in PSIM without a need for complex C++ implementation.</p>"},{"location":"simulator/autoware_learning_based_vehicle_model/#design","title":"Design","text":"<p>The idea behind this package is that the model we want to use for simulation consists of multiple sub-models (e.g., steering model, drive model, vehicle kinematics, etc.). These sub-models are implemented in Python and can be trainable. Each sub-model has string names for all of its inputs/outputs, which are used to create model interconnections automatically (see image below). This allows us to easily switch sub-models for better customization of the simulator.</p> <p></p>"},{"location":"simulator/autoware_learning_based_vehicle_model/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>To use this package <code>python3</code> and <code>pybind11</code> need to be installed. The only assumption on Python sub-models is their interface.</p> <pre><code>class PythonSubmodelInterface:\n\n    def forward(self, action, state):  # Required\n        \"\"\"\n        Calculate forward pass through the model and returns next_state.\n        \"\"\"\n        return list()\n\n    def get_state_names(self):  # Required\n        \"\"\"\n        Return list of string names of the model states (outputs).\n        \"\"\"\n        return list()\n\n    def get_action_names(self):  # Required\n        \"\"\"\n        Return list of string names of the model actions (inputs).\n        \"\"\"\n        return list()\n\n    def reset(self):  # Required\n        \"\"\"\n        Reset model. This function is called after load_params().\n        \"\"\"\n        pass\n\n    def load_params(self, path):  # Required\n        \"\"\"\n        Load parameters of the model.\n        Inputs:\n            - path: Path to a parameter file to load by the model.\n        \"\"\"\n        pass\n\n    def dtSet(self, dt):  # Required\n        \"\"\"\n        Set dt of the model.\n        Inputs:\n            - dt: time step\n        \"\"\"\n        pass\n</code></pre>"},{"location":"simulator/autoware_learning_based_vehicle_model/#api","title":"API","text":"<p>To successfully create a vehicle model an InterconnectedModel class needs to be set up correctly.</p>"},{"location":"simulator/autoware_learning_based_vehicle_model/#interconnectedmodel-class","title":"InterconnectedModel class","text":""},{"location":"simulator/autoware_learning_based_vehicle_model/#constructor","title":"<code>Constructor</code>","text":"<p>The constructor takes no arguments.</p>"},{"location":"simulator/autoware_learning_based_vehicle_model/#void-addsubmodelstdtuplestdstring-stdstring-stdstring-model_descriptor","title":"<code>void addSubmodel(std::tuple&lt;std::string, std::string, std::string&gt; model_descriptor)</code>","text":"<p>Add a new sub-model to the model.</p> <p>Inputs:</p> <ul> <li>model_descriptor: Describes what model should be used. The model descriptor contains three strings:<ul> <li>The first string is a path to a python module where the model is implemented.</li> <li>The second string is a path to the file where model parameters are stored.</li> <li>The third string is the name of the class that implements the model.</li> </ul> </li> </ul> <p>Outputs:</p> <ul> <li>None</li> </ul>"},{"location":"simulator/autoware_learning_based_vehicle_model/#void-generateconnectionsstdvectorchar-in_names-stdvectorchar-out_names","title":"<code>void generateConnections(std::vector&lt;char *&gt; in_names, std::vector&lt;char*&gt; out_names)</code>","text":"<p>Generate connections between sub-models and inputs/outputs of the model.</p> <p>Inputs:</p> <ul> <li>in_names: String names for all of the model inputs in order.</li> <li>out_names: String names for all of the model outputs in order.</li> </ul> <p>Outputs:</p> <ul> <li>None</li> </ul>"},{"location":"simulator/autoware_learning_based_vehicle_model/#void-initstatestdvectordouble-new_state","title":"<code>void initState(std::vector&lt;double&gt; new_state)</code>","text":"<p>Set the initial state of the model.</p> <p>Inputs:</p> <ul> <li>new_state: New state of the model.</li> </ul> <p>Outputs:</p> <ul> <li>None</li> </ul>"},{"location":"simulator/autoware_learning_based_vehicle_model/#stdvectordouble-updatepymodelstdvectordouble-psim_input","title":"<code>std::vector&lt;double&gt; updatePyModel(std::vector&lt;double&gt; psim_input)</code>","text":"<p>Calculate the next state of the model by calculating the next state of all of the sub-models.</p> <p>Inputs:</p> <ul> <li>psim_input: Input to the model.</li> </ul> <p>Outputs:</p> <ul> <li>next_state: Next state of the model.</li> </ul>"},{"location":"simulator/autoware_learning_based_vehicle_model/#dtsetdouble-dt","title":"<code>dtSet(double dt)</code>","text":"<p>Set the time step of the model.</p> <p>Inputs:</p> <ul> <li>dt: time step</li> </ul> <p>Outputs:</p> <ul> <li>None</li> </ul>"},{"location":"simulator/autoware_learning_based_vehicle_model/#example","title":"Example","text":"<p>Firstly we need to set up the model.</p> <pre><code>InterconnectedModel vehicle;\n\n// Example of model descriptors\nstd::tuple&lt;char*, char*, char*&gt; model_descriptor_1 = {\n    (char*)\"path_to_python_module_with_model_class_1\",\n    (char*)nullptr,  // If no param file is needed you can pass 'nullptr'\n    (char*)\"ModelClass1\"\n    };\n\nstd::tuple&lt;char*, char*, char*&gt; model_descriptor_2 =   {\n    (char*)\"path_to_python_module_with_model_class_2\",\n    (char*)\"/path_to/param_file\",\n    (char*)\"ModelClass2\"  // Name of the python class. Needs to use the interface from 'Assumptions'\n    };\n\n// Create sub-models based on descriptors\nvehicle.addSubmodel(model_descriptor_1);\nvehicle.addSubmodel(model_descriptor_2);\n\n// Define STATE and INPUT names of the system\nstd::vector&lt;char*&gt; state_names = {(char*)\"STATE_NAME_1\", (char*)\"STATE_NAME_2\"};\nstd::vector&lt;char*&gt; input_names = {(char*)\"INPUT_NAME_1\", (char*)\"INPUT_NAME_2\"};\n\n// Automatically connect sub-systems with model input\nvehicle.generateConnections(input_names, state_names);\n\n// Set the time step of the model\nvehicle.dtSet(dt);\n</code></pre> <p>After the model is correctly set up, we can use it the following way.</p> <pre><code>// Example of an model input\nstd::vector&lt;double&gt; vehicle_input = {0.0, 1.0}; // INPUT_NAME_1, INPUT_NAME_2\n\n// Example of an model state\nstd::vector&lt;double&gt; current_state = {0.2, 0.5}; // STATE_NAME_1, STATE_NAME_2\n\n// Set model state\nvehicle.initState(current_state);\n\n// Calculate the next state of the model\nstd::vector&lt;double&gt; next_state = vehicle.updatePyModel(vehicle_input);\n</code></pre>"},{"location":"simulator/autoware_learning_based_vehicle_model/#references-external-links","title":"References / External links","text":""},{"location":"simulator/autoware_learning_based_vehicle_model/#related-issues","title":"Related issues","text":""},{"location":"simulator/autoware_simple_planning_simulator/","title":"autoware_simple_planning_simulator","text":""},{"location":"simulator/autoware_simple_planning_simulator/#autoware_simple_planning_simulator","title":"autoware_simple_planning_simulator","text":""},{"location":"simulator/autoware_simple_planning_simulator/#purpose-use-cases","title":"Purpose / Use cases","text":"<p>This node simulates the vehicle motion for a vehicle command in 2D using a simple vehicle model.</p>"},{"location":"simulator/autoware_simple_planning_simulator/#design","title":"Design","text":"<p>The purpose of this simulator is for the integration test of planning and control modules. This does not simulate sensing or perception, but is implemented in pure c++ only and works without GPU.</p>"},{"location":"simulator/autoware_simple_planning_simulator/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<ul> <li>It simulates only in 2D motion.</li> <li>It does not perform physical operations such as collision and sensing, but only calculates the integral results of vehicle dynamics.</li> </ul>"},{"location":"simulator/autoware_simple_planning_simulator/#inputs-outputs-api","title":"Inputs / Outputs / API","text":""},{"location":"simulator/autoware_simple_planning_simulator/#input","title":"input","text":"<ul> <li>input/initialpose [<code>geometry_msgs/msg/PoseWithCovarianceStamped</code>] : for initial pose</li> <li>input/ackermann_control_command [<code>autoware_control_msgs/msg/Control</code>] : target command to drive a vehicle</li> <li>input/manual_ackermann_control_command [<code>autoware_control_msgs/msg/Control</code>] : manual target command to drive a vehicle (used when control_mode_request = Manual)</li> <li>input/gear_command [<code>autoware_vehicle_msgs/msg/GearCommand</code>] : target gear command.</li> <li>input/manual_gear_command [<code>autoware_vehicle_msgs/msg/GearCommand</code>] : target gear command (used when control_mode_request = Manual)</li> <li>input/turn_indicators_command [<code>autoware_vehicle_msgs/msg/TurnIndicatorsCommand</code>] : target turn indicator command</li> <li>input/hazard_lights_command [<code>autoware_vehicle_msgs/msg/HazardLightsCommand</code>] : target hazard lights command</li> <li>input/control_mode_request [<code>tier4_vehicle_msgs::srv::ControlModeRequest</code>] : mode change for Auto/Manual driving</li> </ul>"},{"location":"simulator/autoware_simple_planning_simulator/#output","title":"output","text":"<ul> <li>/tf [<code>tf2_msgs/msg/TFMessage</code>] : simulated vehicle pose (base_link)</li> <li>/output/odometry [<code>nav_msgs/msg/Odometry</code>] : simulated vehicle pose and twist</li> <li>/output/steering [<code>autoware_vehicle_msgs/msg/SteeringReport</code>] : simulated steering angle</li> <li>/output/control_mode_report [<code>autoware_vehicle_msgs/msg/ControlModeReport</code>] : current control mode (Auto/Manual)</li> <li>/output/gear_report [<code>autoware_vehicle_msgs/msg/ControlModeReport</code>] : simulated gear</li> <li>/output/turn_indicators_report [<code>autoware_vehicle_msgs/msg/ControlModeReport</code>] : simulated turn indicator status</li> <li>/output/hazard_lights_report [<code>autoware_vehicle_msgs/msg/ControlModeReport</code>] : simulated hazard lights status</li> </ul>"},{"location":"simulator/autoware_simple_planning_simulator/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"simulator/autoware_simple_planning_simulator/#common-parameters","title":"Common Parameters","text":"Name Type Description Default value simulated_frame_id string set to the child_frame_id in output tf \"base_link\" origin_frame_id string set to the frame_id in output tf \"odom\" initialize_source string If \"ORIGIN\", the initial pose is set at (0,0,0). If \"INITIAL_POSE_TOPIC\", node will wait until the <code>input/initialpose</code> topic is published. \"INITIAL_POSE_TOPIC\" add_measurement_noise bool If true, the Gaussian noise is added to the simulated results. true pos_noise_stddev double Standard deviation for position noise 0.01 rpy_noise_stddev double Standard deviation for Euler angle noise 0.0001 vel_noise_stddev double Standard deviation for longitudinal velocity noise 0.0 angvel_noise_stddev double Standard deviation for angular velocity noise 0.0 steer_noise_stddev double Standard deviation for steering angle noise 0.0001"},{"location":"simulator/autoware_simple_planning_simulator/#vehicle-model-parameters","title":"Vehicle Model Parameters","text":""},{"location":"simulator/autoware_simple_planning_simulator/#vehicle_model_type-options","title":"vehicle_model_type options","text":"<ul> <li><code>IDEAL_STEER_VEL</code></li> <li><code>IDEAL_STEER_ACC</code></li> <li><code>IDEAL_STEER_ACC_GEARED</code></li> <li><code>DELAY_STEER_VEL</code></li> <li><code>DELAY_STEER_ACC</code></li> <li><code>DELAY_STEER_ACC_GEARED</code></li> <li><code>DELAY_STEER_ACC_GEARED_WO_FALL_GUARD</code></li> <li><code>DELAY_STEER_MAP_ACC_GEARED</code>: applies 1D dynamics and time delay to the steering and acceleration commands. The simulated acceleration is determined by a value converted through the provided acceleration map. This model is valuable for an accurate simulation with acceleration deviations in a real vehicle.</li> <li><code>LEARNED_STEER_VEL</code>: launches learned python models. More about this here.</li> </ul> <p>The following models receive <code>ACTUATION_CMD</code> generated from <code>raw_vehicle_cmd_converter</code>. Therefore, when these models are selected, the <code>raw_vehicle_cmd_converter</code> is also launched.</p> <ul> <li><code>ACTUATION_CMD</code>: This model only converts accel/brake commands and use steer command as it is.</li> <li><code>ACTUATION_CMD_STEER_MAP</code>: The steer command is converted to the steer rate command using the steer map.</li> <li><code>ACTUATION_CMD_VGR</code>: The steer command is converted to the steer rate command using the variable gear ratio.</li> <li><code>ACTUATION_CMD_MECHANICAL</code>: This model has mechanical dynamics and controller for that.</li> </ul> <p>The <code>IDEAL</code> model moves ideally as commanded, while the <code>DELAY</code> model moves based on a 1st-order with time delay model. The <code>STEER</code> means the model receives the steer command. The <code>VEL</code> means the model receives the target velocity command, while the <code>ACC</code> model receives the target acceleration command. The <code>GEARED</code> suffix means that the motion will consider the gear command: the vehicle moves only one direction following the gear.</p> <p>The table below shows which models correspond to what parameters. The model names are written in abbreviated form (e.g. IDEAL_STEER_VEL = I_ST_V).</p> Name Type Description I_ST_V I_ST_A I_ST_A_G D_ST_V D_ST_A D_ST_A_G D_ST_A_G_WO_FG D_ST_M_ACC_G L_S_V A_C Default value unit acc_time_delay double dead time for the acceleration input x x x x o o o o x x 0.1 [s] steer_time_delay double dead time for the steering input x x x o o o o o x o 0.24 [s] vel_time_delay double dead time for the velocity input x x x o x x x x x x 0.25 [s] acc_time_constant double time constant of the 1st-order acceleration dynamics x x x x o o o o x x 0.1 [s] steer_time_constant double time constant of the 1st-order steering dynamics x x x o o o o o x o 0.27 [s] steer_dead_band double dead band for steering angle x x x o o o o x x o 0.0 [rad] vel_time_constant double time constant of the 1st-order velocity dynamics x x x o x x x x x x 0.5 [s] vel_lim double limit of velocity x x x o o o o o x x 50.0 [m/s] vel_rate_lim double limit of acceleration x x x o o o o o x x 7.0 [m/ss] steer_lim double limit of steering angle x x x o o o o o x o 1.0 [rad] steer_rate_lim double limit of steering angle change rate x x x o o o o o x o 5.0 [rad/s] steer_bias double bias for steering angle x x x o o o o o x o 0.0 [rad] debug_acc_scaling_factor double scaling factor for accel command x x x x o o o x x x 1.0 [-] debug_steer_scaling_factor double scaling factor for steer command x x x x o o o x x x 1.0 [-] acceleration_map_path string path to csv file for acceleration map which converts velocity and ideal acceleration to actual acceleration x x x x x x x o x x - [-] model_module_paths string path to a python module where the model is implemented x x x x x x x x o x - [-] model_param_paths string path to the file where model parameters are stored (can be empty string if no parameter file is required) x x x x x x x x o x - [-] model_class_names string name of the class that implements the model x x x x x x x x o x - [-] <p>Note: Parameters <code>model_module_paths</code>, <code>model_param_paths</code>, and <code>model_class_names</code> need to have the same length.</p> <p>The <code>acceleration_map</code> is used only for <code>DELAY_STEER_MAP_ACC_GEARED</code> and it shows the acceleration command on the vertical axis and the current velocity on the horizontal axis, with each cell representing the converted acceleration command that is actually used in the simulator's motion calculation. Values in between are linearly interpolated.</p> <p>Example of <code>acceleration_map.csv</code></p> <pre><code>default,  0.00,  1.39,  2.78,  4.17,  5.56,  6.94,  8.33,  9.72, 11.11, 12.50, 13.89, 15.28, 16.67\n-4.0,    -4.40, -4.36, -4.38, -4.12, -4.20, -3.94, -3.98, -3.80, -3.77, -3.76, -3.59, -3.50, -3.40\n-3.5,    -4.00, -3.91, -3.85, -3.64, -3.68, -3.55, -3.42, -3.24, -3.25, -3.00, -3.04, -2.93, -2.80\n-3.0,    -3.40, -3.37, -3.33, -3.00, -3.00, -2.90, -2.88, -2.65, -2.43, -2.44, -2.43, -2.39, -2.30\n-2.5,    -2.80, -2.72, -2.72, -2.62, -2.41, -2.43, -2.26, -2.18, -2.11, -2.03, -1.96, -1.91, -1.85\n-2.0,    -2.30, -2.24, -2.12, -2.02, -1.92, -1.81, -1.67, -1.58, -1.51, -1.49, -1.40, -1.35, -1.30\n-1.5,    -1.70, -1.61, -1.47, -1.46, -1.40, -1.37, -1.29, -1.24, -1.10, -0.99, -0.83, -0.80, -0.78\n-1.0,    -1.30, -1.28, -1.10, -1.09, -1.04, -1.02, -0.98, -0.89, -0.82, -0.61, -0.52, -0.54, -0.56\n-0.8,    -0.96, -0.90, -0.82, -0.74, -0.70, -0.65, -0.63, -0.59, -0.55, -0.44, -0.39, -0.39, -0.35\n-0.6,    -0.77, -0.71, -0.67, -0.65, -0.58, -0.52, -0.51, -0.50, -0.40, -0.33, -0.30, -0.31, -0.30\n-0.4,    -0.45, -0.40, -0.45, -0.44, -0.38, -0.35, -0.31, -0.30, -0.26, -0.30, -0.29, -0.31, -0.25\n-0.2,    -0.24, -0.24, -0.25, -0.22, -0.23, -0.25, -0.27, -0.29, -0.24, -0.22, -0.17, -0.18, -0.12\n 0.0,     0.00,  0.00, -0.05, -0.05, -0.05, -0.05, -0.08, -0.08, -0.08, -0.08, -0.10, -0.10, -0.10\n 0.2,     0.16,  0.12,  0.02,  0.02,  0.00,  0.00, -0.05, -0.05, -0.05, -0.05, -0.08, -0.08, -0.08\n 0.4,     0.38,  0.30,  0.22,  0.25,  0.24,  0.23,  0.20,  0.16,  0.16,  0.14,  0.10,  0.05,  0.05\n 0.6,     0.52,  0.52,  0.51,  0.49,  0.43,  0.40,  0.35,  0.33,  0.33,  0.33,  0.32,  0.34,  0.34\n 0.8,     0.82,  0.81,  0.78,  0.68,  0.63,  0.56,  0.53,  0.48,  0.43,  0.41,  0.37,  0.38,  0.40\n 1.0,     1.00,  1.08,  1.01,  0.88,  0.76,  0.69,  0.66,  0.58,  0.54,  0.49,  0.45,  0.40,  0.40\n 1.5,     1.52,  1.50,  1.38,  1.26,  1.14,  1.03,  0.91,  0.82,  0.67,  0.61,  0.51,  0.41,  0.41\n 2.0,     1.80,  1.80,  1.64,  1.43,  1.25,  1.11,  0.96,  0.81,  0.70,  0.59,  0.51,  0.42,  0.42\n</code></pre> <p></p>"},{"location":"simulator/autoware_simple_planning_simulator/#actuation_cmd-model","title":"ACTUATION_CMD model","text":"<p>The simple_planning_simulator usually operates by receiving Control commands, but when the <code>ACTUATION_CMD*</code> model is selected, it receives Actuation commands instead of Control commands. This model can simulate the motion using the vehicle command that is actually sent to the real vehicle. Therefore, when this model is selected, the <code>raw_vehicle_cmd_converter</code> is also launched. Please refer to the actuation_cmd_sim.md for more details.</p> <p>Note: The steering/velocity/acceleration dynamics is modeled by a first order system with a deadtime in a delay model. The definition of the time constant is the time it takes for the step response to rise up to 63% of its final value. The deadtime is a delay in the response to a control input.</p>"},{"location":"simulator/autoware_simple_planning_simulator/#example-of-learned_steer_vel-model","title":"Example of LEARNED_STEER_VEL model","text":"<p>We created a few basic models to showcase how <code>LEARNED_STEER_VEL</code> works.</p> <ol> <li> <p>Install a library that contains basic Python models. (branch: <code>v0.1_autoware</code>)</p> </li> <li> <p>In a file <code>src/vehicle/sample_vehicle_launch/sample_vehicle_description/config/simulator_model.param.yaml</code> set <code>vehicle_model_type</code> to <code>LEARNED_STEER_VEL</code>. In the same file set the following parameters. These models are for testing and do not require any parameter file.</p> </li> </ol> <pre><code>model_module_paths:\n  [\n    \"control_analysis_pipeline.autoware_models.vehicle.kinematic\",\n    \"control_analysis_pipeline.autoware_models.steering.steer_example\",\n    \"control_analysis_pipeline.autoware_models.drive.drive_example\",\n  ]\nmodel_param_paths: [\"\", \"\", \"\"]\nmodel_class_names: [\"KinematicModel\", \"SteerExample\", \"DriveExample\"]\n</code></pre>"},{"location":"simulator/autoware_simple_planning_simulator/#default-tf-configuration","title":"Default TF configuration","text":"<p>Since the vehicle outputs <code>odom</code>-&gt;<code>base_link</code> tf, this simulator outputs the tf with the same frame_id configuration. In the simple_planning_simulator.launch.py, the node that outputs the <code>map</code>-&gt;<code>odom</code> tf, that usually estimated by the localization module (e.g. NDT), will be launched as well. Since the tf output by this simulator module is an ideal value, <code>odom</code>-&gt;<code>map</code> will always be 0.</p>"},{"location":"simulator/autoware_simple_planning_simulator/#caveat-pitch-calculation","title":"(Caveat) Pitch calculation","text":"<p>Ego vehicle pitch angle is calculated in the following manner.</p> <p></p> <p>NOTE: driving against the line direction (as depicted in image's bottom row) is not supported and only shown for illustration purposes.</p>"},{"location":"simulator/autoware_simple_planning_simulator/#error-detection-and-handling","title":"Error detection and handling","text":"<p>The only validation on inputs being done is testing for a valid vehicle model type.</p>"},{"location":"simulator/autoware_simple_planning_simulator/#security-considerations","title":"Security considerations","text":""},{"location":"simulator/autoware_simple_planning_simulator/#references-external-links","title":"References / External links","text":"<p>This is originally developed in the Autoware.AI. See the link below.</p> <p>https://github.com/Autoware-AI/simulation/tree/master/wf_simulator</p>"},{"location":"simulator/autoware_simple_planning_simulator/#future-extensions-unimplemented-parts","title":"Future extensions / Unimplemented parts","text":"<ul> <li>Improving the accuracy of vehicle models (e.g., adding steering dead zones and slip behavior)</li> <li>Cooperation with modules that output pseudo pointcloud or pseudo perception results</li> </ul>"},{"location":"simulator/autoware_simple_planning_simulator/docs/actuation_cmd_sim/","title":"ACTUATION_CMD model","text":""},{"location":"simulator/autoware_simple_planning_simulator/docs/actuation_cmd_sim/#actuation_cmd-model","title":"ACTUATION_CMD model","text":"<p>The simple_planning_simulator usually operates by receiving Control commands, but when the <code>ACTUATION_CMD*</code> model is selected, it receives Actuation commands instead of Control commands. This model can simulate the motion using the vehicle command that is actually sent to the real vehicle. Therefore, when this model is selected, the <code>raw_vehicle_cmd_converter</code> is also launched.</p>"},{"location":"simulator/autoware_simple_planning_simulator/docs/actuation_cmd_sim/#actuation_cmd","title":"ACTUATION_CMD","text":"<p>This model receives the accel/brake commands and converts them using the map to calculate the motion of the model. The steer command is used as it is. Please make sure that the raw_vehicle_cmd_converter is configured as follows.</p> <pre><code>convert_accel_cmd: true\nconvert_brake_cmd: true\nconvert_steer_cmd: false\n</code></pre>"},{"location":"simulator/autoware_simple_planning_simulator/docs/actuation_cmd_sim/#actuation_cmd_steer_map","title":"ACTUATION_CMD_STEER_MAP","text":"<p>This model is inherited from ACTUATION_CMD and receives steering arbitrary value as the actuation command. The value is converted to the steering tire rate to calculate the motion of the model. An arbitrary value is like EPS (Electric Power Steering) Voltage.</p> <p>Please make sure that the raw_vehicle_cmd_converter is configured as follows.</p> <pre><code>convert_accel_cmd: true\nconvert_brake_cmd: true\nconvert_steer_cmd: true\n</code></pre>"},{"location":"simulator/autoware_simple_planning_simulator/docs/actuation_cmd_sim/#actuation_cmd_vgr","title":"ACTUATION_CMD_VGR","text":"<p>This model is inherited from ACTUATION_CMD and steering wheel angle is sent as the actuation command. The value is converted to the steering tire angle to calculate the motion of the model.</p> <p>Please make sure that the raw_vehicle_cmd_converter is configured as follows.</p> <pre><code>convert_accel_cmd: true\nconvert_brake_cmd: true\nconvert_steer_cmd: true\n</code></pre> <p></p>"},{"location":"simulator/autoware_simple_planning_simulator/docs/actuation_cmd_sim/#actuation_cmd_mechanical","title":"ACTUATION_CMD_MECHANICAL","text":"<p>This model is inherited from ACTUATION_CMD_VGR nad has mechanical dynamics and controller for that to simulate the mechanical structure and software of the real vehicle.</p> <p>Please make sure that the raw_vehicle_cmd_converter is configured as follows.</p> <pre><code>convert_accel_cmd: true\nconvert_brake_cmd: true\nconvert_steer_cmd: true\n</code></pre> <p>The mechanical structure of the vehicle is as follows.</p> <p></p> <p>The vehicle side software assumes that it has limiters, PID controllers, power steering, etc. for the input. The conversion in the power steering is approximated by a polynomial. Steering Dynamics is a model that represents the motion of the tire angle when the Steering Torque is input. It is represented by the following formula.</p> <pre><code>\\begin{align}\n\\dot{\\theta} &amp;= \\omega \\\\\n\\dot{\\omega} &amp;= \\frac{1}{I} (T_{\\text{input}} - D \\omega - K \\theta - \\text{sign}(\\omega) F_{\\text{friction}} ) \\\\\n\\end{align}\n</code></pre> <p>In this case,</p> <ul> <li>\\(\\theta\\) : Tire angle</li> <li>\\(\\omega\\) : Tire angular velocity</li> <li>\\(T_{\\text{input}}\\) : Input torque</li> <li>\\(D\\) : Damping coefficient</li> <li>\\(K\\) : Spring constant</li> <li>\\(F_{\\text{friction}}\\) : Friction force</li> <li>\\(I\\) : Moment of inertia</li> </ul> <p>Also, this dynamics has a dead zone. The steering rotation direction is different from the steering torque input direction, and the steering torque input is less than the dead zone threshold, it enters the dead zone. Once it enters the dead zone, it is judged to be in the dead zone until there is a steering input above the dead zone threshold. When in the dead zone, the steering tire angle does not move.</p> <p>Please refer to the following file for the values of the parameters that have been system-identified using the actual vehicle's driving data. The blue line is the control input, the green line is the actual vehicle's tire angle output, and the red line is the simulator's tire angle output. mechanical_sample_param</p> <p>This model has a smaller sum of errors with the observed values of the actual vehicle than when tuned with a normal first-order lag model. For details, please refer to #9252.</p> <p></p> <p>The parameters used in the ACTUATION_CMD are as follows.</p> Name Type Description unit accel_time_delay double dead time for the acceleration input [s] accel_time_constant double time constant of the 1st-order acceleration dynamics [s] brake_time_delay double dead time for the brake input [s] brake_time_constant double time constant of the 1st-order brake dynamics [s] convert_accel_cmd bool If true, it is assumed that the command is received converted to an accel actuation value, and it is converted back to acceleration value inside the simulator. [-] convert_brake_cmd bool If true, it is assumed that the command is received converted to a brake actuation value, and it is converted back to acceleration value inside the simulator. [-] vgr_coef_a double the value of the coefficient a of the variable gear ratio [-] vgr_coef_b double the value of the coefficient b of the variable gear ratio [-] vgr_coef_c double the value of the coefficient c of the variable gear ratio [-] enable_pub_steer bool whether to publish the steering tire angle. if false, it is expected to be converted and published from actuation_status in other nodes (e.g. raw_vehicle_cmd_converter) [-]"},{"location":"simulator/autoware_vehicle_door_simulator/","title":"autoware_vehicle_door_simulator","text":""},{"location":"simulator/autoware_vehicle_door_simulator/#autoware_vehicle_door_simulator","title":"autoware_vehicle_door_simulator","text":"<p>This package is for testing operations on vehicle devices such as doors.</p>"},{"location":"simulator/tier4_dummy_object_rviz_plugin/","title":"tier4_dummy_object_rviz_plugin","text":""},{"location":"simulator/tier4_dummy_object_rviz_plugin/#tier4_dummy_object_rviz_plugin","title":"tier4_dummy_object_rviz_plugin","text":""},{"location":"simulator/tier4_dummy_object_rviz_plugin/#purpose","title":"Purpose","text":"<p>This plugin is used to generate dummy pedestrians, cars, and obstacles in planning simulator.</p>"},{"location":"simulator/tier4_dummy_object_rviz_plugin/#overview","title":"Overview","text":"<p>The CarInitialPoseTool sends a topic for generating a dummy car. The PedestrianInitialPoseTool sends a topic for generating a dummy pedestrian. The UnknownInitialPoseTool sends a topic for generating a dummy obstacle. The DeleteAllObjectsTool deletes the dummy cars, pedestrians, and obstacles displayed by the above three tools.</p>"},{"location":"simulator/tier4_dummy_object_rviz_plugin/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"simulator/tier4_dummy_object_rviz_plugin/#output","title":"Output","text":"Name Type Description <code>/simulation/dummy_perception_publisher/object_info</code> <code>tier4_simulation_msgs::msg::DummyObject</code> The topic on which to publish dummy object info"},{"location":"simulator/tier4_dummy_object_rviz_plugin/#parameter","title":"Parameter","text":""},{"location":"simulator/tier4_dummy_object_rviz_plugin/#core-parameters","title":"Core Parameters","text":""},{"location":"simulator/tier4_dummy_object_rviz_plugin/#carpose","title":"CarPose","text":"Name Type Default Value Description <code>topic_property_</code> string <code>/simulation/dummy_perception_publisher/object_info</code> The topic on which to publish dummy object info <code>std_dev_x_</code> float 0.03 X standard deviation for initial pose [m] <code>std_dev_y_</code> float 0.03 Y standard deviation for initial pose [m] <code>std_dev_z_</code> float 0.03 Z standard deviation for initial pose [m] <code>std_dev_theta_</code> float 5.0 * M_PI / 180.0 Theta standard deviation for initial pose [rad] <code>length_</code> float 4.0 X standard deviation for initial pose [m] <code>width_</code> float 1.8 Y standard deviation for initial pose [m] <code>height_</code> float 2.0 Z standard deviation for initial pose [m] <code>position_z_</code> float 0.0 Z position for initial pose [m] <code>velocity_</code> float 0.0 Velocity [m/s]"},{"location":"simulator/tier4_dummy_object_rviz_plugin/#buspose","title":"BusPose","text":"Name Type Default Value Description <code>topic_property_</code> string <code>/simulation/dummy_perception_publisher/object_info</code> The topic on which to publish dummy object info <code>std_dev_x_</code> float 0.03 X standard deviation for initial pose [m] <code>std_dev_y_</code> float 0.03 Y standard deviation for initial pose [m] <code>std_dev_z_</code> float 0.03 Z standard deviation for initial pose [m] <code>std_dev_theta_</code> float 5.0 * M_PI / 180.0 Theta standard deviation for initial pose [rad] <code>length_</code> float 10.5 X standard deviation for initial pose [m] <code>width_</code> float 2.5 Y standard deviation for initial pose [m] <code>height_</code> float 3.5 Z standard deviation for initial pose [m] <code>position_z_</code> float 0.0 Z position for initial pose [m] <code>velocity_</code> float 0.0 Velocity [m/s]"},{"location":"simulator/tier4_dummy_object_rviz_plugin/#pedestrianpose","title":"PedestrianPose","text":"Name Type Default Value Description <code>topic_property_</code> string <code>/simulation/dummy_perception_publisher/object_info</code> The topic on which to publish dummy object info <code>std_dev_x_</code> float 0.03 X standard deviation for initial pose [m] <code>std_dev_y_</code> float 0.03 Y standard deviation for initial pose [m] <code>std_dev_z_</code> float 0.03 Z standard deviation for initial pose [m] <code>std_dev_theta_</code> float 5.0 * M_PI / 180.0 Theta standard deviation for initial pose [rad] <code>position_z_</code> float 0.0 Z position for initial pose [m] <code>velocity_</code> float 0.0 Velocity [m/s]"},{"location":"simulator/tier4_dummy_object_rviz_plugin/#unknownpose","title":"UnknownPose","text":"Name Type Default Value Description <code>topic_property_</code> string <code>/simulation/dummy_perception_publisher/object_info</code> The topic on which to publish dummy object info <code>std_dev_x_</code> float 0.03 X standard deviation for initial pose [m] <code>std_dev_y_</code> float 0.03 Y standard deviation for initial pose [m] <code>std_dev_z_</code> float 0.03 Z standard deviation for initial pose [m] <code>std_dev_theta_</code> float 5.0 * M_PI / 180.0 Theta standard deviation for initial pose [rad] <code>position_z_</code> float 0.0 Z position for initial pose [m] <code>velocity_</code> float 0.0 Velocity [m/s]"},{"location":"simulator/tier4_dummy_object_rviz_plugin/#deleteallobjects","title":"DeleteAllObjects","text":"Name Type Default Value Description <code>topic_property_</code> string <code>/simulation/dummy_perception_publisher/object_info</code> The topic on which to publish dummy object info"},{"location":"simulator/tier4_dummy_object_rviz_plugin/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>Using a planning simulator</p>"},{"location":"simulator/tier4_dummy_object_rviz_plugin/#usage","title":"Usage","text":"<ol> <li>Start rviz and select + on the tool tab.    </li> <li>Select one of the following: tier4_dummy_object_rviz_plugin and press OK.    </li> <li>Select the new item in the tool tab (2D Dummy Car in the example) and click on it in rviz.    </li> </ol>"},{"location":"simulator/tier4_dummy_object_rviz_plugin/#interactive-manipulation","title":"Interactive manipulation","text":"<p>You can interactively manipulate the object.</p> <ol> <li>Select \"Tool Properties\" in rviz.</li> <li>Select the corresponding object tab in the Tool Properties.</li> <li>Turn the \"Interactive\" checkbox on.    </li> <li>Select the item in the tool tab in you haven't chosen yet.</li> <li>Key commands are as follows.</li> </ol> action key command ADD Shift + Click Right Button MOVE Hold down Right Button + Drug and Drop DELETE Alt + Click Right Button"},{"location":"simulator/tier4_dummy_object_rviz_plugin/#material-design-icons","title":"Material Design Icons","text":"<p>This project uses Material Design Icons by Google. These icons are used under the terms of the Apache License, Version 2.0.</p> <p>Material Design Icons are a collection of symbols provided by Google that are used to enhance the user interface of applications, websites, and other digital products.</p>"},{"location":"simulator/tier4_dummy_object_rviz_plugin/#license","title":"License","text":"<p>The Material Design Icons are licensed under the Apache License, Version 2.0. You may obtain a copy of the License at:</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"simulator/tier4_dummy_object_rviz_plugin/#acknowledgments","title":"Acknowledgments","text":"<p>We would like to express our gratitude to Google for making these icons available to the community, helping developers and designers enhance the visual appeal and user experience of their projects.</p>"},{"location":"system/autoware_bluetooth_monitor/","title":"bluetooth_monitor","text":""},{"location":"system/autoware_bluetooth_monitor/#bluetooth_monitor","title":"bluetooth_monitor","text":""},{"location":"system/autoware_bluetooth_monitor/#description","title":"Description","text":"<p>This node monitors a Bluetooth connection to a wireless device by using L2ping. L2ping generates PING echo command on Bluetooth L2CAP layer, and it is able to receive and check echo response from a wireless device.</p>"},{"location":"system/autoware_bluetooth_monitor/#block-diagram","title":"Block diagram","text":"<p>L2ping is only allowed for root by default, so this package provides the following approach to minimize security risks as much as possible:</p> <ul> <li>Provide a small program named <code>l2ping_service</code> which performs L2ping and provides wireless device information to <code>bluetooth_monitor</code> by using socket programming.</li> <li><code>bluetooth_monitor</code> is able to know wireless device information and L2ping status as an unprivileged user since those information are sent by socket communication.</li> </ul> <p></p>"},{"location":"system/autoware_bluetooth_monitor/#output","title":"Output","text":""},{"location":"system/autoware_bluetooth_monitor/#bluetooth_monitor-bluetooth_connection","title":"bluetooth_monitor: bluetooth_connection","text":"<p>[summary]</p> level message OK OK WARN RTT warning ERROR Lost Function error <p>[values]</p> key value (example) Device [0-9]: Status OK / RTT warning / Verify error / Lost / Ping rejected / Function error Device [0-9]: Name Wireless Controller Device [0-9]: Manufacturer MediaTek, Inc. Device [0-9]: Address AA:BB:CC:DD:EE:FF Device [0-9]: RTT 0.00ms <ul> <li>The following key will be added when <code>bluetooth_monitor</code> reports <code>Function error</code>.   ex.) The <code>connect</code> system call failed.</li> </ul> key (example) value (example) Device [0-9]: connect No such file or directory"},{"location":"system/autoware_bluetooth_monitor/#parameters","title":"Parameters","text":"Name Type Description Default Range addresses array Bluetooth addresses of the device to monitor ['4C:B9:9B:6E:7F:9A'] N/A port integer Port number to connect to L2ping service on the host 7640 N/A timeout integer Time in seconds to wait for a response from the device 5 N/A rtt_warn float Time in seconds to warn if the round trip time is greater than this value 0.1 N/A <ul> <li><code>rtt_warn</code><ul> <li>0.00(zero): Disable checking RTT</li> <li>otherwise: Check RTT with specified seconds</li> </ul> </li> </ul> <ul> <li><code>addresses</code><ul> <li>*: All connected devices</li> <li>AA:BB:CC:DD:EE:FF: You can specify a device to monitor by setting a Bluetooth address</li> </ul> </li> </ul>"},{"location":"system/autoware_bluetooth_monitor/#instructions-before-starting","title":"Instructions before starting","text":"<ul> <li>You can skip this instructions if you run <code>l2ping_service</code> as root user.</li> </ul> <ol> <li> <p>Assign capability to <code>l2ping_service</code> since L2ping requires <code>cap_net_raw+eip</code> capability.</p> <pre><code>sudo setcap 'cap_net_raw+eip' ./build/bluetooth_monitor/l2ping_service\n</code></pre> </li> <li> <p>Run <code>l2ping_service</code> and <code>bluetooth_monitor</code>.</p> <pre><code>./build/bluetooth_monitor/l2ping_service\nros2 launch autoware_bluetooth_monitor bluetooth_monitor.launch.xml\n</code></pre> </li> </ol>"},{"location":"system/autoware_bluetooth_monitor/#known-limitations-and-issues","title":"Known limitations and issues","text":"<p>None.</p>"},{"location":"system/autoware_command_mode_decider/","title":"autoware_command_mode_decider","text":""},{"location":"system/autoware_command_mode_decider/#autoware_command_mode_decider","title":"autoware_command_mode_decider","text":""},{"location":"system/autoware_command_mode_decider/#overview","title":"Overview","text":"<p>This package determines the priorities of vehicle behavior. The determined behavior is published to the switcher node as command mode. The command mode is the unified concept of operation mode and MRM. Typically, the system will select a specified operation mode when normal, and if it is not available, select an appropriate MRM. Since the command mode selection algorithm is system-dependent, developers can implement any logic they want through plugins.</p> <p></p>"},{"location":"system/autoware_command_mode_decider/#debug-topic","title":"Debug topic","text":"<p>The debug topic type is Int32MultiArrayStamped. Each item represents the following. Here, manual mode means the state where the autoware control is false. Note that operation mode and command mode can be divided into narrow and broad senses depending on whether it contains manual mode.</p> Index Description Mode candidates 0 This is the current command mode. operation mode, manual control, MRM 1 This is the current operation mode. operation mode 2 This is the last operation mode where the transition is complete. operation mode, manual control 3 This is the requested autoware control. boolean value (0 or 1) 4 This is the requested operation mode. operation mode 5+ These are the decided command modes. operation mode, MRM"},{"location":"system/autoware_command_mode_switcher/","title":"autoware_command_mode_switcher","text":""},{"location":"system/autoware_command_mode_switcher/#autoware_command_mode_switcher","title":"autoware_command_mode_switcher","text":""},{"location":"system/autoware_command_mode_switcher/#overview","title":"Overview","text":"<p>This package activates the target command mode from the decider node. The activation process for each command mode is implementation dependent, so extensions using plugins are supported. By implementing the required interface, each plugin will be able to operate under the appropriate state transitions.</p>"},{"location":"system/autoware_command_mode_switcher/#status","title":"Status","text":"<p>The switcher node will output the following status depending on the plugin status.</p> Status Description request This command mode is requested transition This command mode transition is in progress vehicle_gate_selected The vehicle interface is using this command mode network_gate_selected The ECU is selected (if there are multiple ECUs) command_gate_selected The command source of this command mode is selected command_source_exclusive This is the only command mode using the command source command_source_enabled This command mode is enabled command_source_disabled This command mode is disabled"},{"location":"system/autoware_command_mode_types/","title":"autoware_command_mode_types","text":""},{"location":"system/autoware_command_mode_types/#autoware_command_mode_types","title":"autoware_command_mode_types","text":""},{"location":"system/autoware_command_mode_types/#overview","title":"Overview","text":"<p>This package defines constants for command modes and command sources that are commonly used across several packages. Command modes represents a particular behavior in Autoware, and in implementation it is a collective term for operation mode and MRM. Developers can add their command modes and define the decision logic for them. However, since the API currently does not support extending the operation mode, the focus will be on extending MRM.</p>"},{"location":"system/autoware_command_mode_types/#architecture","title":"Architecture","text":"<p>The following diagram shows the architecture of the relevant modules. The command mode decider and command mode switcher nodes allow their behavior to be customized using plugins. The decider node determines the target command mode based on the command mode availability from diagnostic graph aggregator, and the switcher node receives it and actually enables the target mode. Since a single module may support multiple command modes by switching between modes, the commands that are actually output are managed as a command source. Finally, the control command gate node selects one of these command sources and sends it to the vehicle.</p> <p></p>"},{"location":"system/autoware_command_mode_types/#adding-modes-and-sources","title":"Adding modes and sources","text":"<p>If you want to add a new command mode or command source, It is recommended to create a package that defines the mode and source ID like this package. Then, use the defined ID to create plugins and change the settings of each node. For information on assigning IDs, refer to the following sections.</p>"},{"location":"system/autoware_command_mode_types/#command-modes","title":"Command modes","text":"<p>The following table lists the mode IDs defined by this package. To add a new ID, follow the rules below.</p> <ul> <li>The value is a 16-bit unsigned integer.</li> <li>Do not use 0 through 9.</li> <li>It is recommended to assign a value that is not confused with the source IDs.</li> </ul> ID Name Description 0 unknown unknown 1000 manual operation mode autoware control disabled 1001 stop operation mode stop 1002 autonomous operation mode autonomous 1003 local operation mode local 1004 remote operation mode remote 2001 emergency_stop MRM emergency stop 2002 comfortable_stop MRM comfortable stop 2003 pull_over MRM pull over (not yet supported)"},{"location":"system/autoware_command_mode_types/#command-sources","title":"Command sources","text":"<p>The following table lists the source IDs defined by this package. To add a new ID, follow the rules below.</p> <ul> <li>The value is a 16-bit unsigned integer.</li> <li>Do not use 0 through 9.</li> <li>It is recommended to assign a value that is not confused with the mode IDs.</li> </ul> ID Name Description 0 unknown unknown 1 builtin builtin stop command of control command gate 11 stop operation mode stop 12 main operation mode stop, MRM comfortable stop 13 local operation mode local 14 remote operation mode remote 21 emergency_stop MRM emergency stop"},{"location":"system/autoware_component_monitor/","title":"autoware_component_monitor","text":""},{"location":"system/autoware_component_monitor/#autoware_component_monitor","title":"autoware_component_monitor","text":"<p>The <code>autoware_component_monitor</code> package allows monitoring system usage of component containers. The composable node inside the package is attached to a component container, and it publishes CPU and memory usage of the container.</p>"},{"location":"system/autoware_component_monitor/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"system/autoware_component_monitor/#input","title":"Input","text":"<p>None.</p>"},{"location":"system/autoware_component_monitor/#output","title":"Output","text":"Name Type Description <code>~/component_system_usage</code> <code>autoware_internal_msgs::msg::ResourceUsageReport</code> CPU, Memory usage etc."},{"location":"system/autoware_component_monitor/#parameters","title":"Parameters","text":""},{"location":"system/autoware_component_monitor/#core-parameters","title":"Core Parameters","text":"Name Type Description Default Range publish_rate float Publish rate in Hz 5 N/A"},{"location":"system/autoware_component_monitor/#how-to-use","title":"How to use","text":"<p>Add it as a composable node in your launch file:</p> <pre><code>&lt;launch&gt;\n  &lt;group&gt;\n    &lt;push-ros-namespace namespace=\"your_namespace\"/&gt;\n    ...\n\n    &lt;load_composable_node target=\"$(var container_name)\"&gt;\n      &lt;composable_node pkg=\"autoware_component_monitor\"\n                       plugin=\"autoware::component_monitor::ComponentMonitor\"\n                       name=\"component_monitor\"&gt;\n        &lt;param from=\"$(find-pkg-share autoware_component_monitor)/config/component_monitor.param.yaml\"/&gt;\n      &lt;/composable_node&gt;\n    &lt;/load_composable_node&gt;\n\n    ...\n  &lt;/group&gt;\n&lt;/launch&gt;\n</code></pre>"},{"location":"system/autoware_component_monitor/#quick-testing","title":"Quick testing","text":"<p>You can test the package by running the following command:</p> <pre><code>ros2 component load &lt;container_name&gt; autoware_component_monitor autoware::component_monitor::ComponentMonitor -p publish_rate:=10.0 --node-namespace &lt;namespace&gt;\n\n# Example usage\nros2 component load /pointcloud_container autoware_component_monitor autoware::component_monitor::ComponentMonitor -p publish_rate:=10.0 --node-namespace /pointcloud_container\n</code></pre>"},{"location":"system/autoware_component_monitor/#how-it-works","title":"How it works","text":"<p>The package uses the <code>top</code> command under the hood. <code>top -b -n 1 -E k -p PID</code> command is run at 10 Hz to get the system usage of the process.</p> <ul> <li><code>-b</code> activates the batch mode. By default, <code>top</code> doesn't exit and prints to stdout periodically. Batch mode allows   exiting the program.</li> <li><code>-n</code> number of times should <code>top</code> prints the system usage in batch mode.</li> <li><code>-p</code> specifies the PID of the process to monitor.</li> <li><code>-E k</code> changes the memory unit in the summary section to KiB.</li> </ul> <p>Here is a sample output:</p> <pre><code>top - 13:57:26 up  3:14,  1 user,  load average: 1,09, 1,10, 1,04\nTasks:   1 total,   0 running,   1 sleeping,   0 stopped,   0 zombie\n%Cpu(s):  0,0 us,  0,8 sy,  0,0 ni, 99,2 id,  0,0 wa,  0,0 hi,  0,0 si,  0,0 st\nKiB Mem : 65532208 total, 35117428 free, 17669824 used, 12744956 buff/cache\nKiB Swap: 39062524 total, 39062524 free,        0 used. 45520816 avail Mem\n\n    PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND\n   3352 meb       20   0 2905940   1,2g  39292 S   0,0   2,0  23:24.01 awesome\n</code></pre> <p>We get 5th, 8th fields from the last line, which are RES, %CPU respectively.</p>"},{"location":"system/autoware_component_state_monitor/","title":"autoware_component_state_monitor","text":""},{"location":"system/autoware_component_state_monitor/#autoware_component_state_monitor","title":"autoware_component_state_monitor","text":"<p>The component state monitor checks the state of each component using topic state monitor. This is an implementation for backward compatibility with the AD service state monitor. It will be replaced in the future using a diagnostics tree.</p>"},{"location":"system/autoware_default_adapi_helpers/autoware_automatic_pose_initializer/","title":"automatic_pose_initializer","text":""},{"location":"system/autoware_default_adapi_helpers/autoware_automatic_pose_initializer/#automatic_pose_initializer","title":"automatic_pose_initializer","text":""},{"location":"system/autoware_default_adapi_helpers/autoware_automatic_pose_initializer/#automatic_pose_initializer_1","title":"automatic_pose_initializer","text":"<p>This node calls localization initialize API when the localization initialization state is uninitialized. Since the API uses GNSS pose when no pose is specified, initialization using GNSS can be performed automatically.</p> Interface Local Name Global Name Description Subscription - /api/localization/initialization_state The localization initialization state API. Client - /api/localization/initialize The localization initialize API."},{"location":"system/autoware_default_adapi_universe/","title":"autoware_default_adapi_universe","text":""},{"location":"system/autoware_default_adapi_universe/#autoware_default_adapi_universe","title":"autoware_default_adapi_universe","text":""},{"location":"system/autoware_default_adapi_universe/#notes","title":"Notes","text":"<p>Components that relay services must be executed by the Multi-Threaded Executor.</p>"},{"location":"system/autoware_default_adapi_universe/#features","title":"Features","text":"<p>This package is a default implementation AD API.</p> <ul> <li>autoware state (backward compatibility)</li> <li>fail-safe</li> <li>interface</li> <li>localization</li> <li>motion</li> <li>operation mode</li> <li>routing</li> </ul>"},{"location":"system/autoware_default_adapi_universe/#interface","title":"Interface","text":"<ul> <li>Autoware AD API</li> <li>Parameters</li> <li>Diagnostics</li> </ul>"},{"location":"system/autoware_default_adapi_universe/#web-server-script","title":"Web server script","text":"<p>This is a sample to call API using HTTP.</p>"},{"location":"system/autoware_default_adapi_universe/#guide-message-script","title":"Guide message script","text":"<p>This script is no longer supported, please use rqt_diagnostic_graph_monitor instead. This tool is available in autoware_tools.</p> <pre><code>ros2 run rqt_diagnostic_graph_monitor rqt_diagnostic_graph_monitor\n</code></pre>"},{"location":"system/autoware_default_adapi_universe/document/autoware-state/","title":"Autoware state compatibility","text":""},{"location":"system/autoware_default_adapi_universe/document/autoware-state/#autoware-state-compatibility","title":"Autoware state compatibility","text":""},{"location":"system/autoware_default_adapi_universe/document/autoware-state/#overview","title":"Overview","text":"<p>Since <code>/autoware/state</code> was so widely used, this packages creates it from the states of AD API for backwards compatibility. The diagnostic checks that ad_service_state_monitor used to perform have been replaced by autoware_component_state_monitor. The service <code>/autoware/shutdown</code> to change autoware state to finalizing is also supported for compatibility.</p> <p></p>"},{"location":"system/autoware_default_adapi_universe/document/autoware-state/#conversion","title":"Conversion","text":"<p>This is the correspondence between AD API states and autoware states. The launch state is the data that this node holds internally. For the routing state arrival, the first 2.0 seconds correspond to ArrivedGoal, the rest correspond to WaitingForRoute.</p> <p></p>"},{"location":"system/autoware_default_adapi_universe/document/diagnostics/","title":"Diagnostics","text":""},{"location":"system/autoware_default_adapi_universe/document/diagnostics/#diagnostics","title":"Diagnostics","text":""},{"location":"system/autoware_default_adapi_universe/document/diagnostics/#adapinodelocalization-state","title":"/adapi/node/localization: state","text":"<p>The level is OK when the localization state is INITIALIZED. Otherwise ERROR.</p>"},{"location":"system/autoware_default_adapi_universe/document/diagnostics/#adapinoderouting-state","title":"/adapi/node/routing: state","text":"<p>The level is OK when the routing state is SET, REROUTING, or ARRIVED. Otherwise ERROR.</p>"},{"location":"system/autoware_default_adapi_universe/document/diagnostics/#adapinodemrm_request-delegate","title":"/adapi/node/mrm_request: delegate","text":"<p>The level is OK when the delegate strategy is not requested. Otherwise ERROR. The senders of the delegate strategy are set to keys.</p>"},{"location":"system/autoware_default_adapi_universe/document/fail-safe/","title":"Fail-safe API","text":""},{"location":"system/autoware_default_adapi_universe/document/fail-safe/#fail-safe-api","title":"Fail-safe API","text":""},{"location":"system/autoware_default_adapi_universe/document/fail-safe/#overview","title":"Overview","text":"<p>The fail-safe API simply relays the MRM state. See the autoware-documentation for AD API specifications.</p>"},{"location":"system/autoware_default_adapi_universe/document/interface/","title":"Interface API","text":""},{"location":"system/autoware_default_adapi_universe/document/interface/#interface-api","title":"Interface API","text":""},{"location":"system/autoware_default_adapi_universe/document/interface/#overview","title":"Overview","text":"<p>The interface API simply returns a version number. See the autoware-documentation for AD API specifications.</p>"},{"location":"system/autoware_default_adapi_universe/document/localization/","title":"Localization API","text":""},{"location":"system/autoware_default_adapi_universe/document/localization/#localization-api","title":"Localization API","text":""},{"location":"system/autoware_default_adapi_universe/document/localization/#overview","title":"Overview","text":"<p>Unify the location initialization method to the service. The topic <code>/initialpose</code> from rviz is now only subscribed to by adapter node and converted to API call. This API call is forwarded to the pose initializer node so it can centralize the state of pose initialization. For other nodes that require initialpose, pose initializer node publishes as <code>/initialpose3d</code>. See the autoware-documentation for AD API specifications.</p> <p></p>"},{"location":"system/autoware_default_adapi_universe/document/motion/","title":"Motion API","text":""},{"location":"system/autoware_default_adapi_universe/document/motion/#motion-api","title":"Motion API","text":""},{"location":"system/autoware_default_adapi_universe/document/motion/#overview","title":"Overview","text":"<p>Provides a hook for when the vehicle starts. It is typically used for announcements that call attention to the surroundings. Add a pause function to the vehicle_cmd_gate, and API will control it based on vehicle stopped and start requested. See the autoware-documentation for AD API specifications.</p> <p></p>"},{"location":"system/autoware_default_adapi_universe/document/motion/#states","title":"States","text":"<p>The implementation has more detailed state transitions to manage pause state synchronization. The correspondence with the AD API state is as follows.</p> <p></p>"},{"location":"system/autoware_default_adapi_universe/document/operation-mode/","title":"Operation mode API","text":""},{"location":"system/autoware_default_adapi_universe/document/operation-mode/#operation-mode-api","title":"Operation mode API","text":""},{"location":"system/autoware_default_adapi_universe/document/operation-mode/#overview","title":"Overview","text":"<p>Introduce operation mode. It handles autoware engage, gate_mode, external_cmd_selector and control_mode abstractly. When the mode is changed, it will be in-transition state, and if the transition completion condition to that mode is not satisfied, it will be returned to the previous mode. Also, currently, the condition for mode change is only <code>WaitingForEngage</code> in <code>/autoware/state</code>, and the engage state is shared between modes. After introducing the operation mode, each mode will have a transition available flag. See the autoware-documentation for AD API specifications.</p> <p></p>"},{"location":"system/autoware_default_adapi_universe/document/operation-mode/#states","title":"States","text":"<p>The operation mode has the following state transitions. Disabling autoware control and changing operation mode when autoware control is disabled can be done immediately. Otherwise, enabling autoware control and changing operation mode when autoware control is enabled causes the state will be transition state. If the mode change completion condition is not satisfied within the timeout in the transition state, it will return to the previous mode.</p> <p></p>"},{"location":"system/autoware_default_adapi_universe/document/operation-mode/#compatibility","title":"Compatibility","text":"<p>Ideally, vehicle_cmd_gate and external_cmd_selector should be merged so that the operation mode can be handled directly. However, currently the operation mode transition manager performs the following conversions to match the implementation. The transition manager monitors each topic in the previous interface and synchronizes the operation mode when it changes. When the operation mode is changed with the new interface, the transition manager disables synchronization and changes the operation mode using the previous interface.</p> <p></p>"},{"location":"system/autoware_default_adapi_universe/document/parameters/","title":"Parameters","text":""},{"location":"system/autoware_default_adapi_universe/document/parameters/#parameters","title":"Parameters","text":"<p>The default parameters are set by default_adapi.param.yaml.</p>"},{"location":"system/autoware_default_adapi_universe/document/parameters/#adapinodeautoware_state","title":"/adapi/node/autoware_state","text":"Parameter Type Description update_rate double Update rate of autoware state."},{"location":"system/autoware_default_adapi_universe/document/parameters/#adapinodemotion","title":"/adapi/node/motion","text":"Parameter Type Description require_accept_start bool Require accept start API at start if true, stop_check_duration double Stop check duration of the vehicle."},{"location":"system/autoware_default_adapi_universe/document/parameters/#adapinoderouting","title":"/adapi/node/routing","text":"Parameter Type Description stop_check_duration double Stop check duration of the vehicle."},{"location":"system/autoware_default_adapi_universe/document/parameters/#adapinodevehicle_door","title":"/adapi/node/vehicle_door","text":"Parameter Type Description check_autoware_control bool Always allow door control during direct control if false."},{"location":"system/autoware_default_adapi_universe/document/parameters/#adapinodemanuallocal","title":"/adapi/node/manual/local","text":"Parameter Type Description mode string Input source of manual control. Specify \"local\" for local control."},{"location":"system/autoware_default_adapi_universe/document/parameters/#adapinodemanualremote","title":"/adapi/node/manual/remote","text":"Parameter Type Description mode string Input source of manual control. Specify \"remote\" for remote control."},{"location":"system/autoware_default_adapi_universe/document/routing/","title":"Routing API","text":""},{"location":"system/autoware_default_adapi_universe/document/routing/#routing-api","title":"Routing API","text":""},{"location":"system/autoware_default_adapi_universe/document/routing/#overview","title":"Overview","text":"<p>Unify the route setting method to the service. This API supports two waypoint formats, poses and lanelet segments. The goal and checkpoint topics from rviz is only subscribed to by adapter node and converted to API call. This API call is forwarded to the mission planner node so it can centralize the state of routing. For other nodes that require route, mission planner node publishes as <code>/planning/mission_planning/route</code>. See the autoware-documentation for AD API specifications.</p> <p></p>"},{"location":"system/autoware_diagnostic_graph_aggregator/","title":"autoware_diagnostic_graph_aggregator","text":""},{"location":"system/autoware_diagnostic_graph_aggregator/#autoware_diagnostic_graph_aggregator","title":"autoware_diagnostic_graph_aggregator","text":""},{"location":"system/autoware_diagnostic_graph_aggregator/#overview","title":"Overview","text":"<p>The diagnostic graph aggregator node subscribes to diagnostic array and publishes aggregated diagnostic graph. As shown in the diagram below, this node introduces extra diagnostic status for intermediate functional units.</p> <p></p>"},{"location":"system/autoware_diagnostic_graph_aggregator/#diagnostic-graph-structures","title":"Diagnostic graph structures","text":"<p>The diagnostic graph is actually a set of fault tree analysis (FTA) for each operation mode of Autoware. Since the status of the same node may be referenced by multiple nodes, the overall structure is a directed acyclic graph (DAG). Each node in the diagnostic graph represents the diagnostic status of a specific functional unit, including the input diagnostics. So we define this as \"unit\", and call the unit corresponding to the input diagnosis \"diag unit\" and the others \"node unit\".</p> <p>Every unit has an error level that is the same as DiagnosticStatus, a unit type, and optionally a unit path. In addition, every diag unit has a message, a hardware_id, and values that are the same as DiagnosticStatus. The unit type represents how the unit status is calculated, such as AND or OR. The unit path is any unique string that represents the functionality of the unit.</p> <p>NOTE: This feature is currently under development. The diagnostic graph also supports \"link\" because there are cases where connections between units have additional status. For example, it is natural that many functional units will have an error status until initialization is complete.</p>"},{"location":"system/autoware_diagnostic_graph_aggregator/#operation-mode-availability","title":"Operation mode availability","text":"<p>For MRM, this node publishes the status of the top-level functional units in the dedicated message. Therefore, the diagnostic graph must contain functional units with the following names. This feature breaks the generality of the graph and may be changed to a plugin or another node in the future.</p> <ul> <li>/autoware/operation/stop</li> <li>/autoware/operation/autonomous</li> <li>/autoware/operation/local</li> <li>/autoware/operation/remote</li> <li>/autoware/operation/emergency-stop</li> <li>/autoware/operation/comfortable-stop</li> <li>/autoware/operation/pull-over</li> </ul>"},{"location":"system/autoware_diagnostic_graph_aggregator/#interfaces","title":"Interfaces","text":"Interface Type Interface Name Data Type Description subscription <code>/diagnostics</code> <code>diagnostic_msgs/msg/DiagnosticArray</code> Diagnostics input. publisher <code>/diagnostics_graph/unknowns</code> <code>diagnostic_msgs/msg/DiagnosticArray</code> Diagnostics not included in graph. publisher <code>/diagnostics_graph/struct</code> <code>tier4_system_msgs/msg/DiagGraphStruct</code> Diagnostic graph (static part). publisher <code>/diagnostics_graph/status</code> <code>tier4_system_msgs/msg/DiagGraphStatus</code> Diagnostic graph (dynamic part). publisher <code>/system/operation_mode/availability</code> <code>tier4_system_msgs/msg/OperationModeAvailability</code> Operation mode availability."},{"location":"system/autoware_diagnostic_graph_aggregator/#parameters","title":"Parameters","text":"Parameter Name Data Type Description <code>graph_file</code> <code>string</code> Path of the config file. <code>rate</code> <code>double</code> Rate of aggregation and topic publication. <code>input_qos_depth</code> <code>uint</code> QoS depth of input array topic. <code>graph_qos_depth</code> <code>uint</code> QoS depth of output graph topic. <code>use_operation_mode_availability</code> <code>bool</code> Use operation mode availability publisher."},{"location":"system/autoware_diagnostic_graph_aggregator/#examples","title":"Examples","text":"<p>This is an example of a diagnostic graph configuration. The configuration can be split into multiple files.</p> <ul> <li>main.yaml</li> <li>module1.yaml</li> <li>module2.yaml</li> </ul> <pre><code>ros2 launch autoware_diagnostic_graph_aggregator example-main.launch.xml\n</code></pre> <p>If you want to share the same settings with a configuration that is missing some diagnostics due to differences in variations, we recommend that you unify the diagnostics by using dummy_diag_publisher, etc.</p>"},{"location":"system/autoware_diagnostic_graph_aggregator/#debug-tools","title":"Debug tools","text":"<ul> <li>tree</li> <li>autoware_diagnostic_graph_utils</li> </ul>"},{"location":"system/autoware_diagnostic_graph_aggregator/#graph-file-format","title":"Graph file format","text":"<ul> <li>graph</li> <li>path</li> <li>unit</li> </ul>"},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/graph/","title":"Graph","text":""},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/graph/#graph","title":"Graph","text":"<p>The graph object is the top level structure that makes up the configuration file.</p>"},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/graph/#format","title":"Format","text":"Name Type Required Description <code>files</code> <code>list[path]</code> no List of path objects for importing subgraphs. <code>units</code> <code>list[unit]</code> no List of unit objects that make up the graph."},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/path/","title":"Path","text":""},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/path/#path","title":"Path","text":"<p>The path object specifies the file path of the subgraph to be imported. The structure of the subgraph file should be graph object.</p>"},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/path/#format","title":"Format","text":"Name Type Required Description <code>path</code> <code>string</code> yes The file path of the subgraph."},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/path/#substitutions","title":"Substitutions","text":"<p>File paths can contain substitutions like ROS 2 launch. The supported substitutions are as follows.</p> Substitution Description <code>$(dirname)</code> The path of this file directory. <code>$(find-pkg-share &lt;package&gt;)</code> The path of the package."},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/unit/","title":"Unit","text":""},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/unit/#unit","title":"Unit","text":"<p>The <code>unit</code> is a base object that makes up the diagnostic graph. Any derived object can be used where a unit object is required.</p>"},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/unit/#format","title":"Format","text":"Name Type Required Description <code>path</code> <code>string</code> no Any string to reference from other units. <code>type</code> <code>string</code> yes The string indicating the type of derived object. <code>dependent</code> <code>string</code> no The path of the dependent unit. <code>latch</code> <code>float</code> no The latch seconds for this unit. <code>$derived argument</code> <code>any</code> no Additional arguments for constructing the derived object of given <code>type</code>"},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/unit/#derived-object-types","title":"Derived object types","text":"<p><code>$derived_arguments</code> denotes additional arguments to be passed for constructing the object which is specified by <code>type</code> parameter. For example, if \"and\" is provided as <code>type</code>, <code>list</code> parameter is also required.</p> <ul> <li>diag</li> <li>link</li> <li>and</li> <li>or</li> <li>remapping<ul> <li><code>warn-to-ok</code></li> <li><code>warn-to-error</code></li> </ul> </li> <li>constant<ul> <li><code>ok</code></li> <li><code>warn</code></li> <li><code>error</code></li> <li><code>stale</code></li> </ul> </li> </ul>"},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/unit/#dependent-failure","title":"Dependent failure","text":"<p>The <code>dependent</code> field defines dependencies between units that do not have a parent-child relationship in the tree. If the unit specified in this field is not OK, the <code>is_dependent</code> field in the node status will be true. This allows the user to easily rule out units other than the root cause.</p>"},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/unit/#latch","title":"Latch","text":"<p>If the <code>latch</code> field is set to a number of seconds, the worst level will be latched after the specified time has elapsed. If the field is not specified, the latch is disabled and the input and output levels always match.</p> <p></p> <p>This is when the latch is set to 0.0 seconds.</p> <p></p> <p>This is when the latch is set to 2.0 seconds.</p> <p></p> <p>Error level is also considered warning level. This is when the latch is set to 2.0 seconds.</p> <p></p> <p>When the latch is cleared, the decision starts again from that moment, independent of the previous error duration. This is when the latch is set to 2.0 seconds.</p> <p></p>"},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/unit/and/","title":"And","text":""},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/unit/and/#and","title":"And","text":"<p>The <code>and</code> object is a unit that is evaluated as the maximum error level of the input units. Note that error level <code>stale</code> is treated as <code>error</code>.</p>"},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/unit/and/#format","title":"Format","text":"Name Type Required Description <code>type</code> <code>string</code> yes Specify <code>and</code> when using this object. <code>list</code> <code>list[unit]</code> yes List of input unit objects. <p>If <code>list</code> is undeclared or empty, this object is evaluated as <code>ok</code>.</p>"},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/unit/and/#short-circuit-evaluation","title":"Short-circuit evaluation","text":"<p>The type <code>short-circuit-and</code> is deprecated. This currently does exactly the same thing as <code>and</code>. To ignore errors in dependencies, use the <code>dependent</code> field of the unit object.</p>"},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/unit/const/","title":"Constant","text":""},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/unit/const/#constant","title":"Constant","text":"<p>The constant object is a unit with a fixed error level.</p>"},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/unit/const/#format","title":"Format","text":"Name Type Required Description <code>type</code> <code>string</code> yes Specify error level when using this object."},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/unit/const/#error-levels","title":"Error levels","text":"<p>The supported error levels are as follows.</p> <ul> <li><code>ok</code></li> <li><code>warn</code></li> <li><code>error</code></li> <li><code>stale</code></li> </ul>"},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/unit/diag/","title":"Diag","text":""},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/unit/diag/#diag","title":"Diag","text":"<p>The <code>diag</code> object is a unit that refers to a specific status within the source diagnostics.</p>"},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/unit/diag/#format","title":"Format","text":"Name Type Required Description <code>type</code> <code>string</code> yes Specify <code>diag</code> when using this object. <code>node</code> <code>string</code> yes Node name that outputs the diagnostic status. <code>name</code> <code>string</code> yes The name of the diagnostic status. <code>timeout</code> <code>float</code> yes The timeout seconds for the diagnostic status. <code>hysteresis</code> <code>float</code> yes The hysteresis seconds for the diagnostic status."},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/unit/diag/#hysteresis","title":"Hysteresis","text":"<p>If the <code>hysteresis</code> field is set to a number of seconds, level changes less than this time will be ignored. If the field is not specified, it is equivalent to specifying 0.0 seconds, in which case the input and output will always match. This is when the latch is set to 0.0 seconds.</p> <p></p> <p>This is when the hysteresis is set to 2.0 seconds.</p> <p></p> <p>Error level is also considered warning level when the current level is OK. OK level is also considered warning level when the current level is error. This is when the hysteresis is set to 2.0 seconds.</p> <p></p>"},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/unit/link/","title":"Link","text":""},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/unit/link/#link","title":"Link","text":"<p>The <code>link</code> object is a unit that refers to another unit.</p>"},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/unit/link/#format","title":"Format","text":"Name Type Required Description <code>type</code> <code>string</code> yes Specify <code>link</code> when using this object. <code>link</code> <code>string</code> yes The path of the unit to reference."},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/unit/or/","title":"Or","text":""},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/unit/or/#or","title":"Or","text":"<p>The <code>or</code> object is a unit that is evaluated as the minimum error level of the input units. Note that error level <code>stale</code> is treated as <code>error</code>.</p>"},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/unit/or/#format","title":"Format","text":"Name Type Required Description <code>type</code> <code>string</code> yes Specify <code>or</code> when using this object. <code>list</code> <code>list[unit]</code> yes List of input unit objects. <p>If <code>list</code> is undeclared or empty, this object is evaluated as <code>error</code>.</p>"},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/unit/remap/","title":"Constant","text":""},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/unit/remap/#constant","title":"Constant","text":"<p>Warning</p> <p>This object is under development. It may be removed in the future.</p> <p>The remapping object is a unit that converts error levels.</p>"},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/unit/remap/#format","title":"Format","text":"Name Type Required Description <code>type</code> <code>string</code> yes Specify remapping type when using this object. <code>item</code> <code>unit</code> yes Input unit object."},{"location":"system/autoware_diagnostic_graph_aggregator/doc/format/unit/remap/#remapping-types","title":"Remapping types","text":"<p>The supported remapping types are as follows.</p> <ul> <li><code>warn-to-ok</code></li> <li><code>warn-to-error</code></li> </ul>"},{"location":"system/autoware_diagnostic_graph_aggregator/doc/tool/tree/","title":"Tree tool","text":""},{"location":"system/autoware_diagnostic_graph_aggregator/doc/tool/tree/#tree-tool","title":"Tree tool","text":"<p>This tool displays the graph structure of the configuration file in tree format.</p>"},{"location":"system/autoware_diagnostic_graph_aggregator/doc/tool/tree/#usage","title":"Usage","text":"<pre><code>ros2 run autoware_diagnostic_graph_aggregator tree &lt;graph-config-path&gt;\n</code></pre>"},{"location":"system/autoware_diagnostic_graph_aggregator/doc/tool/tree/#examples","title":"Examples","text":"<pre><code>ros2 run autoware_diagnostic_graph_aggregator tree '$(find-pkg-share autoware_diagnostic_graph_aggregator)/example/graph/main.yaml'\n</code></pre> <pre><code>===== Top-level trees ============================\n- /autoware/modes/autonomous (and)\n    - /functions/pose_estimation (and)\n    - /functions/obstacle_detection (or)\n- /autoware/modes/local (and)\n    - /external/joystick_command (diag)\n- /autoware/modes/remote (and)\n    - /external/remote_command (diag)\n- /autoware/modes/comfortable_stop (and)\n    - /functions/obstacle_detection (or)\n- /autoware/modes/pull_over (and)\n    - /functions/pose_estimation (and)\n    - /functions/obstacle_detection (or)\n===== Subtrees ===================================\n- /functions/pose_estimation (and)\n    - /sensing/lidars/top (diag)\n- /functions/obstacle_detection (or)\n    - /sensing/lidars/front (diag)\n    - /sensing/radars/front (diag)\n===== Isolated units =============================\n- /autoware/modes/stop (ok)\n- /autoware/modes/emergency_stop (ok)\n</code></pre>"},{"location":"system/autoware_diagnostic_graph_utils/","title":"autoware_diagnostic_graph_utils","text":""},{"location":"system/autoware_diagnostic_graph_utils/#autoware_diagnostic_graph_utils","title":"autoware_diagnostic_graph_utils","text":"<p>This package is a utility for diagnostic graph published by diagnostic_graph_aggregator.</p>"},{"location":"system/autoware_diagnostic_graph_utils/#ros-node","title":"ROS node","text":"<ul> <li>dump</li> <li>converter</li> </ul>"},{"location":"system/autoware_diagnostic_graph_utils/#c-library","title":"C++ library","text":"<ul> <li>DiagGraph</li> <li>DiagGraphSubscription</li> </ul>"},{"location":"system/autoware_diagnostic_graph_utils/doc/node/converter/","title":"Converter tool","text":""},{"location":"system/autoware_diagnostic_graph_utils/doc/node/converter/#converter-tool","title":"Converter tool","text":"<p>This tool converts <code>/diagnostics_graph</code> to <code>/diagnostics_array</code> so it can be read by tools such as <code>rqt_runtime_monitor</code>.</p>"},{"location":"system/autoware_diagnostic_graph_utils/doc/node/converter/#usage","title":"Usage","text":"<pre><code>ros2 run autoware_diagnostic_graph_utils converter_node\n</code></pre>"},{"location":"system/autoware_diagnostic_graph_utils/doc/node/converter/#examples","title":"Examples","text":"<p>Terminal 1:</p> <pre><code>ros2 launch diagnostic_graph_aggregator example-main.launch.xml\n</code></pre> <p>Terminal 2:</p> <pre><code>ros2 run autoware_diagnostic_graph_utils converter_node\n</code></pre> <p>Terminal 3:</p> <pre><code>ros2 run rqt_runtime_monitor rqt_runtime_monitor --ros-args -r diagnostics:=diagnostics_array\n</code></pre> <p></p>"},{"location":"system/autoware_diagnostic_graph_utils/doc/node/dump/","title":"Dump tool","text":""},{"location":"system/autoware_diagnostic_graph_utils/doc/node/dump/#dump-tool","title":"Dump tool","text":"<p>This tool displays <code>/diagnostics_graph</code> in table format.</p>"},{"location":"system/autoware_diagnostic_graph_utils/doc/node/dump/#usage","title":"Usage","text":"<pre><code>ros2 run autoware_diagnostic_graph_utils dump_node\n</code></pre>"},{"location":"system/autoware_diagnostic_graph_utils/doc/node/dump/#examples","title":"Examples","text":"<p>Terminal 1:</p> <pre><code>ros2 launch diagnostic_graph_aggregator example-main.launch.xml\n</code></pre> <p>Terminal 2:</p> <pre><code>ros2 run autoware_diagnostic_graph_utils dump_node\n</code></pre> <p>Output:</p> <pre><code>|----|-------|----------------------------------|------|----------|\n| No | Level | Path                             | Type | Children |\n|----|-------|----------------------------------|------|----------|\n|  1 | OK    | /autoware/modes/stop             | ok   |          |\n|  2 | ERROR | /autoware/modes/autonomous       | and  | 8 9      |\n|  3 | OK    | /autoware/modes/local            | and  | 13       |\n|  4 | OK    | /autoware/modes/remote           | and  | 14       |\n|  5 | OK    | /autoware/modes/emergency_stop   | ok   |          |\n|  6 | OK    | /autoware/modes/comfortable_stop | and  | 9        |\n|  7 | ERROR | /autoware/modes/pull_over        | and  | 8 9      |\n|  8 | ERROR | /functions/pose_estimation       | and  | 10       |\n|  9 | OK    | /functions/obstacle_detection    | or   | 11 12    |\n| 10 | ERROR | /sensing/lidars/top              | diag |          |\n| 11 | OK    | /sensing/lidars/front            | diag |          |\n| 12 | OK    | /sensing/radars/front            | diag |          |\n| 13 | OK    | /external/joystick_command       | diag |          |\n| 14 | OK    | /external/remote_command         | diag |          |\n</code></pre>"},{"location":"system/autoware_dummy_diag_publisher/","title":"dummy_diag_publisher","text":""},{"location":"system/autoware_dummy_diag_publisher/#dummy_diag_publisher","title":"dummy_diag_publisher","text":""},{"location":"system/autoware_dummy_diag_publisher/#purpose","title":"Purpose","text":"<p>This package outputs a dummy diagnostic data for debugging and developing.</p>"},{"location":"system/autoware_dummy_diag_publisher/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"system/autoware_dummy_diag_publisher/#outputs","title":"Outputs","text":"Name Type Description <code>/diagnostics</code> <code>diagnostic_msgs::msgs::DiagnosticArray</code> Diagnostics outputs"},{"location":"system/autoware_dummy_diag_publisher/#parameters","title":"Parameters","text":""},{"location":"system/autoware_dummy_diag_publisher/#node-parameters","title":"Node Parameters","text":"<p>The parameter <code>DIAGNOSTIC_NAME</code> must be a name that exists in the parameter YAML file. If the parameter <code>status</code> is given from a command line, the parameter <code>is_active</code> is automatically set to <code>true</code>.</p> Name Type Default Value Explanation Reconfigurable <code>update_rate</code> int <code>10</code> Timer callback period [Hz] false <code>DIAGNOSTIC_NAME.is_active</code> bool <code>true</code> Force update or not true <code>DIAGNOSTIC_NAME.status</code> string <code>\"OK\"</code> diag status set by dummy diag publisher true"},{"location":"system/autoware_dummy_diag_publisher/#yaml-format-for-dummy_diag_publisher","title":"YAML format for dummy_diag_publisher","text":"<p>If the value is <code>default</code>, the default value will be set.</p> Key Type Default Value Explanation <code>required_diags.DIAGNOSTIC_NAME.is_active</code> bool <code>true</code> Force update or not <code>required_diags.DIAGNOSTIC_NAME.status</code> string <code>\"OK\"</code> diag status set by dummy diag publisher"},{"location":"system/autoware_dummy_diag_publisher/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"system/autoware_dummy_diag_publisher/#usage","title":"Usage","text":""},{"location":"system/autoware_dummy_diag_publisher/#launch","title":"launch","text":"<pre><code>ros2 launch autoware_dummy_diag_publisher dummy_diag_publisher.launch.xml\n</code></pre>"},{"location":"system/autoware_dummy_diag_publisher/#reconfigure","title":"reconfigure","text":"<pre><code>ros2 param set /dummy_diag_publisher velodyne_connection.status \"Warn\"\nros2 param set /dummy_diag_publisher velodyne_connection.is_active true\n</code></pre>"},{"location":"system/autoware_dummy_infrastructure/","title":"autoware_dummy_infrastructure","text":""},{"location":"system/autoware_dummy_infrastructure/#autoware_dummy_infrastructure","title":"autoware_dummy_infrastructure","text":"<p>This is a debug node for infrastructure communication.</p>"},{"location":"system/autoware_dummy_infrastructure/#usage","title":"Usage","text":"<pre><code>ros2 launch autoware_dummy_infrastructure dummy_infrastructure.launch.xml\nros2 run rqt_reconfigure rqt_reconfigure\n</code></pre>"},{"location":"system/autoware_dummy_infrastructure/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"system/autoware_dummy_infrastructure/#inputs","title":"Inputs","text":"Name Type Description <code>~/input/command_array</code> <code>tier4_v2x_msgs::msg::InfrastructureCommandArray</code> Infrastructure command"},{"location":"system/autoware_dummy_infrastructure/#outputs","title":"Outputs","text":"Name Type Description <code>~/output/state_array</code> <code>tier4_v2x_msgs::msg::VirtualTrafficLightStateArray</code> Virtual traffic light array"},{"location":"system/autoware_dummy_infrastructure/#parameters","title":"Parameters","text":""},{"location":"system/autoware_dummy_infrastructure/#node-parameters","title":"Node Parameters","text":"Name Type Default Value Explanation <code>update_rate</code> double <code>10.0</code> Timer callback period [Hz] <code>use_first_command</code> bool <code>true</code> Consider instrument id or not <code>use_command_state</code> bool <code>false</code> Consider command state or not <code>instrument_id</code> string `` Used as command id <code>approval</code> bool <code>false</code> set approval filed to ros param <code>is_finalized</code> bool <code>false</code> Stop at stop_line if finalization isn't completed"},{"location":"system/autoware_dummy_infrastructure/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"system/autoware_duplicated_node_checker/","title":"Duplicated Node Checker","text":""},{"location":"system/autoware_duplicated_node_checker/#duplicated-node-checker","title":"Duplicated Node Checker","text":""},{"location":"system/autoware_duplicated_node_checker/#purpose","title":"Purpose","text":"<p>This node monitors the ROS 2 environments and detect duplication of node names in the environment. The result is published as diagnostics.</p>"},{"location":"system/autoware_duplicated_node_checker/#standalone-startup","title":"Standalone Startup","text":"<pre><code>ros2 launch autoware_duplicated_node_checker duplicated_node_checker.launch.xml\n</code></pre>"},{"location":"system/autoware_duplicated_node_checker/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The types of topic status and corresponding diagnostic status are following.</p> Duplication status Diagnostic status Description <code>OK</code> OK No duplication is detected <code>Duplicated Detected</code> ERROR Duplication is detected"},{"location":"system/autoware_duplicated_node_checker/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"system/autoware_duplicated_node_checker/#output","title":"Output","text":"Name Type Description <code>/diagnostics</code> <code>diagnostic_msgs/DiagnosticArray</code> Diagnostics outputs"},{"location":"system/autoware_duplicated_node_checker/#parameters","title":"Parameters","text":"Name Type Description Default Range update_rate float The scanning and update frequency of the checker. 10 &gt;2"},{"location":"system/autoware_duplicated_node_checker/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"system/autoware_mrm_comfortable_stop_operator/","title":"mrm_comfortable_stop_operator","text":""},{"location":"system/autoware_mrm_comfortable_stop_operator/#mrm_comfortable_stop_operator","title":"mrm_comfortable_stop_operator","text":""},{"location":"system/autoware_mrm_comfortable_stop_operator/#purpose","title":"Purpose","text":"<p>MRM comfortable stop operator is a node that generates comfortable stop commands according to the comfortable stop MRM order.</p>"},{"location":"system/autoware_mrm_comfortable_stop_operator/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"system/autoware_mrm_comfortable_stop_operator/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"system/autoware_mrm_comfortable_stop_operator/#input","title":"Input","text":"Name Type Description <code>~/input/mrm/comfortable_stop/operate</code> <code>tier4_system_msgs::srv::OperateMrm</code> MRM execution order"},{"location":"system/autoware_mrm_comfortable_stop_operator/#output","title":"Output","text":"Name Type Description <code>~/output/mrm/comfortable_stop/status</code> <code>tier4_system_msgs::msg::MrmBehaviorStatus</code> MRM execution status <code>~/output/velocity_limit</code> <code>autoware_internal_planning_msgs::msg::VelocityLimit</code> Velocity limit command <code>~/output/velocity_limit/clear</code> <code>autoware_internal_planning_msgs::msg::VelocityLimitClearCommand</code> Velocity limit clear command"},{"location":"system/autoware_mrm_comfortable_stop_operator/#parameters","title":"Parameters","text":"Name Type Description Default Range update_rate integer Timer callback frequency [Hz] 10 N/A min_acceleration float Minimum acceleration for comfortable stop [m/s^2] -1 N/A max_jerk float Maximum jerk for comfortable stop [m/s^3] 0.3 N/A min_jerk float Minimum jerk for comfortable stop [m/s^3] -0.3 N/A"},{"location":"system/autoware_mrm_comfortable_stop_operator/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"system/autoware_mrm_emergency_stop_operator/","title":"mrm_emergency_stop_operator","text":""},{"location":"system/autoware_mrm_emergency_stop_operator/#mrm_emergency_stop_operator","title":"mrm_emergency_stop_operator","text":""},{"location":"system/autoware_mrm_emergency_stop_operator/#purpose","title":"Purpose","text":"<p>MRM emergency stop operator is a node that generates emergency stop commands according to the emergency stop MRM order.</p>"},{"location":"system/autoware_mrm_emergency_stop_operator/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"system/autoware_mrm_emergency_stop_operator/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"system/autoware_mrm_emergency_stop_operator/#input","title":"Input","text":"Name Type Description <code>~/input/mrm/emergency_stop/operate</code> <code>tier4_system_msgs::srv::OperateMrm</code> MRM execution order <code>~/input/control/control_cmd</code> <code>autoware_control_msgs::msg::Control</code> Control command output from the last node of the control component. Used for the initial value of the emergency stop command."},{"location":"system/autoware_mrm_emergency_stop_operator/#output","title":"Output","text":"Name Type Description <code>~/output/mrm/emergency_stop/status</code> <code>tier4_system_msgs::msg::MrmBehaviorStatus</code> MRM execution status <code>~/output/mrm/emergency_stop/control_cmd</code> <code>autoware_control_msgs::msg::Control</code> Emergency stop command"},{"location":"system/autoware_mrm_emergency_stop_operator/#parameters","title":"Parameters","text":"Name Type Description Default Range update_rate integer Timer callback frequency [Hz] 30 N/A target_acceleration float Target acceleration for emergency stop [m/s^2] -2.5 N/A target_jerk float Target jerk for emergency stop [m/s^3] -1.5 N/A"},{"location":"system/autoware_mrm_emergency_stop_operator/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"system/autoware_mrm_handler/","title":"mrm_handler","text":""},{"location":"system/autoware_mrm_handler/#mrm_handler","title":"mrm_handler","text":""},{"location":"system/autoware_mrm_handler/#purpose","title":"Purpose","text":"<p>MRM Handler is a node to select a proper MRM from a system failure state contained in OperationModeAvailability.</p>"},{"location":"system/autoware_mrm_handler/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"system/autoware_mrm_handler/#state-transitions","title":"State Transitions","text":""},{"location":"system/autoware_mrm_handler/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"system/autoware_mrm_handler/#input","title":"Input","text":"Name Type Description <code>/localization/kinematic_state</code> <code>nav_msgs::msg::Odometry</code> Used to decide whether vehicle is stopped or not <code>/system/operation_mode/availability</code> <code>tier4_system_msgs::msg::OperationModeAvailability</code> Used to select proper MRM from system available mrm behavior contained in operationModeAvailability <code>/vehicle/status/control_mode</code> <code>autoware_vehicle_msgs::msg::ControlModeReport</code> Used to check vehicle mode: autonomous or manual <code>/system/mrm/emergency_stop/status</code> <code>tier4_system_msgs::msg::MrmBehaviorStatus</code> Used to check if MRM emergency stop operation is available <code>/system/mrm/comfortable_stop/status</code> <code>tier4_system_msgs::msg::MrmBehaviorStatus</code> Used to check if MRM comfortable stop operation is available <code>/system/mrm/pull_over_manager/status</code> <code>tier4_system_msgs::msg::MrmBehaviorStatus</code> Used to check if MRM pull over operation is available <code>/api/operation_mode/state</code> <code>autoware_adapi_v1_msgs::msg::OperationModeState</code> Used to check whether the current operation mode is AUTO or STOP."},{"location":"system/autoware_mrm_handler/#output","title":"Output","text":"Name Type Description <code>/system/emergency/gear_cmd</code> <code>autoware_vehicle_msgs::msg::GearCommand</code> Required to execute proper MRM (send gear cmd) <code>/system/emergency/turn_indicators_cmd</code> <code>autoware_vehicle_msgs::msg::TurnIndicatorsCommand</code> Required to execute proper MRM (send turn signal cmd) <code>/system/emergency/hazard_lights_cmd</code> <code>autoware_vehicle_msgs::msg::HazardLightsCommand</code> Required to execute proper MRM (send hazard signal cmd) <code>/system/fail_safe/mrm_state</code> <code>autoware_adapi_v1_msgs::msg::MrmState</code> Inform MRM execution state and selected MRM behavior <code>/system/mrm/emergency_stop/operate</code> <code>tier4_system_msgs::srv::OperateMrm</code> Execution order for MRM emergency stop <code>/system/mrm/comfortable_stop/operate</code> <code>tier4_system_msgs::srv::OperateMrm</code> Execution order for MRM comfortable stop <code>/system/mrm/pull_over_manager/operate</code> <code>tier4_system_msgs::srv::OperateMrm</code> Execution order for MRM pull over"},{"location":"system/autoware_mrm_handler/#parameters","title":"Parameters","text":"Name Type Description Default Range update_rate integer Timer callback period. 10 N/A timeout_operation_mode_availability float If the input <code>operation_mode_availability</code> topic cannot be received for more than <code>timeout_operation_mode_availability</code>, vehicle will make an emergency stop. 0.5 N/A timeout_call_mrm_behavior float If a service is requested to the mrm operator and the response is not returned by <code>timeout_call_mrm_behavior</code>, the timeout occurs. 0.01 N/A timeout_cancel_mrm_behavior float If a service is requested to the mrm operator and the response is not returned by <code>timeout_cancel_mrm_behavior</code>, the timeout occurs. 0.01 N/A use_emergency_holding boolean If this parameter is true, the handler does not recover to the NORMAL state. False N/A timeout_emergency_recovery float If the duration of the EMERGENCY state is longer than <code>timeout_emergency_recovery</code>, it does not recover to the NORMAL state. 5.0 N/A use_parking_after_stopped boolean If this parameter is true, it will publish PARKING shift command. false N/A use_pull_over boolean If this parameter is true, operate pull over when latent faults occur. false N/A use_comfortable_stop boolean If this parameter is true, operate comfortable stop when latent faults occur. false N/A turning_hazard_on.emergency boolean If this parameter is true, hazard lamps will be turned on during emergency state. true N/A turning_indicator_on.emergency boolean If this parameter is true, turning indicators will be disabled during emergency state. true N/A"},{"location":"system/autoware_mrm_handler/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"system/autoware_pipeline_latency_monitor/","title":"pipeline_latency_monitor","text":""},{"location":"system/autoware_pipeline_latency_monitor/#pipeline_latency_monitor","title":"pipeline_latency_monitor","text":""},{"location":"system/autoware_pipeline_latency_monitor/#purpose","title":"Purpose","text":"<p>This package provides a node for calculating the latency introduced by a sequence of Autoware processing steps. For example, the pipeline from receiving sensor data up to the generation of the control command is made of several steps (e.g., perception, prediction, planning, control). This node calculates the total end-to-end latency by subscribing to messages that report the processing time of each step, combining them based on their timestamps, and publishing the total latency.</p>"},{"location":"system/autoware_pipeline_latency_monitor/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The node is configured with a sequence of processing steps. For each step, it subscribes to a topic that publishes its execution latency.</p> <p>To calculate the total latency, the node works backward from the last step in the defined sequence. It takes the most recent latency measurement from the final step. Then, for each preceding step, it finds a corresponding latency measurement from its history whose timestamp is consistent with the subsequent step. Specifically, it ensures that the end time of a given step occurs before the start time of the next step in the chain.</p> <p>The total latency is the sum of these chronologically-consistent individual latencies. The node also allows for adding fixed offset values to the final sum.</p> <p>The calculated total latency is published continuously and also used to report diagnostic information. An ERROR status is published if the latency exceeds a configurable threshold.</p>"},{"location":"system/autoware_pipeline_latency_monitor/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"system/autoware_pipeline_latency_monitor/#input","title":"Input","text":"<p>Subscribes to topics containing latency information for each processing step. The topic names and types are configured via parameters. Only 2 message types are currently supported.</p> Message Type Timestamp field Latency value field <code>autoware_internal_debug_msgs/msg/Float64Stamped</code> <code>stamp</code> <code>data</code> <code>autoware_planning_validator/msg/PlanningValidatorStatus</code> <code>stamp</code> <code>latency</code>"},{"location":"system/autoware_pipeline_latency_monitor/#output","title":"Output","text":"Name Type Description <code>~/output/total_latency_ms</code> <code>autoware_internal_debug_msgs/msg/Float64Stamped</code> The calculated total pipeline latency in milliseconds. <code>/diagnostics</code> <code>diagnostic_msgs/DiagnosticArray</code> Publishes the diagnostic status. Reports <code>OK</code> if the latency is within the threshold, and <code>ERROR</code> if it is exceeded. <code>~/debug/&lt;step_name&gt;_latency_ms</code> <code>autoware_internal_debug_msgs/msg/Float64Stamped</code> For each processing step, publishes the latest latency value received from its input topic. <code>~/debug/pipeline_total_latency_ms</code> <code>autoware_internal_debug_msgs/msg/Float64Stamped</code> A debug topic that also publishes the calculated total latency."},{"location":"system/autoware_pipeline_latency_monitor/#parameters","title":"Parameters","text":"Name Type Default Value Description <code>update_rate</code> <code>double</code> 10 The rate [Hz] at which the total latency is calculated and published. <code>latency_threshold_ms</code> <code>double</code> 1000 The latency threshold in milliseconds. If the total latency exceeds this value, an <code>ERROR</code> diagnostic is reported. <code>window_size</code> <code>int</code> 10 The number of historical latency messages to store for each processing step. <code>processing_steps.sequence</code> <code>string[]</code> <code>[]</code> An array of names defining the ordered sequence of processing steps to measure. <code>processing_steps.&lt;step_name&gt;.topic</code> <code>string</code> - The input topic that provides the latency for this step. <code>processing_steps.&lt;step_name&gt;.topic_type</code> <code>string</code> - The message type of the input topic. Supported types: <code>autoware_internal_debug_msgs/msg/Float64Stamped</code>, <code>autoware_planning_validator/msg/PlanningValidatorStatus</code>. <code>processing_steps.&lt;step_name&gt;.timestamp_meaning</code> <code>string</code> <code>\"end\"</code> Defines if the timestamp in the message represents the <code>start</code> or <code>end</code> of the processing interval for that step. <code>processing_steps.&lt;step_name&gt;.latency_multiplier</code> <code>double</code> <code>1.0</code> A multiplier to convert the received latency value into milliseconds (e.g., use 1000.0 if the input is in seconds). <code>latency_offsets_ms</code> <code>double[]</code> <code>[]</code> A list of fixed latency values (in milliseconds) to be added to the calculated total latency. Useful for accounting for steps that do not publish their own latency."},{"location":"system/autoware_pipeline_latency_monitor/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<ul> <li>The accuracy of the latency calculation depends on the timestamps of all input messages being synchronized to a common clock.</li> <li>The algorithm for combining latencies assumes a sequential, non-parallel processing pipeline.</li> <li>The node must be configured with the correct sequence of processing steps and their corresponding topics for the calculation to be meaningful.</li> </ul>"},{"location":"system/autoware_processing_time_checker/","title":"Processing Time Checker","text":""},{"location":"system/autoware_processing_time_checker/#processing-time-checker","title":"Processing Time Checker","text":""},{"location":"system/autoware_processing_time_checker/#purpose","title":"Purpose","text":"<p>This node checks whether the processing time of each module is valid or not, and send a diagnostic. NOTE: Currently, there is no validation feature, and \"OK\" is always assigned in the diagnostic.</p>"},{"location":"system/autoware_processing_time_checker/#standalone-startup","title":"Standalone Startup","text":"<pre><code>ros2 launch autoware_processing_time_checker processing_time_checker.launch.xml\n</code></pre>"},{"location":"system/autoware_processing_time_checker/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":""},{"location":"system/autoware_processing_time_checker/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"system/autoware_processing_time_checker/#input","title":"Input","text":"Name Type Description <code>/.../processing_time_ms</code> <code>autoware_internal_debug_msgs/Float64Stamped</code> processing time of each module"},{"location":"system/autoware_processing_time_checker/#output","title":"Output","text":"Name Type Description <code>/system/processing_time_checker/metrics</code> <code>tier4_metric_msgs::msg::MetricArray</code> processing time of all the modules"},{"location":"system/autoware_processing_time_checker/#parameters","title":"Parameters","text":"Name Type Description Default Range update_rate float The scanning and update frequency of the checker. 10 &gt;2 processing_time_topic_name_list array The topic name list of the processing time. [] N/A <p>If <code>output_metrics = true</code>, the node writes the statics of the processing_time measured during its lifetime to <code>&lt;ros2_logging_directory&gt;/autoware_metrics/&lt;node_name&gt;-&lt;time_stamp&gt;.json</code> when shut down.</p>"},{"location":"system/autoware_processing_time_checker/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"system/autoware_system_monitor/","title":"System Monitor for Autoware","text":""},{"location":"system/autoware_system_monitor/#system-monitor-for-autoware","title":"System Monitor for Autoware","text":"<p>Further improvement of system monitor functionality for Autoware.</p>"},{"location":"system/autoware_system_monitor/#description","title":"Description","text":"<p>This package provides the following nodes for monitoring system:</p> <ul> <li>CPU Monitor</li> <li>HDD Monitor</li> <li>Memory Monitor</li> <li>Network Monitor</li> <li>NTP Monitor</li> <li>Process Monitor</li> <li>GPU Monitor</li> <li>Voltage Monitor</li> </ul>"},{"location":"system/autoware_system_monitor/#supported-architecture","title":"Supported architecture","text":"<ul> <li>x86_64</li> <li>arm64v8/aarch64</li> </ul>"},{"location":"system/autoware_system_monitor/#operation-confirmed-platform","title":"Operation confirmed platform","text":"<ul> <li>PC system intel core i7</li> </ul> <p>The following \"once confirmed\" platforms (or their successors) need to be tested again with the latest environment.</p> <ul> <li>NVIDIA Jetson AGX Xavier</li> <li>Raspberry Pi4 Model B</li> </ul>"},{"location":"system/autoware_system_monitor/#how-to-use","title":"How to use","text":"<p>Use colcon build and launch in the same way as other packages.</p> <pre><code>colcon build\nsource install/setup.bash\nros2 launch autoware_system_monitor system_monitor.launch.xml\n</code></pre> <p>CPU and GPU monitoring method differs depending on platform. CMake automatically chooses source to be built according to build environment. If you build this package on intel platform, CPU monitor and GPU monitor which run on intel platform are built.</p>"},{"location":"system/autoware_system_monitor/#ros-topics-published-by-system-monitor","title":"ROS topics published by system monitor","text":"<p>Every topic is published in 1 minute interval.</p> <ul> <li>CPU Monitor</li> <li>HDD Monitor</li> <li>Mem Monitor</li> <li>Net Monitor</li> <li>NTP Monitor</li> <li>Process Monitor</li> <li>GPU Monitor</li> <li>Voltage Monitor</li> </ul> <p>[Usage] \u2713\uff1aSupported, -\uff1aNot supported</p> Node Message Intel arm64(tegra) arm64(raspi) Notes CPU Monitor CPU Temperature \u2713 \u2713 \u2713 CPU Usage \u2713 \u2713 \u2713 CPU Load Average \u2713 \u2713 \u2713 CPU Thermal Throttling \u2713 - \u2713 CPU Frequency \u2713 \u2713 \u2713 Notification of frequency only, normally error not generated. HDD Monitor HDD Temperature \u2713 \u2713 \u2713 HDD PowerOnHours \u2713 \u2713 \u2713 HDD TotalDataWritten \u2713 \u2713 \u2713 HDD RecoveredError \u2713 \u2713 \u2713 HDD Usage \u2713 \u2713 \u2713 HDD ReadDataRate \u2713 \u2713 \u2713 HDD WriteDataRate \u2713 \u2713 \u2713 HDD ReadIOPS \u2713 \u2713 \u2713 HDD WriteIOPS \u2713 \u2713 \u2713 HDD Connection \u2713 \u2713 \u2713 Memory Monitor Memory Usage \u2713 \u2713 \u2713 Net Monitor Network Connection \u2713 \u2713 \u2713 Network Usage \u2713 \u2713 \u2713 Notification of usage only, normally error not generated. Network CRC Error \u2713 \u2713 \u2713 Warning occurs when the number of CRC errors in the period reaches the threshold value. The number of CRC errors that occur is the same as the value that can be confirmed with the ip command. IP Packet Reassembles Failed \u2713 \u2713 \u2713 UDP Buf Errors \u2713 \u2713 \u2713 NTP Monitor NTP Offset \u2713 \u2713 \u2713 Process Monitor Tasks Summary \u2713 \u2713 \u2713 High-load Proc[0-9] \u2713 \u2713 \u2713 High-mem Proc[0-9] \u2713 \u2713 \u2713 GPU Monitor GPU Temperature \u2713 \u2713 - GPU Usage \u2713 \u2713 - GPU Memory Usage \u2713 - - GPU Thermal Throttling \u2713 - - GPU Frequency \u2713 \u2713 - For Intel platform, monitor whether current GPU clock is supported by the GPU. Voltage Monitor CMOS Battery Status \u2713 - - Battery Health for RTC and BIOS -"},{"location":"system/autoware_system_monitor/#ros-parameters","title":"ROS parameters","text":"<p>See ROS parameters.</p>"},{"location":"system/autoware_system_monitor/#notes","title":"Notes","text":""},{"location":"system/autoware_system_monitor/#cpu-monitor-for-intel-platform","title":"CPU monitor for intel platform","text":"<p>Thermal throttling event can be monitored by reading contents of MSR(Model Specific Register), and accessing MSR is only allowed for root by default, so this package provides the following approach to minimize security risks as much as possible:</p> <ul> <li>Provide a small program named 'msr_reader' which accesses MSR and sends thermal throttling status to CPU monitor by using socket programming.</li> <li>Run 'msr_reader' as a specific user instead of root.</li> <li>CPU monitor is able to know the status as an unprivileged user since thermal throttling status is sent by socket communication.</li> </ul>"},{"location":"system/autoware_system_monitor/#instructions-before-starting","title":"Instructions before starting","text":"<ol> <li> <p>Create a user to run 'msr_reader'.</p> <pre><code>sudo adduser &lt;username&gt;\n</code></pre> </li> <li> <p>Load kernel module 'msr' into your target system.    The path '/dev/cpu/CPUNUM/msr' appears.</p> <pre><code>sudo modprobe msr\n</code></pre> </li> <li> <p>Allow user to access MSR with read-only privilege using the Access Control List (ACL).</p> <pre><code>sudo setfacl -m u:&lt;username&gt;:r /dev/cpu/*/msr\n</code></pre> </li> <li> <p>Assign capability to 'msr_reader' since msr kernel module requires rawio capability.</p> <pre><code>sudo setcap cap_sys_rawio=ep install/system_monitor/lib/system_monitor/msr_reader\n</code></pre> </li> <li> <p>Run 'msr_reader' as the user you created, and run system_monitor as a generic user.</p> <pre><code>su &lt;username&gt;\ninstall/system_monitor/lib/system_monitor/msr_reader\n</code></pre> </li> </ol>"},{"location":"system/autoware_system_monitor/#see-also","title":"See also","text":"<p>msr_reader</p>"},{"location":"system/autoware_system_monitor/#hdd-monitor","title":"HDD Monitor","text":"<p>Generally, S.M.A.R.T. information is used to monitor HDD temperature and life of HDD, and normally accessing disk device node is allowed for root user or disk group. As with the CPU monitor, this package provides an approach to minimize security risks as much as possible:</p> <ul> <li>Provide a small program named 'hdd_reader' which accesses S.M.A.R.T. information and sends some items of it to HDD monitor by using socket programming.</li> <li>Run 'hdd_reader' as a specific user.</li> <li>HDD monitor is able to know some items of S.M.A.R.T. information as an unprivileged user since those are sent by socket communication.</li> </ul>"},{"location":"system/autoware_system_monitor/#instructions-before-starting_1","title":"Instructions before starting","text":"<ol> <li> <p>Create a user to run 'hdd_reader'.</p> <pre><code>sudo adduser &lt;username&gt;\n</code></pre> </li> <li> <p>Add user to the disk group.</p> <pre><code>sudo usermod -a -G disk &lt;username&gt;\n</code></pre> </li> <li> <p>Assign capabilities to 'hdd_reader' since SCSI kernel module requires rawio capability to send ATA PASS-THROUGH (12) command and NVMe kernel module requires admin capability to send Admin Command.</p> <pre><code>sudo setcap 'cap_sys_rawio=ep cap_sys_admin=ep' install/system_monitor/lib/system_monitor/hdd_reader\n</code></pre> </li> <li> <p>Run 'hdd_reader' as the user you created, and run system_monitor as a generic user.</p> <pre><code>su &lt;username&gt;\ninstall/system_monitor/lib/system_monitor/hdd_reader\n</code></pre> </li> </ol>"},{"location":"system/autoware_system_monitor/#see-also_1","title":"See also","text":"<p>hdd_reader</p>"},{"location":"system/autoware_system_monitor/#gpu-monitor-for-intel-platform","title":"GPU Monitor for intel platform","text":"<p>Currently GPU monitor for intel platform only supports NVIDIA GPU whose information can be accessed by NVML API.</p> <p>Also you need to install CUDA libraries. For installation instructions for CUDA 10.0, see NVIDIA CUDA Installation Guide for Linux.</p>"},{"location":"system/autoware_system_monitor/#voltage-monitor-for-cmos-battery","title":"Voltage monitor for CMOS Battery","text":"<p>Some platforms have built-in batteries for the RTC and CMOS. This node determines the battery status from the result of executing cat /proc/driver/rtc. Also, if lm-sensors is installed, it is possible to use the results. However, the return value of sensors varies depending on the chipset, so it is necessary to set a string to extract the corresponding voltage. It is also necessary to set the voltage for warning and error. For example, if you want a warning when the voltage is less than 2.9V and an error when it is less than 2.7V. The execution result of sensors on the chipset nct6106 is as follows, and \"in7:\" is the voltage of the CMOS battery.</p> <pre><code>$ sensors\npch_cannonlake-virtual-0\nAdapter: Virtual device\ntemp1:        +42.0\u00b0C\n\nnct6106-isa-0a10\nAdapter: ISA adapter\nin0:           728.00 mV (min =  +0.00 V, max =  +1.74 V)\nin1:             1.01 V  (min =  +0.00 V, max =  +2.04 V)\nin2:             3.34 V  (min =  +0.00 V, max =  +4.08 V)\nin3:             3.34 V  (min =  +0.00 V, max =  +4.08 V)\nin4:             1.07 V  (min =  +0.00 V, max =  +2.04 V)\nin5:             1.05 V  (min =  +0.00 V, max =  +2.04 V)\nin6:             1.67 V  (min =  +0.00 V, max =  +2.04 V)\nin7:             3.06 V  (min =  +0.00 V, max =  +4.08 V)\nin8:             2.10 V  (min =  +0.00 V, max =  +4.08 V)\nfan1:          2789 RPM  (min =    0 RPM)\nfan2:             0 RPM  (min =    0 RPM)\n</code></pre> <p>The setting value of voltage_monitor.param.yaml is as follows.</p> <pre><code>/**:\n  ros__parameters:\n    cmos_battery_warn: 2.90\n    cmos_battery_error: 2.70\n    cmos_battery_label: \"in7:\"\n</code></pre> <p>The above values of 2.7V and 2.90V are hypothetical. Depending on the motherboard and chipset, the value may vary. However, if the voltage of the lithium battery drops below 2.7V, it is recommended to replace it. In the above example, the message output to the topic /diagnostics is as follows. If the voltage &lt; 2.9V then:</p> <pre><code>  name: /autoware/system/resource_monitoring/voltage/cmos_battery\n  message: Warning\n  hardware_id: ''\n  values:\n  - key: 'voltage_monitor: CMOS Battery Status'\n    value: Low Battery\n</code></pre> <p>If the voltage &lt; 2.7V then:</p> <pre><code>  name: /autoware/system/resource_monitoring/voltage/cmos_battery\n  message: Warning\n  hardware_id: ''\n  values:\n  - key: 'voltage_monitor: CMOS Battery Status'\n    value: Battery Died\n</code></pre> <p>If neither, then:</p> <pre><code>  name: /autoware/system/resource_monitoring/voltage/cmos_battery\n  message: OK\n  hardware_id: ''\n  values:\n  - key: 'voltage_monitor: CMOS Battery Status'\n    value: OK\n</code></pre> <p>If the CMOS battery voltage drops less than voltage_error or voltage_warn,It will be a warning. If the battery runs out, the RTC will stop working when the power is turned off. However, since the vehicle can run, it is not an error. The vehicle will stop when an error occurs, but there is no need to stop immediately. It can be determined by the value of \"Low Battery\" or \"Battery Died\".</p>"},{"location":"system/autoware_system_monitor/#uml-diagrams","title":"UML diagrams","text":"<p>See Class diagrams. See Sequence diagrams.</p>"},{"location":"system/autoware_system_monitor/docs/class_diagrams/","title":"Class diagrams","text":""},{"location":"system/autoware_system_monitor/docs/class_diagrams/#class-diagrams","title":"Class diagrams","text":""},{"location":"system/autoware_system_monitor/docs/class_diagrams/#cpu-monitor","title":"CPU Monitor","text":""},{"location":"system/autoware_system_monitor/docs/class_diagrams/#hdd-monitor","title":"HDD Monitor","text":""},{"location":"system/autoware_system_monitor/docs/class_diagrams/#memory-monitor","title":"Memory Monitor","text":""},{"location":"system/autoware_system_monitor/docs/class_diagrams/#net-monitor","title":"Net Monitor","text":""},{"location":"system/autoware_system_monitor/docs/class_diagrams/#ntp-monitor","title":"NTP Monitor","text":""},{"location":"system/autoware_system_monitor/docs/class_diagrams/#process-monitor","title":"Process Monitor","text":""},{"location":"system/autoware_system_monitor/docs/class_diagrams/#gpu-monitor","title":"GPU Monitor","text":""},{"location":"system/autoware_system_monitor/docs/hdd_reader/","title":"hdd_reader","text":""},{"location":"system/autoware_system_monitor/docs/hdd_reader/#hdd_reader","title":"hdd_reader","text":""},{"location":"system/autoware_system_monitor/docs/hdd_reader/#name","title":"Name","text":"<p>hdd_reader - Read S.M.A.R.T. information for monitoring HDD temperature and life of HDD</p>"},{"location":"system/autoware_system_monitor/docs/hdd_reader/#synopsis","title":"Synopsis","text":"<p>hdd_reader [OPTION]</p>"},{"location":"system/autoware_system_monitor/docs/hdd_reader/#description","title":"Description","text":"<p>Read S.M.A.R.T. information for monitoring HDD temperature and life of HDD. This runs as a daemon process and listens to a UNIX domain socket (\"/tmp/hdd_reader.sock\" by default).</p> <p>Options: -h, --help \u00a0\u00a0\u00a0\u00a0Display help -s, --socket PATH \u00a0\u00a0\u00a0\u00a0UNIX domain socket path</p> <p>Exit status: Returns 0 if OK; non-zero otherwise.</p>"},{"location":"system/autoware_system_monitor/docs/hdd_reader/#notes","title":"Notes","text":"<p>The 'hdd_reader' accesses minimal data enough to get Model number, Serial number, HDD temperature, and life of HDD. This is an approach to limit its functionality, however, the functionality can be expanded for further improvements and considerations in the future.</p>"},{"location":"system/autoware_system_monitor/docs/hdd_reader/#ata","title":"[ATA]","text":"Purpose Name Length Model number, Serial number IDENTIFY DEVICE data 256 words(512 bytes) HDD temperature, life of HDD SMART READ DATA 256 words(512 bytes) <p>For details please see the documents below.</p> <ul> <li>ATA Command Set - 4 (ACS-4)</li> <li>ATA/ATAPI Command Set - 3 (ACS-3)</li> <li>SMART Attribute Overview</li> <li>SMART Attribute Annex</li> </ul>"},{"location":"system/autoware_system_monitor/docs/hdd_reader/#nvme","title":"[NVMe]","text":"Purpose Name Length Model number, Serial number Identify Controller data structure 4096 bytes HDD temperature, life of HDD SMART / Health Information 36 Dword(144 bytes) <p>For details please see the documents below.</p> <ul> <li>NVM Express 1.2b</li> </ul>"},{"location":"system/autoware_system_monitor/docs/hdd_reader/#operation-confirmed-drives","title":"Operation confirmed drives","text":"<ul> <li>SAMSUNG MZVLB1T0HALR (SSD)</li> <li>Western Digital My Passport (Portable HDD)</li> </ul>"},{"location":"system/autoware_system_monitor/docs/msr_reader/","title":"msr_reader","text":""},{"location":"system/autoware_system_monitor/docs/msr_reader/#msr_reader","title":"msr_reader","text":""},{"location":"system/autoware_system_monitor/docs/msr_reader/#name","title":"Name","text":"<p>msr_reader - Read MSR register for monitoring thermal throttling event</p>"},{"location":"system/autoware_system_monitor/docs/msr_reader/#synopsis","title":"Synopsis","text":"<p>msr_reader [OPTION]</p>"},{"location":"system/autoware_system_monitor/docs/msr_reader/#description","title":"Description","text":"<p>Read MSR register for monitoring thermal throttling event. This runs as a daemon process and listens to a UNIX domain socket (\"/tmp/msr_reader.sock\" by default).</p> <p>Options: -h, --help \u00a0\u00a0\u00a0\u00a0Display help -s, --socket PATH \u00a0\u00a0\u00a0\u00a0UNIX domain socket path</p> <p>Exit status: Returns 0 if OK; non-zero otherwise.</p>"},{"location":"system/autoware_system_monitor/docs/msr_reader/#notes","title":"Notes","text":"<p>The 'msr_reader' accesses minimal data enough to get thermal throttling event. This is an approach to limit its functionality, however, the functionality can be expanded for further improvements and considerations in the future.</p> Register Address Name Length 1B1H IA32_PACKAGE_THERM_STATUS 64bit <p>For details please see the documents below.</p> <ul> <li>Intel\u00ae 64 and IA-32 ArchitecturesSoftware Developer\u2019s Manual</li> </ul>"},{"location":"system/autoware_system_monitor/docs/msr_reader/#operation-confirmed-platform","title":"Operation confirmed platform","text":"<ul> <li>PC system intel core i7</li> </ul>"},{"location":"system/autoware_system_monitor/docs/ros_parameters/","title":"ROS parameters","text":""},{"location":"system/autoware_system_monitor/docs/ros_parameters/#ros-parameters","title":"ROS parameters","text":""},{"location":"system/autoware_system_monitor/docs/ros_parameters/#cpu-monitor","title":"CPU Monitor","text":"<p>cpu_monitor:</p> Name Type Unit Default Notes temp_warn float DegC 90.0 Generates warning when CPU temperature reaches a specified value or higher. temp_error float DegC 95.0 Generates error when CPU temperature reaches a specified value or higher. usage_warn float %(1e-2) 0.90 Generates warning when CPU usage reaches a specified value or higher and last for usage_warn_count counts. usage_error float %(1e-2) 1.00 Generates error when CPU usage reaches a specified value or higher and last for usage_error_count counts. usage_warn_count int n/a 2 Generates warning when CPU usage reaches usage_warn value or higher and last for a specified counts. usage_error_count int n/a 2 Generates error when CPU usage reaches usage_error value or higher and last for a specified counts. load1_warn float %(1e-2) 0.90 Generates warning when load average 1min reaches a specified value or higher. load5_warn float %(1e-2) 0.80 Generates warning when load average 5min reaches a specified value or higher. msr_reader_socket_path string n/a /tmp/msr_reader.sock UNIX domain socket path to connect to msr_reader."},{"location":"system/autoware_system_monitor/docs/ros_parameters/#hdd-monitor","title":"HDD Monitor","text":"<p>hdd_monitor:</p> <p>\u00a0\u00a0disks:</p> Name Type Unit Default Notes name string n/a none The disk name to monitor temperature. (e.g. /dev/sda) temp_attribute_id int n/a 0xC2 S.M.A.R.T attribute ID of temperature. temp_warn float DegC 55.0 Generates warning when HDD temperature reaches a specified value or higher. temp_error float DegC 70.0 Generates error when HDD temperature reaches a specified value or higher. power_on_hours_attribute_id int n/a 0x09 S.M.A.R.T attribute ID of power-on hours. power_on_hours_warn int Hour 3000000 Generates warning when HDD power-on hours reaches a specified value or higher. total_data_written_attribute_id int n/a 0xF1 S.M.A.R.T attribute ID of total data written. total_data_written_warn int depends on device 4915200 Generates warning when HDD total data written reaches a specified value or higher. total_data_written_safety_factor int %(1e-2) 0.05 Safety factor of HDD total data written. recovered_error_attribute_id int n/a 0xC3 S.M.A.R.T attribute ID of recovered error. recovered_error_warn int n/a 1 Generates warning when HDD recovered error reaches a specified value or higher. read_data_rate_warn float MB/s 360.0 Generates warning when HDD read data rate reaches a specified value or higher. write_data_rate_warn float MB/s 103.5 Generates warning when HDD write data rate reaches a specified value or higher. read_iops_warn float IOPS 63360.0 Generates warning when HDD read IOPS reaches a specified value or higher. write_iops_warn float IOPS 24120.0 Generates warning when HDD write IOPS reaches a specified value or higher. <p>hdd_monitor:</p> Name Type Unit Default Notes hdd_reader_socket_path string n/a /tmp/hdd_reader.sock UNIX domain socket path to connect to hdd_reader. usage_warn float %(1e-2) 0.95 Generates warning when disk usage reaches a specified value or higher. usage_error float %(1e-2) 0.99 Generates error when disk usage reaches a specified value or higher."},{"location":"system/autoware_system_monitor/docs/ros_parameters/#memory-monitor","title":"Memory Monitor","text":"<p>mem_monitor:</p> Name Type Unit Default Notes usage_warn float %(1e-2) 0.95 Generates warning when physical memory usage reaches a specified value or higher. usage_error float %(1e-2) 0.99 Generates error when physical memory usage reaches a specified value or higher."},{"location":"system/autoware_system_monitor/docs/ros_parameters/#net-monitor","title":"Net Monitor","text":"<p>net_monitor:</p> Name Type Unit Default Notes devices list[string] n/a none The name of network interface to monitor. (e.g. eth0, * for all network interfaces) monitor_program string n/a greengrass program name to be monitored by nethogs name. crc_error_check_duration int sec 1 CRC error check duration. crc_error_count_threshold int n/a 1 Generates warning when count of CRC errors during CRC error check duration reaches a specified value or higher. reassembles_failed_check_duration int sec 1 IP packet reassembles failed check duration. reassembles_failed_check_count int n/a 1 Generates warning when count of IP packet reassembles failed during IP packet reassembles failed check duration reaches a specified value or higher. udp_buf_errors_check_duration int sec 1 UDP buf errors check duration. udp_buf_errors_check_count int n/a 1 Generates warning when count of UDP buf errors during udp_buf_errors_check_duration reaches a specified value or higher."},{"location":"system/autoware_system_monitor/docs/ros_parameters/#ntp-monitor","title":"NTP Monitor","text":"<p>ntp_monitor:</p> Name Type Unit Default Notes server string n/a ntp.ubuntu.com The name of NTP server to synchronize date and time. (e.g. ntp.nict.jp for Japan) offset_warn float sec 0.1 Generates warning when NTP offset reaches a specified value or higher. (default is 100ms) offset_error float sec 5.0 Generates warning when NTP offset reaches a specified value or higher. (default is 5sec)"},{"location":"system/autoware_system_monitor/docs/ros_parameters/#process-monitor","title":"Process Monitor","text":"<p>process_monitor:</p> Name Type Unit Default Notes num_of_procs int n/a 5 The number of processes to generate High-load Proc[0-9] and High-mem Proc[0-9]."},{"location":"system/autoware_system_monitor/docs/ros_parameters/#gpu-monitor","title":"GPU Monitor","text":"<p>gpu_monitor:</p> Name Type Unit Default Notes temp_warn float DegC 90.0 Generates warning when GPU temperature reaches a specified value or higher. temp_error float DegC 95.0 Generates error when GPU temperature reaches a specified value or higher. gpu_usage_warn float %(1e-2) 0.90 Generates warning when GPU usage reaches a specified value or higher. gpu_usage_error float %(1e-2) 1.00 Generates error when GPU usage reaches a specified value or higher. memory_usage_warn float %(1e-2) 0.90 Generates warning when GPU memory usage reaches a specified value or higher. memory_usage_error float %(1e-2) 1.00 Generates error when GPU memory usage reaches a specified value or higher."},{"location":"system/autoware_system_monitor/docs/ros_parameters/#voltage-monitor","title":"Voltage Monitor","text":"<p>voltage_monitor:</p> Name Type Unit Default Notes cmos_battery_warn float volt 2.9 Generates warning when voltage of CMOS Battery is lower. cmos_battery_error float volt 2.7 Generates error when voltage of CMOS Battery is lower. cmos_battery_label string n/a \"\" voltage string in sensors command outputs. if empty no voltage will be checked."},{"location":"system/autoware_system_monitor/docs/seq_diagrams/","title":"Sequence diagrams","text":""},{"location":"system/autoware_system_monitor/docs/seq_diagrams/#sequence-diagrams","title":"Sequence diagrams","text":""},{"location":"system/autoware_system_monitor/docs/seq_diagrams/#cpu-monitor","title":"CPU Monitor","text":""},{"location":"system/autoware_system_monitor/docs/seq_diagrams/#hdd-monitor","title":"HDD Monitor","text":""},{"location":"system/autoware_system_monitor/docs/seq_diagrams/#memory-monitor","title":"Memory Monitor","text":""},{"location":"system/autoware_system_monitor/docs/seq_diagrams/#net-monitor","title":"Net Monitor","text":""},{"location":"system/autoware_system_monitor/docs/seq_diagrams/#ntp-monitor","title":"NTP Monitor","text":""},{"location":"system/autoware_system_monitor/docs/seq_diagrams/#process-monitor","title":"Process Monitor","text":""},{"location":"system/autoware_system_monitor/docs/seq_diagrams/#gpu-monitor","title":"GPU Monitor","text":""},{"location":"system/autoware_system_monitor/docs/topics_cpu_monitor/","title":"ROS topics: CPU Monitor","text":""},{"location":"system/autoware_system_monitor/docs/topics_cpu_monitor/#ros-topics-cpu-monitor","title":"ROS topics: CPU Monitor","text":""},{"location":"system/autoware_system_monitor/docs/topics_cpu_monitor/#cpu-temperature","title":"CPU Temperature","text":"<p>/diagnostics/cpu_monitor: CPU Temperature</p> <p>[summary]</p> level message OK OK <p>[values]</p> key (example) value (example) Package id 0, Core [0-9], thermal_zone[0-9] 50.0 DegC <p>*key: thermal_zone[0-9] for ARM architecture.</p>"},{"location":"system/autoware_system_monitor/docs/topics_cpu_monitor/#cpu-usage","title":"CPU Usage","text":"<p>/diagnostics/cpu_monitor: CPU Usage</p> <p>[summary]</p> level message OK OK WARN high load ERROR very high load <p>[values]</p> key value (example) CPU [all,0-9]: status OK / high load / very high load CPU [all,0-9]: usr 2.00% CPU [all,0-9]: nice 0.00% CPU [all,0-9]: sys 1.00% CPU [all,0-9]: idle 97.00%"},{"location":"system/autoware_system_monitor/docs/topics_cpu_monitor/#cpu-load-average","title":"CPU Load Average","text":"<p>/diagnostics/cpu_monitor: CPU Load Average</p> <p>[summary]</p> level message OK OK WARN high load <p>[values]</p> key value (example) 1min 14.50% 5min 14.55% 15min 9.67%"},{"location":"system/autoware_system_monitor/docs/topics_cpu_monitor/#cpu-thermal-throttling","title":"CPU Thermal Throttling","text":"<p>Intel and raspi platform only. Tegra platform not supported.</p> <p>/diagnostics/cpu_monitor: CPU Thermal Throttling</p> <p>[summary]</p> level message OK OK ERROR throttling <p>[values for intel platform]</p> key value (example) CPU [0-9]: Pkg Thermal Status OK / throttling <p>[values for raspi platform]</p> key value (example) status All clear / Currently throttled / Soft temperature limit active"},{"location":"system/autoware_system_monitor/docs/topics_cpu_monitor/#cpu-frequency","title":"CPU Frequency","text":"<p>/diagnostics/cpu_monitor: CPU Frequency</p> <p>[summary]</p> level message OK OK <p>[values]</p> key value (example) CPU [0-9]: clock 2879MHz"},{"location":"system/autoware_system_monitor/docs/topics_gpu_monitor/","title":"ROS topics: GPU Monitor","text":""},{"location":"system/autoware_system_monitor/docs/topics_gpu_monitor/#ros-topics-gpu-monitor","title":"ROS topics: GPU Monitor","text":"<p>Intel and tegra platform only. Raspi platform not supported.</p>"},{"location":"system/autoware_system_monitor/docs/topics_gpu_monitor/#gpu-temperature","title":"GPU Temperature","text":"<p>/diagnostics/gpu_monitor: GPU Temperature</p> <p>[summary]</p> level message OK OK WARN warm ERROR hot <p>[values]</p> key (example) value (example) GeForce GTX 1650, thermal_zone[0-9] 46.0 DegC <p>*key: thermal_zone[0-9] for ARM architecture.</p>"},{"location":"system/autoware_system_monitor/docs/topics_gpu_monitor/#gpu-usage","title":"GPU Usage","text":"<p>/diagnostics/gpu_monitor: GPU Usage</p> <p>[summary]</p> level message OK OK WARN high load ERROR very high load <p>[values]</p> key value (example) GPU [0-9]: status OK / high load / very high load GPU [0-9]: name GeForce GTX 1650, gpu.[0-9] GPU [0-9]: usage 19.0% <p>*key: gpu.[0-9] for ARM architecture.</p>"},{"location":"system/autoware_system_monitor/docs/topics_gpu_monitor/#gpu-memory-usage","title":"GPU Memory Usage","text":"<p>Intel platform only. There is no separate gpu memory in tegra. Both cpu and gpu uses cpu memory.</p> <p>/diagnostics/gpu_monitor: GPU Memory Usage</p> <p>[summary]</p> level message OK OK WARN high load ERROR very high load <p>[values]</p> key value (example) GPU [0-9]: status OK / high load / very high load GPU [0-9]: name GeForce GTX 1650 GPU [0-9]: usage 13.0% GPU [0-9]: total 3G GPU [0-9]: used 1G GPU [0-9]: free 2G"},{"location":"system/autoware_system_monitor/docs/topics_gpu_monitor/#gpu-thermal-throttling","title":"GPU Thermal Throttling","text":"<p>Intel platform only. Tegra platform not supported.</p> <p>/diagnostics/gpu_monitor: GPU Thermal Throttling</p> <p>[summary]</p> level message OK OK ERROR throttling <p>[values]</p> key value (example) GPU [0-9]: status OK / throttling GPU [0-9]: name GeForce GTX 1650 GPU [0-9]: graphics clock 1020 MHz GPU [0-9]: reasons GpuIdle / SwThermalSlowdown etc."},{"location":"system/autoware_system_monitor/docs/topics_gpu_monitor/#gpu-frequency","title":"GPU Frequency","text":"<p>/diagnostics/gpu_monitor: GPU Frequency</p>"},{"location":"system/autoware_system_monitor/docs/topics_gpu_monitor/#intel-platform","title":"Intel platform","text":"<p>[summary]</p> level message OK OK WARN unsupported clock <p>[values]</p> key value (example) GPU [0-9]: status OK / unsupported clock GPU [0-9]: name GeForce GTX 1650 GPU [0-9]: graphics clock 1020 MHz"},{"location":"system/autoware_system_monitor/docs/topics_gpu_monitor/#tegra-platform","title":"Tegra platform","text":"<p>[summary]</p> level message OK OK <p>[values]</p> key (example) value (example) GPU 17000000.gv11b: clock 318 MHz"},{"location":"system/autoware_system_monitor/docs/topics_hdd_monitor/","title":"ROS topics: HDD Monitor","text":""},{"location":"system/autoware_system_monitor/docs/topics_hdd_monitor/#ros-topics-hdd-monitor","title":"ROS topics: HDD Monitor","text":""},{"location":"system/autoware_system_monitor/docs/topics_hdd_monitor/#hdd-temperature","title":"HDD Temperature","text":"<p>/diagnostics/hdd_monitor: HDD Temperature</p> <p>[summary]</p> level message OK OK WARN hot ERROR critical hot <p>[values]</p> key value (example) HDD [0-9]: status OK / hot / critical hot HDD [0-9]: name /dev/nvme0 HDD [0-9]: model SAMSUNG MZVLB1T0HBLR-000L7 HDD [0-9]: serial S4EMNF0M820682 HDD [0-9]: temperature 37.0 DegC  not available"},{"location":"system/autoware_system_monitor/docs/topics_hdd_monitor/#hdd-poweronhours","title":"HDD PowerOnHours","text":"<p>/diagnostics/hdd_monitor: HDD PowerOnHours</p> <p>[summary]</p> level message OK OK WARN lifetime limit <p>[values]</p> key value (example) HDD [0-9]: status OK / lifetime limit HDD [0-9]: name /dev/nvme0 HDD [0-9]: model PHISON PS5012-E12S-512G HDD [0-9]: serial FB590709182505050767 HDD [0-9]: power on hours 4834 Hours  not available"},{"location":"system/autoware_system_monitor/docs/topics_hdd_monitor/#hdd-totaldatawritten","title":"HDD TotalDataWritten","text":"<p>/diagnostics/hdd_monitor: HDD TotalDataWritten</p> <p>[summary]</p> level message OK OK WARN warranty period <p>[values]</p> key value (example) HDD [0-9]: status OK / warranty period HDD [0-9]: name /dev/nvme0 HDD [0-9]: model PHISON PS5012-E12S-512G HDD [0-9]: serial FB590709182505050767 HDD [0-9]: total data written 146295330  not available"},{"location":"system/autoware_system_monitor/docs/topics_hdd_monitor/#hdd-recoverederror","title":"HDD RecoveredError","text":"<p>/diagnostics/hdd_monitor: HDD RecoveredError</p> <p>[summary]</p> level message OK OK WARN high soft error rate <p>[values]</p> key value (example) HDD [0-9]: status OK / high soft error rate HDD [0-9]: name /dev/nvme0 HDD [0-9]: model PHISON PS5012-E12S-512G HDD [0-9]: serial FB590709182505050767 HDD [0-9]: recovered error 0  not available"},{"location":"system/autoware_system_monitor/docs/topics_hdd_monitor/#hdd-usage","title":"HDD Usage","text":"<p>/diagnostics/hdd_monitor: HDD Usage</p> <p>[summary]</p> level message OK OK WARN low disk space ERROR very low disk space <p>[values]</p> key value (example) HDD [0-9]: status OK / low disk space / very low disk space HDD [0-9]: filesystem /dev/nvme0n1p4 HDD [0-9]: size 264G HDD [0-9]: used 172G HDD [0-9]: avail 749G HDD [0-9]: use 69% HDD [0-9]: mounted on /"},{"location":"system/autoware_system_monitor/docs/topics_hdd_monitor/#hdd-readdatarate","title":"HDD ReadDataRate","text":"<p>/diagnostics/hdd_monitor: HDD ReadDataRate</p> <p>[summary]</p> level message OK OK WARN high data rate of read <p>[values]</p> key value (example) HDD [0-9]: status OK / high data rate of read HDD [0-9]: name /dev/nvme0 HDD [0-9]: data rate of read 0.00 MB/s"},{"location":"system/autoware_system_monitor/docs/topics_hdd_monitor/#hdd-writedatarate","title":"HDD WriteDataRate","text":"<p>/diagnostics/hdd_monitor: HDD WriteDataRate</p> <p>[summary]</p> level message OK OK WARN high data rate of write <p>[values]</p> key value (example) HDD [0-9]: status OK / high data rate of write HDD [0-9]: name /dev/nvme0 HDD [0-9]: data rate of write 0.00 MB/s"},{"location":"system/autoware_system_monitor/docs/topics_hdd_monitor/#hdd-readiops","title":"HDD ReadIOPS","text":"<p>/diagnostics/hdd_monitor: HDD ReadIOPS</p> <p>[summary]</p> level message OK OK WARN high IOPS of read <p>[values]</p> key value (example) HDD [0-9]: status OK / high IOPS of read HDD [0-9]: name /dev/nvme0 HDD [0-9]: IOPS of read 0.00 IOPS"},{"location":"system/autoware_system_monitor/docs/topics_hdd_monitor/#hdd-writeiops","title":"HDD WriteIOPS","text":"<p>/diagnostics/hdd_monitor: HDD WriteIOPS</p> <p>[summary]</p> level message OK OK WARN high IOPS of write <p>[values]</p> key value (example) HDD [0-9]: status OK / high IOPS of write HDD [0-9]: name /dev/nvme0 HDD [0-9]: IOPS of write 0.00 IOPS"},{"location":"system/autoware_system_monitor/docs/topics_hdd_monitor/#hdd-connection","title":"HDD Connection","text":"<p>/diagnostics/hdd_monitor: HDD Connection</p> <p>[summary]</p> level message OK OK WARN not connected <p>[values]</p> key value (example) HDD [0-9]: status OK / not connected HDD [0-9]: name /dev/nvme0 HDD [0-9]: mount point /"},{"location":"system/autoware_system_monitor/docs/topics_mem_monitor/","title":"ROS topics: Memory Monitor","text":""},{"location":"system/autoware_system_monitor/docs/topics_mem_monitor/#ros-topics-memory-monitor","title":"ROS topics: Memory Monitor","text":""},{"location":"system/autoware_system_monitor/docs/topics_mem_monitor/#memory-usage","title":"Memory Usage","text":"<p>/diagnostics/mem_monitor: Memory Usage</p> <p>[summary]</p> level message OK OK WARN high load ERROR very high load <p>[values]</p> key value (example) Mem: usage 29.72% Mem: total 31.2G Mem: used 6.0G Mem: free 20.7G Mem: shared 2.9G Mem: buff/cache 4.5G Mem: available 21.9G Swap: total 2.0G Swap: used 218M Swap: free 1.8G Total: total 33.2G Total: used 6.2G Total: free 22.5G Total: used+ 9.1G"},{"location":"system/autoware_system_monitor/docs/topics_net_monitor/","title":"ROS topics: Net Monitor","text":""},{"location":"system/autoware_system_monitor/docs/topics_net_monitor/#ros-topics-net-monitor","title":"ROS topics: Net Monitor","text":""},{"location":"system/autoware_system_monitor/docs/topics_net_monitor/#network-connection","title":"Network Connection","text":"<p>/diagnostics/net_monitor: Network Connection</p> <p>[summary]</p> level message OK OK WARN no such device <p>[values]</p> key value (example) Network [0-9]: status OK / no such device HDD [0-9]: name wlp82s0"},{"location":"system/autoware_system_monitor/docs/topics_net_monitor/#network-usage","title":"Network Usage","text":"<p>/diagnostics/net_monitor: Network Usage</p> <p>[summary]</p> level message OK OK <p>[values]</p> key value (example) Network [0-9]: status OK Network [0-9]: interface name wlp82s0 Network [0-9]: rx_usage 0.00% Network [0-9]: tx_usage 0.00% Network [0-9]: rx_traffic 0.00 MB/s Network [0-9]: tx_traffic 0.00 MB/s Network [0-9]: capacity 400.0 MB/s Network [0-9]: mtu 1500 Network [0-9]: rx_bytes 58455228 Network [0-9]: rx_errors 0 Network [0-9]: tx_bytes 11069136 Network [0-9]: tx_errors 0 Network [0-9]: collisions 0"},{"location":"system/autoware_system_monitor/docs/topics_net_monitor/#network-traffic","title":"Network Traffic","text":"<p>/diagnostics/net_monitor: Network Traffic</p> <p>[summary]</p> level message OK OK <p>[values when specified program is detected]</p> key value (example) nethogs [0-9]: program /lambda/greengrassSystemComponents/1384/999 nethogs [0-9]: sent (KB/Sec) 1.13574 nethogs [0-9]: received (KB/Sec) 0.261914 <p>[values when error is occurring]</p> key value (example) error execve failed: No such file or directory"},{"location":"system/autoware_system_monitor/docs/topics_net_monitor/#network-crc-error","title":"Network CRC Error","text":"<p>/diagnostics/net_monitor: Network CRC Error</p> <p>[summary]</p> level message OK OK WARN CRC error <p>[values]</p> key value (example) Network [0-9]: interface name wlp82s0 Network [0-9]: total rx_crc_errors 0 Network [0-9]: rx_crc_errors per unit time 0"},{"location":"system/autoware_system_monitor/docs/topics_net_monitor/#ip-packet-reassembles-failed","title":"IP Packet Reassembles Failed","text":"<p>/diagnostics/net_monitor: IP Packet Reassembles Failed</p> <p>[summary]</p> level message OK OK WARN reassembles failed <p>[values]</p> key value (example) total packet reassembles failed 0 packet reassembles failed per unit time 0"},{"location":"system/autoware_system_monitor/docs/topics_net_monitor/#udp-buf-errors","title":"UDP Buf Errors","text":"<p>/diagnostics/net_monitor: UDP Buf Errors</p> <p>[summary]</p> level message OK OK WARN UDP buf errors <p>[values]</p> key value (example) total UDP rcv buf errors 0 UDP rcv buf errors per unit time 0 total UDP snd buf errors 0 UDP snd buf errors per unit time 0"},{"location":"system/autoware_system_monitor/docs/topics_ntp_monitor/","title":"ROS topics: NTP Monitor","text":""},{"location":"system/autoware_system_monitor/docs/topics_ntp_monitor/#ros-topics-ntp-monitor","title":"ROS topics: NTP Monitor","text":""},{"location":"system/autoware_system_monitor/docs/topics_ntp_monitor/#ntp-offset","title":"NTP Offset","text":"<p>/diagnostics/ntp_monitor: NTP Offset</p> <p>[summary]</p> level message OK OK WARN high ERROR too high <p>[values]</p> key value (example) NTP Offset -0.013181 sec NTP Delay 0.053880 sec"},{"location":"system/autoware_system_monitor/docs/topics_process_monitor/","title":"ROS topics: Process Monitor","text":""},{"location":"system/autoware_system_monitor/docs/topics_process_monitor/#ros-topics-process-monitor","title":"ROS topics: Process Monitor","text":""},{"location":"system/autoware_system_monitor/docs/topics_process_monitor/#tasks-summary","title":"Tasks Summary","text":"<p>/diagnostics/process_monitor: Tasks Summary</p> <p>[summary]</p> level message OK OK <p>[values]</p> key value (example) total 409 running 2 sleeping 321 stopped 0 zombie 0"},{"location":"system/autoware_system_monitor/docs/topics_process_monitor/#high-load-proc0-9","title":"High-load Proc[0-9]","text":"<p>/diagnostics/process_monitor: High-load Proc[0-9]</p> <p>[summary]</p> level message OK OK <p>[values]</p> key value (example) COMMAND /usr/lib/firefox/firefox %CPU 37.5 %MEM 2.1 PID 14062 USER autoware PR 20 NI 0 VIRT 3461152 RES 669052 SHR 481208 S S TIME+ 23:57.49"},{"location":"system/autoware_system_monitor/docs/topics_process_monitor/#high-mem-proc0-9","title":"High-mem Proc[0-9]","text":"<p>/diagnostics/process_monitor: High-mem Proc[0-9]</p> <p>[summary]</p> level message OK OK <p>[values]</p> key value (example) COMMAND /snap/multipass/1784/usr/bin/qemu-system-x86_64 %CPU 0 %MEM 2.5 PID 1565 USER root PR 20 NI 0 VIRT 3722320 RES 812432 SHR 20340 S S TIME+ 0:22.84"},{"location":"system/autoware_system_monitor/docs/topics_voltage_monitor/","title":"ROS topics: Voltage Monitor","text":""},{"location":"system/autoware_system_monitor/docs/topics_voltage_monitor/#ros-topics-voltage-monitor","title":"ROS topics: Voltage Monitor","text":"<p>\"CMOS Battery Status\" and \"CMOS battery voltage\" are exclusive. Only one or the other is generated. Which one is generated depends on the value of cmos_battery_label.</p>"},{"location":"system/autoware_system_monitor/docs/topics_voltage_monitor/#cmos-battery-status","title":"CMOS Battery Status","text":"<p>/diagnostics/voltage_monitor: CMOS Battery Status</p> <p>[summary]</p> level message OK OK WARN Battery Dead <p>[values]</p> key (example) value (example) CMOS battery status OK / Battery Dead <p>*key: thermal_zone[0-9] for ARM architecture.</p>"},{"location":"system/autoware_system_monitor/docs/topics_voltage_monitor/#cmos-battery-voltage","title":"CMOS Battery Voltage","text":"<p>/diagnostics/voltage_monitor: CMOS battery voltage</p> <p>[summary]</p> level message OK OK WARN Low Battery WARN Battery Died <p>[values]</p> key value (example) CMOS battery voltage 3.06"},{"location":"system/autoware_system_monitor/docs/traffic_reader/","title":"traffic_reader","text":""},{"location":"system/autoware_system_monitor/docs/traffic_reader/#traffic_reader","title":"traffic_reader","text":""},{"location":"system/autoware_system_monitor/docs/traffic_reader/#name","title":"Name","text":"<p>traffic_reader - monitoring network traffic by process</p>"},{"location":"system/autoware_system_monitor/docs/traffic_reader/#synopsis","title":"Synopsis","text":"<p>traffic_reader [OPTION]</p>"},{"location":"system/autoware_system_monitor/docs/traffic_reader/#description","title":"Description","text":"<p>Monitoring network traffic by process. This runs as a daemon process and listens to a UNIX domain socket (\"/tmp/traffic_reader\" by default).</p> <p>Options: -h, --help \u00a0\u00a0\u00a0\u00a0Display help -s, --socket PATH \u00a0\u00a0\u00a0\u00a0UNIX domain socket path</p> <p>Exit status: Returns 0 if OK; non-zero otherwise.</p>"},{"location":"system/autoware_system_monitor/docs/traffic_reader/#notes","title":"Notes","text":"<p>The 'traffic_reader' requires nethogs command.</p>"},{"location":"system/autoware_system_monitor/docs/traffic_reader/#operation-confirmed-platform","title":"Operation confirmed platform","text":"<ul> <li>Ubuntu 20.04.3 LTS (GNU/Linux 5.11.0-40-generic x86_64)</li> </ul>"},{"location":"system/autoware_topic_relay_controller/","title":"topic_relay_controller","text":""},{"location":"system/autoware_topic_relay_controller/#topic_relay_controller","title":"topic_relay_controller","text":""},{"location":"system/autoware_topic_relay_controller/#purpose","title":"Purpose","text":"<p>The node subscribes to a specified topic, remaps it, and republishes it. Additionally, it has the capability to continue publishing the last received value if the subscription stops.</p>"},{"location":"system/autoware_topic_relay_controller/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"system/autoware_topic_relay_controller/#input","title":"Input","text":"Name Type Description <code>/&lt;topic&gt;</code> <code>&lt;specified message type&gt;</code> Topic to be subscribed, as defined by the <code>topic</code> parameter. <code>/tf</code> <code>tf2_msgs::msg::TFMessage</code> (Optional) If the topic is <code>/tf</code>, used for transform message relay. <code>/tf_static</code> <code>tf2_msgs::msg::TFMessage</code> (Optional) If the topic is <code>/tf_static</code>, used for static transforms."},{"location":"system/autoware_topic_relay_controller/#output","title":"Output","text":"Name Type Description <code>/&lt;remap_topic&gt;</code> <code>&lt;specified message type&gt;</code> Republished topic after remapping, as defined by the <code>remap_topic</code> parameter."},{"location":"system/autoware_topic_relay_controller/#parameters","title":"Parameters","text":"Variable Type Description topic string The name of the input topic to subscribe to remap_topic string The name of the output topic to publish to topic_type string The type of messages being relayed qos integer QoS profile to use for subscriptions and publications (default: <code>1</code>) transient_local boolean Enables transient local QoS for subscribers (default: <code>false</code>) best_effort boolean Enables best-effort QoS for subscribers (default: <code>false</code>) enable_relay_control boolean Allows dynamic relay control via a service (default: <code>true</code>) srv_name string The service name for relay control when <code>enable_relay_control</code> is <code>true</code> enable_keep_publishing boolean Keeps publishing the last received topic value when not subscribed (default: <code>false</code>) update_rate integer The rate (Hz) for publishing the last topic value when <code>enable_keep_publishing</code> is <code>true</code> (optional) frame_id string Frame ID for transform messages when subscribing to <code>/tf</code> or <code>/tf_static</code> (optional) child_frame_id string Child frame ID for transform messages when subscribing to <code>/tf</code> or <code>/tf_static</code> (optional)"},{"location":"system/autoware_topic_relay_controller/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<ul> <li>The node assumes that the specified <code>topic</code> and <code>remap_topic</code> are valid and accessible within the ROS 2 environment.</li> <li>If <code>enable_keep_publishing</code> is <code>true</code>, the node continuously republishes the last received value even if no new messages are being received.</li> <li>For <code>/tf</code> and <code>/tf_static</code>, additional parameters like <code>frame_id</code> and <code>child_frame_id</code> are required for selective transformation relays.</li> <li>QoS settings must be carefully chosen to match the requirements of the subscribed and published topics.</li> </ul>"},{"location":"system/autoware_topic_state_monitor/","title":"autoware_topic_state_monitor","text":""},{"location":"system/autoware_topic_state_monitor/#autoware_topic_state_monitor","title":"autoware_topic_state_monitor","text":""},{"location":"system/autoware_topic_state_monitor/#purpose","title":"Purpose","text":"<p>This node monitors input topic for abnormalities such as timeout and low frequency. The result of topic status is published as diagnostics.</p>"},{"location":"system/autoware_topic_state_monitor/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The types of topic status and corresponding diagnostic status are following.</p> Topic status Diagnostic status Description <code>OK</code> OK The topic has no abnormalities <code>NotReceived</code> ERROR The topic has not been received yet <code>WarnRate</code> WARN The frequency of the topic is dropped <code>ErrorRate</code> ERROR The frequency of the topic is significantly dropped <code>Timeout</code> ERROR The topic subscription is stopped for a certain time"},{"location":"system/autoware_topic_state_monitor/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"system/autoware_topic_state_monitor/#input","title":"Input","text":"Name Type Description any name any type Subscribe target topic to monitor"},{"location":"system/autoware_topic_state_monitor/#output","title":"Output","text":"Name Type Description <code>/diagnostics</code> <code>diagnostic_msgs/DiagnosticArray</code> Diagnostics outputs"},{"location":"system/autoware_topic_state_monitor/#parameters","title":"Parameters","text":""},{"location":"system/autoware_topic_state_monitor/#node-parameters","title":"Node Parameters","text":"Name Type Default Value Description <code>topic</code> string - Name of target topic <code>topic_type</code> string - Type of target topic (used if the topic is not transform) <code>frame_id</code> string - Frame ID of transform parent (used if the topic is transform) <code>child_frame_id</code> string - Frame ID of transform child (used if the topic is transform) <code>transient_local</code> bool false QoS policy of topic subscription (Transient Local/Volatile) <code>best_effort</code> bool false QoS policy of topic subscription (Best Effort/Reliable) <code>diag_name</code> string - Name used for the diagnostics to publish <code>update_rate</code> double 10.0 Timer callback period [Hz]"},{"location":"system/autoware_topic_state_monitor/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Description <code>warn_rate</code> double 0.5 If the topic rate is lower than this value, the topic status becomes <code>WarnRate</code> <code>error_rate</code> double 0.1 If the topic rate is lower than this value, the topic status becomes <code>ErrorRate</code> <code>timeout</code> double 1.0 If the topic subscription is stopped for more than this time [s], the topic status becomes <code>Timeout</code> <code>window_size</code> int 10 Window size of target topic for calculating frequency"},{"location":"system/autoware_topic_state_monitor/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"system/autoware_velodyne_monitor/","title":"autoware_velodyne_monitor","text":""},{"location":"system/autoware_velodyne_monitor/#autoware_velodyne_monitor","title":"autoware_velodyne_monitor","text":""},{"location":"system/autoware_velodyne_monitor/#purpose","title":"Purpose","text":"<p>This node monitors the status of Velodyne LiDARs. The result of the status is published as diagnostics. Take care not to use this diagnostics to decide the lidar error. Please read Assumptions / Known limits for the detail reason.</p>"},{"location":"system/autoware_velodyne_monitor/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>The status of Velodyne LiDAR can be retrieved from <code>http://[ip_address]/cgi/{info, settings, status, diag}.json</code>.</p> <p>The types of abnormal status and corresponding diagnostics status are following.</p> Abnormal status Diagnostic status No abnormality OK Top board temperature is too cold ERROR Top board temperature is cold WARN Top board temperature is too hot ERROR Top board temperature is hot WARN Bottom board temperature is too cold ERROR Bottom board temperature is cold WARN Bottom board temperature is too hot ERROR Bottom board temperature is hot WARN Rpm(Rotations per minute) of the motor is too low ERROR Rpm(Rotations per minute) of the motor is low WARN Connection error (cannot get Velodyne LiDAR status) ERROR"},{"location":"system/autoware_velodyne_monitor/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"system/autoware_velodyne_monitor/#input","title":"Input","text":"<p>None</p>"},{"location":"system/autoware_velodyne_monitor/#output","title":"Output","text":"Name Type Description <code>/diagnostics</code> <code>diagnostic_msgs/DiagnosticArray</code> Diagnostics outputs"},{"location":"system/autoware_velodyne_monitor/#parameters","title":"Parameters","text":""},{"location":"system/autoware_velodyne_monitor/#node-parameters","title":"Node Parameters","text":"Name Type Default Value Description <code>timeout</code> double 0.5 Timeout for HTTP request to get Velodyne LiDAR status [s]"},{"location":"system/autoware_velodyne_monitor/#core-parameters","title":"Core Parameters","text":"Name Type Default Value Description <code>ip_address</code> string \"192.168.1.201\" IP address of target Velodyne LiDAR <code>temp_cold_warn</code> double -5.0 If the temperature of Velodyne LiDAR is lower than this value, the diagnostics status becomes WARN [\u00b0C] <code>temp_cold_error</code> double -10.0 If the temperature of Velodyne LiDAR is lower than this value, the diagnostics status becomes ERROR [\u00b0C] <code>temp_hot_warn</code> double 75.0 If the temperature of Velodyne LiDAR is higher than this value, the diagnostics status becomes WARN [\u00b0C] <code>temp_hot_error</code> double 80.0 If the temperature of Velodyne LiDAR is higher than this value, the diagnostics status becomes ERROR [\u00b0C] <code>rpm_ratio_warn</code> double 0.80 If the rpm rate of the motor (= current rpm / default rpm) is lower than this value, the diagnostics status becomes WARN <code>rpm_ratio_error</code> double 0.70 If the rpm rate of the motor (= current rpm / default rpm) is lower than this value, the diagnostics status becomes ERROR"},{"location":"system/autoware_velodyne_monitor/#config-files","title":"Config files","text":"<p>Config files for several velodyne models are prepared. The <code>temp_***</code> parameters are set with reference to the operational temperature from each datasheet. Moreover, the <code>temp_hot_***</code> of each model are set highly as 20 from operational temperature. Now, <code>VLP-16.param.yaml</code> is used as default argument because it is lowest spec.</p> Model Name Config name Operational Temperature [\u2103] VLP-16 VLP-16.param.yaml -10 to 60 VLP-32C VLP-32C.param.yaml -20 to 60 VLS-128 VLS-128.param.yaml -20 to 60 Velarray M1600 Velarray_M1600.param.yaml -40 to 85 HDL-32E HDL-32E.param.yaml -10 to 60"},{"location":"system/autoware_velodyne_monitor/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>This node uses the http_client and request results by GET method. It takes a few seconds to get results, or generate a timeout exception if it does not succeed the GET request. This occurs frequently and the diagnostics aggregator output STALE. Therefore I recommend to stop using this results to decide the lidar error, and only monitor it to confirm lidar status.</p>"},{"location":"tools/reaction_analyzer/","title":"Reaction Analyzer","text":""},{"location":"tools/reaction_analyzer/#reaction-analyzer","title":"Reaction Analyzer","text":""},{"location":"tools/reaction_analyzer/#description","title":"Description","text":"<p>The main purpose of the reaction analyzer package is to measure the reaction times of various nodes within a ROS-based autonomous driving simulation environment by subscribing to pre-determined topics. This tool is particularly useful for evaluating the performance of perception, planning, and control pipelines in response to dynamic changes in the environment, such as sudden obstacles. To be able to measure both control outputs and perception outputs, it was necessary to divide the node into two running_mode: <code>planning_control</code> and <code>perception_planning</code>.</p> <p></p>"},{"location":"tools/reaction_analyzer/#planning-control-mode","title":"Planning Control Mode","text":"<p>In this mode, the reaction analyzer creates a dummy publisher for the PredictedObjects and PointCloud2 topics. In the beginning of the test, it publishes the initial position of the ego vehicle and the goal position to set the test environment. Then, it spawns a sudden obstacle in front of the ego vehicle. After the obstacle is spawned, it starts to search reacted messages of the planning and control nodes in the pre-determined topics. When all the topics are reacted, it calculates the reaction time of the nodes and statistics by comparing <code>reacted_times</code> of each of the nodes with <code>spawn_cmd_time</code>, and it creates a csv file to store the results.</p>"},{"location":"tools/reaction_analyzer/#perception-planning-mode","title":"Perception Planning Mode","text":"<p>In this mode, the reaction analyzer reads the rosbag files which are recorded from AWSIM, and it creates a topic publisher for each topic inside the rosbag to replay the rosbag. It reads two rosbag files: <code>path_bag_without_object</code> and <code>path_bag_with_object</code>. Firstly, it replays the <code>path_bag_without_object</code> to set the initial position of the ego vehicle and the goal position. After <code>spawn_time_after_init</code> seconds , it replays the <code>path_bag_with_object</code> to spawn a sudden obstacle in front of the ego vehicle. After the obstacle is spawned, it starts to search the reacted messages of the perception and planning nodes in the pre-determined topics. When all the topics are reacted, it calculates the reaction time of the nodes and statistics by comparing <code>reacted_times</code> of each of the nodes with <code>spawn_cmd_time</code>, and it creates a csv file to store the results.</p>"},{"location":"tools/reaction_analyzer/#point-cloud-publisher-type","title":"Point Cloud Publisher Type","text":"<p>To get better analyze for Perception &amp; Sensing pipeline, the reaction analyzer can publish the point cloud messages in 3 different ways: <code>async_header_sync_publish</code>, <code>sync_header_sync_publish</code> or <code>async_publish</code>. (<code>T</code> is the period of the lidar's output)</p> <p></p> <ul> <li><code>async_header_sync_publish</code>: It publishes the point cloud messages synchronously with asynchronous header times. It   means that each of the lidar's output will be published at the same time, but the headers of the point cloud messages   includes different timestamps because of the phase difference.</li> <li><code>sync_header_sync_publish</code>: It publishes the point cloud messages synchronously with synchronous header times. It   means that each of the lidar's output will be published at the same time, and the headers of the point cloud messages   includes the same timestamps.</li> <li><code>async_publish</code>: It publishes the point cloud messages asynchronously. It means that each of the lidar's output will   be published at different times.</li> </ul>"},{"location":"tools/reaction_analyzer/#usage","title":"Usage","text":"<p>The common parameters you need to define for both running modes are <code>output_file_path</code>, <code>test_iteration</code>, and <code>reaction_chain</code> list. <code>output_file_path</code> is the output file path is the path where the results and statistics will be stored. <code>test_iteration</code> defines how many tests will be performed. The <code>reaction_chain</code> list is the list of the pre-defined topics you want to measure their reaction times.</p> <p>IMPORTANT: Ensure the <code>reaction_chain</code> list is correctly defined:</p> <ul> <li>For <code>perception_planning</code> mode, do not define <code>Control</code> nodes.</li> <li>For <code>planning_control</code> mode, do not define <code>Perception</code> nodes.</li> </ul>"},{"location":"tools/reaction_analyzer/#prepared-test-environment","title":"Prepared Test Environment","text":"<ul> <li>Download the demonstration test map from the   link here. After   downloading,   extract the zip file and use its path as <code>[MAP_PATH]</code> in the following commands.</li> </ul>"},{"location":"tools/reaction_analyzer/#planning-control-mode_1","title":"Planning Control Mode","text":"<ul> <li>You need to define only Planning and Control nodes in the <code>reaction_chain</code> list. With the default parameters,   you can start to test with the following command:</li> </ul> <pre><code>ros2 launch reaction_analyzer reaction_analyzer.launch.xml running_mode:=planning_control vehicle_model:=sample_vehicle sensor_model:=sample_sensor_kit map_path:=[MAP_PATH]\n</code></pre> <p>After the command, the <code>simple_planning_simulator</code> and the <code>reaction_analyzer</code> will be launched. It will automatically start to test. After the test is completed, the results will be stored in the <code>output_file_path</code> you defined.</p>"},{"location":"tools/reaction_analyzer/#perception-planning-mode_1","title":"Perception Planning Mode","text":"<ul> <li>Download the rosbag files from the Google Drive   link here.</li> <li>Extract the zip file and set the path of the <code>.db3</code> files to parameters <code>path_bag_without_object</code>   and <code>path_bag_with_object</code>.</li> <li>You can start to test with the following command:</li> </ul> <pre><code>ros2 launch reaction_analyzer reaction_analyzer.launch.xml running_mode:=perception_planning vehicle_model:=sample_vehicle sensor_model:=awsim_labs_sensor_kit map_path:=[MAP_PATH]\n</code></pre> <ul> <li>On the first run of the tool in perception_planning mode, initialization might take longer than expected. Please allow some time for the process to complete.</li> </ul> <p>After the command, the <code>e2e_simulator</code> and the <code>reaction_analyzer</code> will be launched. It will automatically start to test. After the test is completed, the results will be stored in the <code>output_file_path</code> you defined.</p>"},{"location":"tools/reaction_analyzer/#prepared-test-environment_1","title":"Prepared Test Environment","text":"<p>Scene without object: </p> <p>Scene object: </p>"},{"location":"tools/reaction_analyzer/#custom-test-environment","title":"Custom Test Environment","text":"<p>If you want to run the reaction analyzer with your custom test environment, you need to redefine some of the parameters. The parameters you need to redefine are <code>initialization_pose</code>, <code>entity_params</code>, <code>goal_pose</code>, and <code>topic_publisher</code> ( for <code>perception_planning</code> mode) parameters.</p> <ul> <li>To set <code>initialization_pose</code>, <code>entity_params</code>, <code>goal_pose</code>:</li> <li>Run the AWSIM environment. Tutorial for AWSIM can be found   here.</li> <li>Run the e2e_simulator with the following command:</li> </ul> <pre><code>ros2 launch autoware_launch e2e_simulator.launch.xml vehicle_model:=sample_vehicle sensor_model:=awsim_labs_sensor_kit map_path:=[MAP_PATH]\n</code></pre> <ul> <li>After the EGO is initialized, you can position the ego vehicle in the desired location using the <code>2D Pose Estimate</code> button in RViz.</li> <li>After the EGO located in desired position, please localize the dummy obstacle by using the traffic controller. You can access the traffic control section by pressing the 'ESC' key.</li> </ul> <p>After localize EGO and dummy vehicle, we should write the positions of these entities in the map frame in <code>reaction_analyzer.param.yaml</code>. To achieve this:</p> <ul> <li>Get initialization pose from <code>/awsim/ground_truth/vehicle/pose</code> topic.</li> <li>Get entity params from <code>/perception/object_recognition/objects</code> topic.</li> <li>Get goal pose from <code>/planning/mission_planning/goal</code> topic.</li> </ul> <p>PS: <code>initialization_pose</code> is only valid for <code>planning_control</code> mode.</p> <ul> <li>After the parameters were noted, we should record the rosbags for the test. To record the rosbags, you can use the   following command:</li> </ul> <pre><code>ros2 bag record --all\n</code></pre> <ul> <li>You should record two rosbags: one without the object and one with the object. You can use the traffic controller to   spawn the object in front of the EGO vehicle or remove it.</li> </ul> <p>NOTE: You should record the rosbags in the same environment with the same position of the EGO vehicle. You don't need to run Autoware while recording.</p> <ul> <li>After you record the rosbags, you can set the <code>path_bag_without_object</code> and <code>path_bag_with_object</code> parameters with the   paths of the recorded rosbags.</li> </ul>"},{"location":"tools/reaction_analyzer/#results","title":"Results","text":"<p>The results will be stored in the <code>csv</code> file format and written to the <code>output_file_path</code> you defined. It shows each pipeline of the Autoware by using header timestamp of the messages, and it reports <code>Node Latency</code>, <code>Pipeline Latency</code>, and <code>Total Latency</code> for each of the nodes.</p> <ul> <li><code>Node Latency</code>: The time difference between previous and current node's reaction timestamps. If it is the first node   in the pipeline, it is same as <code>Pipeline Latency</code>.</li> <li><code>Pipeline Latency</code>: The time difference between published time of the message and pipeline header time.</li> <li><code>Total Latency</code>: The time difference between the message's published timestamp and the spawn obstacle command sent   timestamp.</li> </ul>"},{"location":"tools/reaction_analyzer/#parameters","title":"Parameters","text":"Name Type Description <code>timer_period</code> double [s] Period for the main processing timer. <code>test_iteration</code> int Number of iterations for the test. <code>output_file_path</code> string Directory path where test results and statistics will be stored. <code>spawn_time_after_init</code> double [s] Time delay after initialization before spawning objects. Only valid <code>perception_planning</code> mode. <code>spawn_distance_threshold</code> double [m] Distance threshold for spawning objects. Only valid <code>planning_control</code> mode. <code>poses.initialization_pose</code> struct Initial pose of the vehicle, containing <code>x</code>, <code>y</code>, <code>z</code>, <code>roll</code>, <code>pitch</code>, and <code>yaw</code> fields. Only valid <code>planning_control</code> mode. <code>poses.entity_params</code> struct Parameters for entities (e.g., obstacles), containing <code>x</code>, <code>y</code>, <code>z</code>, <code>roll</code>, <code>pitch</code>, <code>yaw</code>, <code>x_dimension</code>, <code>y_dimension</code>, and <code>z_dimension</code>. <code>poses.goal_pose</code> struct Goal pose of the vehicle, containing <code>x</code>, <code>y</code>, <code>z</code>, <code>roll</code>, <code>pitch</code>, and <code>yaw</code> fields. <code>topic_publisher.path_bag_without_object</code> string Path to the ROS bag file without objects. Only valid <code>perception_planning</code> mode. <code>topic_publisher.path_bag_with_object</code> string Path to the ROS bag file with objects. Only valid <code>perception_planning</code> mode. <code>topic_publisher.spawned_pointcloud_sampling_distance</code> double [m] Sampling distance for point clouds of spawned objects. Only valid <code>planning_control</code> mode. <code>topic_publisher.dummy_perception_publisher_period</code> double [s] Publishing period for the dummy perception data. Only valid <code>planning_control</code> mode. <code>topic_publisher.pointcloud_publisher.pointcloud_publisher_type</code> string Defines how the PointCloud2 messages are going to be published. Modes explained above. <code>topic_publisher.pointcloud_publisher.pointcloud_publisher_period</code> double [s] Publishing period of the PointCloud2 messages. <code>topic_publisher.pointcloud_publisher.publish_only_pointcloud_with_object</code> bool Default false. Publish only the point cloud messages with the object. <code>reaction_params.first_brake_params.debug_control_commands</code> bool Debug publish flag. <code>reaction_params.first_brake_params.control_cmd_buffer_time_interval</code> double [s] Time interval for buffering control commands. <code>reaction_params.first_brake_params.min_number_descending_order_control_cmd</code> int Minimum number of control commands in descending order for triggering brake. <code>reaction_params.first_brake_params.min_jerk_for_brake_cmd</code> double [m/s\u00b3] Minimum jerk value for issuing a brake command. <code>reaction_params.search_zero_vel_params.max_looking_distance</code> double [m] Maximum looking distance for zero velocity on trajectory <code>reaction_params.search_entity_params.search_radius</code> double [m] Searching radius for spawned entity. Distance between ego pose and entity pose. <code>reaction_chain</code> struct List of the nodes with their topics and topic's message types."},{"location":"tools/reaction_analyzer/#limitations","title":"Limitations","text":"<ul> <li>Reaction analyzer has some limitation like <code>PublisherMessageType</code>, <code>SubscriberMessageType</code> and <code>ReactionType</code>. It is   currently supporting following list:</li> </ul> <ul> <li>Publisher Message Types:<ul> <li><code>sensor_msgs/msg/PointCloud2</code></li> <li><code>sensor_msgs/msg/CameraInfo</code></li> <li><code>sensor_msgs/msg/Image</code></li> <li><code>geometry_msgs/msg/PoseWithCovarianceStamped</code></li> <li><code>sensor_msgs/msg/Imu</code></li> <li><code>autoware_vehicle_msgs/msg/ControlModeReport</code></li> <li><code>autoware_vehicle_msgs/msg/GearReport</code></li> <li><code>autoware_vehicle_msgs/msg/HazardLightsReport</code></li> <li><code>autoware_vehicle_msgs/msg/SteeringReport</code></li> <li><code>autoware_vehicle_msgs/msg/TurnIndicatorsReport</code></li> <li><code>autoware_vehicle_msgs/msg/VelocityReport</code></li> </ul> </li> </ul> <ul> <li>Subscriber Message Types:<ul> <li><code>sensor_msgs/msg/PointCloud2</code></li> <li><code>autoware_perception_msgs/msg/DetectedObjects</code></li> <li><code>autoware_perception_msgs/msg/TrackedObjects</code></li> <li><code>autoware_perception_msgs/msg/PredictedObject</code></li> <li><code>autoware_planning_msgs/msg/Trajectory</code></li> <li><code>autoware_control_msgs/msg/Control</code></li> </ul> </li> </ul> <ul> <li>Reaction Types:<ul> <li><code>FIRST_BRAKE</code></li> <li><code>SEARCH_ZERO_VEL</code></li> <li><code>SEARCH_ENTITY</code></li> </ul> </li> </ul>"},{"location":"tools/reaction_analyzer/#future-improvements","title":"Future improvements","text":"<ul> <li>The reaction analyzer can be improved by adding more reaction types. Currently, it is supporting only <code>FIRST_BRAKE</code>,   <code>SEARCH_ZERO_VEL</code>, and <code>SEARCH_ENTITY</code> reaction types. It can be extended by adding more reaction types for each of   the message types.</li> </ul>"},{"location":"vehicle/autoware_accel_brake_map_calibrator/","title":"accel_brake_map_calibrator","text":""},{"location":"vehicle/autoware_accel_brake_map_calibrator/#accel_brake_map_calibrator","title":"accel_brake_map_calibrator","text":"<p>The role of this node is to automatically calibrate <code>accel_map.csv</code> / <code>brake_map.csv</code> used in the <code>autoware_raw_vehicle_cmd_converter</code> node.</p> <p>The base map, which is lexus's one by default, is updated iteratively with the loaded driving data.</p>"},{"location":"vehicle/autoware_accel_brake_map_calibrator/#how-to-calibrate","title":"How to calibrate","text":""},{"location":"vehicle/autoware_accel_brake_map_calibrator/#launch-calibrator","title":"Launch Calibrator","text":"<p>After launching Autoware, run the <code>autoware_accel_brake_map_calibrator</code> by the following command and then perform autonomous driving. Note: You can collect data with manual driving if it is possible to use the same vehicle interface as during autonomous driving (e.g. using a joystick).</p> <pre><code>ros2 launch autoware_accel_brake_map_calibrator accel_brake_map_calibrator.launch.xml rviz:=true\n</code></pre> <p>Or if you want to use rosbag files, run the following commands.</p> <pre><code>ros2 launch autoware_accel_brake_map_calibrator accel_brake_map_calibrator.launch.xml rviz:=true use_sim_time:=true\nros2 bag play &lt;rosbag_file&gt; --clock\n</code></pre> <p>During the calibration with setting the parameter <code>progress_file_output</code> to true, the log file is output in [directory of autoware_accel_brake_map_calibrator]/config/ . You can also see accel and brake maps in [directory of autoware_accel_brake_map_calibrator]/config/accel_map.csv and [directory of autoware_accel_brake_map_calibrator]/config/brake_map.csv after calibration.</p>"},{"location":"vehicle/autoware_accel_brake_map_calibrator/#calibration-plugin","title":"Calibration plugin","text":"<p>The <code>rviz:=true</code> option displays the RViz with a calibration plugin as below.</p> <p> </p> <p>The current status (velocity and pedal) is shown in the plugin. The color on the current cell varies green/red depending on the current data is valid/invalid. The data that doesn't satisfy the following conditions are considered invalid and will not be used for estimation since aggressive data (e.g. when the pedal is moving fast) causes bad calibration accuracy.</p> <ul> <li>The velocity and pedal conditions are within certain ranges from the index values.</li> <li>The steer value, pedal speed, pitch value, etc. are less than corresponding thresholds.</li> <li>The velocity is higher than a threshold.</li> </ul> <p>The detailed parameters are described in the parameter section.</p> <p>Note: You don't need to worry about whether the current state is red or green during calibration. Just keep getting data until all the cells turn red.</p> <p>The value of each cell in the map is gray at first, and it changes from blue to red as the number of valid data in the cell accumulates. It is preferable to continue the calibration until each cell of the map becomes close to red. In particular, the performance near the stop depends strongly on the velocity of 0 ~ 6m/s range and the pedal value of +0.2 ~ -0.4, range so it is desirable to focus on those areas.</p>"},{"location":"vehicle/autoware_accel_brake_map_calibrator/#diagnostics","title":"Diagnostics","text":"<p>The <code>accel brake map_calibrator</code> publishes diagnostics message depending on the calibration status. Diagnostic type <code>WARN</code> indicates that the current accel/brake map is estimated to be inaccurate. In this situation, it is strongly recommended to perform a re-calibration of the accel/brake map.</p> Status Diagnostics Type Diagnostics message Description No calibration required <code>OK</code> \"OK\" Calibration Required <code>WARN</code> \"Accel/brake map Calibration is required.\" The accuracy of current accel/brake map may be low. <p>This diagnostics status can be also checked on the following ROS topic.</p> <pre><code>ros2 topic echo /accel_brake_map_calibrator/output/update_suggest\n</code></pre> <p>When the diagnostics type is <code>WARN</code>, <code>True</code> is published on this topic and the update of the accel/brake map is suggested.</p>"},{"location":"vehicle/autoware_accel_brake_map_calibrator/#evaluation-of-the-accel-brake-map-accuracy","title":"Evaluation of the accel / brake map accuracy","text":"<p>The accuracy of map is evaluated by the Root Mean Squared Error (RMSE) between the observed acceleration and predicted acceleration.</p> <p>TERMS:</p> <ul> <li><code>Observed acceleration</code>: the current vehicle acceleration which is calculated as a derivative value of the wheel speed.</li> </ul> <ul> <li><code>Predicted acceleration</code>: the output of the original accel/brake map, which the Autoware is expecting. The value is calculated using the current pedal and velocity.</li> </ul> <p>You can check additional error information with the following topics.</p> <ul> <li><code>/accel_brake_map_calibrator/output/current_map_error</code> : The error of the original map set in the <code>csv_path_accel/brake_map</code> path. The original map is not accurate if this value is large.</li> <li><code>/accel_brake_map_calibrator/output/updated_map_error</code> : The error of the map calibrated in this node. The calibration quality is low if this value is large.</li> <li><code>/accel_brake_map_calibrator/output/map_error_ratio</code> : The error ratio between the original map and updated map (ratio = updated / current). If this value is less than 1, it is desirable to update the map.</li> </ul>"},{"location":"vehicle/autoware_accel_brake_map_calibrator/#how-to-visualize-calibration-data","title":"How to visualize calibration data","text":"<p>The process of calibration can be visualized as below. Since these scripts need the log output of the calibration, the <code>pedal_accel_graph_output</code> parameter must be set to true while the calibration is running for the visualization.</p>"},{"location":"vehicle/autoware_accel_brake_map_calibrator/#visualize-plot-of-relation-between-acceleration-and-pedal","title":"Visualize plot of relation between acceleration and pedal","text":"<p>The following command shows the plot of used data in the calibration. In each plot of velocity ranges, you can see the distribution of the relationship between pedal and acceleration, and raw data points with colors according to their pitch angles.</p> <pre><code>ros2 run autoware_accel_brake_map_calibrator view_plot.py\n</code></pre> <p></p>"},{"location":"vehicle/autoware_accel_brake_map_calibrator/#visualize-statistics-about-accelerationvelocitypedal-data","title":"Visualize statistics about acceleration/velocity/pedal data","text":"<p>The following command shows the statistics of the calibration:</p> <ul> <li>mean value</li> <li>standard deviation</li> <li>number of data</li> </ul> <p>of all data in each map cell.</p> <pre><code>ros2 run autoware_accel_brake_map_calibrator view_statistics.py\n</code></pre> <p></p>"},{"location":"vehicle/autoware_accel_brake_map_calibrator/#how-to-save-the-calibrated-accel-brake-map-anytime-you-want","title":"How to save the calibrated accel / brake map anytime you want","text":"<p>You can save accel and brake map anytime with the following command.</p> <pre><code>ros2 service call /accel_brake_map_calibrator/update_map_dir tier4_vehicle_msgs/srv/UpdateAccelBrakeMap \"path: '&lt;accel/brake map directory&gt;'\"\n</code></pre> <p>You can also save accel and brake map in the default directory where Autoware reads accel_map.csv/brake_map.csv using the RViz plugin (AccelBrakeMapCalibratorButtonPanel) as following.</p> <ol> <li> <p>Click Panels tab, and select AccelBrakeMapCalibratorButtonPanel.</p> <p></p> </li> <li> <p>Select the panel, and the button will appear at the bottom of RViz.</p> <p></p> </li> <li> <p>Press the button, and the accel / brake map will be saved.    (The button cannot be pressed in certain situations, such as when the calibrator node is not running.)</p> <p></p> </li> </ol>"},{"location":"vehicle/autoware_accel_brake_map_calibrator/#parameters","title":"Parameters","text":""},{"location":"vehicle/autoware_accel_brake_map_calibrator/#system-parameters","title":"System Parameters","text":"Name Type Description Default value update_method string you can select map calibration method. \"update_offset_each_cell\" calculates offsets for each grid cells on the map. \"update_offset_total\" calculates the total offset of the map. \"update_offset_each_cell\" get_pitch_method string \"tf\": get pitch from tf, \"none\": unable to perform pitch validation and pitch compensation \"tf\" pedal_accel_graph_output bool if true, it will output a log of the pedal accel graph. true progress_file_output bool if true, it will output a log and csv file of the update process. false default_map_dir str directory of default map [directory of autoware_raw_vehicle_cmd_converter]/data/default/ calibrated_map_dir str directory of calibrated map [directory of autoware_accel_brake_map_calibrator]/config/ update_hz double hz for update 10.0"},{"location":"vehicle/autoware_accel_brake_map_calibrator/#algorithm-parameters","title":"Algorithm Parameters","text":"Name Type Description Default value initial_covariance double Covariance of initial acceleration map (larger covariance makes the update speed faster) 0.05 velocity_min_threshold double Speeds smaller than this are not used for updating. 0.1 velocity_diff_threshold double When the velocity data is more than this threshold away from the grid reference speed (center value), the associated data is not used for updating. 0.556 max_steer_threshold double If the steer angle is greater than this value, the associated data is not used for updating. 0.2 max_pitch_threshold double If the pitch angle is greater than this value, the associated data is not used for updating. 0.02 max_jerk_threshold double If the ego jerk calculated from ego acceleration is greater than this value, the associated data is not used for updating. 0.7 pedal_velocity_thresh double If the pedal moving speed is greater than this value, the associated data is not used for updating. 0.15 pedal_diff_threshold double If the current pedal value is more then this threshold away from the previous value, the associated data is not used for updating. 0.03 max_accel double Maximum value of acceleration calculated from velocity source. 5.0 min_accel double Minimum value of acceleration calculated from velocity source. -5.0 pedal_to_accel_delay double The delay time between actuation_cmd to acceleration, considered in the update logic. 0.3 update_suggest_thresh double threshold of RMSE ratio that update suggest flag becomes true. ( RMSE ratio: [RMSE of new map] / [RMSE of original map] ) 0.7 max_data_count int For visualization. When the data num of each grid gets this value, the grid color gets red. 100 accel_brake_value_source string Whether to use actuation_status or actuation_command as accel/brake sources. value status"},{"location":"vehicle/autoware_accel_brake_map_calibrator/#test-utility-scripts","title":"Test utility scripts","text":""},{"location":"vehicle/autoware_accel_brake_map_calibrator/#constant-accelbrake-command-test","title":"Constant accel/brake command test","text":"<p>These scripts are useful to test for accel brake map calibration. These generate an <code>ActuationCmd</code> with a constant accel/brake value given interactively by a user through CLI.</p> <ul> <li>accel_tester.py</li> <li>brake_tester.py</li> <li>actuation_cmd_publisher.py</li> </ul> <p>The <code>accel/brake_tester.py</code> receives a target accel/brake command from CLI. It sends a target value to <code>actuation_cmd_publisher.py</code> which generates the <code>ActuationCmd</code>. You can run these scripts by the following commands in the different terminals, and it will be as in the screenshot below.</p> <pre><code>ros2 run autoware_accel_brake_map_calibrator accel_tester.py\nros2 run autoware_accel_brake_map_calibrator brake_tester.py\nros2 run autoware_accel_brake_map_calibrator actuation_cmd_publisher.py\n</code></pre> <p></p>"},{"location":"vehicle/autoware_accel_brake_map_calibrator/#calibration-method","title":"Calibration Method","text":"<p>Two algorithms are selectable for the acceleration map update, update_offset_four_cell_around and update_offset_each_cell. Please see the link for details.</p>"},{"location":"vehicle/autoware_accel_brake_map_calibrator/#data-preprocessing","title":"Data Preprocessing","text":"<p>Before calibration, missing or unusable data (e.g., too large handle angles) must first be eliminated. The following parameters are used to determine which data to remove.</p>"},{"location":"vehicle/autoware_accel_brake_map_calibrator/#parameters_1","title":"Parameters","text":"Name Description Default Value velocity_min_threshold Exclude minimal velocity 0.1 max_steer_threshold Exclude large steering angle 0.2 max_pitch_threshold Exclude large pitch angle 0.02 max_jerk_threshold Exclude large jerk 0.7 pedal_velocity_thresh Exclude large pedaling speed 0.15"},{"location":"vehicle/autoware_accel_brake_map_calibrator/#update_offset_each_cell","title":"update_offset_each_cell","text":"<p>Update by Recursive Least Squares(RLS) method using data close enough to each grid.</p> <p>Advantage : Only data close enough to each grid is used for calibration, allowing accurate updates at each point.</p> <p>Disadvantage : Calibration is time-consuming due to a large amount of data to be excluded.</p>"},{"location":"vehicle/autoware_accel_brake_map_calibrator/#parameters_2","title":"Parameters","text":"<p>Data selection is determined by the following thresholds.</p> Name Default Value velocity_diff_threshold 0.556 pedal_diff_threshold 0.03"},{"location":"vehicle/autoware_accel_brake_map_calibrator/#update-formula","title":"Update formula","text":"\\[ \\begin{align}     \\theta[n]=&amp;     \\theta[n-1]+\\frac{p[n-1]x^{(n)}}{\\lambda+p[n-1]{(x^{(n)})}^2}(y^{(n)}-\\theta[n-1]x^{(n)})\\\\     p[n]=&amp;\\frac{p[n-1]}{\\lambda+p[n-1]{(x^{(n)})}^2} \\end{align} \\]"},{"location":"vehicle/autoware_accel_brake_map_calibrator/#variables","title":"Variables","text":"Variable name Symbol covariance \\(p[n-1]\\) map_offset \\(\\theta[n]\\) forgettingfactor \\(\\lambda\\) phi \\(x(=1)\\) measured_acc \\(y\\)"},{"location":"vehicle/autoware_accel_brake_map_calibrator/#update_offset_four_cell_around-1","title":"update_offset_four_cell_around [1]","text":"<p>Update the offsets by RLS in four grids around newly obtained data. By considering linear interpolation, the update takes into account appropriate weights. Therefore, there is no need to remove data by thresholding.</p> <p>Advantage : No data is wasted because updates are performed on the 4 grids around the data with appropriate weighting. Disadvantage : Accuracy may be degraded due to extreme bias of the data. For example, if data \\(z(k)\\) is biased near \\(Z_{RR}\\) in Fig. 2, updating is performed at the four surrounding points ( \\(Z_{RR}\\), \\(Z_{RL}\\), \\(Z_{LR}\\), and \\(Z_{LL}\\)), but accuracy at \\(Z_{LL}\\) is not expected.</p> <p> </p>"},{"location":"vehicle/autoware_accel_brake_map_calibrator/#implementation","title":"Implementation","text":"<p>See eq.(7)-(10) in [1] for the updated formula. In addition, eq.(17),(18) from [1] are used for Anti-Windup.</p>"},{"location":"vehicle/autoware_accel_brake_map_calibrator/#references","title":"References","text":"<p>[1] Gabrielle Lochrie, Michael Doljevic, Mario Nona, Yongsoon Yoon, Anti-Windup Recursive Least Squares Method for Adaptive Lookup Tables with Application to Automotive Powertrain Control Systems, IFAC-PapersOnLine, Volume 54, Issue 20, 2021, Pages 840-845</p>"},{"location":"vehicle/autoware_external_cmd_converter/","title":"external_cmd_converter","text":""},{"location":"vehicle/autoware_external_cmd_converter/#external_cmd_converter","title":"external_cmd_converter","text":"<p><code>external_cmd_converter</code> is a node that converts desired mechanical input to acceleration and velocity by using accel/brake map.</p>"},{"location":"vehicle/autoware_external_cmd_converter/#algorithm","title":"Algorithm","text":""},{"location":"vehicle/autoware_external_cmd_converter/#how-to-calculate-reference-acceleration-and-velocity","title":"How to calculate reference acceleration and velocity","text":"<p>A reference acceleration and velocity are derived from the throttle and brake values of external control commands.</p>"},{"location":"vehicle/autoware_external_cmd_converter/#reference-acceleration","title":"Reference Acceleration","text":"<p>A reference acceleration is calculated from accel_brake_map based on values of a desired_pedal and a current velocity;</p> \\[     pedal_d = throttle_d - brake_d, \\] \\[     acc_{ref} = Acc(pedal_d, v_{x,current}). \\] Parameter Description \\(throttle_d\\) throttle value of external control command (<code>~/in/external_control_cmd.control.throttle</code>) \\(brake_d\\) brake value of external control command (<code>~/in/external_control_cmd.control.brake</code>) \\(v_{x,current}\\) current longitudinal velocity (<code>~/in/odometry.twist.twist.linear.x</code>) Acc accel_brake_map"},{"location":"vehicle/autoware_external_cmd_converter/#reference-velocity","title":"Reference Velocity","text":"<p>A reference velocity is calculated based on a current velocity and a reference acceleration:</p> \\[ v_{ref} =     v_{x,current} + k_{v_{ref}} \\cdot \\text{sign}_{gear} \\cdot acc_{ref}. \\] Parameter Description \\(acc_{ref}\\) reference acceleration \\(k_{v_{ref}}\\) reference velocity gain \\(\\text{sign}_{gear}\\) gear command (<code>~/in/shift_cmd</code>) (Drive/Low: 1, Reverse: -1, Other: 0)"},{"location":"vehicle/autoware_external_cmd_converter/#input-topics","title":"Input topics","text":"Name Type Description <code>~/in/external_control_cmd</code> tier4_external_api_msgs::msg::ControlCommand target <code>throttle/brake/steering_angle/steering_angle_velocity</code> is necessary to calculate desired control command. <code>~/input/shift_cmd\"</code> autoware_vehicle_msgs::GearCommand current command of gear. <code>~/input/emergency_stop</code> tier4_external_api_msgs::msg::Heartbeat emergency heart beat for external command. <code>~/input/current_gate_mode</code> tier4_control_msgs::msg::GateMode topic for gate mode. <code>~/input/odometry</code> navigation_msgs::Odometry twist topic in odometry is used."},{"location":"vehicle/autoware_external_cmd_converter/#output-topics","title":"Output topics","text":"Name Type Description <code>~/out/control_cmd</code> autoware_control_msgs::msg::Control ackermann control command converted from selected external command"},{"location":"vehicle/autoware_external_cmd_converter/#parameters","title":"Parameters","text":"Parameter Type Description <code>ref_vel_gain_</code> double reference velocity gain <code>timer_rate</code> double timer's update rate <code>wait_for_first_topic</code> double if time out check is done after receiving first topic <code>control_command_timeout</code> double time out check for control command <code>emergency_stop_timeout</code> double time out check for emergency stop command"},{"location":"vehicle/autoware_external_cmd_converter/#limitation","title":"Limitation","text":"<p>tbd.</p>"},{"location":"vehicle/autoware_raw_vehicle_cmd_converter/","title":"autoware_raw_vehicle_cmd_converter","text":""},{"location":"vehicle/autoware_raw_vehicle_cmd_converter/#autoware_raw_vehicle_cmd_converter","title":"autoware_raw_vehicle_cmd_converter","text":""},{"location":"vehicle/autoware_raw_vehicle_cmd_converter/#overview","title":"Overview","text":"<p>The raw_vehicle_command_converter is a crucial node in vehicle automation systems, responsible for translating desired steering and acceleration inputs into specific vehicle control commands. This process is achieved through a combination of a lookup table and an optional feedback control system.</p>"},{"location":"vehicle/autoware_raw_vehicle_cmd_converter/#lookup-table","title":"Lookup Table","text":"<p>The core of the converter's functionality lies in its use of a CSV-formatted lookup table. This table encapsulates the relationship between the throttle/brake pedal (depending on your vehicle control interface) and the corresponding vehicle acceleration across various speeds. The converter utilizes this data to accurately translate target accelerations into appropriate throttle/brake values.</p> <p></p>"},{"location":"vehicle/autoware_raw_vehicle_cmd_converter/#creation-of-reference-data","title":"Creation of Reference Data","text":"<p>Reference data for the lookup table is generated through the following steps:</p> <ol> <li>Data Collection: On a flat road, a constant value command (e.g., throttle/brake pedal) is applied to accelerate or decelerate the vehicle.</li> <li>Recording Data: During this phase, both the IMU acceleration and vehicle velocity data are recorded.</li> <li>CSV File Generation: A CSV file is created, detailing the relationship between command values, vehicle speed, and resulting acceleration.</li> </ol> <p>Once the acceleration map is crafted, it should be loaded when the RawVehicleCmdConverter node is launched, with the file path defined in the launch file.</p>"},{"location":"vehicle/autoware_raw_vehicle_cmd_converter/#auto-calibration-tool","title":"Auto-Calibration Tool","text":"<p>For ease of calibration and adjustments to the lookup table, an auto-calibration tool is available. More information and instructions for this tool can be found here.</p>"},{"location":"vehicle/autoware_raw_vehicle_cmd_converter/#variable-gear-ratio-vgr","title":"Variable Gear Ratio (VGR)","text":"<p>This is a gear ratio for converting tire angle to steering angle. Generally, to improve operability, the gear ratio becomes dynamically larger as the speed increases or the steering angle becomes smaller. For a certain vehicle, data was acquired and the gear ratio was approximated by the following formula.</p> \\[ a + b \\times v^2 - c \\times \\lvert \\delta \\rvert \\] <p>For that vehicle, the coefficients were as follows.</p> <pre><code>vgr_coef_a: 15.713\nvgr_coef_b: 0.053\nvgr_coef_c: 0.042\n</code></pre> <p></p> <p>When <code>convert_steer_cmd_method: \"vgr\"</code> is selected, the node receives the control command from the controller as the desired tire angle and calculates the desired steering angle to output. Also, when <code>convert_actuation_to_steering_status: true</code>, this node receives the <code>actuation_status</code> topic and calculates the steer tire angle from the <code>steer_wheel_angle</code> and publishes it.</p>"},{"location":"vehicle/autoware_raw_vehicle_cmd_converter/#vehicle-adaptor","title":"Vehicle Adaptor","text":"<p>Under development A feature that compensates for control commands according to the dynamic characteristics of the vehicle. This feature works when <code>use_vehicle_adaptor: true</code> is set and requires <code>control_horizon</code> to be enabled, so you need to set <code>enable_control_cmd_horizon_pub: true</code> in the trajectory_follower node.</p>"},{"location":"vehicle/autoware_raw_vehicle_cmd_converter/#latency-measurement","title":"Latency Measurement","text":"<p>This node includes a latency measurement feature that tracks the time difference between receiving a control command and publishing the corresponding actuation command. The measured latency is published as a debug topic, which can be useful for performance monitoring and system optimization.</p> <p>The latency is calculated as the time difference between the timestamp of the incoming control command and the current time when the actuation command is published. When the input <code>control_cmd</code> contains a timestamp that represents the time when the control module started processing (propagated through the control pipeline), the <code>control_component_latency</code> represents the overall processing time of the entire control module system.</p>"},{"location":"vehicle/autoware_raw_vehicle_cmd_converter/#input-topics","title":"Input topics","text":"Name Type Description <code>~/input/control_cmd</code> autoware_control_msgs::msg::Control target <code>velocity/acceleration/steering_angle/steering_angle_velocity</code> is necessary to calculate actuation command. <code>~/input/steering\"</code> autoware_vehicle_msgs::msg::SteeringReport subscribe only when <code>convert_actuation_to_steering_status: false</code>. current status of steering used for steering feed back control <code>~/input/odometry</code> navigation_msgs::Odometry twist topic in odometry is used. <code>~/input/actuation_status</code> tier4_vehicle_msgs::msg::ActuationStatus actuation status is assumed to receive the same type of status as sent to the vehicle side. For example, if throttle/brake pedal/steer_wheel_angle is sent, the same type of status is received. In the case of steer_wheel_angle, it is used to calculate steer_tire_angle and VGR in this node. <p>Input topics when vehicle_adaptor is enabled</p> Name Type Description <code>~/input/accel</code> geometry_msgs::msg::AccelWithCovarianceStamped; acceleration status <code>~/input/operation_mode_state</code> autoware_adapi_v1_msgs::msg::OperationModeState operation mode status <code>~/input/control_horizon</code> autoware_control_msgs::msg::ControlHorizon control horizon command"},{"location":"vehicle/autoware_raw_vehicle_cmd_converter/#output-topics","title":"Output topics","text":"Name Type Description <code>~/output/actuation_cmd</code> tier4_vehicle_msgs::msg::ActuationCommandStamped actuation command for vehicle to apply mechanical input <code>~/output/steering_status</code> autoware_vehicle_msgs::msg::SteeringReport publish only when <code>convert_actuation_to_steering_status: true</code>. steer tire angle is calculated from steer wheel angle and published. <code>~/output/control_component_latency</code> autoware_internal_debug_msgs::msg::Float64Stamped control system latency measurement from control command reception to actuation command publication"},{"location":"vehicle/autoware_raw_vehicle_cmd_converter/#parameters","title":"Parameters","text":"Name Type Description Default Range csv_path_accel_map string path for acceleration map csv file $(find-pkg-share autoware_raw_vehicle_cmd_converter)/data/default/accel_map.csv N/A csv_path_brake_map string path for brake map csv file $(find-pkg-share autoware_raw_vehicle_cmd_converter)/data/default/brake_map.csv N/A csv_path_steer_map string path for steer map csv file $(find-pkg-share autoware_raw_vehicle_cmd_converter)/data/default/steer_map.csv N/A convert_accel_cmd boolean use accel or not true N/A convert_brake_cmd boolean use brake or not true N/A convert_steer_cmd boolean use steer or not true N/A use_steer_ff boolean steering steer controller using steer feed forward or not true N/A use_steer_fb boolean steering steer controller using steer feed back or not true N/A is_debugging boolean debugging mode or not false N/A max_throttle float maximum value of throttle 0.4 \u22650.0 max_brake float maximum value of brake 0.8 \u22650.0 max_steer float maximum value of steer 10.0 N/A min_steer float minimum value of steer -10.0 N/A steer_pid.kp float proportional coefficient value in PID control 150.0 N/A steer_pid.ki float integral coefficient value in PID control 15.0 &gt;0.0 steer_pid.kd float derivative coefficient value in PID control 0.0 N/A steer_pid.max float maximum value of PID 8.0 N/A steer_pid.min float minimum value of PID -8.0. N/A steer_pid.max_p float maximum value of Proportional in PID 8.0 N/A steer_pid.min_p float minimum value of Proportional in PID -8.0 N/A steer_pid.max_i float maximum value of Integral in PID 8.0 N/A steer_pid.min_i float minimum value of Integral in PID -8.0 N/A steer_pid.max_d float maximum value of Derivative in PID 0.0 N/A steer_pid.min_d float minimum value of Derivative in PID 0.0 N/A steer_pid.invalid_integration_decay float invalid integration decay value in PID control 0.97 &gt;0.0 convert_steer_cmd_method string method for converting steer command vgr ['vgr', 'steer_map'] vgr_coef_a float coefficient a for variable gear ratio 15.713 N/A vgr_coef_b float coefficient b for variable gear ratio 0.053 N/A vgr_coef_c float coefficient c for variable gear ratio 0.042 N/A convert_actuation_to_steering_status boolean convert actuation to steering status or not. Whether to subscribe to actuation_status and calculate and publish steering_status For example, receive the steering wheel angle and calculate the steering wheel angle based on the gear ratio. If false, the vehicle interface must publish steering_status. true N/A use_vehicle_adaptor boolean flag to enable feature that compensates control commands according to vehicle dynamics. false N/A"},{"location":"vehicle/autoware_raw_vehicle_cmd_converter/#limitation","title":"Limitation","text":"<p>The current feed back implementation is only applied to steering control.</p>"},{"location":"vehicle/autoware_steer_offset_estimator/Readme/","title":"steer_offset_estimator","text":""},{"location":"vehicle/autoware_steer_offset_estimator/Readme/#steer_offset_estimator","title":"steer_offset_estimator","text":""},{"location":"vehicle/autoware_steer_offset_estimator/Readme/#purpose","title":"Purpose","text":"<p>The role of this node is to automatically calibrate <code>steer_offset</code> used in the <code>vehicle_interface</code> node.</p> <p>The base steer offset value is 0 by default, which is standard, is updated iteratively with the loaded driving data. This module is supposed to be used in below straight driving situation. </p>"},{"location":"vehicle/autoware_steer_offset_estimator/Readme/#inner-workings-algorithms","title":"Inner-workings / Algorithms","text":"<p>Estimates sequential steering offsets from kinematic model and state observations.  Calculate yaw rate error and then calculate steering error recursively by least squared method, for more details see <code>updateSteeringOffset()</code> function.</p>"},{"location":"vehicle/autoware_steer_offset_estimator/Readme/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"vehicle/autoware_steer_offset_estimator/Readme/#input","title":"Input","text":"Name Type Description <code>~/input/twist</code> <code>geometry_msgs::msg::TwistStamped</code> vehicle twist <code>~/input/steer</code> <code>autoware_vehicle_msgs::msg::SteeringReport</code> steering"},{"location":"vehicle/autoware_steer_offset_estimator/Readme/#output","title":"Output","text":"Name Type Description <code>~/output/steering_offset</code> <code>autoware_internal_debug_msgs::msg::Float32Stamped</code> steering offset <code>~/output/steering_offset_covariance</code> <code>autoware_internal_debug_msgs::msg::Float32Stamped</code> covariance of steering offset"},{"location":"vehicle/autoware_steer_offset_estimator/Readme/#launch-calibrator","title":"Launch Calibrator","text":"<p>After launching Autoware, run the <code>steer_offset_estimator</code> by the following command and then perform autonomous driving. Note: You can collect data with manual driving if it is possible to use the same vehicle interface as during autonomous driving (e.g. using a joystick).</p> <pre><code>ros2 launch steer_offset_estimator steer_offset_estimator.launch.xml\n</code></pre> <p>Or if you want to use rosbag files, run the following commands.</p> <pre><code>ros2 param set /use_sim_time true\nros2 bag play &lt;rosbag_file&gt; --clock\n</code></pre>"},{"location":"vehicle/autoware_steer_offset_estimator/Readme/#parameters","title":"Parameters","text":"Name Type Description Default Range initial_covariance float steer offset is larger than tolerance 1000 N/A steer_update_hz float update hz of steer data 10 \u22650.0 forgetting_factor float weight of using previous value 0.999 \u22650.0 valid_min_velocity float velocity below this value is not used 5 \u22650.0 valid_max_steer float steer above this value is not used 0.05 N/A warn_steer_offset_deg float Warn if offset is above this value. ex. if absolute estimated offset is larger than 2.5[deg] =&gt; warning 2.5 N/A"},{"location":"vehicle/autoware_steer_offset_estimator/Readme/#diagnostics","title":"Diagnostics","text":"<p>The <code>steer_offset_estimator</code> publishes diagnostics message depending on the calibration status. Diagnostic type <code>WARN</code> indicates that the current steer_offset is estimated to be inaccurate. In this situation, it is strongly recommended to perform a re-calibration of the steer_offset.</p> Status Diagnostics Type Diagnostics message No calibration required <code>OK</code> \"Preparation\" Calibration Required <code>WARN</code> \"Steer offset is larger than tolerance\" <p>This diagnostics status can be also checked on the following ROS topic.</p> <pre><code>ros2 topic echo /vehicle/status/steering_offset\n</code></pre>"},{"location":"visualization/autoware_bag_time_manager_rviz_plugin/","title":"autoware_bag_time_manager_rviz_plugin","text":""},{"location":"visualization/autoware_bag_time_manager_rviz_plugin/#autoware_bag_time_manager_rviz_plugin","title":"autoware_bag_time_manager_rviz_plugin","text":""},{"location":"visualization/autoware_bag_time_manager_rviz_plugin/#purpose","title":"Purpose","text":"<p>This plugin allows publishing and controlling the ros bag time.</p>"},{"location":"visualization/autoware_bag_time_manager_rviz_plugin/#output","title":"Output","text":"<p>tbd.</p>"},{"location":"visualization/autoware_bag_time_manager_rviz_plugin/#howtouse","title":"HowToUse","text":"<ol> <li> <p>Start rviz and select panels/Add new panel.</p> <p></p> </li> <li> <p>Select BagTimeManagerPanel and press OK.</p> <p></p> </li> <li> <p>See bag_time_manager_rviz_plugin/BagTimeManagerPanel is added.</p> <p></p> </li> </ol> <ul> <li>Pause/Resume: pause/resume the clock.</li> <li>ApplyRate: apply rate of the clock.</li> </ul>"},{"location":"visualization/autoware_traffic_light_rviz_plugin/","title":"Autoware Traffic Light RViz Plugin","text":""},{"location":"visualization/autoware_traffic_light_rviz_plugin/#autoware-traffic-light-rviz-plugin","title":"Autoware Traffic Light RViz Plugin","text":"<p>This package provides a RViz2 plugin for visualizing traffic light recognition results in Autoware.</p> <p></p>"},{"location":"visualization/autoware_traffic_light_rviz_plugin/#property-description","title":"Property Description","text":""},{"location":"visualization/autoware_traffic_light_rviz_plugin/#topic-settings","title":"Topic Settings","text":"<ul> <li>Lanelet Map Topic: Topic to receive Lanelet2 map data<ul> <li>Type: <code>autoware_map_msgs/msg/LaneletMapBin</code></li> </ul> </li> <li>Traffic Light Topic: Topic to receive traffic light recognition results<ul> <li>Type: <code>autoware_perception_msgs/msg/TrafficLightGroupArray</code></li> </ul> </li> </ul>"},{"location":"visualization/autoware_traffic_light_rviz_plugin/#display-settings","title":"Display Settings","text":"<ul> <li>Timeout: Time in seconds before clearing traffic light state display</li> <li>Show Text: Toggle text display of traffic light states</li> <li>Show Bulb: Toggle visual representation of traffic lights</li> <li>Text Prefix: Prefix for traffic light state text</li> <li>Font Size: Size of the traffic light state text</li> <li>Text Color: Color of the traffic light state text</li> </ul>"},{"location":"visualization/autoware_traffic_light_rviz_plugin/#text-position-adjustment","title":"Text Position Adjustment","text":"<ul> <li>Text X Offset: Offset for text display along X-axis</li> <li>Text Y Offset: Offset for text display along Y-axis</li> <li>Text Z Offset: Offset for text display along Z-axis</li> </ul>"},{"location":"visualization/tier4_adapi_rviz_plugin/","title":"tier4_adapi_rviz_plugin","text":""},{"location":"visualization/tier4_adapi_rviz_plugin/#tier4_adapi_rviz_plugin","title":"tier4_adapi_rviz_plugin","text":"<p>This package contains tools for testing AD API. For general AD API usage, we recommend using tier4_state_rviz_plugin.</p>"},{"location":"visualization/tier4_adapi_rviz_plugin/#routepanel","title":"RoutePanel","text":"<p>To use the panel, set the topic name from 2D Goal Pose Tool to <code>/rviz/routing/pose</code>. By default, when a tool publish a pose, the panel immediately sets a route with that as the goal. Enable or disable of allow_goal_modification option can be set with the check box.</p> <p>Push the mode button in the waypoint to enter waypoint mode. In this mode, the pose is added to waypoints. Press the apply button to set the route using the saved waypoints (the last one is a goal). Reset the saved waypoints with the reset button.</p>"},{"location":"visualization/tier4_adapi_rviz_plugin/#material-design-icons","title":"Material Design Icons","text":"<p>This project uses Material Design Icons by Google. These icons are used under the terms of the Apache License, Version 2.0.</p> <p>Material Design Icons are a collection of symbols provided by Google that are used to enhance the user interface of applications, websites, and other digital products.</p>"},{"location":"visualization/tier4_adapi_rviz_plugin/#license","title":"License","text":"<p>The Material Design Icons are licensed under the Apache License, Version 2.0. You may obtain a copy of the License at:</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"visualization/tier4_adapi_rviz_plugin/#acknowledgments","title":"Acknowledgments","text":"<p>We would like to express our gratitude to Google for making these icons available to the community, helping developers and designers enhance the visual appeal and user experience of their projects.</p>"},{"location":"visualization/tier4_camera_view_rviz_plugin/","title":"tier4_camera_view_rviz_plugin","text":""},{"location":"visualization/tier4_camera_view_rviz_plugin/#tier4_camera_view_rviz_plugin","title":"tier4_camera_view_rviz_plugin","text":""},{"location":"visualization/tier4_camera_view_rviz_plugin/#thirdpersonview-tool","title":"ThirdPersonView Tool","text":"<p>Add the <code>tier4_camera_view_rviz_plugin/ThirdPersonViewTool</code> tool to the RViz. Push the button, the camera will focus on the vehicle and set the target frame to <code>base_link</code>. Short cut key 'o'.</p>"},{"location":"visualization/tier4_camera_view_rviz_plugin/#birdeyeview-tool","title":"BirdEyeView Tool","text":"<p>Add the <code>tier4_camera_view_rviz_plugin/BirdEyeViewTool</code> tool to the RViz. Push the button, the camera will turn to the BEV view, the target frame is consistent with the latest frame. Short cut key 'r'.</p>"},{"location":"visualization/tier4_camera_view_rviz_plugin/#material-design-icons","title":"Material Design Icons","text":"<p>This project uses Material Design Icons by Google. These icons are used under the terms of the Apache License, Version 2.0.</p> <p>Material Design Icons are a collection of symbols provided by Google that are used to enhance the user interface of applications, websites, and other digital products.</p>"},{"location":"visualization/tier4_camera_view_rviz_plugin/#license","title":"License","text":"<p>The Material Design Icons are licensed under the Apache License, Version 2.0. You may obtain a copy of the License at:</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"visualization/tier4_camera_view_rviz_plugin/#acknowledgments","title":"Acknowledgments","text":"<p>We would like to express our gratitude to Google for making these icons available to the community, helping developers and designers enhance the visual appeal and user experience of their projects.</p>"},{"location":"visualization/tier4_control_mode_rviz_plugin/","title":"tier4_control_mode_rviz_plugin","text":""},{"location":"visualization/tier4_control_mode_rviz_plugin/#tier4_control_mode_rviz_plugin","title":"tier4_control_mode_rviz_plugin","text":""},{"location":"visualization/tier4_control_mode_rviz_plugin/#purpose","title":"Purpose","text":"<p>This plugin displays the current control mode status of Autoware. The background color changes according to the mode, enabling intuitive status recognition.</p>"},{"location":"visualization/tier4_control_mode_rviz_plugin/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"visualization/tier4_control_mode_rviz_plugin/#input","title":"Input","text":"Name Type Description <code>/vehicle/status/control_mode</code> <code>autoware_vehicle_msgs::msg::ControlModeReport</code> Topic representing the current control mode"},{"location":"visualization/tier4_control_mode_rviz_plugin/#control-mode-types","title":"Control Mode Types","text":"Mode Value Color Description NO_COMMAND 0 Dark Gray No command state AUTONOMOUS 1 Green Autonomous driving mode AUTONOMOUS_STEER_ONLY 2 Dark Gray Autonomous steering control only AUTONOMOUS_VELOCITY_ONLY 3 Dark Gray Autonomous velocity control only MANUAL 4 Red Manual driving mode DISENGAGED 5 Orange Control disengaged state NOT_READY 6 Dark Gray System not ready"},{"location":"visualization/tier4_control_mode_rviz_plugin/#how-to-use","title":"How to Use","text":"<ol> <li>Launch RViz</li> <li>Select <code>Panels</code> \u2192 <code>Add New Panel</code> from the menu</li> <li>Choose <code>rviz_plugins/ControlModeDisplay</code></li> <li>The panel will display the current control mode</li> </ol>"},{"location":"visualization/tier4_control_mode_rviz_plugin/#rviz-configuration-example","title":"RViz Configuration Example","text":"<pre><code>Panels:\n  - Class: rviz_plugins/ControlModeDisplay\n    Name: ControlModeDisplay\n</code></pre>"},{"location":"visualization/tier4_datetime_rviz_plugin/","title":"tier4_datetime_rviz_plugin","text":""},{"location":"visualization/tier4_datetime_rviz_plugin/#tier4_datetime_rviz_plugin","title":"tier4_datetime_rviz_plugin","text":""},{"location":"visualization/tier4_datetime_rviz_plugin/#purpose","title":"Purpose","text":"<p>This plugin displays the ROS Time and Wall Time in rviz.</p>"},{"location":"visualization/tier4_datetime_rviz_plugin/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"visualization/tier4_datetime_rviz_plugin/#usage","title":"Usage","text":"<ol> <li>Start rviz and select panels/Add new panel.    </li> <li>Select tier4_datetime_rviz_plugin/AutowareDateTimePanel and press OK.    </li> </ol>"},{"location":"visualization/tier4_perception_rviz_plugin/","title":"tier4_perception_rviz_plugin","text":""},{"location":"visualization/tier4_perception_rviz_plugin/#tier4_perception_rviz_plugin","title":"tier4_perception_rviz_plugin","text":""},{"location":"visualization/tier4_perception_rviz_plugin/#purpose","title":"Purpose","text":"<p>It is an rviz plugin for visualizing the result from tier4 perception module. This package is based on the implementation of the rviz plugin developed by Autoware.Auto.</p> <p>See Autoware.Auto design documentation for the original design philosophy. [1]</p>"},{"location":"visualization/tier4_perception_rviz_plugin/#input-types-visualization-results","title":"Input Types / Visualization Results","text":""},{"location":"visualization/tier4_perception_rviz_plugin/#detectedobjectswithfeature","title":"DetectedObjectsWithFeature","text":""},{"location":"visualization/tier4_perception_rviz_plugin/#input-types","title":"Input Types","text":"Name Type Description <code>tier4_perception_msgs::msg::DetectedObjectsWithFeature</code> detection result array"},{"location":"visualization/tier4_perception_rviz_plugin/#visualization-result","title":"Visualization Result","text":""},{"location":"visualization/tier4_perception_rviz_plugin/#referencesexternal-links","title":"References/External links","text":"<p>[1] https://gitlab.com/autowarefoundation/autoware.auto/AutowareAuto/-/tree/master/src/tools/visualization/autoware_rviz_plugins</p>"},{"location":"visualization/tier4_perception_rviz_plugin/#future-extensions-unimplemented-parts","title":"Future extensions / Unimplemented parts","text":""},{"location":"visualization/tier4_planning_factor_rviz_plugin/","title":"tier4_planning_factor_rviz_plugin","text":""},{"location":"visualization/tier4_state_rviz_plugin/","title":"tier4_state_rviz_plugin","text":""},{"location":"visualization/tier4_state_rviz_plugin/#tier4_state_rviz_plugin","title":"tier4_state_rviz_plugin","text":""},{"location":"visualization/tier4_state_rviz_plugin/#purpose","title":"Purpose","text":"<p>This plugin displays the current status of autoware. This plugin also can engage from the panel.</p>"},{"location":"visualization/tier4_state_rviz_plugin/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"visualization/tier4_state_rviz_plugin/#input","title":"Input","text":"Name Type Description <code>/api/operation_mode/state</code> <code>autoware_adapi_v1_msgs::msg::OperationModeState</code> The topic represents the state of operation mode <code>/api/routing/state</code> <code>autoware_adapi_v1_msgs::msg::RouteState</code> The topic represents the state of route <code>/api/localization/initialization_state</code> <code>autoware_adapi_v1_msgs::msg::LocalizationInitializationState</code> The topic represents the state of localization initialization <code>/api/motion/state</code> <code>autoware_adapi_v1_msgs::msg::MotionState</code> The topic represents the state of motion <code>/api/autoware/get/emergency</code> <code>tier4_external_api_msgs::msg::Emergency</code> The topic represents the state of external emergency <code>/vehicle/status/gear_status</code> <code>autoware_vehicle_msgs::msg::GearReport</code> The topic represents the state of gear"},{"location":"visualization/tier4_state_rviz_plugin/#output","title":"Output","text":"Name Type Description <code>/api/operation_mode/change_to_autonomous</code> <code>autoware_adapi_v1_msgs::srv::ChangeOperationMode</code> The service to change operation mode to autonomous <code>/api/operation_mode/change_to_stop</code> <code>autoware_adapi_v1_msgs::srv::ChangeOperationMode</code> The service to change operation mode to stop <code>/api/operation_mode/change_to_local</code> <code>autoware_adapi_v1_msgs::srv::ChangeOperationMode</code> The service to change operation mode to local <code>/api/operation_mode/change_to_remote</code> <code>autoware_adapi_v1_msgs::srv::ChangeOperationMode</code> The service to change operation mode to remote <code>/api/operation_mode/enable_autoware_control</code> <code>autoware_adapi_v1_msgs::srv::ChangeOperationMode</code> The service to enable vehicle control by Autoware <code>/api/operation_mode/disable_autoware_control</code> <code>autoware_adapi_v1_msgs::srv::ChangeOperationMode</code> The service to disable vehicle control by Autoware <code>/api/routing/clear_route</code> <code>autoware_adapi_v1_msgs::srv::ClearRoute</code> The service to clear route state <code>/api/motion/accept_start</code> <code>autoware_adapi_v1_msgs::srv::AcceptStart</code> The service to accept the vehicle to start <code>/api/autoware/set/emergency</code> <code>tier4_external_api_msgs::srv::SetEmergency</code> The service to set external emergency <code>/planning/scenario_planning/max_velocity_default</code> <code>autoware_internal_planning_msgs::msg::VelocityLimit</code> The topic to set maximum speed of the vehicle"},{"location":"visualization/tier4_state_rviz_plugin/#howtouse","title":"HowToUse","text":"<ol> <li> <p>Start rviz and select panels/Add new panel.</p> <p></p> </li> <li> <p>Select tier4_state_rviz_plugin/AutowareStatePanel and press OK.</p> <p></p> </li> <li> <p>If the auto button is activated, can engage by clicking it.</p> <p></p> </li> </ol>"},{"location":"visualization/tier4_traffic_light_rviz_plugin/","title":"tier4_traffic_light_rviz_plugin","text":""},{"location":"visualization/tier4_traffic_light_rviz_plugin/#tier4_traffic_light_rviz_plugin","title":"tier4_traffic_light_rviz_plugin","text":""},{"location":"visualization/tier4_traffic_light_rviz_plugin/#purpose","title":"Purpose","text":"<p>This plugin panel publishes dummy traffic light signals.</p>"},{"location":"visualization/tier4_traffic_light_rviz_plugin/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"visualization/tier4_traffic_light_rviz_plugin/#output","title":"Output","text":"Name Type Description <code>/perception/traffic_light_recognition/traffic_signals</code> <code>autoware_perception_msgs::msg::TrafficLightGroupArray</code> Publish traffic light signals"},{"location":"visualization/tier4_traffic_light_rviz_plugin/#howtouse","title":"HowToUse","text":"<ol> <li>Start rviz and select panels/Add new panel.</li> <li>Select TrafficLightPublishPanel and press OK.</li> <li>Set <code>Traffic Light ID</code> &amp; <code>Traffic Light Status</code> and press <code>SET</code> button.</li> <li>Traffic light signals are published, while <code>PUBLISH</code> button is pushed.</li> </ol>"},{"location":"visualization/tier4_vehicle_rviz_plugin/","title":"tier4_vehicle_rviz_plugin","text":""},{"location":"visualization/tier4_vehicle_rviz_plugin/#tier4_vehicle_rviz_plugin","title":"tier4_vehicle_rviz_plugin","text":"<p>This package is including jsk code. Note that jsk_overlay_utils.cpp and jsk_overlay_utils.hpp are BSD license.</p>"},{"location":"visualization/tier4_vehicle_rviz_plugin/#purpose","title":"Purpose","text":"<p>This plugin provides a visual and easy-to-understand display of vehicle speed, turn signal, steering status and acceleration.</p>"},{"location":"visualization/tier4_vehicle_rviz_plugin/#inputs-outputs","title":"Inputs / Outputs","text":""},{"location":"visualization/tier4_vehicle_rviz_plugin/#input","title":"Input","text":"Name Type Description <code>/vehicle/status/velocity_status</code> <code>autoware_vehicle_msgs::msg::VelocityReport</code> The topic is vehicle twist <code>/control/turn_signal_cmd</code> <code>autoware_vehicle_msgs::msg::TurnIndicatorsReport</code> The topic is status of turn signal <code>/vehicle/status/steering_status</code> <code>autoware_vehicle_msgs::msg::SteeringReport</code> The topic is status of steering <code>/localization/acceleration</code> <code>geometry_msgs::msg::AccelWithCovarianceStamped</code> The topic is the acceleration"},{"location":"visualization/tier4_vehicle_rviz_plugin/#parameter","title":"Parameter","text":""},{"location":"visualization/tier4_vehicle_rviz_plugin/#core-parameters","title":"Core Parameters","text":""},{"location":"visualization/tier4_vehicle_rviz_plugin/#consolemeter","title":"ConsoleMeter","text":"Name Type Default Value Description <code>property_text_color_</code> QColor QColor(25, 255, 240) Text color <code>property_left_</code> int 128 Left of the plotter window [px] <code>property_top_</code> int 128 Top of the plotter window [px] <code>property_length_</code> int 256 Height of the plotter window [px] <code>property_value_height_offset_</code> int 0 Height offset of the plotter window [px] <code>property_value_scale_</code> float 1.0 / 6.667 Value scale"},{"location":"visualization/tier4_vehicle_rviz_plugin/#steeringangle","title":"SteeringAngle","text":"Name Type Default Value Description <code>property_text_color_</code> QColor QColor(25, 255, 240) Text color <code>property_left_</code> int 128 Left of the plotter window [px] <code>property_top_</code> int 128 Top of the plotter window [px] <code>property_length_</code> int 256 Height of the plotter window [px] <code>property_value_height_offset_</code> int 0 Height offset of the plotter window [px] <code>property_value_scale_</code> float 1.0 / 6.667 Value scale <code>property_handle_angle_scale_</code> float 3.0 Scale is steering angle to handle angle"},{"location":"visualization/tier4_vehicle_rviz_plugin/#turnsignal","title":"TurnSignal","text":"Name Type Default Value Description <code>property_left_</code> int 128 Left of the plotter window [px] <code>property_top_</code> int 128 Top of the plotter window [px] <code>property_width_</code> int 256 Left of the plotter window [px] <code>property_height_</code> int 256 Width of the plotter window [px]"},{"location":"visualization/tier4_vehicle_rviz_plugin/#velocityhistory","title":"VelocityHistory","text":"Name Type Default Value Description <code>property_velocity_timeout_</code> float 10.0 Timeout of velocity [s] <code>property_velocity_alpha_</code> float 1.0 Alpha of velocity <code>property_velocity_scale_</code> float 0.3 Scale of velocity <code>property_velocity_color_view_</code> bool false Use Constant Color or not <code>property_velocity_color_</code> QColor Qt::black Color of velocity history <code>property_vel_max_</code> float 3.0 Color Border Vel Max [m/s]"},{"location":"visualization/tier4_vehicle_rviz_plugin/#accelerationmeter","title":"AccelerationMeter","text":"Name Type Default Value Description <code>property_normal_text_color_</code> QColor QColor(25, 255, 240) Normal text color <code>property_emergency_text_color_</code> QColor QColor(255, 80, 80) Emergency acceleration color <code>property_left_</code> int 896 Left of the plotter window [px] <code>property_top_</code> int 128 Top of the plotter window [px] <code>property_length_</code> int 256 Height of the plotter window [px] <code>property_value_height_offset_</code> int 0 Height offset of the plotter window [px] <code>property_value_scale_</code> float 1 / 6.667 Value text scale <code>property_emergency_threshold_max_</code> float 1.0 Max acceleration threshold for emergency [m/s^2] <code>property_emergency_threshold_min_</code> float -2.5 Min acceleration threshold for emergency [m/s^2]"},{"location":"visualization/tier4_vehicle_rviz_plugin/#assumptions-known-limits","title":"Assumptions / Known limits","text":"<p>TBD.</p>"},{"location":"visualization/tier4_vehicle_rviz_plugin/#usage","title":"Usage","text":"<ol> <li>Start rviz and select Add under the Displays panel.    </li> <li>Select any one of the tier4_vehicle_rviz_plugin and press OK.    </li> <li>Enter the name of the topic where you want to view the status.    </li> </ol>"}]}